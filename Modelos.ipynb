{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentación de clientes y modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('base_limpia.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Etiqueta']!= 'Por Definir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seguimiento = pd.read_excel('base_limpia.xlsx')\n",
    "seguimiento = seguimiento[seguimiento['Etiqueta']== 'Por Definir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Operacion Con Credito</th>\n",
       "      <th>Carta Oferta</th>\n",
       "      <th>Zona</th>\n",
       "      <th>Region Etapa</th>\n",
       "      <th>Rut Cliente</th>\n",
       "      <th>Estado Carta Oferta</th>\n",
       "      <th>Estado Comercial COF</th>\n",
       "      <th>Monto Carta oferta</th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>...</th>\n",
       "      <th>N° Grupo Familiar</th>\n",
       "      <th>Tipo Compra</th>\n",
       "      <th>Antiguedad Laboral</th>\n",
       "      <th>Renta Liquida</th>\n",
       "      <th>Rango de Ingresos</th>\n",
       "      <th>Profesion (Estandar)</th>\n",
       "      <th>Cargo Estandarizado</th>\n",
       "      <th>Rango de Edad</th>\n",
       "      <th>Etiqueta</th>\n",
       "      <th>Comuna Estandarizada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Si</td>\n",
       "      <td>212515871</td>\n",
       "      <td>5-ZONA ANTOFAGASTA</td>\n",
       "      <td>ANTOFAGASTA</td>\n",
       "      <td>18010236-1</td>\n",
       "      <td>PPM</td>\n",
       "      <td>Normal</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>7648.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>1</td>\n",
       "      <td>3500000</td>\n",
       "      <td>3.000.001 y 4.000.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Gerente</td>\n",
       "      <td>26-30</td>\n",
       "      <td>Por Definir</td>\n",
       "      <td>II Región de Antofagasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Si</td>\n",
       "      <td>212515848</td>\n",
       "      <td>5-ZONA ANTOFAGASTA</td>\n",
       "      <td>ANTOFAGASTA</td>\n",
       "      <td>19104385-5</td>\n",
       "      <td>Con Promesa</td>\n",
       "      <td>Normal</td>\n",
       "      <td>5250.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>2</td>\n",
       "      <td>2300000</td>\n",
       "      <td>2.000.001 y 2.500.000</td>\n",
       "      <td>Universitarios</td>\n",
       "      <td>Operario</td>\n",
       "      <td>19-25</td>\n",
       "      <td>Por Definir</td>\n",
       "      <td>II Región de Antofagasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>212516137</td>\n",
       "      <td>5-ZONA ANTOFAGASTA</td>\n",
       "      <td>ANTOFAGASTA</td>\n",
       "      <td>17718590-6</td>\n",
       "      <td>PPM</td>\n",
       "      <td>Normal</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>1</td>\n",
       "      <td>450000</td>\n",
       "      <td>Hasta 500.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Tecnico</td>\n",
       "      <td>26-30</td>\n",
       "      <td>Por Definir</td>\n",
       "      <td>II Región de Antofagasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Si</td>\n",
       "      <td>212516126</td>\n",
       "      <td>5-ZONA ANTOFAGASTA</td>\n",
       "      <td>ANTOFAGASTA</td>\n",
       "      <td>17718590-6</td>\n",
       "      <td>PPM</td>\n",
       "      <td>Normal</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>1</td>\n",
       "      <td>450000</td>\n",
       "      <td>Hasta 500.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Tecnico</td>\n",
       "      <td>26-30</td>\n",
       "      <td>Por Definir</td>\n",
       "      <td>II Región de Antofagasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>212516095</td>\n",
       "      <td>3-ZONA SUR</td>\n",
       "      <td>BIO-BIO</td>\n",
       "      <td>9213925-5</td>\n",
       "      <td>PPM</td>\n",
       "      <td>Normal</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Segunda Vivienda</td>\n",
       "      <td>9</td>\n",
       "      <td>3800000</td>\n",
       "      <td>3.000.001 y 4.000.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Gerente</td>\n",
       "      <td>56-60</td>\n",
       "      <td>Por Definir</td>\n",
       "      <td>VIII Región del Biobío</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29551</th>\n",
       "      <td>33889</td>\n",
       "      <td>Si</td>\n",
       "      <td>212415942</td>\n",
       "      <td>3-ZONA SUR</td>\n",
       "      <td>METROPOLITANA</td>\n",
       "      <td>15890264-8</td>\n",
       "      <td>Con Promesa</td>\n",
       "      <td>Traspasada</td>\n",
       "      <td>1679.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>2</td>\n",
       "      <td>810000</td>\n",
       "      <td>Entre 500.001 y 1.000.000</td>\n",
       "      <td>Universitarios</td>\n",
       "      <td>Administrativo</td>\n",
       "      <td>31-35</td>\n",
       "      <td>Por Definir</td>\n",
       "      <td>Puente Alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29558</th>\n",
       "      <td>33897</td>\n",
       "      <td>Si</td>\n",
       "      <td>212408378</td>\n",
       "      <td>3-ZONA SUR</td>\n",
       "      <td>METROPOLITANA</td>\n",
       "      <td>13915513-0</td>\n",
       "      <td>Con Promesa</td>\n",
       "      <td>Traspasada</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Vivienda Inversion</td>\n",
       "      <td>3</td>\n",
       "      <td>1000000</td>\n",
       "      <td>Entre 500.001 y 1.000.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Docente</td>\n",
       "      <td>41-45</td>\n",
       "      <td>Por Definir</td>\n",
       "      <td>Maipu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29564</th>\n",
       "      <td>33903</td>\n",
       "      <td>Si</td>\n",
       "      <td>212408355</td>\n",
       "      <td>2-ZONA CENTRO</td>\n",
       "      <td>METROPOLITANA</td>\n",
       "      <td>10464464-3</td>\n",
       "      <td>Con Promesa</td>\n",
       "      <td>Traspasada</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>2</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1.000.001 y 1.500.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Administrativo</td>\n",
       "      <td>51-55</td>\n",
       "      <td>Por Definir</td>\n",
       "      <td>Maipu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29565</th>\n",
       "      <td>33904</td>\n",
       "      <td>Si</td>\n",
       "      <td>212408355</td>\n",
       "      <td>2-ZONA CENTRO</td>\n",
       "      <td>METROPOLITANA</td>\n",
       "      <td>10464464-3</td>\n",
       "      <td>Con Promesa</td>\n",
       "      <td>Traspasada</td>\n",
       "      <td>3571.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>2</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1.000.001 y 1.500.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Administrativo</td>\n",
       "      <td>51-55</td>\n",
       "      <td>Por Definir</td>\n",
       "      <td>Maipu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29673</th>\n",
       "      <td>34027</td>\n",
       "      <td>Si</td>\n",
       "      <td>212453918</td>\n",
       "      <td>3-ZONA SUR</td>\n",
       "      <td>BIO-BIO</td>\n",
       "      <td>9943048-6</td>\n",
       "      <td>Con Promesa</td>\n",
       "      <td>Traspasada</td>\n",
       "      <td>3235.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>12</td>\n",
       "      <td>1000000</td>\n",
       "      <td>Entre 500.001 y 1.000.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Docente</td>\n",
       "      <td>51-55</td>\n",
       "      <td>Por Definir</td>\n",
       "      <td>VIII Región del Biobío</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3046 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 Operacion Con Credito  Carta Oferta                Zona  \\\n",
       "0               0                    Si     212515871  5-ZONA ANTOFAGASTA   \n",
       "1               1                    Si     212515848  5-ZONA ANTOFAGASTA   \n",
       "2               2                    No     212516137  5-ZONA ANTOFAGASTA   \n",
       "3               3                    Si     212516126  5-ZONA ANTOFAGASTA   \n",
       "4               4                    No     212516095          3-ZONA SUR   \n",
       "...           ...                   ...           ...                 ...   \n",
       "29551       33889                    Si     212415942          3-ZONA SUR   \n",
       "29558       33897                    Si     212408378          3-ZONA SUR   \n",
       "29564       33903                    Si     212408355       2-ZONA CENTRO   \n",
       "29565       33904                    Si     212408355       2-ZONA CENTRO   \n",
       "29673       34027                    Si     212453918          3-ZONA SUR   \n",
       "\n",
       "        Region Etapa Rut Cliente Estado Carta Oferta Estado Comercial COF  \\\n",
       "0        ANTOFAGASTA  18010236-1                 PPM               Normal   \n",
       "1        ANTOFAGASTA  19104385-5         Con Promesa               Normal   \n",
       "2        ANTOFAGASTA  17718590-6                 PPM               Normal   \n",
       "3        ANTOFAGASTA  17718590-6                 PPM               Normal   \n",
       "4            BIO-BIO   9213925-5                 PPM               Normal   \n",
       "...              ...         ...                 ...                  ...   \n",
       "29551  METROPOLITANA  15890264-8         Con Promesa           Traspasada   \n",
       "29558  METROPOLITANA  13915513-0         Con Promesa           Traspasada   \n",
       "29564  METROPOLITANA  10464464-3         Con Promesa           Traspasada   \n",
       "29565  METROPOLITANA  10464464-3         Con Promesa           Traspasada   \n",
       "29673        BIO-BIO   9943048-6         Con Promesa           Traspasada   \n",
       "\n",
       "       Monto Carta oferta  Monto Reserva  ...  N° Grupo Familiar  \\\n",
       "0                  6400.0         7648.0  ...                  5   \n",
       "1                  5250.0           62.0  ...                  2   \n",
       "2                    78.0            0.0  ...                  2   \n",
       "3                  2400.0            0.0  ...                  2   \n",
       "4                    50.0           10.0  ...                  3   \n",
       "...                   ...            ...  ...                ...   \n",
       "29551              1679.0            5.0  ...                  2   \n",
       "29558              1460.0           10.0  ...                  2   \n",
       "29564              3571.0           10.0  ...                  4   \n",
       "29565              3571.0           10.0  ...                  4   \n",
       "29673              3235.0           10.0  ...                  4   \n",
       "\n",
       "              Tipo Compra  Antiguedad Laboral  Renta Liquida  \\\n",
       "0        Primera Vivienda                   1        3500000   \n",
       "1        Primera Vivienda                   2        2300000   \n",
       "2        Primera Vivienda                   1         450000   \n",
       "3        Primera Vivienda                   1         450000   \n",
       "4        Segunda Vivienda                   9        3800000   \n",
       "...                   ...                 ...            ...   \n",
       "29551    Primera Vivienda                   2         810000   \n",
       "29558  Vivienda Inversion                   3        1000000   \n",
       "29564    Primera Vivienda                   2        1500000   \n",
       "29565    Primera Vivienda                   2        1500000   \n",
       "29673    Primera Vivienda                  12        1000000   \n",
       "\n",
       "               Rango de Ingresos  Profesion (Estandar)  Cargo Estandarizado  \\\n",
       "0          3.000.001 y 4.000.000                 Otros              Gerente   \n",
       "1          2.000.001 y 2.500.000        Universitarios             Operario   \n",
       "2                  Hasta 500.000                 Otros              Tecnico   \n",
       "3                  Hasta 500.000                 Otros              Tecnico   \n",
       "4          3.000.001 y 4.000.000                 Otros              Gerente   \n",
       "...                          ...                   ...                  ...   \n",
       "29551  Entre 500.001 y 1.000.000        Universitarios       Administrativo   \n",
       "29558  Entre 500.001 y 1.000.000                 Otros              Docente   \n",
       "29564      1.000.001 y 1.500.000                 Otros       Administrativo   \n",
       "29565      1.000.001 y 1.500.000                 Otros       Administrativo   \n",
       "29673  Entre 500.001 y 1.000.000                 Otros              Docente   \n",
       "\n",
       "       Rango de Edad     Etiqueta      Comuna Estandarizada  \n",
       "0              26-30  Por Definir  II Región de Antofagasta  \n",
       "1              19-25  Por Definir  II Región de Antofagasta  \n",
       "2              26-30  Por Definir  II Región de Antofagasta  \n",
       "3              26-30  Por Definir  II Región de Antofagasta  \n",
       "4              56-60  Por Definir    VIII Región del Biobío  \n",
       "...              ...          ...                       ...  \n",
       "29551          31-35  Por Definir               Puente Alto  \n",
       "29558          41-45  Por Definir                     Maipu  \n",
       "29564          51-55  Por Definir                     Maipu  \n",
       "29565          51-55  Por Definir                     Maipu  \n",
       "29673          51-55  Por Definir    VIII Región del Biobío  \n",
       "\n",
       "[3046 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seguimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Operacion Con Credito', 'Carta Oferta', 'Zona', 'Region Etapa',\n",
       "       'Rut Cliente', 'Estado Carta Oferta', 'Estado Comercial COF',\n",
       "       'Monto Carta oferta', 'Monto Reserva', 'Monto Pie',\n",
       "       'Monto Carta de instruccion', 'Monto Beneficio Minero',\n",
       "       'Monto CH FFAA y Otros', 'Monto Crédito Complementario',\n",
       "       'Monto LEASING', 'Monto Vivienda principal', 'Monto CDP',\n",
       "       'Monto CDP Cheque', 'Monto CH', 'Monto Subsidio', 'Monto Ahorro',\n",
       "       'Monto BAP', 'Nombre Completo', 'Tipo', 'Nacionalidad Cliente',\n",
       "       'Estado Civil', 'N° Grupo Familiar', 'Tipo Compra',\n",
       "       'Antiguedad Laboral', 'Renta Liquida', 'Rango de Ingresos',\n",
       "       'Profesion (Estandar)', 'Cargo Estandarizado', 'Rango de Edad',\n",
       "       'Etiqueta', 'Comuna Estandarizada'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_train = ['Operacion Con Credito', 'Zona',  'Monto Carta oferta', 'Monto Reserva', 'Monto Pie',\n",
    "       'Monto Carta de instruccion', 'Monto Beneficio Minero',\n",
    "       'Monto CH FFAA y Otros', 'Monto Crédito Complementario',\n",
    "       'Monto LEASING', 'Monto Vivienda principal', 'Monto CDP',\n",
    "       'Monto CDP Cheque', 'Monto CH', 'Monto Subsidio', 'Monto Ahorro',\n",
    "       'Monto BAP', 'Tipo', 'Nacionalidad Cliente',\n",
    "       'Estado Civil', 'N° Grupo Familiar', 'Tipo Compra',\n",
    "       'Antiguedad Laboral', 'Renta Liquida', 'Rango de Ingresos',\n",
    "       'Profesion (Estandar)', 'Cargo Estandarizado', 'Rango de Edad', 'Comuna Estandarizada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df[columnas_train]\n",
    "y = df['Etiqueta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas = ['Operacion Con Credito', 'Zona', 'Tipo', 'Nacionalidad Cliente', 'Estado Civil', 'N° Grupo Familiar',\n",
    "               'Tipo Compra', 'Rango de Ingresos', 'Profesion (Estandar)', 'Cargo Estandarizado', 'Rango de Edad',\n",
    "               'Comuna Estandarizada']\n",
    "\n",
    "numericas = ['Monto Reserva', 'Monto Pie','Monto Carta de instruccion', 'Monto Beneficio Minero',\n",
    "                'Monto CH FFAA y Otros', 'Monto Crédito Complementario',\n",
    "                'Monto LEASING', 'Monto Vivienda principal', 'Monto CDP',\n",
    "                'Monto CDP Cheque', 'Monto CH', 'Monto Subsidio', 'Monto Ahorro', 'Monto BAP',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>Monto CH</th>\n",
       "      <th>Monto Subsidio</th>\n",
       "      <th>Monto Ahorro</th>\n",
       "      <th>Monto BAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4616.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4448.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>10.0</td>\n",
       "      <td>90.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>3.5</td>\n",
       "      <td>221.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2200.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.0</td>\n",
       "      <td>400.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30575</th>\n",
       "      <td>10.0</td>\n",
       "      <td>300.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3156.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2783.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30576</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3973.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2324.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30577</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1481.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5260.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3664.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30578</th>\n",
       "      <td>10.0</td>\n",
       "      <td>504.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5148.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4634.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30579</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5472.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4979.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27534 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "95              18.0       0.00                         0.0   \n",
       "103             10.0    4616.13                         0.0   \n",
       "132             10.0      90.00                         0.0   \n",
       "146              3.5     221.50                         0.0   \n",
       "176              0.0     400.00                         0.0   \n",
       "...              ...        ...                         ...   \n",
       "30575           10.0     300.00                         0.0   \n",
       "30576           10.0    1480.00                         0.0   \n",
       "30577           10.0    1481.00                         0.0   \n",
       "30578           10.0     504.00                         0.0   \n",
       "30579            0.0       0.00                         0.0   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "95                        0.0                    0.0   \n",
       "103                       0.0                    0.0   \n",
       "132                       0.0                    0.0   \n",
       "146                       0.0                    0.0   \n",
       "176                       0.0                    0.0   \n",
       "...                       ...                    ...   \n",
       "30575                     0.0                    0.0   \n",
       "30576                     0.0                    0.0   \n",
       "30577                     0.0                    0.0   \n",
       "30578                     0.0                    0.0   \n",
       "30579                     0.0                    0.0   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "95                              0.0            0.0                      0.00   \n",
       "103                             0.0            0.0                   4448.56   \n",
       "132                             0.0            0.0                      0.00   \n",
       "146                             0.0            0.0                   2200.00   \n",
       "176                             0.0            0.0                      0.00   \n",
       "...                             ...            ...                       ...   \n",
       "30575                           0.0            0.0                   3156.00   \n",
       "30576                           0.0            0.0                   3973.00   \n",
       "30577                           0.0            0.0                   5260.00   \n",
       "30578                           0.0            0.0                   5148.00   \n",
       "30579                           0.0            0.0                   5472.00   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  Monto CH  Monto Subsidio  Monto Ahorro  \\\n",
       "95           0.0               0.0       0.0             0.0           0.0   \n",
       "103          0.0               0.0       0.0             0.0           0.0   \n",
       "132          0.0               0.0       0.0             0.0           0.0   \n",
       "146          0.0               0.0    1650.0           325.0           0.0   \n",
       "176          0.0               0.0       0.0             0.0           0.0   \n",
       "...          ...               ...       ...             ...           ...   \n",
       "30575        0.0               0.0    2783.0             0.0           0.0   \n",
       "30576        0.0               0.0    2324.0             0.0           0.0   \n",
       "30577        0.0               0.0    3664.0             0.0           0.0   \n",
       "30578        0.0               0.0    4634.0             0.0           0.0   \n",
       "30579        0.0               0.0    4979.0             0.0           0.0   \n",
       "\n",
       "       Monto BAP  \n",
       "95           0.0  \n",
       "103          0.0  \n",
       "132          0.0  \n",
       "146          0.0  \n",
       "176          0.0  \n",
       "...          ...  \n",
       "30575        0.0  \n",
       "30576        0.0  \n",
       "30577        0.0  \n",
       "30578        0.0  \n",
       "30579        0.0  \n",
       "\n",
       "[27534 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_numericas = df[numericas]\n",
    "df_numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Operacion Con Credito</th>\n",
       "      <th>Zona</th>\n",
       "      <th>Tipo</th>\n",
       "      <th>Nacionalidad Cliente</th>\n",
       "      <th>Estado Civil</th>\n",
       "      <th>N° Grupo Familiar</th>\n",
       "      <th>Tipo Compra</th>\n",
       "      <th>Rango de Ingresos</th>\n",
       "      <th>Profesion (Estandar)</th>\n",
       "      <th>Cargo Estandarizado</th>\n",
       "      <th>Rango de Edad</th>\n",
       "      <th>Comuna Estandarizada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>No</td>\n",
       "      <td>2-ZONA CENTRO</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Chilena</td>\n",
       "      <td>Soltero(a)</td>\n",
       "      <td>3</td>\n",
       "      <td>Segunda Vivienda</td>\n",
       "      <td>1.000.001 y 1.500.000</td>\n",
       "      <td>Tecnicos</td>\n",
       "      <td>Ejecutivo</td>\n",
       "      <td>36-40</td>\n",
       "      <td>Puente Alto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>No</td>\n",
       "      <td>3-ZONA SUR</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Chilena</td>\n",
       "      <td>Casado(a)</td>\n",
       "      <td>3</td>\n",
       "      <td>Otros</td>\n",
       "      <td>2.500.001 y 3.000.000</td>\n",
       "      <td>Universitarios</td>\n",
       "      <td>Gerente</td>\n",
       "      <td>41-45</td>\n",
       "      <td>VIII Región del Biobío</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>No</td>\n",
       "      <td>1-ZONA NORTE</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Chilena</td>\n",
       "      <td>Soltero(a)</td>\n",
       "      <td>2</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>Entre 500.001 y 1.000.000</td>\n",
       "      <td>Universitarios</td>\n",
       "      <td>Apoyo</td>\n",
       "      <td>26-30</td>\n",
       "      <td>IV Región de Coquimbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Si</td>\n",
       "      <td>1-ZONA NORTE</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Chilena</td>\n",
       "      <td>Soltero(a)</td>\n",
       "      <td>2</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>Entre 500.001 y 1.000.000</td>\n",
       "      <td>Universitarios</td>\n",
       "      <td>Apoyo</td>\n",
       "      <td>26-30</td>\n",
       "      <td>IV Región de Coquimbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>No</td>\n",
       "      <td>2-ZONA CENTRO</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Chilena</td>\n",
       "      <td>Soltero(a)</td>\n",
       "      <td>3</td>\n",
       "      <td>Vivienda Inversion</td>\n",
       "      <td>1.500.001 y 2.000.000</td>\n",
       "      <td>Tecnicos</td>\n",
       "      <td>Ejecutivo</td>\n",
       "      <td>41-45</td>\n",
       "      <td>San Miguel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30575</th>\n",
       "      <td>Si</td>\n",
       "      <td>3-ZONA SUR</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Chilena</td>\n",
       "      <td>Soltero(a)</td>\n",
       "      <td>3</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>Entre 500.001 y 1.000.000</td>\n",
       "      <td>Tecnicos</td>\n",
       "      <td>Administrativo</td>\n",
       "      <td>51-55</td>\n",
       "      <td>VIII Región del Biobío</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30576</th>\n",
       "      <td>Si</td>\n",
       "      <td>3-ZONA SUR</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Chilena</td>\n",
       "      <td>Casado(a)</td>\n",
       "      <td>2</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>1.000.001 y 1.500.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Jefe</td>\n",
       "      <td>41-45</td>\n",
       "      <td>VIII Región del Biobío</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30577</th>\n",
       "      <td>Si</td>\n",
       "      <td>3-ZONA SUR</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Chilena</td>\n",
       "      <td>Soltero(a)</td>\n",
       "      <td>6</td>\n",
       "      <td>Primera Vivienda</td>\n",
       "      <td>2.000.001 y 2.500.000</td>\n",
       "      <td>Universitarios</td>\n",
       "      <td>Jefe</td>\n",
       "      <td>31-35</td>\n",
       "      <td>VIII Región del Biobío</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30578</th>\n",
       "      <td>Si</td>\n",
       "      <td>3-ZONA SUR</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Chilena</td>\n",
       "      <td>Casado(a)</td>\n",
       "      <td>4</td>\n",
       "      <td>Segunda Vivienda</td>\n",
       "      <td>2.500.001 y 3.000.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Jefe</td>\n",
       "      <td>51-55</td>\n",
       "      <td>VIII Región del Biobío</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30579</th>\n",
       "      <td>Si</td>\n",
       "      <td>3-ZONA SUR</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Chilena</td>\n",
       "      <td>Casado(a)</td>\n",
       "      <td>5</td>\n",
       "      <td>Segunda Vivienda</td>\n",
       "      <td>3.000.001 y 4.000.000</td>\n",
       "      <td>Otros</td>\n",
       "      <td>Gerente</td>\n",
       "      <td>51-55</td>\n",
       "      <td>VIII Región del Biobío</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27534 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Operacion Con Credito           Zona     Tipo Nacionalidad Cliente  \\\n",
       "95                       No  2-ZONA CENTRO  Natural              Chilena   \n",
       "103                      No     3-ZONA SUR  Natural              Chilena   \n",
       "132                      No   1-ZONA NORTE  Natural              Chilena   \n",
       "146                      Si   1-ZONA NORTE  Natural              Chilena   \n",
       "176                      No  2-ZONA CENTRO  Natural              Chilena   \n",
       "...                     ...            ...      ...                  ...   \n",
       "30575                    Si     3-ZONA SUR  Natural              Chilena   \n",
       "30576                    Si     3-ZONA SUR  Natural              Chilena   \n",
       "30577                    Si     3-ZONA SUR  Natural              Chilena   \n",
       "30578                    Si     3-ZONA SUR  Natural              Chilena   \n",
       "30579                    Si     3-ZONA SUR  Natural              Chilena   \n",
       "\n",
       "      Estado Civil  N° Grupo Familiar         Tipo Compra  \\\n",
       "95      Soltero(a)                  3    Segunda Vivienda   \n",
       "103      Casado(a)                  3               Otros   \n",
       "132     Soltero(a)                  2    Primera Vivienda   \n",
       "146     Soltero(a)                  2    Primera Vivienda   \n",
       "176     Soltero(a)                  3  Vivienda Inversion   \n",
       "...            ...                ...                 ...   \n",
       "30575   Soltero(a)                  3    Primera Vivienda   \n",
       "30576    Casado(a)                  2    Primera Vivienda   \n",
       "30577   Soltero(a)                  6    Primera Vivienda   \n",
       "30578    Casado(a)                  4    Segunda Vivienda   \n",
       "30579    Casado(a)                  5    Segunda Vivienda   \n",
       "\n",
       "               Rango de Ingresos Profesion (Estandar) Cargo Estandarizado  \\\n",
       "95         1.000.001 y 1.500.000             Tecnicos           Ejecutivo   \n",
       "103        2.500.001 y 3.000.000       Universitarios             Gerente   \n",
       "132    Entre 500.001 y 1.000.000       Universitarios               Apoyo   \n",
       "146    Entre 500.001 y 1.000.000       Universitarios               Apoyo   \n",
       "176        1.500.001 y 2.000.000             Tecnicos           Ejecutivo   \n",
       "...                          ...                  ...                 ...   \n",
       "30575  Entre 500.001 y 1.000.000             Tecnicos      Administrativo   \n",
       "30576      1.000.001 y 1.500.000                Otros                Jefe   \n",
       "30577      2.000.001 y 2.500.000       Universitarios                Jefe   \n",
       "30578      2.500.001 y 3.000.000                Otros                Jefe   \n",
       "30579      3.000.001 y 4.000.000                Otros             Gerente   \n",
       "\n",
       "      Rango de Edad    Comuna Estandarizada  \n",
       "95            36-40             Puente Alto  \n",
       "103           41-45  VIII Región del Biobío  \n",
       "132           26-30   IV Región de Coquimbo  \n",
       "146           26-30   IV Región de Coquimbo  \n",
       "176           41-45              San Miguel  \n",
       "...             ...                     ...  \n",
       "30575         51-55  VIII Región del Biobío  \n",
       "30576         41-45  VIII Región del Biobío  \n",
       "30577         31-35  VIII Región del Biobío  \n",
       "30578         51-55  VIII Región del Biobío  \n",
       "30579         51-55  VIII Región del Biobío  \n",
       "\n",
       "[27534 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_categoricas = df[categoricas]\n",
    "df_categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categoricas = pd.get_dummies(df_categoricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N° Grupo Familiar</th>\n",
       "      <th>Operacion Con Credito_No</th>\n",
       "      <th>Operacion Con Credito_Si</th>\n",
       "      <th>Zona_1-ZONA NORTE</th>\n",
       "      <th>Zona_2-ZONA CENTRO</th>\n",
       "      <th>Zona_3-ZONA SUR</th>\n",
       "      <th>Zona_4-ZONA VERTICAL</th>\n",
       "      <th>Zona_5-ZONA ANTOFAGASTA</th>\n",
       "      <th>Tipo_Jurídico</th>\n",
       "      <th>Tipo_Natural</th>\n",
       "      <th>...</th>\n",
       "      <th>Comuna Estandarizada_VII Región del Maule</th>\n",
       "      <th>Comuna Estandarizada_VIII Región del Biobío</th>\n",
       "      <th>Comuna Estandarizada_Vitacura</th>\n",
       "      <th>Comuna Estandarizada_X Región de Los Lagos</th>\n",
       "      <th>Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo</th>\n",
       "      <th>Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena</th>\n",
       "      <th>Comuna Estandarizada_XIV Región de Los Ríos</th>\n",
       "      <th>Comuna Estandarizada_XV Región de Arica y Parinacota</th>\n",
       "      <th>Comuna Estandarizada_XVI Región de Ñuble</th>\n",
       "      <th>Comuna Estandarizada_Ñuñoa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30575</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30576</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30577</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30578</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30579</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27534 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N° Grupo Familiar  Operacion Con Credito_No  Operacion Con Credito_Si  \\\n",
       "95                     3                         1                         0   \n",
       "103                    3                         1                         0   \n",
       "132                    2                         1                         0   \n",
       "146                    2                         0                         1   \n",
       "176                    3                         1                         0   \n",
       "...                  ...                       ...                       ...   \n",
       "30575                  3                         0                         1   \n",
       "30576                  2                         0                         1   \n",
       "30577                  6                         0                         1   \n",
       "30578                  4                         0                         1   \n",
       "30579                  5                         0                         1   \n",
       "\n",
       "       Zona_1-ZONA NORTE  Zona_2-ZONA CENTRO  Zona_3-ZONA SUR  \\\n",
       "95                     0                   1                0   \n",
       "103                    0                   0                1   \n",
       "132                    1                   0                0   \n",
       "146                    1                   0                0   \n",
       "176                    0                   1                0   \n",
       "...                  ...                 ...              ...   \n",
       "30575                  0                   0                1   \n",
       "30576                  0                   0                1   \n",
       "30577                  0                   0                1   \n",
       "30578                  0                   0                1   \n",
       "30579                  0                   0                1   \n",
       "\n",
       "       Zona_4-ZONA VERTICAL  Zona_5-ZONA ANTOFAGASTA  Tipo_Jurídico  \\\n",
       "95                        0                        0              0   \n",
       "103                       0                        0              0   \n",
       "132                       0                        0              0   \n",
       "146                       0                        0              0   \n",
       "176                       0                        0              0   \n",
       "...                     ...                      ...            ...   \n",
       "30575                     0                        0              0   \n",
       "30576                     0                        0              0   \n",
       "30577                     0                        0              0   \n",
       "30578                     0                        0              0   \n",
       "30579                     0                        0              0   \n",
       "\n",
       "       Tipo_Natural  ...  Comuna Estandarizada_VII Región del Maule  \\\n",
       "95                1  ...                                          0   \n",
       "103               1  ...                                          0   \n",
       "132               1  ...                                          0   \n",
       "146               1  ...                                          0   \n",
       "176               1  ...                                          0   \n",
       "...             ...  ...                                        ...   \n",
       "30575             1  ...                                          0   \n",
       "30576             1  ...                                          0   \n",
       "30577             1  ...                                          0   \n",
       "30578             1  ...                                          0   \n",
       "30579             1  ...                                          0   \n",
       "\n",
       "       Comuna Estandarizada_VIII Región del Biobío  \\\n",
       "95                                               0   \n",
       "103                                              1   \n",
       "132                                              0   \n",
       "146                                              0   \n",
       "176                                              0   \n",
       "...                                            ...   \n",
       "30575                                            1   \n",
       "30576                                            1   \n",
       "30577                                            1   \n",
       "30578                                            1   \n",
       "30579                                            1   \n",
       "\n",
       "       Comuna Estandarizada_Vitacura  \\\n",
       "95                                 0   \n",
       "103                                0   \n",
       "132                                0   \n",
       "146                                0   \n",
       "176                                0   \n",
       "...                              ...   \n",
       "30575                              0   \n",
       "30576                              0   \n",
       "30577                              0   \n",
       "30578                              0   \n",
       "30579                              0   \n",
       "\n",
       "       Comuna Estandarizada_X Región de Los Lagos  \\\n",
       "95                                              0   \n",
       "103                                             0   \n",
       "132                                             0   \n",
       "146                                             0   \n",
       "176                                             0   \n",
       "...                                           ...   \n",
       "30575                                           0   \n",
       "30576                                           0   \n",
       "30577                                           0   \n",
       "30578                                           0   \n",
       "30579                                           0   \n",
       "\n",
       "       Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo  \\\n",
       "95                                                     0                          \n",
       "103                                                    0                          \n",
       "132                                                    0                          \n",
       "146                                                    0                          \n",
       "176                                                    0                          \n",
       "...                                                  ...                          \n",
       "30575                                                  0                          \n",
       "30576                                                  0                          \n",
       "30577                                                  0                          \n",
       "30578                                                  0                          \n",
       "30579                                                  0                          \n",
       "\n",
       "       Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena  \\\n",
       "95                                                     0                         \n",
       "103                                                    0                         \n",
       "132                                                    0                         \n",
       "146                                                    0                         \n",
       "176                                                    0                         \n",
       "...                                                  ...                         \n",
       "30575                                                  0                         \n",
       "30576                                                  0                         \n",
       "30577                                                  0                         \n",
       "30578                                                  0                         \n",
       "30579                                                  0                         \n",
       "\n",
       "       Comuna Estandarizada_XIV Región de Los Ríos  \\\n",
       "95                                               0   \n",
       "103                                              0   \n",
       "132                                              0   \n",
       "146                                              0   \n",
       "176                                              0   \n",
       "...                                            ...   \n",
       "30575                                            0   \n",
       "30576                                            0   \n",
       "30577                                            0   \n",
       "30578                                            0   \n",
       "30579                                            0   \n",
       "\n",
       "       Comuna Estandarizada_XV Región de Arica y Parinacota  \\\n",
       "95                                                     0      \n",
       "103                                                    0      \n",
       "132                                                    0      \n",
       "146                                                    0      \n",
       "176                                                    0      \n",
       "...                                                  ...      \n",
       "30575                                                  0      \n",
       "30576                                                  0      \n",
       "30577                                                  0      \n",
       "30578                                                  0      \n",
       "30579                                                  0      \n",
       "\n",
       "       Comuna Estandarizada_XVI Región de Ñuble  Comuna Estandarizada_Ñuñoa  \n",
       "95                                            0                           0  \n",
       "103                                           0                           0  \n",
       "132                                           0                           0  \n",
       "146                                           0                           0  \n",
       "176                                           0                           0  \n",
       "...                                         ...                         ...  \n",
       "30575                                         0                           0  \n",
       "30576                                         0                           0  \n",
       "30577                                         0                           0  \n",
       "30578                                         0                           0  \n",
       "30579                                         0                           0  \n",
       "\n",
       "[27534 rows x 148 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro = pd.merge(df_numericas.reset_index(), df_categoricas.reset_index(), on='index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>...</th>\n",
       "      <th>Comuna Estandarizada_VII Región del Maule</th>\n",
       "      <th>Comuna Estandarizada_VIII Región del Biobío</th>\n",
       "      <th>Comuna Estandarizada_Vitacura</th>\n",
       "      <th>Comuna Estandarizada_X Región de Los Lagos</th>\n",
       "      <th>Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo</th>\n",
       "      <th>Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena</th>\n",
       "      <th>Comuna Estandarizada_XIV Región de Los Ríos</th>\n",
       "      <th>Comuna Estandarizada_XV Región de Arica y Parinacota</th>\n",
       "      <th>Comuna Estandarizada_XVI Región de Ñuble</th>\n",
       "      <th>Comuna Estandarizada_Ñuñoa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4616.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4448.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>90.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>221.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2200.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>400.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27529</th>\n",
       "      <td>10.0</td>\n",
       "      <td>300.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3156.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27530</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3973.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27531</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1481.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5260.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27532</th>\n",
       "      <td>10.0</td>\n",
       "      <td>504.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5148.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27533</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5472.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27534 rows × 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "0               18.0       0.00                         0.0   \n",
       "1               10.0    4616.13                         0.0   \n",
       "2               10.0      90.00                         0.0   \n",
       "3                3.5     221.50                         0.0   \n",
       "4                0.0     400.00                         0.0   \n",
       "...              ...        ...                         ...   \n",
       "27529           10.0     300.00                         0.0   \n",
       "27530           10.0    1480.00                         0.0   \n",
       "27531           10.0    1481.00                         0.0   \n",
       "27532           10.0     504.00                         0.0   \n",
       "27533            0.0       0.00                         0.0   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "0                         0.0                    0.0   \n",
       "1                         0.0                    0.0   \n",
       "2                         0.0                    0.0   \n",
       "3                         0.0                    0.0   \n",
       "4                         0.0                    0.0   \n",
       "...                       ...                    ...   \n",
       "27529                     0.0                    0.0   \n",
       "27530                     0.0                    0.0   \n",
       "27531                     0.0                    0.0   \n",
       "27532                     0.0                    0.0   \n",
       "27533                     0.0                    0.0   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "0                               0.0            0.0                      0.00   \n",
       "1                               0.0            0.0                   4448.56   \n",
       "2                               0.0            0.0                      0.00   \n",
       "3                               0.0            0.0                   2200.00   \n",
       "4                               0.0            0.0                      0.00   \n",
       "...                             ...            ...                       ...   \n",
       "27529                           0.0            0.0                   3156.00   \n",
       "27530                           0.0            0.0                   3973.00   \n",
       "27531                           0.0            0.0                   5260.00   \n",
       "27532                           0.0            0.0                   5148.00   \n",
       "27533                           0.0            0.0                   5472.00   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  ...  \\\n",
       "0            0.0               0.0  ...   \n",
       "1            0.0               0.0  ...   \n",
       "2            0.0               0.0  ...   \n",
       "3            0.0               0.0  ...   \n",
       "4            0.0               0.0  ...   \n",
       "...          ...               ...  ...   \n",
       "27529        0.0               0.0  ...   \n",
       "27530        0.0               0.0  ...   \n",
       "27531        0.0               0.0  ...   \n",
       "27532        0.0               0.0  ...   \n",
       "27533        0.0               0.0  ...   \n",
       "\n",
       "       Comuna Estandarizada_VII Región del Maule  \\\n",
       "0                                              0   \n",
       "1                                              0   \n",
       "2                                              0   \n",
       "3                                              0   \n",
       "4                                              0   \n",
       "...                                          ...   \n",
       "27529                                          0   \n",
       "27530                                          0   \n",
       "27531                                          0   \n",
       "27532                                          0   \n",
       "27533                                          0   \n",
       "\n",
       "       Comuna Estandarizada_VIII Región del Biobío  \\\n",
       "0                                                0   \n",
       "1                                                1   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "...                                            ...   \n",
       "27529                                            1   \n",
       "27530                                            1   \n",
       "27531                                            1   \n",
       "27532                                            1   \n",
       "27533                                            1   \n",
       "\n",
       "       Comuna Estandarizada_Vitacura  \\\n",
       "0                                  0   \n",
       "1                                  0   \n",
       "2                                  0   \n",
       "3                                  0   \n",
       "4                                  0   \n",
       "...                              ...   \n",
       "27529                              0   \n",
       "27530                              0   \n",
       "27531                              0   \n",
       "27532                              0   \n",
       "27533                              0   \n",
       "\n",
       "       Comuna Estandarizada_X Región de Los Lagos  \\\n",
       "0                                               0   \n",
       "1                                               0   \n",
       "2                                               0   \n",
       "3                                               0   \n",
       "4                                               0   \n",
       "...                                           ...   \n",
       "27529                                           0   \n",
       "27530                                           0   \n",
       "27531                                           0   \n",
       "27532                                           0   \n",
       "27533                                           0   \n",
       "\n",
       "       Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo  \\\n",
       "0                                                      0                          \n",
       "1                                                      0                          \n",
       "2                                                      0                          \n",
       "3                                                      0                          \n",
       "4                                                      0                          \n",
       "...                                                  ...                          \n",
       "27529                                                  0                          \n",
       "27530                                                  0                          \n",
       "27531                                                  0                          \n",
       "27532                                                  0                          \n",
       "27533                                                  0                          \n",
       "\n",
       "       Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena  \\\n",
       "0                                                      0                         \n",
       "1                                                      0                         \n",
       "2                                                      0                         \n",
       "3                                                      0                         \n",
       "4                                                      0                         \n",
       "...                                                  ...                         \n",
       "27529                                                  0                         \n",
       "27530                                                  0                         \n",
       "27531                                                  0                         \n",
       "27532                                                  0                         \n",
       "27533                                                  0                         \n",
       "\n",
       "       Comuna Estandarizada_XIV Región de Los Ríos  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "...                                            ...   \n",
       "27529                                            0   \n",
       "27530                                            0   \n",
       "27531                                            0   \n",
       "27532                                            0   \n",
       "27533                                            0   \n",
       "\n",
       "       Comuna Estandarizada_XV Región de Arica y Parinacota  \\\n",
       "0                                                      0      \n",
       "1                                                      0      \n",
       "2                                                      0      \n",
       "3                                                      0      \n",
       "4                                                      0      \n",
       "...                                                  ...      \n",
       "27529                                                  0      \n",
       "27530                                                  0      \n",
       "27531                                                  0      \n",
       "27532                                                  0      \n",
       "27533                                                  0      \n",
       "\n",
       "       Comuna Estandarizada_XVI Región de Ñuble  Comuna Estandarizada_Ñuñoa  \n",
       "0                                             0                           0  \n",
       "1                                             0                           0  \n",
       "2                                             0                           0  \n",
       "3                                             0                           0  \n",
       "4                                             0                           0  \n",
       "...                                         ...                         ...  \n",
       "27529                                         0                           0  \n",
       "27530                                         0                           0  \n",
       "27531                                         0                           0  \n",
       "27532                                         0                           0  \n",
       "27533                                         0                           0  \n",
       "\n",
       "[27534 rows x 162 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro = df_prepro.drop('index', axis=1)\n",
    "df_prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_prepro\n",
    "y = df['Etiqueta']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)\n",
    "prob = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Desiste', 'Escritura'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.17457678 0.82542322]\n",
      "El cliente  Escritura con probabilidad [0.28454838 0.71545162]\n",
      "El cliente  Escritura con probabilidad [0.11830279 0.88169721]\n",
      "El cliente  Escritura con probabilidad [0.23153613 0.76846387]\n",
      "El cliente  Escritura con probabilidad [0.1671258 0.8328742]\n",
      "El cliente  Escritura con probabilidad [0.14267866 0.85732134]\n",
      "El cliente  Escritura con probabilidad [0.00541635 0.99458365]\n",
      "El cliente  Escritura con probabilidad [0.13974947 0.86025053]\n",
      "El cliente  Escritura con probabilidad [0.19380607 0.80619393]\n",
      "El cliente  Escritura con probabilidad [0.27083697 0.72916303]\n",
      "El cliente  Escritura con probabilidad [0.03282603 0.96717397]\n",
      "El cliente  Escritura con probabilidad [0.06554646 0.93445354]\n",
      "El cliente  Escritura con probabilidad [0.27980351 0.72019649]\n",
      "El cliente  Escritura con probabilidad [0.09925779 0.90074221]\n",
      "El cliente  Escritura con probabilidad [0.00862931 0.99137069]\n",
      "El cliente  Escritura con probabilidad [0.4329968 0.5670032]\n",
      "El cliente  Escritura con probabilidad [0.1840628 0.8159372]\n",
      "El cliente  Escritura con probabilidad [0.06576601 0.93423399]\n",
      "El cliente  Escritura con probabilidad [0.12019572 0.87980428]\n",
      "El cliente  Escritura con probabilidad [0.0759416 0.9240584]\n",
      "El cliente  Escritura con probabilidad [0.10966251 0.89033749]\n",
      "El cliente  Escritura con probabilidad [0.4269398 0.5730602]\n",
      "El cliente  Escritura con probabilidad [2.19584351e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.19531451 0.80468549]\n",
      "El cliente  Escritura con probabilidad [0.18828274 0.81171726]\n",
      "El cliente  Escritura con probabilidad [0.12445446 0.87554554]\n",
      "El cliente  Escritura con probabilidad [0.02359383 0.97640617]\n",
      "El cliente  Escritura con probabilidad [0.20883465 0.79116535]\n",
      "El cliente  Escritura con probabilidad [0.32645354 0.67354646]\n",
      "El cliente  Escritura con probabilidad [2.57677074e-06 9.99997423e-01]\n",
      "El cliente  Escritura con probabilidad [0.14783482 0.85216518]\n",
      "El cliente  Escritura con probabilidad [0.01585755 0.98414245]\n",
      "El cliente  Escritura con probabilidad [0.39556913 0.60443087]\n",
      "El cliente  Escritura con probabilidad [2.46846987e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.00685619 0.99314381]\n",
      "El cliente  Escritura con probabilidad [0.22268408 0.77731592]\n",
      "El cliente  Escritura con probabilidad [0.03103935 0.96896065]\n",
      "El cliente  Escritura con probabilidad [5.68745051e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.19131639 0.80868361]\n",
      "El cliente  Escritura con probabilidad [0.21474808 0.78525192]\n",
      "El cliente  Escritura con probabilidad [0.30374889 0.69625111]\n",
      "El cliente  Escritura con probabilidad [0.19195888 0.80804112]\n",
      "El cliente  Escritura con probabilidad [1.14728410e-07 9.99999885e-01]\n",
      "El cliente  Escritura con probabilidad [0.16658178 0.83341822]\n",
      "El cliente  Escritura con probabilidad [0.18022548 0.81977452]\n",
      "El cliente  Escritura con probabilidad [0.11170584 0.88829416]\n",
      "El cliente  Escritura con probabilidad [0.20028466 0.79971534]\n",
      "El cliente  Escritura con probabilidad [0.12685694 0.87314306]\n",
      "El cliente  Escritura con probabilidad [0.01766235 0.98233765]\n",
      "El cliente  Escritura con probabilidad [0.13255401 0.86744599]\n",
      "El cliente  Escritura con probabilidad [0.32104686 0.67895314]\n",
      "El cliente  Escritura con probabilidad [0.06181657 0.93818343]\n",
      "El cliente  Escritura con probabilidad [0.27286008 0.72713992]\n",
      "El cliente  Escritura con probabilidad [0.21893764 0.78106236]\n",
      "El cliente  Escritura con probabilidad [0.0529394 0.9470606]\n",
      "El cliente  Escritura con probabilidad [0.0161084 0.9838916]\n",
      "El cliente  Escritura con probabilidad [0.0325882 0.9674118]\n",
      "El cliente  Escritura con probabilidad [0.03892069 0.96107931]\n",
      "El cliente  Escritura con probabilidad [0.20808748 0.79191252]\n",
      "El cliente  Escritura con probabilidad [0.01990503 0.98009497]\n",
      "El cliente  Escritura con probabilidad [0.0424525 0.9575475]\n",
      "El cliente  Escritura con probabilidad [0.03747045 0.96252955]\n",
      "El cliente  Escritura con probabilidad [0.07673484 0.92326516]\n",
      "El cliente  Escritura con probabilidad [4.62876965e-06 9.99995371e-01]\n",
      "El cliente  Escritura con probabilidad [0.18165876 0.81834124]\n",
      "El cliente  Escritura con probabilidad [0.21878426 0.78121574]\n",
      "El cliente  Escritura con probabilidad [0.21223976 0.78776024]\n",
      "El cliente  Escritura con probabilidad [0.12794103 0.87205897]\n",
      "El cliente  Escritura con probabilidad [0.15809354 0.84190646]\n",
      "El cliente  Escritura con probabilidad [0.21163004 0.78836996]\n",
      "El cliente  Escritura con probabilidad [0.05089057 0.94910943]\n",
      "El cliente  Escritura con probabilidad [0.10983376 0.89016624]\n",
      "El cliente  Escritura con probabilidad [0.13164662 0.86835338]\n",
      "El cliente  Escritura con probabilidad [0.00629445 0.99370555]\n",
      "El cliente  Escritura con probabilidad [0.01824577 0.98175423]\n",
      "El cliente  Escritura con probabilidad [0.05680554 0.94319446]\n",
      "El cliente  Escritura con probabilidad [0.00413011 0.99586989]\n",
      "El cliente  Escritura con probabilidad [0.2427175 0.7572825]\n",
      "El cliente  Escritura con probabilidad [0.23029582 0.76970418]\n",
      "El cliente  Escritura con probabilidad [0.1285508 0.8714492]\n",
      "El cliente  Escritura con probabilidad [0.07157437 0.92842563]\n",
      "El cliente  Escritura con probabilidad [0.17282015 0.82717985]\n",
      "El cliente  Escritura con probabilidad [0.04085211 0.95914789]\n",
      "El cliente  Escritura con probabilidad [0.05987586 0.94012414]\n",
      "El cliente  Escritura con probabilidad [0.14580738 0.85419262]\n",
      "El cliente  Escritura con probabilidad [0.00733537 0.99266463]\n",
      "El cliente  Escritura con probabilidad [0.11593784 0.88406216]\n",
      "El cliente  Escritura con probabilidad [0.10451404 0.89548596]\n",
      "El cliente  Escritura con probabilidad [0.02028593 0.97971407]\n",
      "El cliente  Escritura con probabilidad [0.20009857 0.79990143]\n",
      "El cliente  Escritura con probabilidad [0.14895286 0.85104714]\n",
      "El cliente  Escritura con probabilidad [0.17398437 0.82601563]\n",
      "El cliente  Escritura con probabilidad [0.16910488 0.83089512]\n",
      "El cliente  Escritura con probabilidad [0.05018925 0.94981075]\n",
      "El cliente  Escritura con probabilidad [0.00166431 0.99833569]\n",
      "El cliente  Escritura con probabilidad [5.08472024e-04 9.99491528e-01]\n",
      "El cliente  Escritura con probabilidad [0.10516652 0.89483348]\n",
      "El cliente  Escritura con probabilidad [2.14308180e-04 9.99785692e-01]\n",
      "El cliente  Escritura con probabilidad [0.11822399 0.88177601]\n",
      "El cliente  Escritura con probabilidad [1.29089983e-06 9.99998709e-01]\n",
      "El cliente  Escritura con probabilidad [0.12907544 0.87092456]\n",
      "El cliente  Escritura con probabilidad [0.16198418 0.83801582]\n",
      "El cliente  Escritura con probabilidad [0.13696625 0.86303375]\n",
      "El cliente  Escritura con probabilidad [0.03141122 0.96858878]\n",
      "El cliente  Escritura con probabilidad [0.07947072 0.92052928]\n",
      "El cliente  Escritura con probabilidad [2.02170363e-05 9.99979783e-01]\n",
      "El cliente  Escritura con probabilidad [0.34192947 0.65807053]\n",
      "El cliente  Escritura con probabilidad [0.18021987 0.81978013]\n",
      "El cliente  Escritura con probabilidad [1.81329574e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.26385055 0.73614945]\n",
      "El cliente  Escritura con probabilidad [0.28854928 0.71145072]\n",
      "El cliente  Escritura con probabilidad [0.08442361 0.91557639]\n",
      "El cliente  Escritura con probabilidad [0.01056129 0.98943871]\n",
      "El cliente  Escritura con probabilidad [0.01170267 0.98829733]\n",
      "El cliente  Escritura con probabilidad [0.24485743 0.75514257]\n",
      "El cliente  Escritura con probabilidad [2.37256743e-06 9.99997627e-01]\n",
      "El cliente  Escritura con probabilidad [0.07416512 0.92583488]\n",
      "El cliente  Escritura con probabilidad [0.12777663 0.87222337]\n",
      "El cliente  Escritura con probabilidad [0.06798806 0.93201194]\n",
      "El cliente  Escritura con probabilidad [5.01011293e-06 9.99994990e-01]\n",
      "El cliente  Escritura con probabilidad [0.06775394 0.93224606]\n",
      "El cliente  Escritura con probabilidad [0.09039482 0.90960518]\n",
      "El cliente  Escritura con probabilidad [0.22375908 0.77624092]\n",
      "El cliente  Escritura con probabilidad [0.3124845 0.6875155]\n",
      "El cliente  Escritura con probabilidad [0.25820606 0.74179394]\n",
      "El cliente  Escritura con probabilidad [0.01904692 0.98095308]\n",
      "El cliente  Escritura con probabilidad [0.42813695 0.57186305]\n",
      "El cliente  Escritura con probabilidad [0.05707687 0.94292313]\n",
      "El cliente  Escritura con probabilidad [2.23378152e-06 9.99997766e-01]\n",
      "El cliente  Escritura con probabilidad [0.09583672 0.90416328]\n",
      "El cliente  Escritura con probabilidad [6.73136478e-05 9.99932686e-01]\n",
      "El cliente  Escritura con probabilidad [0.1705441 0.8294559]\n",
      "El cliente  Escritura con probabilidad [0.02640141 0.97359859]\n",
      "El cliente  Escritura con probabilidad [0.25923089 0.74076911]\n",
      "El cliente  Escritura con probabilidad [0.20238462 0.79761538]\n",
      "El cliente  Escritura con probabilidad [0.24231121 0.75768879]\n",
      "El cliente  Escritura con probabilidad [0.23570998 0.76429002]\n",
      "El cliente  Escritura con probabilidad [6.29712038e-09 9.99999994e-01]\n",
      "El cliente  Escritura con probabilidad [0.17614884 0.82385116]\n",
      "El cliente  Escritura con probabilidad [0.00335946 0.99664054]\n",
      "El cliente  Escritura con probabilidad [0.01854163 0.98145837]\n",
      "El cliente  Escritura con probabilidad [0.09224109 0.90775891]\n",
      "El cliente  Escritura con probabilidad [0.13781622 0.86218378]\n",
      "El cliente  Escritura con probabilidad [0.22243473 0.77756527]\n",
      "El cliente  Escritura con probabilidad [0.00424873 0.99575127]\n",
      "El cliente  Escritura con probabilidad [0.07123166 0.92876834]\n",
      "El cliente  Escritura con probabilidad [0.02103323 0.97896677]\n",
      "El cliente  Escritura con probabilidad [0.22657726 0.77342274]\n",
      "El cliente  Escritura con probabilidad [0.23705599 0.76294401]\n",
      "El cliente  Escritura con probabilidad [0.15679039 0.84320961]\n",
      "El cliente  Escritura con probabilidad [0.44478803 0.55521197]\n",
      "El cliente  Escritura con probabilidad [0.10057834 0.89942166]\n",
      "El cliente  Escritura con probabilidad [0.05809391 0.94190609]\n",
      "El cliente  Escritura con probabilidad [0.04035425 0.95964575]\n",
      "El cliente  Escritura con probabilidad [2.64733110e-04 9.99735267e-01]\n",
      "El cliente  Escritura con probabilidad [0.31079512 0.68920488]\n",
      "El cliente  Escritura con probabilidad [0.06420166 0.93579834]\n",
      "El cliente  Escritura con probabilidad [0.00921045 0.99078955]\n",
      "El cliente  Escritura con probabilidad [0.06254494 0.93745506]\n",
      "El cliente  Escritura con probabilidad [0.22151143 0.77848857]\n",
      "El cliente  Escritura con probabilidad [0.20069984 0.79930016]\n",
      "El cliente  Escritura con probabilidad [0.24357365 0.75642635]\n",
      "El cliente  Escritura con probabilidad [0.13764732 0.86235268]\n",
      "El cliente  Escritura con probabilidad [0.09525838 0.90474162]\n",
      "El cliente  Escritura con probabilidad [1.37920850e-06 9.99998621e-01]\n",
      "El cliente  Escritura con probabilidad [0.17252882 0.82747118]\n",
      "El cliente  Escritura con probabilidad [0.04806583 0.95193417]\n",
      "El cliente  Escritura con probabilidad [0.38908108 0.61091892]\n",
      "El cliente  Escritura con probabilidad [0.12371338 0.87628662]\n",
      "El cliente  Escritura con probabilidad [1.25772955e-04 9.99874227e-01]\n",
      "El cliente  Escritura con probabilidad [0.0822687 0.9177313]\n",
      "El cliente  Escritura con probabilidad [1.00960807e-05 9.99989904e-01]\n",
      "El cliente  Escritura con probabilidad [0.28633913 0.71366087]\n",
      "El cliente  Escritura con probabilidad [0.09638354 0.90361646]\n",
      "El cliente  Escritura con probabilidad [0.2814835 0.7185165]\n",
      "El cliente  Escritura con probabilidad [0.19356216 0.80643784]\n",
      "El cliente  Escritura con probabilidad [0.17115317 0.82884683]\n",
      "El cliente  Escritura con probabilidad [3.41948692e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.10504277 0.89495723]\n",
      "El cliente  Escritura con probabilidad [0.00118896 0.99881104]\n",
      "El cliente  Escritura con probabilidad [0.43597268 0.56402732]\n",
      "El cliente  Escritura con probabilidad [0.0390006 0.9609994]\n",
      "El cliente  Escritura con probabilidad [0.20035143 0.79964857]\n",
      "El cliente  Escritura con probabilidad [0.10599601 0.89400399]\n",
      "El cliente  Escritura con probabilidad [0.18584112 0.81415888]\n",
      "El cliente  Escritura con probabilidad [2.19337637e-07 9.99999781e-01]\n",
      "El cliente  Escritura con probabilidad [0.09340364 0.90659636]\n",
      "El cliente  Escritura con probabilidad [0.23270732 0.76729268]\n",
      "El cliente  Escritura con probabilidad [3.42216733e-06 9.99996578e-01]\n",
      "El cliente  Escritura con probabilidad [1.00093928e-05 9.99989991e-01]\n",
      "El cliente  Escritura con probabilidad [0.01383041 0.98616959]\n",
      "El cliente  Escritura con probabilidad [0.0777311 0.9222689]\n",
      "El cliente  Escritura con probabilidad [0.07097316 0.92902684]\n",
      "El cliente  Escritura con probabilidad [0.05457011 0.94542989]\n",
      "El cliente  Escritura con probabilidad [0.22771486 0.77228514]\n",
      "El cliente  Escritura con probabilidad [0.02852293 0.97147707]\n",
      "El cliente  Escritura con probabilidad [0.31503603 0.68496397]\n",
      "El cliente  Escritura con probabilidad [0.08257779 0.91742221]\n",
      "El cliente  Escritura con probabilidad [0.44486427 0.55513573]\n",
      "El cliente  Escritura con probabilidad [0.30064215 0.69935785]\n",
      "El cliente  Escritura con probabilidad [0.0305397 0.9694603]\n",
      "El cliente  Escritura con probabilidad [4.88498131e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.02634555 0.97365445]\n",
      "El cliente  Escritura con probabilidad [0.05490602 0.94509398]\n",
      "El cliente  Escritura con probabilidad [0.13997366 0.86002634]\n",
      "El cliente  Escritura con probabilidad [0.01557631 0.98442369]\n",
      "El cliente  Escritura con probabilidad [0.08600062 0.91399938]\n",
      "El cliente  Escritura con probabilidad [0.1607122 0.8392878]\n",
      "El cliente  Escritura con probabilidad [0.13244699 0.86755301]\n",
      "El cliente  Escritura con probabilidad [0.00785302 0.99214698]\n",
      "El cliente  Escritura con probabilidad [0.24300286 0.75699714]\n",
      "El cliente  Escritura con probabilidad [4.08411817e-05 9.99959159e-01]\n",
      "El cliente  Escritura con probabilidad [0.18816146 0.81183854]\n",
      "El cliente  Escritura con probabilidad [0.17156312 0.82843688]\n",
      "El cliente  Escritura con probabilidad [0.02606881 0.97393119]\n",
      "El cliente  Escritura con probabilidad [0.01679221 0.98320779]\n",
      "El cliente  Escritura con probabilidad [0.01488873 0.98511127]\n",
      "El cliente  Escritura con probabilidad [6.48329555e-04 9.99351670e-01]\n",
      "El cliente  Escritura con probabilidad [0.03662879 0.96337121]\n",
      "El cliente  Escritura con probabilidad [1.60174560e-04 9.99839825e-01]\n",
      "El cliente  Escritura con probabilidad [0.09599037 0.90400963]\n",
      "El cliente  Escritura con probabilidad [0.06741416 0.93258584]\n",
      "El cliente  Escritura con probabilidad [0.0046146 0.9953854]\n",
      "El cliente  Escritura con probabilidad [0.12638429 0.87361571]\n",
      "El cliente  Escritura con probabilidad [0.16102783 0.83897217]\n",
      "El cliente  Desiste con probabilidad [0.7101181 0.2898819]\n",
      "El cliente  Escritura con probabilidad [0.1883411 0.8116589]\n",
      "El cliente  Escritura con probabilidad [0.03113382 0.96886618]\n",
      "El cliente  Escritura con probabilidad [0.17851845 0.82148155]\n",
      "El cliente  Escritura con probabilidad [2.81763707e-06 9.99997182e-01]\n",
      "El cliente  Escritura con probabilidad [2.33672699e-04 9.99766327e-01]\n",
      "El cliente  Escritura con probabilidad [0.04559232 0.95440768]\n",
      "El cliente  Escritura con probabilidad [1.69942841e-06 9.99998301e-01]\n",
      "El cliente  Escritura con probabilidad [0.05509285 0.94490715]\n",
      "El cliente  Escritura con probabilidad [0.22052908 0.77947092]\n",
      "El cliente  Escritura con probabilidad [0.01927878 0.98072122]\n",
      "El cliente  Escritura con probabilidad [0.16649015 0.83350985]\n",
      "El cliente  Escritura con probabilidad [0.17820022 0.82179978]\n",
      "El cliente  Escritura con probabilidad [0.08761126 0.91238874]\n",
      "El cliente  Escritura con probabilidad [0.10976016 0.89023984]\n",
      "El cliente  Escritura con probabilidad [0.21786177 0.78213823]\n",
      "El cliente  Escritura con probabilidad [1.92833907e-04 9.99807166e-01]\n",
      "El cliente  Escritura con probabilidad [0.00402651 0.99597349]\n",
      "El cliente  Escritura con probabilidad [0.00921467 0.99078533]\n",
      "El cliente  Escritura con probabilidad [0.38587534 0.61412466]\n",
      "El cliente  Escritura con probabilidad [4.64401664e-05 9.99953560e-01]\n",
      "El cliente  Escritura con probabilidad [0.15943122 0.84056878]\n",
      "El cliente  Escritura con probabilidad [0.01280009 0.98719991]\n",
      "El cliente  Escritura con probabilidad [0.01536579 0.98463421]\n",
      "El cliente  Escritura con probabilidad [0.17420141 0.82579859]\n",
      "El cliente  Escritura con probabilidad [0.11817313 0.88182687]\n",
      "El cliente  Escritura con probabilidad [0.0432501 0.9567499]\n",
      "El cliente  Escritura con probabilidad [0.42171734 0.57828266]\n",
      "El cliente  Escritura con probabilidad [0.05018682 0.94981318]\n",
      "El cliente  Escritura con probabilidad [0.20744889 0.79255111]\n",
      "El cliente  Escritura con probabilidad [0.21526503 0.78473497]\n",
      "El cliente  Escritura con probabilidad [0.07978992 0.92021008]\n",
      "El cliente  Escritura con probabilidad [0.22480276 0.77519724]\n",
      "El cliente  Escritura con probabilidad [1.10186305e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [9.66604574e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.01950399 0.98049601]\n",
      "El cliente  Escritura con probabilidad [0.02094831 0.97905169]\n",
      "El cliente  Escritura con probabilidad [0.23839713 0.76160287]\n",
      "El cliente  Escritura con probabilidad [1.26418875e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.20603868 0.79396132]\n",
      "El cliente  Escritura con probabilidad [0.22940173 0.77059827]\n",
      "El cliente  Escritura con probabilidad [0.17077953 0.82922047]\n",
      "El cliente  Escritura con probabilidad [0.14421542 0.85578458]\n",
      "El cliente  Escritura con probabilidad [0.21049132 0.78950868]\n",
      "El cliente  Escritura con probabilidad [0.28250521 0.71749479]\n",
      "El cliente  Escritura con probabilidad [0.02836383 0.97163617]\n",
      "El cliente  Escritura con probabilidad [0.02642835 0.97357165]\n",
      "El cliente  Escritura con probabilidad [0.39056693 0.60943307]\n",
      "El cliente  Escritura con probabilidad [0.10534506 0.89465494]\n",
      "El cliente  Escritura con probabilidad [0.08613924 0.91386076]\n",
      "El cliente  Escritura con probabilidad [8.67819459e-08 9.99999913e-01]\n",
      "El cliente  Escritura con probabilidad [0.02389789 0.97610211]\n",
      "El cliente  Escritura con probabilidad [0.04182479 0.95817521]\n",
      "El cliente  Escritura con probabilidad [0.12499994 0.87500006]\n",
      "El cliente  Escritura con probabilidad [0.16704738 0.83295262]\n",
      "El cliente  Escritura con probabilidad [0.17847749 0.82152251]\n",
      "El cliente  Escritura con probabilidad [9.04240919e-04 9.99095759e-01]\n",
      "El cliente  Escritura con probabilidad [0.19548856 0.80451144]\n",
      "El cliente  Escritura con probabilidad [0.01668011 0.98331989]\n",
      "El cliente  Escritura con probabilidad [7.16658666e-05 9.99928334e-01]\n",
      "El cliente  Escritura con probabilidad [0.15991451 0.84008549]\n",
      "El cliente  Escritura con probabilidad [0.08673023 0.91326977]\n",
      "El cliente  Escritura con probabilidad [0.20382038 0.79617962]\n",
      "El cliente  Escritura con probabilidad [0.19902064 0.80097936]\n",
      "El cliente  Escritura con probabilidad [0.27915048 0.72084952]\n",
      "El cliente  Escritura con probabilidad [0.04782 0.95218]\n",
      "El cliente  Escritura con probabilidad [0.17104403 0.82895597]\n",
      "El cliente  Escritura con probabilidad [0.01691886 0.98308114]\n",
      "El cliente  Escritura con probabilidad [0.07752708 0.92247292]\n",
      "El cliente  Escritura con probabilidad [5.46510993e-07 9.99999453e-01]\n",
      "El cliente  Escritura con probabilidad [0.21021327 0.78978673]\n",
      "El cliente  Escritura con probabilidad [0.19056933 0.80943067]\n",
      "El cliente  Escritura con probabilidad [0.03271654 0.96728346]\n",
      "El cliente  Escritura con probabilidad [0.02632345 0.97367655]\n",
      "El cliente  Escritura con probabilidad [0.13469702 0.86530298]\n",
      "El cliente  Escritura con probabilidad [0.14250769 0.85749231]\n",
      "El cliente  Escritura con probabilidad [0.02826854 0.97173146]\n",
      "El cliente  Escritura con probabilidad [0.01677628 0.98322372]\n",
      "El cliente  Escritura con probabilidad [0.19677691 0.80322309]\n",
      "El cliente  Escritura con probabilidad [0.20255624 0.79744376]\n",
      "El cliente  Escritura con probabilidad [0.05762939 0.94237061]\n",
      "El cliente  Escritura con probabilidad [0.01947695 0.98052305]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.0859771 0.9140229]\n",
      "El cliente  Escritura con probabilidad [0.25236953 0.74763047]\n",
      "El cliente  Escritura con probabilidad [0.1399668 0.8600332]\n",
      "El cliente  Escritura con probabilidad [0.26409591 0.73590409]\n",
      "El cliente  Escritura con probabilidad [0.23990089 0.76009911]\n",
      "El cliente  Escritura con probabilidad [0.01382525 0.98617475]\n",
      "El cliente  Escritura con probabilidad [0.02951543 0.97048457]\n",
      "El cliente  Escritura con probabilidad [0.23827559 0.76172441]\n",
      "El cliente  Escritura con probabilidad [0.23184546 0.76815454]\n",
      "El cliente  Escritura con probabilidad [0.28401453 0.71598547]\n",
      "El cliente  Escritura con probabilidad [0.20427517 0.79572483]\n",
      "El cliente  Escritura con probabilidad [0.06086648 0.93913352]\n",
      "El cliente  Escritura con probabilidad [0.21805751 0.78194249]\n",
      "El cliente  Escritura con probabilidad [0.04410455 0.95589545]\n",
      "El cliente  Escritura con probabilidad [0.13257384 0.86742616]\n",
      "El cliente  Escritura con probabilidad [0.16803987 0.83196013]\n",
      "El cliente  Escritura con probabilidad [0.19723809 0.80276191]\n",
      "El cliente  Escritura con probabilidad [0.01712458 0.98287542]\n",
      "El cliente  Escritura con probabilidad [0.20120792 0.79879208]\n",
      "El cliente  Escritura con probabilidad [0.22306444 0.77693556]\n",
      "El cliente  Escritura con probabilidad [0.10160911 0.89839089]\n",
      "El cliente  Escritura con probabilidad [0.24328502 0.75671498]\n",
      "El cliente  Escritura con probabilidad [0.09403527 0.90596473]\n",
      "El cliente  Escritura con probabilidad [0.08249758 0.91750242]\n",
      "El cliente  Escritura con probabilidad [0.17742151 0.82257849]\n",
      "El cliente  Escritura con probabilidad [0.17106177 0.82893823]\n",
      "El cliente  Escritura con probabilidad [0.00210231 0.99789769]\n",
      "El cliente  Escritura con probabilidad [0.04241046 0.95758954]\n",
      "El cliente  Escritura con probabilidad [0.12281365 0.87718635]\n",
      "El cliente  Escritura con probabilidad [0.1273221 0.8726779]\n",
      "El cliente  Escritura con probabilidad [0.07060371 0.92939629]\n",
      "El cliente  Escritura con probabilidad [0.20125746 0.79874254]\n",
      "El cliente  Escritura con probabilidad [0.04378263 0.95621737]\n",
      "El cliente  Escritura con probabilidad [0.2098179 0.7901821]\n",
      "El cliente  Escritura con probabilidad [4.05442342e-04 9.99594558e-01]\n",
      "El cliente  Escritura con probabilidad [0.25573313 0.74426687]\n",
      "El cliente  Escritura con probabilidad [6.64469861e-08 9.99999934e-01]\n",
      "El cliente  Escritura con probabilidad [0.22758282 0.77241718]\n",
      "El cliente  Escritura con probabilidad [0.02631701 0.97368299]\n",
      "El cliente  Escritura con probabilidad [0.17323024 0.82676976]\n",
      "El cliente  Escritura con probabilidad [0.20161068 0.79838932]\n",
      "El cliente  Escritura con probabilidad [0.00615247 0.99384753]\n",
      "El cliente  Escritura con probabilidad [0.07519965 0.92480035]\n",
      "El cliente  Escritura con probabilidad [0.23679167 0.76320833]\n",
      "El cliente  Escritura con probabilidad [0.01945056 0.98054944]\n",
      "El cliente  Escritura con probabilidad [0.00677447 0.99322553]\n",
      "El cliente  Escritura con probabilidad [5.17885300e-05 9.99948211e-01]\n",
      "El cliente  Escritura con probabilidad [0.19156513 0.80843487]\n",
      "El cliente  Escritura con probabilidad [0.13061499 0.86938501]\n",
      "El cliente  Escritura con probabilidad [0.2270366 0.7729634]\n",
      "El cliente  Escritura con probabilidad [0.01549197 0.98450803]\n",
      "El cliente  Escritura con probabilidad [0.27363201 0.72636799]\n",
      "El cliente  Escritura con probabilidad [0.00681875 0.99318125]\n",
      "El cliente  Escritura con probabilidad [0.29367425 0.70632575]\n",
      "El cliente  Escritura con probabilidad [5.27202629e-05 9.99947280e-01]\n",
      "El cliente  Escritura con probabilidad [0.23934036 0.76065964]\n",
      "El cliente  Escritura con probabilidad [0.11218732 0.88781268]\n",
      "El cliente  Escritura con probabilidad [0.03122946 0.96877054]\n",
      "El cliente  Escritura con probabilidad [0.05512695 0.94487305]\n",
      "El cliente  Escritura con probabilidad [0.16029949 0.83970051]\n",
      "El cliente  Escritura con probabilidad [0.20953454 0.79046546]\n",
      "El cliente  Escritura con probabilidad [0.18494207 0.81505793]\n",
      "El cliente  Escritura con probabilidad [0.10576015 0.89423985]\n",
      "El cliente  Escritura con probabilidad [0.12251942 0.87748058]\n",
      "El cliente  Escritura con probabilidad [0.21129588 0.78870412]\n",
      "El cliente  Escritura con probabilidad [2.11094611e-08 9.99999979e-01]\n",
      "El cliente  Escritura con probabilidad [0.13557198 0.86442802]\n",
      "El cliente  Escritura con probabilidad [0.00328588 0.99671412]\n",
      "El cliente  Escritura con probabilidad [0.3209092 0.6790908]\n",
      "El cliente  Escritura con probabilidad [0.27239018 0.72760982]\n",
      "El cliente  Escritura con probabilidad [0.04993853 0.95006147]\n",
      "El cliente  Escritura con probabilidad [0.02482853 0.97517147]\n",
      "El cliente  Escritura con probabilidad [0.02437173 0.97562827]\n",
      "El cliente  Escritura con probabilidad [0.02266954 0.97733046]\n",
      "El cliente  Escritura con probabilidad [5.59578738e-07 9.99999440e-01]\n",
      "El cliente  Escritura con probabilidad [2.61199862e-06 9.99997388e-01]\n",
      "El cliente  Escritura con probabilidad [0.20529197 0.79470803]\n",
      "El cliente  Escritura con probabilidad [1.89485172e-05 9.99981051e-01]\n",
      "El cliente  Escritura con probabilidad [0.13478264 0.86521736]\n",
      "El cliente  Escritura con probabilidad [0.07505767 0.92494233]\n",
      "El cliente  Escritura con probabilidad [7.15443701e-07 9.99999285e-01]\n",
      "El cliente  Escritura con probabilidad [1.49217968e-04 9.99850782e-01]\n",
      "El cliente  Escritura con probabilidad [0.20563626 0.79436374]\n",
      "El cliente  Escritura con probabilidad [0.15914016 0.84085984]\n",
      "El cliente  Escritura con probabilidad [1.51937396e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.18005211 0.81994789]\n",
      "El cliente  Escritura con probabilidad [0.23410697 0.76589303]\n",
      "El cliente  Escritura con probabilidad [0.13322523 0.86677477]\n",
      "El cliente  Escritura con probabilidad [0.1169919 0.8830081]\n",
      "El cliente  Escritura con probabilidad [0.16548701 0.83451299]\n",
      "El cliente  Escritura con probabilidad [0.28256434 0.71743566]\n",
      "El cliente  Escritura con probabilidad [0.11986904 0.88013096]\n",
      "El cliente  Escritura con probabilidad [0.0760023 0.9239977]\n",
      "El cliente  Escritura con probabilidad [0.23462033 0.76537967]\n",
      "El cliente  Escritura con probabilidad [0.20848742 0.79151258]\n",
      "El cliente  Escritura con probabilidad [0.34057434 0.65942566]\n",
      "El cliente  Escritura con probabilidad [0.05680043 0.94319957]\n",
      "El cliente  Escritura con probabilidad [0.05222473 0.94777527]\n",
      "El cliente  Escritura con probabilidad [0.20881387 0.79118613]\n",
      "El cliente  Escritura con probabilidad [0.11960356 0.88039644]\n",
      "El cliente  Escritura con probabilidad [0.22595573 0.77404427]\n",
      "El cliente  Escritura con probabilidad [0.1051944 0.8948056]\n",
      "El cliente  Escritura con probabilidad [0.01572582 0.98427418]\n",
      "El cliente  Escritura con probabilidad [8.19406765e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.06747718 0.93252282]\n",
      "El cliente  Escritura con probabilidad [0.14600171 0.85399829]\n",
      "El cliente  Escritura con probabilidad [3.46542830e-04 9.99653457e-01]\n",
      "El cliente  Escritura con probabilidad [0.20332376 0.79667624]\n",
      "El cliente  Escritura con probabilidad [0.14136421 0.85863579]\n",
      "El cliente  Escritura con probabilidad [0.08997145 0.91002855]\n",
      "El cliente  Escritura con probabilidad [0.24388313 0.75611687]\n",
      "El cliente  Escritura con probabilidad [0.26426164 0.73573836]\n",
      "El cliente  Escritura con probabilidad [0.09143905 0.90856095]\n",
      "El cliente  Escritura con probabilidad [0.08615785 0.91384215]\n",
      "El cliente  Escritura con probabilidad [0.06870065 0.93129935]\n",
      "El cliente  Escritura con probabilidad [0.20838848 0.79161152]\n",
      "El cliente  Escritura con probabilidad [6.00833480e-06 9.99993992e-01]\n",
      "El cliente  Escritura con probabilidad [0.10663304 0.89336696]\n",
      "El cliente  Escritura con probabilidad [0.1232333 0.8767667]\n",
      "El cliente  Escritura con probabilidad [0.2971322 0.7028678]\n",
      "El cliente  Escritura con probabilidad [0.19306642 0.80693358]\n",
      "El cliente  Escritura con probabilidad [0.13729584 0.86270416]\n",
      "El cliente  Escritura con probabilidad [0.04183976 0.95816024]\n",
      "El cliente  Escritura con probabilidad [0.02551856 0.97448144]\n",
      "El cliente  Escritura con probabilidad [0.21949819 0.78050181]\n",
      "El cliente  Escritura con probabilidad [0.09913319 0.90086681]\n",
      "El cliente  Escritura con probabilidad [0.22003873 0.77996127]\n",
      "El cliente  Escritura con probabilidad [0.09468352 0.90531648]\n",
      "El cliente  Escritura con probabilidad [0.26468195 0.73531805]\n",
      "El cliente  Escritura con probabilidad [4.40043745e-08 9.99999956e-01]\n",
      "El cliente  Escritura con probabilidad [0.18131656 0.81868344]\n",
      "El cliente  Escritura con probabilidad [0.21559681 0.78440319]\n",
      "El cliente  Escritura con probabilidad [0.13653381 0.86346619]\n",
      "El cliente  Escritura con probabilidad [0.13399924 0.86600076]\n",
      "El cliente  Escritura con probabilidad [0.0090036 0.9909964]\n",
      "El cliente  Escritura con probabilidad [0.01259876 0.98740124]\n",
      "El cliente  Escritura con probabilidad [0.20810871 0.79189129]\n",
      "El cliente  Escritura con probabilidad [0.41596443 0.58403557]\n",
      "El cliente  Escritura con probabilidad [0.07569897 0.92430103]\n",
      "El cliente  Escritura con probabilidad [0.16825212 0.83174788]\n",
      "El cliente  Escritura con probabilidad [0.284474 0.715526]\n",
      "El cliente  Escritura con probabilidad [0.22234289 0.77765711]\n",
      "El cliente  Escritura con probabilidad [0.20997719 0.79002281]\n",
      "El cliente  Escritura con probabilidad [0.24513734 0.75486266]\n",
      "El cliente  Escritura con probabilidad [0.07350112 0.92649888]\n",
      "El cliente  Escritura con probabilidad [0.29862233 0.70137767]\n",
      "El cliente  Escritura con probabilidad [0.2471091 0.7528909]\n",
      "El cliente  Escritura con probabilidad [0.15783851 0.84216149]\n",
      "El cliente  Escritura con probabilidad [0.17204978 0.82795022]\n",
      "El cliente  Escritura con probabilidad [0.01753111 0.98246889]\n",
      "El cliente  Escritura con probabilidad [1.32111042e-07 9.99999868e-01]\n",
      "El cliente  Escritura con probabilidad [3.15753120e-07 9.99999684e-01]\n",
      "El cliente  Escritura con probabilidad [0.28979207 0.71020793]\n",
      "El cliente  Escritura con probabilidad [0.14519408 0.85480592]\n",
      "El cliente  Escritura con probabilidad [0.17712262 0.82287738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [1.61577136e-07 9.99999838e-01]\n",
      "El cliente  Escritura con probabilidad [1.03288892e-05 9.99989671e-01]\n",
      "El cliente  Escritura con probabilidad [0.17706344 0.82293656]\n",
      "El cliente  Escritura con probabilidad [0.18195397 0.81804603]\n",
      "El cliente  Escritura con probabilidad [0.16814276 0.83185724]\n",
      "El cliente  Escritura con probabilidad [0.29832385 0.70167615]\n",
      "El cliente  Escritura con probabilidad [0.16177369 0.83822631]\n",
      "El cliente  Escritura con probabilidad [7.23498228e-06 9.99992765e-01]\n",
      "El cliente  Escritura con probabilidad [0.16309168 0.83690832]\n",
      "El cliente  Escritura con probabilidad [2.59529378e-08 9.99999974e-01]\n",
      "El cliente  Escritura con probabilidad [0.00510713 0.99489287]\n",
      "El cliente  Escritura con probabilidad [0.16872661 0.83127339]\n",
      "El cliente  Escritura con probabilidad [0.29233269 0.70766731]\n",
      "El cliente  Escritura con probabilidad [0.05447491 0.94552509]\n",
      "El cliente  Escritura con probabilidad [0.08451336 0.91548664]\n",
      "El cliente  Escritura con probabilidad [0.28064727 0.71935273]\n",
      "El cliente  Escritura con probabilidad [0.08242504 0.91757496]\n",
      "El cliente  Escritura con probabilidad [4.04121181e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [2.32291374e-04 9.99767709e-01]\n",
      "El cliente  Escritura con probabilidad [0.10202606 0.89797394]\n",
      "El cliente  Escritura con probabilidad [0.28298202 0.71701798]\n",
      "El cliente  Escritura con probabilidad [0.14581336 0.85418664]\n",
      "El cliente  Escritura con probabilidad [0.02088065 0.97911935]\n",
      "El cliente  Escritura con probabilidad [0.02469942 0.97530058]\n",
      "El cliente  Escritura con probabilidad [0.03573078 0.96426922]\n",
      "El cliente  Escritura con probabilidad [0.01531264 0.98468736]\n",
      "El cliente  Escritura con probabilidad [3.10029247e-06 9.99996900e-01]\n",
      "El cliente  Escritura con probabilidad [0.1214599 0.8785401]\n",
      "El cliente  Escritura con probabilidad [0.26412259 0.73587741]\n",
      "El cliente  Escritura con probabilidad [0.22814757 0.77185243]\n",
      "El cliente  Escritura con probabilidad [0.15985162 0.84014838]\n",
      "El cliente  Escritura con probabilidad [0.00317765 0.99682235]\n",
      "El cliente  Escritura con probabilidad [0.18016463 0.81983537]\n",
      "El cliente  Escritura con probabilidad [2.89155047e-05 9.99971084e-01]\n",
      "El cliente  Escritura con probabilidad [0.14061838 0.85938162]\n",
      "El cliente  Escritura con probabilidad [0.02621726 0.97378274]\n",
      "El cliente  Escritura con probabilidad [0.31344066 0.68655934]\n",
      "El cliente  Escritura con probabilidad [0.06572687 0.93427313]\n",
      "El cliente  Escritura con probabilidad [0.07986537 0.92013463]\n",
      "El cliente  Escritura con probabilidad [0.37510207 0.62489793]\n",
      "El cliente  Escritura con probabilidad [0.06221332 0.93778668]\n",
      "El cliente  Escritura con probabilidad [0.07729961 0.92270039]\n",
      "El cliente  Escritura con probabilidad [0.01528559 0.98471441]\n",
      "El cliente  Escritura con probabilidad [0.3611383 0.6388617]\n",
      "El cliente  Escritura con probabilidad [0.01093794 0.98906206]\n",
      "El cliente  Escritura con probabilidad [0.03989587 0.96010413]\n",
      "El cliente  Escritura con probabilidad [0.15262219 0.84737781]\n",
      "El cliente  Escritura con probabilidad [0.15793124 0.84206876]\n",
      "El cliente  Escritura con probabilidad [0.09528332 0.90471668]\n",
      "El cliente  Escritura con probabilidad [0.12776775 0.87223225]\n",
      "El cliente  Escritura con probabilidad [0.01176309 0.98823691]\n",
      "El cliente  Escritura con probabilidad [0.29952025 0.70047975]\n",
      "El cliente  Escritura con probabilidad [0.13917514 0.86082486]\n",
      "El cliente  Escritura con probabilidad [0.08400612 0.91599388]\n",
      "El cliente  Escritura con probabilidad [1.78194606e-05 9.99982181e-01]\n",
      "El cliente  Escritura con probabilidad [1.86002701e-07 9.99999814e-01]\n",
      "El cliente  Escritura con probabilidad [0.15307704 0.84692296]\n",
      "El cliente  Escritura con probabilidad [0.15315967 0.84684033]\n",
      "El cliente  Escritura con probabilidad [6.66290800e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [4.76690634e-08 9.99999952e-01]\n",
      "El cliente  Escritura con probabilidad [0.07970498 0.92029502]\n",
      "El cliente  Escritura con probabilidad [0.09388234 0.90611766]\n",
      "El cliente  Escritura con probabilidad [0.17825977 0.82174023]\n",
      "El cliente  Escritura con probabilidad [4.41742198e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.10929063 0.89070937]\n",
      "El cliente  Escritura con probabilidad [0.18704073 0.81295927]\n",
      "El cliente  Escritura con probabilidad [0.02463193 0.97536807]\n",
      "El cliente  Escritura con probabilidad [0.03065705 0.96934295]\n",
      "El cliente  Escritura con probabilidad [0.01294923 0.98705077]\n",
      "El cliente  Escritura con probabilidad [1.30225423e-05 9.99986977e-01]\n",
      "El cliente  Escritura con probabilidad [0.23475678 0.76524322]\n",
      "El cliente  Escritura con probabilidad [0.0887705 0.9112295]\n",
      "El cliente  Escritura con probabilidad [6.33651009e-08 9.99999937e-01]\n",
      "El cliente  Escritura con probabilidad [0.1799619 0.8200381]\n",
      "El cliente  Escritura con probabilidad [0.1723553 0.8276447]\n",
      "El cliente  Escritura con probabilidad [0.19033936 0.80966064]\n",
      "El cliente  Escritura con probabilidad [0.04196259 0.95803741]\n",
      "El cliente  Escritura con probabilidad [0.20712945 0.79287055]\n",
      "El cliente  Escritura con probabilidad [0.13293625 0.86706375]\n",
      "El cliente  Escritura con probabilidad [0.11900781 0.88099219]\n",
      "El cliente  Escritura con probabilidad [0.0672926 0.9327074]\n",
      "El cliente  Escritura con probabilidad [0.05849173 0.94150827]\n",
      "El cliente  Escritura con probabilidad [0.09290961 0.90709039]\n",
      "El cliente  Escritura con probabilidad [0.20337656 0.79662344]\n",
      "El cliente  Escritura con probabilidad [0.00530459 0.99469541]\n",
      "El cliente  Escritura con probabilidad [8.26651006e-06 9.99991733e-01]\n",
      "El cliente  Escritura con probabilidad [0.19556164 0.80443836]\n",
      "El cliente  Escritura con probabilidad [0.04868845 0.95131155]\n",
      "El cliente  Escritura con probabilidad [0.08252295 0.91747705]\n",
      "El cliente  Escritura con probabilidad [0.00801309 0.99198691]\n",
      "El cliente  Escritura con probabilidad [0.18537784 0.81462216]\n",
      "El cliente  Escritura con probabilidad [2.17541899e-06 9.99997825e-01]\n",
      "El cliente  Escritura con probabilidad [0.16185448 0.83814552]\n",
      "El cliente  Escritura con probabilidad [0.00201293 0.99798707]\n",
      "El cliente  Escritura con probabilidad [0.15939011 0.84060989]\n",
      "El cliente  Escritura con probabilidad [0.19185667 0.80814333]\n",
      "El cliente  Escritura con probabilidad [0.22352891 0.77647109]\n",
      "El cliente  Escritura con probabilidad [0.13288227 0.86711773]\n",
      "El cliente  Escritura con probabilidad [0.26584268 0.73415732]\n",
      "El cliente  Escritura con probabilidad [0.15019619 0.84980381]\n",
      "El cliente  Escritura con probabilidad [0.21627138 0.78372862]\n",
      "El cliente  Escritura con probabilidad [0.08849958 0.91150042]\n",
      "El cliente  Escritura con probabilidad [0.18213265 0.81786735]\n",
      "El cliente  Escritura con probabilidad [9.0545349e-12 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.45379263 0.54620737]\n",
      "El cliente  Escritura con probabilidad [0.15321032 0.84678968]\n",
      "El cliente  Escritura con probabilidad [0.0594452 0.9405548]\n",
      "El cliente  Escritura con probabilidad [0.24920988 0.75079012]\n",
      "El cliente  Escritura con probabilidad [0.08409016 0.91590984]\n",
      "El cliente  Escritura con probabilidad [0.23526911 0.76473089]\n",
      "El cliente  Escritura con probabilidad [0.27653205 0.72346795]\n",
      "El cliente  Escritura con probabilidad [0.14535977 0.85464023]\n",
      "El cliente  Escritura con probabilidad [0.20429726 0.79570274]\n",
      "El cliente  Escritura con probabilidad [0.06336158 0.93663842]\n",
      "El cliente  Escritura con probabilidad [0.06928518 0.93071482]\n",
      "El cliente  Escritura con probabilidad [0.08893192 0.91106808]\n",
      "El cliente  Escritura con probabilidad [0.05101622 0.94898378]\n",
      "El cliente  Escritura con probabilidad [0.22046902 0.77953098]\n",
      "El cliente  Escritura con probabilidad [0.21795934 0.78204066]\n",
      "El cliente  Escritura con probabilidad [0.08312712 0.91687288]\n",
      "El cliente  Escritura con probabilidad [1.90706441e-07 9.99999809e-01]\n",
      "El cliente  Escritura con probabilidad [0.03996309 0.96003691]\n",
      "El cliente  Escritura con probabilidad [0.22160893 0.77839107]\n",
      "El cliente  Escritura con probabilidad [0.23062261 0.76937739]\n",
      "El cliente  Escritura con probabilidad [0.13799501 0.86200499]\n",
      "El cliente  Escritura con probabilidad [0.03695187 0.96304813]\n",
      "El cliente  Escritura con probabilidad [0.213228 0.786772]\n",
      "El cliente  Escritura con probabilidad [0.01565866 0.98434134]\n",
      "El cliente  Escritura con probabilidad [0.02371155 0.97628845]\n",
      "El cliente  Escritura con probabilidad [0.17450917 0.82549083]\n",
      "El cliente  Escritura con probabilidad [0.17238406 0.82761594]\n",
      "El cliente  Escritura con probabilidad [0.21391862 0.78608138]\n",
      "El cliente  Escritura con probabilidad [0.13037499 0.86962501]\n",
      "El cliente  Escritura con probabilidad [0.10337924 0.89662076]\n",
      "El cliente  Escritura con probabilidad [5.09803534e-04 9.99490196e-01]\n",
      "El cliente  Escritura con probabilidad [0.2723759 0.7276241]\n",
      "El cliente  Escritura con probabilidad [0.00857178 0.99142822]\n",
      "El cliente  Escritura con probabilidad [0.23378673 0.76621327]\n",
      "El cliente  Escritura con probabilidad [0.02334765 0.97665235]\n",
      "El cliente  Escritura con probabilidad [0.03997723 0.96002277]\n",
      "El cliente  Escritura con probabilidad [0.01515871 0.98484129]\n",
      "El cliente  Escritura con probabilidad [0.00577568 0.99422432]\n",
      "El cliente  Escritura con probabilidad [0.04422468 0.95577532]\n",
      "El cliente  Escritura con probabilidad [0.09774602 0.90225398]\n",
      "El cliente  Escritura con probabilidad [0.03891079 0.96108921]\n",
      "El cliente  Escritura con probabilidad [0.17087244 0.82912756]\n",
      "El cliente  Escritura con probabilidad [0.25957487 0.74042513]\n",
      "El cliente  Escritura con probabilidad [0.0781591 0.9218409]\n",
      "El cliente  Escritura con probabilidad [0.21308016 0.78691984]\n",
      "El cliente  Escritura con probabilidad [0.19625378 0.80374622]\n",
      "El cliente  Escritura con probabilidad [0.09729169 0.90270831]\n",
      "El cliente  Escritura con probabilidad [0.39699808 0.60300192]\n",
      "El cliente  Escritura con probabilidad [0.15062675 0.84937325]\n",
      "El cliente  Escritura con probabilidad [0.19983704 0.80016296]\n",
      "El cliente  Escritura con probabilidad [0.01158682 0.98841318]\n",
      "El cliente  Escritura con probabilidad [0.01326071 0.98673929]\n",
      "El cliente  Escritura con probabilidad [2.6398439e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.15889252 0.84110748]\n",
      "El cliente  Escritura con probabilidad [4.96787527e-08 9.99999950e-01]\n",
      "El cliente  Escritura con probabilidad [0.13958352 0.86041648]\n",
      "El cliente  Escritura con probabilidad [0.22732183 0.77267817]\n",
      "El cliente  Escritura con probabilidad [4.8003379e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.22643923 0.77356077]\n",
      "El cliente  Escritura con probabilidad [0.00758983 0.99241017]\n",
      "El cliente  Escritura con probabilidad [0.00169658 0.99830342]\n",
      "El cliente  Escritura con probabilidad [0.01924957 0.98075043]\n",
      "El cliente  Escritura con probabilidad [0.10876259 0.89123741]\n",
      "El cliente  Escritura con probabilidad [0.00416029 0.99583971]\n",
      "El cliente  Escritura con probabilidad [0.25399578 0.74600422]\n",
      "El cliente  Escritura con probabilidad [0.15416345 0.84583655]\n",
      "El cliente  Escritura con probabilidad [0.18345154 0.81654846]\n",
      "El cliente  Escritura con probabilidad [0.21895789 0.78104211]\n",
      "El cliente  Escritura con probabilidad [0.1475708 0.8524292]\n",
      "El cliente  Escritura con probabilidad [0.23442971 0.76557029]\n",
      "El cliente  Escritura con probabilidad [9.59706759e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.07856938 0.92143062]\n",
      "El cliente  Escritura con probabilidad [0.21081212 0.78918788]\n",
      "El cliente  Escritura con probabilidad [0.00137976 0.99862024]\n",
      "El cliente  Escritura con probabilidad [0.36573405 0.63426595]\n",
      "El cliente  Escritura con probabilidad [0.14002214 0.85997786]\n",
      "El cliente  Escritura con probabilidad [0.04134956 0.95865044]\n",
      "El cliente  Escritura con probabilidad [0.01054627 0.98945373]\n",
      "El cliente  Escritura con probabilidad [0.13444179 0.86555821]\n",
      "El cliente  Escritura con probabilidad [0.27115591 0.72884409]\n",
      "El cliente  Escritura con probabilidad [0.13536834 0.86463166]\n",
      "El cliente  Escritura con probabilidad [0.25609059 0.74390941]\n",
      "El cliente  Escritura con probabilidad [0.16655736 0.83344264]\n",
      "El cliente  Escritura con probabilidad [0.10597082 0.89402918]\n",
      "El cliente  Escritura con probabilidad [0.18547865 0.81452135]\n",
      "El cliente  Escritura con probabilidad [0.20040032 0.79959968]\n",
      "El cliente  Escritura con probabilidad [0.45138387 0.54861613]\n",
      "El cliente  Escritura con probabilidad [0.13058695 0.86941305]\n",
      "El cliente  Escritura con probabilidad [0.16252272 0.83747728]\n",
      "El cliente  Escritura con probabilidad [0.0628046 0.9371954]\n",
      "El cliente  Escritura con probabilidad [0.40530836 0.59469164]\n",
      "El cliente  Escritura con probabilidad [0.14071792 0.85928208]\n",
      "El cliente  Escritura con probabilidad [2.38278286e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.16917544 0.83082456]\n",
      "El cliente  Escritura con probabilidad [3.64954582e-08 9.99999964e-01]\n",
      "El cliente  Escritura con probabilidad [0.08613795 0.91386205]\n",
      "El cliente  Escritura con probabilidad [0.15352536 0.84647464]\n",
      "El cliente  Escritura con probabilidad [0.06771793 0.93228207]\n",
      "El cliente  Escritura con probabilidad [0.10188903 0.89811097]\n",
      "El cliente  Escritura con probabilidad [0.15250147 0.84749853]\n",
      "El cliente  Escritura con probabilidad [0.16752853 0.83247147]\n",
      "El cliente  Escritura con probabilidad [2.57791944e-04 9.99742208e-01]\n",
      "El cliente  Escritura con probabilidad [0.27837999 0.72162001]\n",
      "El cliente  Escritura con probabilidad [2.45615528e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.10729961 0.89270039]\n",
      "El cliente  Escritura con probabilidad [0.22422938 0.77577062]\n",
      "El cliente  Desiste con probabilidad [0.5210794 0.4789206]\n",
      "El cliente  Escritura con probabilidad [0.14048012 0.85951988]\n",
      "El cliente  Escritura con probabilidad [0.16022197 0.83977803]\n",
      "El cliente  Escritura con probabilidad [8.78933815e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.26121176 0.73878824]\n",
      "El cliente  Escritura con probabilidad [3.61741076e-06 9.99996383e-01]\n",
      "El cliente  Escritura con probabilidad [0.11815846 0.88184154]\n",
      "El cliente  Escritura con probabilidad [0.16986081 0.83013919]\n",
      "El cliente  Escritura con probabilidad [0.13361653 0.86638347]\n",
      "El cliente  Escritura con probabilidad [0.01562381 0.98437619]\n",
      "El cliente  Escritura con probabilidad [1.67199588e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.28900103 0.71099897]\n",
      "El cliente  Escritura con probabilidad [0.1801193 0.8198807]\n",
      "El cliente  Escritura con probabilidad [0.17810385 0.82189615]\n",
      "El cliente  Escritura con probabilidad [0.00911342 0.99088658]\n",
      "El cliente  Escritura con probabilidad [0.16271568 0.83728432]\n",
      "El cliente  Escritura con probabilidad [0.17860161 0.82139839]\n",
      "El cliente  Escritura con probabilidad [0.35399121 0.64600879]\n",
      "El cliente  Escritura con probabilidad [9.28518296e-09 9.99999991e-01]\n",
      "El cliente  Escritura con probabilidad [0.19362399 0.80637601]\n",
      "El cliente  Escritura con probabilidad [0.25593572 0.74406428]\n",
      "El cliente  Escritura con probabilidad [1.85922050e-04 9.99814078e-01]\n",
      "El cliente  Escritura con probabilidad [0.00971611 0.99028389]\n",
      "El cliente  Escritura con probabilidad [0.18116242 0.81883758]\n",
      "El cliente  Escritura con probabilidad [0.05577778 0.94422222]\n",
      "El cliente  Escritura con probabilidad [0.1383204 0.8616796]\n",
      "El cliente  Escritura con probabilidad [0.21069095 0.78930905]\n",
      "El cliente  Escritura con probabilidad [0.11035562 0.88964438]\n",
      "El cliente  Escritura con probabilidad [0.03513592 0.96486408]\n",
      "El cliente  Escritura con probabilidad [0.0563909 0.9436091]\n",
      "El cliente  Escritura con probabilidad [0.16224415 0.83775585]\n",
      "El cliente  Escritura con probabilidad [0.03829525 0.96170475]\n",
      "El cliente  Escritura con probabilidad [0.14676818 0.85323182]\n",
      "El cliente  Escritura con probabilidad [0.01248996 0.98751004]\n",
      "El cliente  Escritura con probabilidad [0.13483548 0.86516452]\n",
      "El cliente  Escritura con probabilidad [0.05420699 0.94579301]\n",
      "El cliente  Escritura con probabilidad [0.19713737 0.80286263]\n",
      "El cliente  Escritura con probabilidad [0.00524746 0.99475254]\n",
      "El cliente  Escritura con probabilidad [0.06011669 0.93988331]\n",
      "El cliente  Escritura con probabilidad [0.24367495 0.75632505]\n",
      "El cliente  Escritura con probabilidad [0.00802414 0.99197586]\n",
      "El cliente  Escritura con probabilidad [0.11008842 0.88991158]\n",
      "El cliente  Escritura con probabilidad [0.03513866 0.96486134]\n",
      "El cliente  Escritura con probabilidad [0.13257081 0.86742919]\n",
      "El cliente  Escritura con probabilidad [0.33370863 0.66629137]\n",
      "El cliente  Escritura con probabilidad [0.28137975 0.71862025]\n",
      "El cliente  Escritura con probabilidad [0.19453982 0.80546018]\n",
      "El cliente  Escritura con probabilidad [1.33895259e-08 9.99999987e-01]\n",
      "El cliente  Escritura con probabilidad [2.48987935e-08 9.99999975e-01]\n",
      "El cliente  Escritura con probabilidad [0.06065248 0.93934752]\n",
      "El cliente  Escritura con probabilidad [0.0642395 0.9357605]\n",
      "El cliente  Escritura con probabilidad [0.23642049 0.76357951]\n",
      "El cliente  Escritura con probabilidad [0.05804685 0.94195315]\n",
      "El cliente  Escritura con probabilidad [0.12015447 0.87984553]\n",
      "El cliente  Escritura con probabilidad [0.06530885 0.93469115]\n",
      "El cliente  Escritura con probabilidad [0.11757849 0.88242151]\n",
      "El cliente  Escritura con probabilidad [1.98471883e-04 9.99801528e-01]\n",
      "El cliente  Escritura con probabilidad [0.02937616 0.97062384]\n",
      "El cliente  Escritura con probabilidad [0.12248867 0.87751133]\n",
      "El cliente  Escritura con probabilidad [3.8048853e-10 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.12101137 0.87898863]\n",
      "El cliente  Escritura con probabilidad [0.03337315 0.96662685]\n",
      "El cliente  Escritura con probabilidad [0.24133774 0.75866226]\n",
      "El cliente  Escritura con probabilidad [0.09848259 0.90151741]\n",
      "El cliente  Escritura con probabilidad [0.10200358 0.89799642]\n",
      "El cliente  Escritura con probabilidad [0.16824978 0.83175022]\n",
      "El cliente  Escritura con probabilidad [0.12376645 0.87623355]\n",
      "El cliente  Escritura con probabilidad [0.28166025 0.71833975]\n",
      "El cliente  Escritura con probabilidad [0.28850272 0.71149728]\n",
      "El cliente  Escritura con probabilidad [0.127973 0.872027]\n",
      "El cliente  Escritura con probabilidad [0.25312819 0.74687181]\n",
      "El cliente  Escritura con probabilidad [0.20285849 0.79714151]\n",
      "El cliente  Escritura con probabilidad [0.03519922 0.96480078]\n",
      "El cliente  Escritura con probabilidad [0.12359443 0.87640557]\n",
      "El cliente  Escritura con probabilidad [0.1878375 0.8121625]\n",
      "El cliente  Escritura con probabilidad [0.03297826 0.96702174]\n",
      "El cliente  Escritura con probabilidad [0.20484149 0.79515851]\n",
      "El cliente  Escritura con probabilidad [0.11205776 0.88794224]\n",
      "El cliente  Escritura con probabilidad [0.0862017 0.9137983]\n",
      "El cliente  Escritura con probabilidad [0.09651893 0.90348107]\n",
      "El cliente  Escritura con probabilidad [0.02370127 0.97629873]\n",
      "El cliente  Escritura con probabilidad [2.98136080e-06 9.99997019e-01]\n",
      "El cliente  Escritura con probabilidad [0.2444638 0.7555362]\n",
      "El cliente  Escritura con probabilidad [0.21066014 0.78933986]\n",
      "El cliente  Escritura con probabilidad [0.2317876 0.7682124]\n",
      "El cliente  Escritura con probabilidad [0.095316 0.904684]\n",
      "El cliente  Escritura con probabilidad [0.21549599 0.78450401]\n",
      "El cliente  Escritura con probabilidad [0.00872357 0.99127643]\n",
      "El cliente  Escritura con probabilidad [0.15080192 0.84919808]\n",
      "El cliente  Escritura con probabilidad [0.20909896 0.79090104]\n",
      "El cliente  Escritura con probabilidad [0.31529061 0.68470939]\n",
      "El cliente  Escritura con probabilidad [0.18844434 0.81155566]\n",
      "El cliente  Escritura con probabilidad [0.15812124 0.84187876]\n",
      "El cliente  Escritura con probabilidad [0.14646209 0.85353791]\n",
      "El cliente  Escritura con probabilidad [0.13907121 0.86092879]\n",
      "El cliente  Escritura con probabilidad [0.24469352 0.75530648]\n",
      "El cliente  Escritura con probabilidad [0.14838226 0.85161774]\n",
      "El cliente  Escritura con probabilidad [0.25983267 0.74016733]\n",
      "El cliente  Escritura con probabilidad [0.04971254 0.95028746]\n",
      "El cliente  Escritura con probabilidad [0.00743203 0.99256797]\n",
      "El cliente  Escritura con probabilidad [0.06337923 0.93662077]\n",
      "El cliente  Escritura con probabilidad [0.09798456 0.90201544]\n",
      "El cliente  Escritura con probabilidad [0.24159073 0.75840927]\n",
      "El cliente  Escritura con probabilidad [0.10699899 0.89300101]\n",
      "El cliente  Escritura con probabilidad [0.2180586 0.7819414]\n",
      "El cliente  Escritura con probabilidad [0.06184037 0.93815963]\n",
      "El cliente  Escritura con probabilidad [2.94964719e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.10675257 0.89324743]\n",
      "El cliente  Escritura con probabilidad [0.26944121 0.73055879]\n",
      "El cliente  Escritura con probabilidad [4.22924976e-05 9.99957708e-01]\n",
      "El cliente  Escritura con probabilidad [0.1344313 0.8655687]\n",
      "El cliente  Escritura con probabilidad [0.2019259 0.7980741]\n",
      "El cliente  Escritura con probabilidad [0.21943798 0.78056202]\n",
      "El cliente  Escritura con probabilidad [0.07236865 0.92763135]\n",
      "El cliente  Escritura con probabilidad [1.86222149e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.11235943 0.88764057]\n",
      "El cliente  Escritura con probabilidad [0.37173464 0.62826536]\n",
      "El cliente  Escritura con probabilidad [0.18593958 0.81406042]\n",
      "El cliente  Escritura con probabilidad [0.28655645 0.71344355]\n",
      "El cliente  Escritura con probabilidad [9.42768170e-06 9.99990572e-01]\n",
      "El cliente  Escritura con probabilidad [0.02163253 0.97836747]\n",
      "El cliente  Escritura con probabilidad [0.29852139 0.70147861]\n",
      "El cliente  Escritura con probabilidad [0.22673272 0.77326728]\n",
      "El cliente  Escritura con probabilidad [7.80109298e-05 9.99921989e-01]\n",
      "El cliente  Escritura con probabilidad [0.0673807 0.9326193]\n",
      "El cliente  Escritura con probabilidad [0.14329824 0.85670176]\n",
      "El cliente  Escritura con probabilidad [0.25648094 0.74351906]\n",
      "El cliente  Escritura con probabilidad [0.02478754 0.97521246]\n",
      "El cliente  Escritura con probabilidad [0.21034412 0.78965588]\n",
      "El cliente  Escritura con probabilidad [0.16402514 0.83597486]\n",
      "El cliente  Escritura con probabilidad [0.24994399 0.75005601]\n",
      "El cliente  Escritura con probabilidad [0.02571809 0.97428191]\n",
      "El cliente  Escritura con probabilidad [0.17431094 0.82568906]\n",
      "El cliente  Escritura con probabilidad [4.14492505e-06 9.99995855e-01]\n",
      "El cliente  Escritura con probabilidad [0.12206261 0.87793739]\n",
      "El cliente  Escritura con probabilidad [0.10231325 0.89768675]\n",
      "El cliente  Escritura con probabilidad [0.02812158 0.97187842]\n",
      "El cliente  Escritura con probabilidad [0.1665845 0.8334155]\n",
      "El cliente  Escritura con probabilidad [0.11125125 0.88874875]\n",
      "El cliente  Escritura con probabilidad [0.02142647 0.97857353]\n",
      "El cliente  Escritura con probabilidad [0.15064139 0.84935861]\n",
      "El cliente  Escritura con probabilidad [4.93507150e-04 9.99506493e-01]\n",
      "El cliente  Escritura con probabilidad [2.3578961e-10 1.0000000e+00]\n",
      "El cliente  Desiste con probabilidad [0.58470124 0.41529876]\n",
      "El cliente  Escritura con probabilidad [9.64240232e-09 9.99999990e-01]\n",
      "El cliente  Escritura con probabilidad [0.03466605 0.96533395]\n",
      "El cliente  Escritura con probabilidad [0.17099525 0.82900475]\n",
      "El cliente  Escritura con probabilidad [0.23619196 0.76380804]\n",
      "El cliente  Escritura con probabilidad [0.27371819 0.72628181]\n",
      "El cliente  Escritura con probabilidad [0.29324937 0.70675063]\n",
      "El cliente  Escritura con probabilidad [0.02821698 0.97178302]\n",
      "El cliente  Escritura con probabilidad [4.19630578e-05 9.99958037e-01]\n",
      "El cliente  Escritura con probabilidad [0.01107588 0.98892412]\n",
      "El cliente  Escritura con probabilidad [0.24540991 0.75459009]\n",
      "El cliente  Escritura con probabilidad [0.05510617 0.94489383]\n",
      "El cliente  Escritura con probabilidad [0.1925814 0.8074186]\n",
      "El cliente  Escritura con probabilidad [0.14389366 0.85610634]\n",
      "El cliente  Escritura con probabilidad [0.44693341 0.55306659]\n",
      "El cliente  Escritura con probabilidad [4.41727745e-06 9.99995583e-01]\n",
      "El cliente  Escritura con probabilidad [0.09697869 0.90302131]\n",
      "El cliente  Escritura con probabilidad [0.22871691 0.77128309]\n",
      "El cliente  Escritura con probabilidad [0.44624903 0.55375097]\n",
      "El cliente  Escritura con probabilidad [0.21916971 0.78083029]\n",
      "El cliente  Escritura con probabilidad [0.04835287 0.95164713]\n",
      "El cliente  Escritura con probabilidad [1.74812387e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [8.30259143e-05 9.99916974e-01]\n",
      "El cliente  Escritura con probabilidad [0.14486596 0.85513404]\n",
      "El cliente  Escritura con probabilidad [0.18501341 0.81498659]\n",
      "El cliente  Escritura con probabilidad [0.04762287 0.95237713]\n",
      "El cliente  Escritura con probabilidad [0.06083807 0.93916193]\n",
      "El cliente  Escritura con probabilidad [0.07244708 0.92755292]\n",
      "El cliente  Escritura con probabilidad [2.22044605e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [6.95502763e-06 9.99993045e-01]\n",
      "El cliente  Escritura con probabilidad [0.2022693 0.7977307]\n",
      "El cliente  Escritura con probabilidad [0.13989999 0.86010001]\n",
      "El cliente  Escritura con probabilidad [0.02020877 0.97979123]\n",
      "El cliente  Escritura con probabilidad [0.16031857 0.83968143]\n",
      "El cliente  Escritura con probabilidad [0.14208857 0.85791143]\n",
      "El cliente  Escritura con probabilidad [0.11128781 0.88871219]\n",
      "El cliente  Escritura con probabilidad [0.17159479 0.82840521]\n",
      "El cliente  Escritura con probabilidad [0.19779332 0.80220668]\n",
      "El cliente  Escritura con probabilidad [0.20727538 0.79272462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.13014997 0.86985003]\n",
      "El cliente  Escritura con probabilidad [0.05618823 0.94381177]\n",
      "El cliente  Escritura con probabilidad [0.27722847 0.72277153]\n",
      "El cliente  Escritura con probabilidad [8.58669871e-04 9.99141330e-01]\n",
      "El cliente  Escritura con probabilidad [0.23131686 0.76868314]\n",
      "El cliente  Escritura con probabilidad [0.23218961 0.76781039]\n",
      "El cliente  Escritura con probabilidad [0.16546144 0.83453856]\n",
      "El cliente  Escritura con probabilidad [2.57685811e-05 9.99974231e-01]\n",
      "El cliente  Escritura con probabilidad [0.16472734 0.83527266]\n",
      "El cliente  Escritura con probabilidad [0.00465314 0.99534686]\n",
      "El cliente  Escritura con probabilidad [0.01632145 0.98367855]\n",
      "El cliente  Escritura con probabilidad [0.18631141 0.81368859]\n",
      "El cliente  Escritura con probabilidad [3.59230066e-08 9.99999964e-01]\n",
      "El cliente  Escritura con probabilidad [0.14209449 0.85790551]\n",
      "El cliente  Escritura con probabilidad [0.21505465 0.78494535]\n",
      "El cliente  Escritura con probabilidad [0.00517192 0.99482808]\n",
      "El cliente  Escritura con probabilidad [0.18178657 0.81821343]\n",
      "El cliente  Escritura con probabilidad [0.15597703 0.84402297]\n",
      "El cliente  Escritura con probabilidad [0.14079293 0.85920707]\n",
      "El cliente  Escritura con probabilidad [0.14273912 0.85726088]\n",
      "El cliente  Escritura con probabilidad [0.23660179 0.76339821]\n",
      "El cliente  Escritura con probabilidad [0.13229542 0.86770458]\n",
      "El cliente  Escritura con probabilidad [0.45485456 0.54514544]\n",
      "El cliente  Escritura con probabilidad [0.17102752 0.82897248]\n",
      "El cliente  Escritura con probabilidad [0.01840894 0.98159106]\n",
      "El cliente  Escritura con probabilidad [0.18439626 0.81560374]\n",
      "El cliente  Escritura con probabilidad [0.12715482 0.87284518]\n",
      "El cliente  Escritura con probabilidad [0.02027171 0.97972829]\n",
      "El cliente  Escritura con probabilidad [0.19511159 0.80488841]\n",
      "El cliente  Escritura con probabilidad [0.13487201 0.86512799]\n",
      "El cliente  Escritura con probabilidad [0.08877624 0.91122376]\n",
      "El cliente  Escritura con probabilidad [0.23077379 0.76922621]\n",
      "El cliente  Escritura con probabilidad [0.21269476 0.78730524]\n",
      "El cliente  Escritura con probabilidad [1.82076576e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.18969518 0.81030482]\n",
      "El cliente  Escritura con probabilidad [0.17579935 0.82420065]\n",
      "El cliente  Escritura con probabilidad [0.05467606 0.94532394]\n",
      "El cliente  Escritura con probabilidad [0.13948785 0.86051215]\n",
      "El cliente  Escritura con probabilidad [0.18491446 0.81508554]\n",
      "El cliente  Escritura con probabilidad [0.30308196 0.69691804]\n",
      "El cliente  Escritura con probabilidad [0.29619449 0.70380551]\n",
      "El cliente  Escritura con probabilidad [0.18123505 0.81876495]\n",
      "El cliente  Escritura con probabilidad [0.17577953 0.82422047]\n",
      "El cliente  Escritura con probabilidad [0.17645581 0.82354419]\n",
      "El cliente  Escritura con probabilidad [0.24300004 0.75699996]\n",
      "El cliente  Escritura con probabilidad [0.10479821 0.89520179]\n",
      "El cliente  Escritura con probabilidad [0.177536 0.822464]\n",
      "El cliente  Escritura con probabilidad [1.81812128e-07 9.99999818e-01]\n",
      "El cliente  Escritura con probabilidad [0.01379138 0.98620862]\n",
      "El cliente  Escritura con probabilidad [2.10123285e-04 9.99789877e-01]\n",
      "El cliente  Escritura con probabilidad [9.28544977e-04 9.99071455e-01]\n",
      "El cliente  Escritura con probabilidad [0.21387248 0.78612752]\n",
      "El cliente  Escritura con probabilidad [2.54358723e-04 9.99745641e-01]\n",
      "El cliente  Escritura con probabilidad [4.97268449e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.16647409 0.83352591]\n",
      "El cliente  Escritura con probabilidad [0.04167336 0.95832664]\n",
      "El cliente  Escritura con probabilidad [0.18899475 0.81100525]\n",
      "El cliente  Escritura con probabilidad [0.21264761 0.78735239]\n",
      "El cliente  Escritura con probabilidad [5.27580052e-08 9.99999947e-01]\n",
      "El cliente  Escritura con probabilidad [4.47169858e-04 9.99552830e-01]\n",
      "El cliente  Escritura con probabilidad [0.1893167 0.8106833]\n",
      "El cliente  Escritura con probabilidad [0.12818104 0.87181896]\n",
      "El cliente  Escritura con probabilidad [3.77347487e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.13145479 0.86854521]\n",
      "El cliente  Escritura con probabilidad [0.13182135 0.86817865]\n",
      "El cliente  Escritura con probabilidad [0.11499859 0.88500141]\n",
      "El cliente  Escritura con probabilidad [0.18949411 0.81050589]\n",
      "El cliente  Escritura con probabilidad [1.78532194e-06 9.99998215e-01]\n",
      "El cliente  Escritura con probabilidad [0.2105606 0.7894394]\n",
      "El cliente  Escritura con probabilidad [0.06152788 0.93847212]\n",
      "El cliente  Escritura con probabilidad [3.91893162e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.07987659 0.92012341]\n",
      "El cliente  Escritura con probabilidad [1.02962501e-07 9.99999897e-01]\n",
      "El cliente  Escritura con probabilidad [1.18830065e-08 9.99999988e-01]\n",
      "El cliente  Escritura con probabilidad [0.02218631 0.97781369]\n",
      "El cliente  Escritura con probabilidad [0.07556813 0.92443187]\n",
      "El cliente  Escritura con probabilidad [1.03908018e-04 9.99896092e-01]\n",
      "El cliente  Escritura con probabilidad [0.14480571 0.85519429]\n",
      "El cliente  Escritura con probabilidad [0.27623471 0.72376529]\n",
      "El cliente  Escritura con probabilidad [0.08748477 0.91251523]\n",
      "El cliente  Escritura con probabilidad [0.14763165 0.85236835]\n",
      "El cliente  Escritura con probabilidad [0.20063008 0.79936992]\n",
      "El cliente  Escritura con probabilidad [0.01906438 0.98093562]\n",
      "El cliente  Escritura con probabilidad [7.88636461e-08 9.99999921e-01]\n",
      "El cliente  Escritura con probabilidad [0.07305859 0.92694141]\n",
      "El cliente  Escritura con probabilidad [0.11256801 0.88743199]\n",
      "El cliente  Escritura con probabilidad [0.01728456 0.98271544]\n",
      "El cliente  Escritura con probabilidad [0.02278488 0.97721512]\n",
      "El cliente  Escritura con probabilidad [0.2124473 0.7875527]\n",
      "El cliente  Escritura con probabilidad [0.17353037 0.82646963]\n",
      "El cliente  Escritura con probabilidad [0.170675 0.829325]\n",
      "El cliente  Escritura con probabilidad [0.02891408 0.97108592]\n",
      "El cliente  Escritura con probabilidad [0.26119933 0.73880067]\n",
      "El cliente  Escritura con probabilidad [0.07412211 0.92587789]\n",
      "El cliente  Escritura con probabilidad [3.24639382e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.24133011 0.75866989]\n",
      "El cliente  Escritura con probabilidad [0.03722902 0.96277098]\n",
      "El cliente  Escritura con probabilidad [0.19912379 0.80087621]\n",
      "El cliente  Escritura con probabilidad [0.12904647 0.87095353]\n",
      "El cliente  Escritura con probabilidad [5.23281272e-04 9.99476719e-01]\n",
      "El cliente  Escritura con probabilidad [0.07425906 0.92574094]\n",
      "El cliente  Escritura con probabilidad [0.06654191 0.93345809]\n",
      "El cliente  Escritura con probabilidad [7.73071432e-06 9.99992269e-01]\n",
      "El cliente  Escritura con probabilidad [0.15027002 0.84972998]\n",
      "El cliente  Escritura con probabilidad [0.01903659 0.98096341]\n",
      "El cliente  Escritura con probabilidad [0.10893592 0.89106408]\n",
      "El cliente  Escritura con probabilidad [0.02799036 0.97200964]\n",
      "El cliente  Desiste con probabilidad [0.60368707 0.39631293]\n",
      "El cliente  Escritura con probabilidad [1.22439445e-05 9.99987756e-01]\n",
      "El cliente  Escritura con probabilidad [4.39904335e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.24576888 0.75423112]\n",
      "El cliente  Escritura con probabilidad [0.21751402 0.78248598]\n",
      "El cliente  Escritura con probabilidad [1.55919596e-07 9.99999844e-01]\n",
      "El cliente  Escritura con probabilidad [0.07093306 0.92906694]\n",
      "El cliente  Escritura con probabilidad [0.16712073 0.83287927]\n",
      "El cliente  Escritura con probabilidad [1.10689508e-04 9.99889310e-01]\n",
      "El cliente  Escritura con probabilidad [0.10280506 0.89719494]\n",
      "El cliente  Escritura con probabilidad [0.23454112 0.76545888]\n",
      "El cliente  Escritura con probabilidad [0.01132992 0.98867008]\n",
      "El cliente  Escritura con probabilidad [0.06179269 0.93820731]\n",
      "El cliente  Escritura con probabilidad [0.15992217 0.84007783]\n",
      "El cliente  Escritura con probabilidad [0.35034627 0.64965373]\n",
      "El cliente  Escritura con probabilidad [0.09206346 0.90793654]\n",
      "El cliente  Escritura con probabilidad [0.06707414 0.93292586]\n",
      "El cliente  Escritura con probabilidad [0.03921307 0.96078693]\n",
      "El cliente  Escritura con probabilidad [0.44363266 0.55636734]\n",
      "El cliente  Escritura con probabilidad [0.11252565 0.88747435]\n",
      "El cliente  Escritura con probabilidad [0.18139808 0.81860192]\n",
      "El cliente  Escritura con probabilidad [0.01685494 0.98314506]\n",
      "El cliente  Escritura con probabilidad [0.02209866 0.97790134]\n",
      "El cliente  Escritura con probabilidad [0.44378648 0.55621352]\n",
      "El cliente  Escritura con probabilidad [0.04885563 0.95114437]\n",
      "El cliente  Escritura con probabilidad [0.39137072 0.60862928]\n",
      "El cliente  Escritura con probabilidad [0.05047529 0.94952471]\n",
      "El cliente  Escritura con probabilidad [0.02926994 0.97073006]\n",
      "El cliente  Escritura con probabilidad [0.01416167 0.98583833]\n",
      "El cliente  Escritura con probabilidad [0.22072657 0.77927343]\n",
      "El cliente  Escritura con probabilidad [0.08515012 0.91484988]\n",
      "El cliente  Escritura con probabilidad [0.04206245 0.95793755]\n",
      "El cliente  Escritura con probabilidad [0.2005416 0.7994584]\n",
      "El cliente  Escritura con probabilidad [0.01492059 0.98507941]\n",
      "El cliente  Escritura con probabilidad [1.1090104e-05 9.9998891e-01]\n",
      "El cliente  Escritura con probabilidad [4.62232947e-04 9.99537767e-01]\n",
      "El cliente  Escritura con probabilidad [0.00507252 0.99492748]\n",
      "El cliente  Escritura con probabilidad [0.12742128 0.87257872]\n",
      "El cliente  Escritura con probabilidad [0.2184742 0.7815258]\n",
      "El cliente  Escritura con probabilidad [0.19263199 0.80736801]\n",
      "El cliente  Escritura con probabilidad [0.107565 0.892435]\n",
      "El cliente  Escritura con probabilidad [5.81929826e-07 9.99999418e-01]\n",
      "El cliente  Escritura con probabilidad [0.13636331 0.86363669]\n",
      "El cliente  Escritura con probabilidad [0.06536937 0.93463063]\n",
      "El cliente  Escritura con probabilidad [0.11313549 0.88686451]\n",
      "El cliente  Escritura con probabilidad [0.11280854 0.88719146]\n",
      "El cliente  Escritura con probabilidad [2.09876561e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.24317802 0.75682198]\n",
      "El cliente  Escritura con probabilidad [0.43316989 0.56683011]\n",
      "El cliente  Escritura con probabilidad [0.24089242 0.75910758]\n",
      "El cliente  Escritura con probabilidad [5.51114709e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [5.02915008e-04 9.99497085e-01]\n",
      "El cliente  Escritura con probabilidad [1.68176148e-04 9.99831824e-01]\n",
      "El cliente  Escritura con probabilidad [0.19695133 0.80304867]\n",
      "El cliente  Escritura con probabilidad [0.12462052 0.87537948]\n",
      "El cliente  Escritura con probabilidad [0.06467211 0.93532789]\n",
      "El cliente  Escritura con probabilidad [0.20263924 0.79736076]\n",
      "El cliente  Escritura con probabilidad [0.21790757 0.78209243]\n",
      "El cliente  Escritura con probabilidad [0.11950461 0.88049539]\n",
      "El cliente  Escritura con probabilidad [0.17105552 0.82894448]\n",
      "El cliente  Escritura con probabilidad [0.27358065 0.72641935]\n",
      "El cliente  Escritura con probabilidad [0.06961725 0.93038275]\n",
      "El cliente  Escritura con probabilidad [0.19486568 0.80513432]\n",
      "El cliente  Escritura con probabilidad [0.16089985 0.83910015]\n",
      "El cliente  Escritura con probabilidad [0.26967959 0.73032041]\n",
      "El cliente  Escritura con probabilidad [0.08370637 0.91629363]\n",
      "El cliente  Escritura con probabilidad [0.31728329 0.68271671]\n",
      "El cliente  Escritura con probabilidad [0.13258642 0.86741358]\n",
      "El cliente  Escritura con probabilidad [1.64029115e-04 9.99835971e-01]\n",
      "El cliente  Escritura con probabilidad [2.05044277e-04 9.99794956e-01]\n",
      "El cliente  Escritura con probabilidad [0.28800242 0.71199758]\n",
      "El cliente  Escritura con probabilidad [4.61155297e-06 9.99995388e-01]\n",
      "El cliente  Escritura con probabilidad [0.22029654 0.77970346]\n",
      "El cliente  Escritura con probabilidad [0.30253394 0.69746606]\n",
      "El cliente  Escritura con probabilidad [0.19057864 0.80942136]\n",
      "El cliente  Escritura con probabilidad [0.0224142 0.9775858]\n",
      "El cliente  Escritura con probabilidad [0.10020495 0.89979505]\n",
      "El cliente  Escritura con probabilidad [1.43909905e-07 9.99999856e-01]\n",
      "El cliente  Escritura con probabilidad [0.01441956 0.98558044]\n",
      "El cliente  Escritura con probabilidad [0.20437812 0.79562188]\n",
      "El cliente  Escritura con probabilidad [0.15015099 0.84984901]\n",
      "El cliente  Escritura con probabilidad [4.85706948e-06 9.99995143e-01]\n",
      "El cliente  Escritura con probabilidad [0.03417493 0.96582507]\n",
      "El cliente  Escritura con probabilidad [3.43951201e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.11542391 0.88457609]\n",
      "El cliente  Escritura con probabilidad [0.01750894 0.98249106]\n",
      "El cliente  Escritura con probabilidad [0.07202188 0.92797812]\n",
      "El cliente  Escritura con probabilidad [0.05811042 0.94188958]\n",
      "El cliente  Escritura con probabilidad [0.09309934 0.90690066]\n",
      "El cliente  Escritura con probabilidad [0.23935061 0.76064939]\n",
      "El cliente  Escritura con probabilidad [3.32128840e-08 9.99999967e-01]\n",
      "El cliente  Escritura con probabilidad [0.2624551 0.7375449]\n",
      "El cliente  Escritura con probabilidad [0.23430655 0.76569345]\n",
      "El cliente  Escritura con probabilidad [9.14929466e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.15058051 0.84941949]\n",
      "El cliente  Escritura con probabilidad [0.29310824 0.70689176]\n",
      "El cliente  Escritura con probabilidad [0.23592054 0.76407946]\n",
      "El cliente  Escritura con probabilidad [0.03237213 0.96762787]\n",
      "El cliente  Escritura con probabilidad [1.16037627e-05 9.99988396e-01]\n",
      "El cliente  Escritura con probabilidad [9.96380756e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.09387821 0.90612179]\n",
      "El cliente  Escritura con probabilidad [0.03336136 0.96663864]\n",
      "El cliente  Escritura con probabilidad [0.16584107 0.83415893]\n",
      "El cliente  Escritura con probabilidad [0.01652546 0.98347454]\n",
      "El cliente  Escritura con probabilidad [1.29167932e-04 9.99870832e-01]\n",
      "El cliente  Escritura con probabilidad [0.15420386 0.84579614]\n",
      "El cliente  Escritura con probabilidad [0.27934013 0.72065987]\n",
      "El cliente  Escritura con probabilidad [2.34324427e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.26576078 0.73423922]\n",
      "El cliente  Escritura con probabilidad [0.15578346 0.84421654]\n",
      "El cliente  Escritura con probabilidad [0.04134498 0.95865502]\n",
      "El cliente  Escritura con probabilidad [0.01704329 0.98295671]\n",
      "El cliente  Escritura con probabilidad [2.72343748e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.17105181 0.82894819]\n",
      "El cliente  Escritura con probabilidad [0.06287274 0.93712726]\n",
      "El cliente  Escritura con probabilidad [0.16579977 0.83420023]\n",
      "El cliente  Escritura con probabilidad [2.54171097e-04 9.99745829e-01]\n",
      "El cliente  Escritura con probabilidad [0.28133215 0.71866785]\n",
      "El cliente  Escritura con probabilidad [0.07688726 0.92311274]\n",
      "El cliente  Escritura con probabilidad [0.14658482 0.85341518]\n",
      "El cliente  Escritura con probabilidad [0.01279612 0.98720388]\n",
      "El cliente  Escritura con probabilidad [0.01306681 0.98693319]\n",
      "El cliente  Escritura con probabilidad [0.09191988 0.90808012]\n",
      "El cliente  Escritura con probabilidad [9.98627401e-05 9.99900137e-01]\n",
      "El cliente  Escritura con probabilidad [0.07213625 0.92786375]\n",
      "El cliente  Escritura con probabilidad [1.56235247e-05 9.99984376e-01]\n",
      "El cliente  Escritura con probabilidad [0.39343041 0.60656959]\n",
      "El cliente  Escritura con probabilidad [0.401151 0.598849]\n",
      "El cliente  Escritura con probabilidad [5.35829935e-04 9.99464170e-01]\n",
      "El cliente  Escritura con probabilidad [0.28418357 0.71581643]\n",
      "El cliente  Escritura con probabilidad [0.20776174 0.79223826]\n",
      "El cliente  Escritura con probabilidad [0.20187387 0.79812613]\n",
      "El cliente  Escritura con probabilidad [0.24672751 0.75327249]\n",
      "El cliente  Escritura con probabilidad [0.18696484 0.81303516]\n",
      "El cliente  Escritura con probabilidad [0.34112634 0.65887366]\n",
      "El cliente  Escritura con probabilidad [0.00612962 0.99387038]\n",
      "El cliente  Escritura con probabilidad [0.44047093 0.55952907]\n",
      "El cliente  Escritura con probabilidad [0.00993761 0.99006239]\n",
      "El cliente  Escritura con probabilidad [0.17411329 0.82588671]\n",
      "El cliente  Escritura con probabilidad [0.06779823 0.93220177]\n",
      "El cliente  Escritura con probabilidad [6.85811055e-04 9.99314189e-01]\n",
      "El cliente  Escritura con probabilidad [0.19866523 0.80133477]\n",
      "El cliente  Escritura con probabilidad [0.20826179 0.79173821]\n",
      "El cliente  Escritura con probabilidad [0.11747706 0.88252294]\n",
      "El cliente  Escritura con probabilidad [0.18968524 0.81031476]\n",
      "El cliente  Escritura con probabilidad [0.1006084 0.8993916]\n",
      "El cliente  Escritura con probabilidad [0.24883074 0.75116926]\n",
      "El cliente  Escritura con probabilidad [0.25024981 0.74975019]\n",
      "El cliente  Escritura con probabilidad [0.24012547 0.75987453]\n",
      "El cliente  Escritura con probabilidad [6.11066753e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.37773226 0.62226774]\n",
      "El cliente  Escritura con probabilidad [0.20029773 0.79970227]\n",
      "El cliente  Escritura con probabilidad [0.2792384 0.7207616]\n",
      "El cliente  Escritura con probabilidad [0.09052536 0.90947464]\n",
      "El cliente  Escritura con probabilidad [0.01287526 0.98712474]\n",
      "El cliente  Escritura con probabilidad [0.04728905 0.95271095]\n",
      "El cliente  Escritura con probabilidad [7.12595927e-08 9.99999929e-01]\n",
      "El cliente  Escritura con probabilidad [0.06571878 0.93428122]\n",
      "El cliente  Escritura con probabilidad [1.66717235e-05 9.99983328e-01]\n",
      "El cliente  Escritura con probabilidad [7.30253591e-09 9.99999993e-01]\n",
      "El cliente  Escritura con probabilidad [0.03450847 0.96549153]\n",
      "El cliente  Escritura con probabilidad [0.19343126 0.80656874]\n",
      "El cliente  Escritura con probabilidad [0.06284732 0.93715268]\n",
      "El cliente  Escritura con probabilidad [6.15604570e-06 9.99993844e-01]\n",
      "El cliente  Escritura con probabilidad [1.37542352e-04 9.99862458e-01]\n",
      "El cliente  Escritura con probabilidad [0.01592559 0.98407441]\n",
      "El cliente  Escritura con probabilidad [0.13970063 0.86029937]\n",
      "El cliente  Escritura con probabilidad [0.01622125 0.98377875]\n",
      "El cliente  Escritura con probabilidad [0.01744671 0.98255329]\n",
      "El cliente  Escritura con probabilidad [0.10488408 0.89511592]\n",
      "El cliente  Escritura con probabilidad [0.43220511 0.56779489]\n",
      "El cliente  Escritura con probabilidad [0.05285527 0.94714473]\n",
      "El cliente  Escritura con probabilidad [0.15138721 0.84861279]\n",
      "El cliente  Escritura con probabilidad [6.30008511e-04 9.99369991e-01]\n",
      "El cliente  Escritura con probabilidad [0.15916275 0.84083725]\n",
      "El cliente  Escritura con probabilidad [0.17420337 0.82579663]\n",
      "El cliente  Escritura con probabilidad [0.20179212 0.79820788]\n",
      "El cliente  Escritura con probabilidad [2.22871552e-04 9.99777128e-01]\n",
      "El cliente  Escritura con probabilidad [3.80620257e-04 9.99619380e-01]\n",
      "El cliente  Escritura con probabilidad [0.17355889 0.82644111]\n",
      "El cliente  Escritura con probabilidad [0.10206525 0.89793475]\n",
      "El cliente  Escritura con probabilidad [0.18685555 0.81314445]\n",
      "El cliente  Escritura con probabilidad [0.05142703 0.94857297]\n",
      "El cliente  Escritura con probabilidad [0.08095809 0.91904191]\n",
      "El cliente  Escritura con probabilidad [0.22544161 0.77455839]\n",
      "El cliente  Escritura con probabilidad [0.20184098 0.79815902]\n",
      "El cliente  Escritura con probabilidad [0.09177444 0.90822556]\n",
      "El cliente  Escritura con probabilidad [0.21406298 0.78593702]\n",
      "El cliente  Escritura con probabilidad [0.03103755 0.96896245]\n",
      "El cliente  Escritura con probabilidad [0.23382084 0.76617916]\n",
      "El cliente  Escritura con probabilidad [0.11186008 0.88813992]\n",
      "El cliente  Escritura con probabilidad [0.12032523 0.87967477]\n",
      "El cliente  Escritura con probabilidad [5.24722327e-06 9.99994753e-01]\n",
      "El cliente  Escritura con probabilidad [0.07067737 0.92932263]\n",
      "El cliente  Escritura con probabilidad [3.85459893e-06 9.99996145e-01]\n",
      "El cliente  Escritura con probabilidad [4.40980585e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.03452136 0.96547864]\n",
      "El cliente  Escritura con probabilidad [0.29046697 0.70953303]\n",
      "El cliente  Escritura con probabilidad [0.23719825 0.76280175]\n",
      "El cliente  Escritura con probabilidad [0.07060687 0.92939313]\n",
      "El cliente  Escritura con probabilidad [0.12950846 0.87049154]\n",
      "El cliente  Escritura con probabilidad [0.24474711 0.75525289]\n",
      "El cliente  Escritura con probabilidad [0.22378075 0.77621925]\n",
      "El cliente  Escritura con probabilidad [0.17647606 0.82352394]\n",
      "El cliente  Escritura con probabilidad [0.00577093 0.99422907]\n",
      "El cliente  Desiste con probabilidad [0.50922261 0.49077739]\n",
      "El cliente  Escritura con probabilidad [0.1212887 0.8787113]\n",
      "El cliente  Escritura con probabilidad [0.20051239 0.79948761]\n",
      "El cliente  Escritura con probabilidad [0.22277249 0.77722751]\n",
      "El cliente  Escritura con probabilidad [3.15969207e-05 9.99968403e-01]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.24568235 0.75431765]\n",
      "El cliente  Escritura con probabilidad [0.21239267 0.78760733]\n",
      "El cliente  Escritura con probabilidad [3.21365246e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.08020745 0.91979255]\n",
      "El cliente  Escritura con probabilidad [0.0405361 0.9594639]\n",
      "El cliente  Escritura con probabilidad [0.14880946 0.85119054]\n",
      "El cliente  Escritura con probabilidad [0.04617434 0.95382566]\n",
      "El cliente  Escritura con probabilidad [0.14013983 0.85986017]\n",
      "El cliente  Escritura con probabilidad [0.2468547 0.7531453]\n",
      "El cliente  Escritura con probabilidad [0.24525415 0.75474585]\n",
      "El cliente  Escritura con probabilidad [0.02511803 0.97488197]\n",
      "El cliente  Escritura con probabilidad [0.04163565 0.95836435]\n",
      "El cliente  Escritura con probabilidad [0.24955586 0.75044414]\n",
      "El cliente  Escritura con probabilidad [0.02660146 0.97339854]\n",
      "El cliente  Escritura con probabilidad [0.19054752 0.80945248]\n",
      "El cliente  Escritura con probabilidad [0.21160407 0.78839593]\n",
      "El cliente  Escritura con probabilidad [0.02022648 0.97977352]\n",
      "El cliente  Escritura con probabilidad [0.18006333 0.81993667]\n",
      "El cliente  Escritura con probabilidad [5.35675126e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [7.05480119e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.23327428 0.76672572]\n",
      "El cliente  Escritura con probabilidad [0.22450448 0.77549552]\n",
      "El cliente  Escritura con probabilidad [0.21078367 0.78921633]\n",
      "El cliente  Escritura con probabilidad [0.24092727 0.75907273]\n",
      "El cliente  Escritura con probabilidad [0.03411512 0.96588488]\n",
      "El cliente  Escritura con probabilidad [0.06709486 0.93290514]\n",
      "El cliente  Escritura con probabilidad [6.15925468e-05 9.99938407e-01]\n",
      "El cliente  Escritura con probabilidad [0.00726103 0.99273897]\n",
      "El cliente  Escritura con probabilidad [0.16374894 0.83625106]\n",
      "El cliente  Escritura con probabilidad [0.38231203 0.61768797]\n",
      "El cliente  Escritura con probabilidad [0.17025085 0.82974915]\n",
      "El cliente  Escritura con probabilidad [0.16690826 0.83309174]\n",
      "El cliente  Escritura con probabilidad [0.02149733 0.97850267]\n",
      "El cliente  Escritura con probabilidad [0.41058356 0.58941644]\n",
      "El cliente  Escritura con probabilidad [0.04326765 0.95673235]\n",
      "El cliente  Escritura con probabilidad [0.2562998 0.7437002]\n",
      "El cliente  Escritura con probabilidad [0.02037805 0.97962195]\n",
      "El cliente  Escritura con probabilidad [0.17219854 0.82780146]\n",
      "El cliente  Escritura con probabilidad [0.16665323 0.83334677]\n",
      "El cliente  Escritura con probabilidad [0.10131677 0.89868323]\n",
      "El cliente  Escritura con probabilidad [0.44930761 0.55069239]\n",
      "El cliente  Escritura con probabilidad [0.00116999 0.99883001]\n",
      "El cliente  Escritura con probabilidad [1.46717649e-07 9.99999853e-01]\n",
      "El cliente  Escritura con probabilidad [4.06769399e-08 9.99999959e-01]\n",
      "El cliente  Escritura con probabilidad [0.22794266 0.77205734]\n",
      "El cliente  Escritura con probabilidad [0.28260646 0.71739354]\n",
      "El cliente  Escritura con probabilidad [0.10057899 0.89942101]\n",
      "El cliente  Escritura con probabilidad [6.13538244e-04 9.99386462e-01]\n",
      "El cliente  Escritura con probabilidad [0.14934305 0.85065695]\n",
      "El cliente  Escritura con probabilidad [0.10112916 0.89887084]\n",
      "El cliente  Escritura con probabilidad [0.22051243 0.77948757]\n",
      "El cliente  Escritura con probabilidad [0.23243631 0.76756369]\n",
      "El cliente  Escritura con probabilidad [0.05374327 0.94625673]\n",
      "El cliente  Escritura con probabilidad [1.65962311e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.19522267 0.80477733]\n",
      "El cliente  Escritura con probabilidad [0.00921929 0.99078071]\n",
      "El cliente  Escritura con probabilidad [0.1234434 0.8765566]\n",
      "El cliente  Escritura con probabilidad [0.00952842 0.99047158]\n",
      "El cliente  Escritura con probabilidad [1.42136583e-05 9.99985786e-01]\n",
      "El cliente  Escritura con probabilidad [4.42298842e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [6.1194827e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.18017275 0.81982725]\n",
      "El cliente  Escritura con probabilidad [0.15354236 0.84645764]\n",
      "El cliente  Escritura con probabilidad [0.12109346 0.87890654]\n",
      "El cliente  Escritura con probabilidad [0.16464552 0.83535448]\n",
      "El cliente  Escritura con probabilidad [0.15266449 0.84733551]\n",
      "El cliente  Escritura con probabilidad [0.08985783 0.91014217]\n",
      "El cliente  Escritura con probabilidad [0.12189446 0.87810554]\n",
      "El cliente  Escritura con probabilidad [0.20482001 0.79517999]\n",
      "El cliente  Escritura con probabilidad [0.18323145 0.81676855]\n",
      "El cliente  Escritura con probabilidad [0.01889543 0.98110457]\n",
      "El cliente  Escritura con probabilidad [3.58601551e-06 9.99996414e-01]\n",
      "El cliente  Escritura con probabilidad [0.18345881 0.81654119]\n",
      "El cliente  Escritura con probabilidad [0.15702051 0.84297949]\n",
      "El cliente  Escritura con probabilidad [4.18339014e-06 9.99995817e-01]\n",
      "El cliente  Escritura con probabilidad [0.17605434 0.82394566]\n",
      "El cliente  Escritura con probabilidad [1.42161550e-05 9.99985784e-01]\n",
      "El cliente  Escritura con probabilidad [7.82297359e-06 9.99992177e-01]\n",
      "El cliente  Escritura con probabilidad [0.16427778 0.83572222]\n",
      "El cliente  Escritura con probabilidad [0.27534583 0.72465417]\n",
      "El cliente  Escritura con probabilidad [0.17112038 0.82887962]\n",
      "El cliente  Escritura con probabilidad [0.04079162 0.95920838]\n",
      "El cliente  Escritura con probabilidad [0.14751235 0.85248765]\n",
      "El cliente  Escritura con probabilidad [0.02163104 0.97836896]\n",
      "El cliente  Escritura con probabilidad [0.23108308 0.76891692]\n",
      "El cliente  Escritura con probabilidad [0.01888137 0.98111863]\n",
      "El cliente  Escritura con probabilidad [5.21415870e-05 9.99947858e-01]\n",
      "El cliente  Escritura con probabilidad [0.09229149 0.90770851]\n",
      "El cliente  Escritura con probabilidad [0.16286779 0.83713221]\n",
      "El cliente  Escritura con probabilidad [0.15456547 0.84543453]\n",
      "El cliente  Escritura con probabilidad [0.07488791 0.92511209]\n",
      "El cliente  Escritura con probabilidad [0.11896954 0.88103046]\n",
      "El cliente  Escritura con probabilidad [0.017399 0.982601]\n",
      "El cliente  Escritura con probabilidad [0.28052797 0.71947203]\n",
      "El cliente  Escritura con probabilidad [0.06437557 0.93562443]\n",
      "El cliente  Escritura con probabilidad [0.20315888 0.79684112]\n",
      "El cliente  Escritura con probabilidad [0.04528425 0.95471575]\n",
      "El cliente  Escritura con probabilidad [0.11293966 0.88706034]\n",
      "El cliente  Escritura con probabilidad [0.02008193 0.97991807]\n",
      "El cliente  Escritura con probabilidad [0.02161631 0.97838369]\n",
      "El cliente  Escritura con probabilidad [0.18580966 0.81419034]\n",
      "El cliente  Escritura con probabilidad [0.14354659 0.85645341]\n",
      "El cliente  Escritura con probabilidad [0.01844885 0.98155115]\n",
      "El cliente  Escritura con probabilidad [0.21374979 0.78625021]\n",
      "El cliente  Escritura con probabilidad [0.11473028 0.88526972]\n",
      "El cliente  Escritura con probabilidad [0.01611955 0.98388045]\n",
      "El cliente  Escritura con probabilidad [1.60589764e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.69913054e-05 9.99983009e-01]\n",
      "El cliente  Escritura con probabilidad [0.02212026 0.97787974]\n",
      "El cliente  Escritura con probabilidad [0.24848627 0.75151373]\n",
      "El cliente  Escritura con probabilidad [0.19589782 0.80410218]\n",
      "El cliente  Escritura con probabilidad [0.14219489 0.85780511]\n",
      "El cliente  Escritura con probabilidad [0.2511188 0.7488812]\n",
      "El cliente  Escritura con probabilidad [0.08502174 0.91497826]\n",
      "El cliente  Escritura con probabilidad [0.06539807 0.93460193]\n",
      "El cliente  Escritura con probabilidad [3.30262382e-05 9.99966974e-01]\n",
      "El cliente  Escritura con probabilidad [0.32749521 0.67250479]\n",
      "El cliente  Escritura con probabilidad [0.14447158 0.85552842]\n",
      "El cliente  Escritura con probabilidad [0.01255801 0.98744199]\n",
      "El cliente  Escritura con probabilidad [8.93341801e-07 9.99999107e-01]\n",
      "El cliente  Escritura con probabilidad [0.22653384 0.77346616]\n",
      "El cliente  Escritura con probabilidad [0.28133215 0.71866785]\n",
      "El cliente  Escritura con probabilidad [0.29104441 0.70895559]\n",
      "El cliente  Escritura con probabilidad [1.76177903e-05 9.99982382e-01]\n",
      "El cliente  Escritura con probabilidad [0.05704082 0.94295918]\n",
      "El cliente  Escritura con probabilidad [2.05139753e-04 9.99794860e-01]\n",
      "El cliente  Escritura con probabilidad [0.24368472 0.75631528]\n",
      "El cliente  Escritura con probabilidad [0.04934425 0.95065575]\n",
      "El cliente  Escritura con probabilidad [0.0455458 0.9544542]\n",
      "El cliente  Escritura con probabilidad [1.19262378e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.27908495 0.72091505]\n",
      "El cliente  Escritura con probabilidad [0.27735749 0.72264251]\n",
      "El cliente  Escritura con probabilidad [5.94401838e-07 9.99999406e-01]\n",
      "El cliente  Escritura con probabilidad [2.65625177e-05 9.99973437e-01]\n",
      "El cliente  Escritura con probabilidad [7.74954284e-04 9.99225046e-01]\n",
      "El cliente  Escritura con probabilidad [0.10899952 0.89100048]\n",
      "El cliente  Escritura con probabilidad [0.15292099 0.84707901]\n",
      "El cliente  Escritura con probabilidad [0.1800844 0.8199156]\n",
      "El cliente  Escritura con probabilidad [0.102454 0.897546]\n",
      "El cliente  Escritura con probabilidad [0.2349194 0.7650806]\n",
      "El cliente  Desiste con probabilidad [0.51042657 0.48957343]\n",
      "El cliente  Escritura con probabilidad [0.02982605 0.97017395]\n",
      "El cliente  Escritura con probabilidad [0.16045844 0.83954156]\n",
      "El cliente  Escritura con probabilidad [0.17993503 0.82006497]\n",
      "El cliente  Escritura con probabilidad [0.1088096 0.8911904]\n",
      "El cliente  Escritura con probabilidad [0.03245344 0.96754656]\n",
      "El cliente  Escritura con probabilidad [4.10692458e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.12775517 0.87224483]\n",
      "El cliente  Escritura con probabilidad [0.08636244 0.91363756]\n",
      "El cliente  Escritura con probabilidad [0.08561423 0.91438577]\n",
      "El cliente  Escritura con probabilidad [2.35793194e-05 9.99976421e-01]\n",
      "El cliente  Escritura con probabilidad [0.02225087 0.97774913]\n",
      "El cliente  Escritura con probabilidad [1.88811914e-06 9.99998112e-01]\n",
      "El cliente  Escritura con probabilidad [1.84052416e-05 9.99981595e-01]\n",
      "El cliente  Escritura con probabilidad [0.25725508 0.74274492]\n",
      "El cliente  Escritura con probabilidad [0.09951149 0.90048851]\n",
      "El cliente  Escritura con probabilidad [0.04284723 0.95715277]\n",
      "El cliente  Escritura con probabilidad [0.023744 0.976256]\n",
      "El cliente  Escritura con probabilidad [0.19386548 0.80613452]\n",
      "El cliente  Escritura con probabilidad [0.21432597 0.78567403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [2.50981138e-08 9.99999975e-01]\n",
      "El cliente  Escritura con probabilidad [0.17539344 0.82460656]\n",
      "El cliente  Escritura con probabilidad [0.19167638 0.80832362]\n",
      "El cliente  Escritura con probabilidad [3.99648137e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.44676562 0.55323438]\n",
      "El cliente  Escritura con probabilidad [0.06225323 0.93774677]\n",
      "El cliente  Escritura con probabilidad [3.20865987e-07 9.99999679e-01]\n",
      "El cliente  Escritura con probabilidad [0.13990344 0.86009656]\n",
      "El cliente  Escritura con probabilidad [8.68475236e-09 9.99999991e-01]\n",
      "El cliente  Escritura con probabilidad [1.24181621e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.20438052 0.79561948]\n",
      "El cliente  Escritura con probabilidad [0.24827249 0.75172751]\n",
      "El cliente  Escritura con probabilidad [3.7341108e-10 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.00637897 0.99362103]\n",
      "El cliente  Escritura con probabilidad [0.21708057 0.78291943]\n",
      "El cliente  Escritura con probabilidad [0.12376645 0.87623355]\n",
      "El cliente  Escritura con probabilidad [0.16195973 0.83804027]\n",
      "El cliente  Escritura con probabilidad [0.00356047 0.99643953]\n",
      "El cliente  Escritura con probabilidad [0.02853144 0.97146856]\n",
      "El cliente  Escritura con probabilidad [3.41762837e-07 9.99999658e-01]\n",
      "El cliente  Escritura con probabilidad [0.15544457 0.84455543]\n",
      "El cliente  Escritura con probabilidad [0.09031916 0.90968084]\n",
      "El cliente  Escritura con probabilidad [0.2803053 0.7196947]\n",
      "El cliente  Escritura con probabilidad [0.10908762 0.89091238]\n",
      "El cliente  Escritura con probabilidad [0.08327763 0.91672237]\n",
      "El cliente  Escritura con probabilidad [0.24437465 0.75562535]\n",
      "El cliente  Escritura con probabilidad [0.2082275 0.7917725]\n",
      "El cliente  Escritura con probabilidad [0.02004994 0.97995006]\n",
      "El cliente  Escritura con probabilidad [0.1755167 0.8244833]\n",
      "El cliente  Escritura con probabilidad [0.41573064 0.58426936]\n",
      "El cliente  Escritura con probabilidad [0.04140568 0.95859432]\n",
      "El cliente  Escritura con probabilidad [0.42464955 0.57535045]\n",
      "El cliente  Escritura con probabilidad [0.31854244 0.68145756]\n",
      "El cliente  Escritura con probabilidad [3.14570432e-07 9.99999685e-01]\n",
      "El cliente  Escritura con probabilidad [2.0180579e-08 9.9999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.1465002 0.8534998]\n",
      "El cliente  Escritura con probabilidad [0.29617775 0.70382225]\n",
      "El cliente  Escritura con probabilidad [0.01566783 0.98433217]\n",
      "El cliente  Escritura con probabilidad [4.09955827e-06 9.99995900e-01]\n",
      "El cliente  Escritura con probabilidad [0.19172248 0.80827752]\n",
      "El cliente  Escritura con probabilidad [2.58519339e-05 9.99974148e-01]\n",
      "El cliente  Escritura con probabilidad [0.00264722 0.99735278]\n",
      "El cliente  Escritura con probabilidad [0.05567258 0.94432742]\n",
      "El cliente  Escritura con probabilidad [0.17136623 0.82863377]\n",
      "El cliente  Escritura con probabilidad [0.01086543 0.98913457]\n",
      "El cliente  Escritura con probabilidad [0.14690843 0.85309157]\n",
      "El cliente  Escritura con probabilidad [2.82164950e-08 9.99999972e-01]\n",
      "El cliente  Escritura con probabilidad [0.03553752 0.96446248]\n",
      "El cliente  Escritura con probabilidad [0.25880433 0.74119567]\n",
      "El cliente  Escritura con probabilidad [1.00321265e-04 9.99899679e-01]\n",
      "El cliente  Escritura con probabilidad [0.08977385 0.91022615]\n",
      "El cliente  Escritura con probabilidad [0.00825258 0.99174742]\n",
      "El cliente  Escritura con probabilidad [0.17006248 0.82993752]\n",
      "El cliente  Escritura con probabilidad [0.01452598 0.98547402]\n",
      "El cliente  Escritura con probabilidad [0.01239533 0.98760467]\n",
      "El cliente  Escritura con probabilidad [2.54589001e-07 9.99999745e-01]\n",
      "El cliente  Escritura con probabilidad [0.18123945 0.81876055]\n",
      "El cliente  Escritura con probabilidad [0.14741743 0.85258257]\n",
      "El cliente  Escritura con probabilidad [0.2020898 0.7979102]\n",
      "El cliente  Escritura con probabilidad [0.02321051 0.97678949]\n",
      "El cliente  Escritura con probabilidad [1.97169243e-08 9.99999980e-01]\n",
      "El cliente  Escritura con probabilidad [0.06072012 0.93927988]\n",
      "El cliente  Escritura con probabilidad [1.94389838e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.07524318 0.92475682]\n",
      "El cliente  Escritura con probabilidad [0.09941785 0.90058215]\n",
      "El cliente  Escritura con probabilidad [0.07519427 0.92480573]\n",
      "El cliente  Escritura con probabilidad [0.26142377 0.73857623]\n",
      "El cliente  Escritura con probabilidad [0.22607557 0.77392443]\n",
      "El cliente  Escritura con probabilidad [1.21274276e-07 9.99999879e-01]\n",
      "El cliente  Escritura con probabilidad [0.04210161 0.95789839]\n",
      "El cliente  Escritura con probabilidad [1.86169247e-06 9.99998138e-01]\n",
      "El cliente  Escritura con probabilidad [0.12404241 0.87595759]\n",
      "El cliente  Escritura con probabilidad [0.19360524 0.80639476]\n",
      "El cliente  Escritura con probabilidad [0.14651113 0.85348887]\n",
      "El cliente  Escritura con probabilidad [0.25878878 0.74121122]\n",
      "El cliente  Escritura con probabilidad [0.24976256 0.75023744]\n",
      "El cliente  Escritura con probabilidad [4.16535184e-08 9.99999958e-01]\n",
      "El cliente  Escritura con probabilidad [0.00824902 0.99175098]\n",
      "El cliente  Escritura con probabilidad [0.0937782 0.9062218]\n",
      "El cliente  Escritura con probabilidad [0.0422108 0.9577892]\n",
      "El cliente  Escritura con probabilidad [0.24882409 0.75117591]\n",
      "El cliente  Escritura con probabilidad [0.00509212 0.99490788]\n",
      "El cliente  Escritura con probabilidad [0.16182838 0.83817162]\n",
      "El cliente  Escritura con probabilidad [0.16450016 0.83549984]\n",
      "El cliente  Escritura con probabilidad [0.2340698 0.7659302]\n",
      "El cliente  Escritura con probabilidad [0.12038421 0.87961579]\n",
      "El cliente  Escritura con probabilidad [0.02726533 0.97273467]\n",
      "El cliente  Escritura con probabilidad [0.15183624 0.84816376]\n",
      "El cliente  Escritura con probabilidad [0.20478252 0.79521748]\n",
      "El cliente  Escritura con probabilidad [0.03532576 0.96467424]\n",
      "El cliente  Escritura con probabilidad [0.12170962 0.87829038]\n",
      "El cliente  Escritura con probabilidad [0.17272217 0.82727783]\n",
      "El cliente  Escritura con probabilidad [2.97640306e-05 9.99970236e-01]\n",
      "El cliente  Escritura con probabilidad [0.12699871 0.87300129]\n",
      "El cliente  Escritura con probabilidad [0.02419548 0.97580452]\n",
      "El cliente  Escritura con probabilidad [0.15184125 0.84815875]\n",
      "El cliente  Escritura con probabilidad [0.15623645 0.84376355]\n",
      "El cliente  Escritura con probabilidad [0.216094 0.783906]\n",
      "El cliente  Escritura con probabilidad [0.02797474 0.97202526]\n",
      "El cliente  Escritura con probabilidad [0.12491444 0.87508556]\n",
      "El cliente  Escritura con probabilidad [3.98810544e-06 9.99996012e-01]\n",
      "El cliente  Escritura con probabilidad [0.25453806 0.74546194]\n",
      "El cliente  Escritura con probabilidad [0.05230041 0.94769959]\n",
      "El cliente  Escritura con probabilidad [0.14159158 0.85840842]\n",
      "El cliente  Escritura con probabilidad [0.16937279 0.83062721]\n",
      "El cliente  Escritura con probabilidad [0.22539455 0.77460545]\n",
      "El cliente  Escritura con probabilidad [0.06260357 0.93739643]\n",
      "El cliente  Escritura con probabilidad [0.14359852 0.85640148]\n",
      "El cliente  Escritura con probabilidad [0.23679795 0.76320205]\n",
      "El cliente  Escritura con probabilidad [0.07231567 0.92768433]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.39081169 0.60918831]\n",
      "El cliente  Escritura con probabilidad [0.08361156 0.91638844]\n",
      "El cliente  Escritura con probabilidad [0.19376178 0.80623822]\n",
      "El cliente  Escritura con probabilidad [0.28416887 0.71583113]\n",
      "El cliente  Escritura con probabilidad [0.28016662 0.71983338]\n",
      "El cliente  Escritura con probabilidad [0.03570528 0.96429472]\n",
      "El cliente  Escritura con probabilidad [0.05659115 0.94340885]\n",
      "El cliente  Escritura con probabilidad [0.42217973 0.57782027]\n",
      "El cliente  Escritura con probabilidad [0.06704496 0.93295504]\n",
      "El cliente  Escritura con probabilidad [1.19756491e-06 9.99998802e-01]\n",
      "El cliente  Escritura con probabilidad [9.28257471e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17187405 0.82812595]\n",
      "El cliente  Escritura con probabilidad [0.0107447 0.9892553]\n",
      "El cliente  Escritura con probabilidad [0.25798632 0.74201368]\n",
      "El cliente  Escritura con probabilidad [0.01960859 0.98039141]\n",
      "El cliente  Escritura con probabilidad [2.89819548e-06 9.99997102e-01]\n",
      "El cliente  Escritura con probabilidad [0.03953172 0.96046828]\n",
      "El cliente  Escritura con probabilidad [0.18580774 0.81419226]\n",
      "El cliente  Escritura con probabilidad [0.16946352 0.83053648]\n",
      "El cliente  Escritura con probabilidad [0.16178525 0.83821475]\n",
      "El cliente  Escritura con probabilidad [0.00776956 0.99223044]\n",
      "El cliente  Escritura con probabilidad [0.00739068 0.99260932]\n",
      "El cliente  Escritura con probabilidad [0.01266593 0.98733407]\n",
      "El cliente  Escritura con probabilidad [0.14201115 0.85798885]\n",
      "El cliente  Escritura con probabilidad [0.09969934 0.90030066]\n",
      "El cliente  Escritura con probabilidad [0.02125335 0.97874665]\n",
      "El cliente  Escritura con probabilidad [0.02147099 0.97852901]\n",
      "El cliente  Escritura con probabilidad [0.14992864 0.85007136]\n",
      "El cliente  Escritura con probabilidad [0.23174928 0.76825072]\n",
      "El cliente  Escritura con probabilidad [1.68847414e-06 9.99998312e-01]\n",
      "El cliente  Escritura con probabilidad [0.00198982 0.99801018]\n",
      "El cliente  Escritura con probabilidad [0.07006842 0.92993158]\n",
      "El cliente  Escritura con probabilidad [0.07795916 0.92204084]\n",
      "El cliente  Escritura con probabilidad [0.1628556 0.8371444]\n",
      "El cliente  Escritura con probabilidad [0.04189858 0.95810142]\n",
      "El cliente  Escritura con probabilidad [0.14032147 0.85967853]\n",
      "El cliente  Escritura con probabilidad [0.31068269 0.68931731]\n",
      "El cliente  Escritura con probabilidad [0.05543397 0.94456603]\n",
      "El cliente  Escritura con probabilidad [0.11266622 0.88733378]\n",
      "El cliente  Escritura con probabilidad [0.11432504 0.88567496]\n",
      "El cliente  Escritura con probabilidad [0.02880139 0.97119861]\n",
      "El cliente  Escritura con probabilidad [0.2604236 0.7395764]\n",
      "El cliente  Escritura con probabilidad [0.28316436 0.71683564]\n",
      "El cliente  Escritura con probabilidad [0.1877028 0.8122972]\n",
      "El cliente  Escritura con probabilidad [0.02867366 0.97132634]\n",
      "El cliente  Escritura con probabilidad [0.2555989 0.7444011]\n",
      "El cliente  Escritura con probabilidad [0.15860219 0.84139781]\n",
      "El cliente  Escritura con probabilidad [0.08516629 0.91483371]\n",
      "El cliente  Escritura con probabilidad [2.50272219e-04 9.99749728e-01]\n",
      "El cliente  Escritura con probabilidad [0.35233006 0.64766994]\n",
      "El cliente  Escritura con probabilidad [2.81498573e-04 9.99718501e-01]\n",
      "El cliente  Escritura con probabilidad [0.02600005 0.97399995]\n",
      "El cliente  Escritura con probabilidad [7.31284020e-05 9.99926872e-01]\n",
      "El cliente  Escritura con probabilidad [0.35556573 0.64443427]\n",
      "El cliente  Escritura con probabilidad [0.01811073 0.98188927]\n",
      "El cliente  Escritura con probabilidad [0.18236999 0.81763001]\n",
      "El cliente  Escritura con probabilidad [0.15451996 0.84548004]\n",
      "El cliente  Escritura con probabilidad [0.0670694 0.9329306]\n",
      "El cliente  Escritura con probabilidad [0.27982054 0.72017946]\n",
      "El cliente  Escritura con probabilidad [0.01671597 0.98328403]\n",
      "El cliente  Escritura con probabilidad [0.1965557 0.8034443]\n",
      "El cliente  Escritura con probabilidad [0.18188549 0.81811451]\n",
      "El cliente  Escritura con probabilidad [0.11550904 0.88449096]\n",
      "El cliente  Escritura con probabilidad [4.28292933e-06 9.99995717e-01]\n",
      "El cliente  Escritura con probabilidad [0.02756472 0.97243528]\n",
      "El cliente  Escritura con probabilidad [0.19417068 0.80582932]\n",
      "El cliente  Escritura con probabilidad [0.04194107 0.95805893]\n",
      "El cliente  Escritura con probabilidad [0.03209319 0.96790681]\n",
      "El cliente  Escritura con probabilidad [0.06112923 0.93887077]\n",
      "El cliente  Escritura con probabilidad [0.07281687 0.92718313]\n",
      "El cliente  Escritura con probabilidad [0.07237534 0.92762466]\n",
      "El cliente  Escritura con probabilidad [0.0716183 0.9283817]\n",
      "El cliente  Escritura con probabilidad [0.19954283 0.80045717]\n",
      "El cliente  Escritura con probabilidad [3.45539145e-04 9.99654461e-01]\n",
      "El cliente  Escritura con probabilidad [0.01818137 0.98181863]\n",
      "El cliente  Escritura con probabilidad [0.03194601 0.96805399]\n",
      "El cliente  Escritura con probabilidad [0.17216526 0.82783474]\n",
      "El cliente  Escritura con probabilidad [0.21991369 0.78008631]\n",
      "El cliente  Escritura con probabilidad [0.16446886 0.83553114]\n",
      "El cliente  Escritura con probabilidad [0.17682049 0.82317951]\n",
      "El cliente  Escritura con probabilidad [0.26034965 0.73965035]\n",
      "El cliente  Escritura con probabilidad [0.24736553 0.75263447]\n",
      "El cliente  Escritura con probabilidad [0.17800583 0.82199417]\n",
      "El cliente  Escritura con probabilidad [0.09709497 0.90290503]\n",
      "El cliente  Escritura con probabilidad [0.44982665 0.55017335]\n",
      "El cliente  Escritura con probabilidad [0.21267557 0.78732443]\n",
      "El cliente  Escritura con probabilidad [0.12608356 0.87391644]\n",
      "El cliente  Escritura con probabilidad [0.11453773 0.88546227]\n",
      "El cliente  Escritura con probabilidad [1.39885343e-08 9.99999986e-01]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.02724641 0.97275359]\n",
      "El cliente  Escritura con probabilidad [0.06307154 0.93692846]\n",
      "El cliente  Escritura con probabilidad [0.22410362 0.77589638]\n",
      "El cliente  Escritura con probabilidad [1.27884135e-07 9.99999872e-01]\n",
      "El cliente  Escritura con probabilidad [0.05159692 0.94840308]\n",
      "El cliente  Escritura con probabilidad [0.09432129 0.90567871]\n",
      "El cliente  Escritura con probabilidad [0.16323699 0.83676301]\n",
      "El cliente  Escritura con probabilidad [0.02460372 0.97539628]\n",
      "El cliente  Escritura con probabilidad [0.02780926 0.97219074]\n",
      "El cliente  Escritura con probabilidad [0.01937671 0.98062329]\n",
      "El cliente  Escritura con probabilidad [0.23148381 0.76851619]\n",
      "El cliente  Escritura con probabilidad [0.15823997 0.84176003]\n",
      "El cliente  Escritura con probabilidad [0.0010357 0.9989643]\n",
      "El cliente  Escritura con probabilidad [0.08858212 0.91141788]\n",
      "El cliente  Escritura con probabilidad [0.23031993 0.76968007]\n",
      "El cliente  Escritura con probabilidad [0.26802929 0.73197071]\n",
      "El cliente  Escritura con probabilidad [0.03770314 0.96229686]\n",
      "El cliente  Escritura con probabilidad [0.08362226 0.91637774]\n",
      "El cliente  Escritura con probabilidad [0.05034994 0.94965006]\n",
      "El cliente  Escritura con probabilidad [0.1884754 0.8115246]\n",
      "El cliente  Escritura con probabilidad [0.18253282 0.81746718]\n",
      "El cliente  Escritura con probabilidad [0.01474582 0.98525418]\n",
      "El cliente  Escritura con probabilidad [0.02191325 0.97808675]\n",
      "El cliente  Escritura con probabilidad [0.19297777 0.80702223]\n",
      "El cliente  Escritura con probabilidad [3.67805103e-06 9.99996322e-01]\n",
      "El cliente  Escritura con probabilidad [0.14117495 0.85882505]\n",
      "El cliente  Escritura con probabilidad [0.1271909 0.8728091]\n",
      "El cliente  Escritura con probabilidad [0.23710903 0.76289097]\n",
      "El cliente  Escritura con probabilidad [0.05637381 0.94362619]\n",
      "El cliente  Escritura con probabilidad [0.29712649 0.70287351]\n",
      "El cliente  Escritura con probabilidad [0.06093391 0.93906609]\n",
      "El cliente  Escritura con probabilidad [0.20060589 0.79939411]\n",
      "El cliente  Escritura con probabilidad [0.09496127 0.90503873]\n",
      "El cliente  Escritura con probabilidad [0.05519694 0.94480306]\n",
      "El cliente  Escritura con probabilidad [0.03882205 0.96117795]\n",
      "El cliente  Escritura con probabilidad [0.22196633 0.77803367]\n",
      "El cliente  Escritura con probabilidad [0.12235032 0.87764968]\n",
      "El cliente  Escritura con probabilidad [0.1407743 0.8592257]\n",
      "El cliente  Escritura con probabilidad [0.16044811 0.83955189]\n",
      "El cliente  Escritura con probabilidad [0.10984667 0.89015333]\n",
      "El cliente  Escritura con probabilidad [0.15934035 0.84065965]\n",
      "El cliente  Escritura con probabilidad [0.1453391 0.8546609]\n",
      "El cliente  Escritura con probabilidad [0.06032086 0.93967914]\n",
      "El cliente  Escritura con probabilidad [0.07079473 0.92920527]\n",
      "El cliente  Escritura con probabilidad [0.11580736 0.88419264]\n",
      "El cliente  Escritura con probabilidad [0.06018033 0.93981967]\n",
      "El cliente  Escritura con probabilidad [0.07607897 0.92392103]\n",
      "El cliente  Escritura con probabilidad [0.4109081 0.5890919]\n",
      "El cliente  Escritura con probabilidad [0.0183805 0.9816195]\n",
      "El cliente  Escritura con probabilidad [0.31330891 0.68669109]\n",
      "El cliente  Escritura con probabilidad [0.16299105 0.83700895]\n",
      "El cliente  Escritura con probabilidad [8.29470972e-08 9.99999917e-01]\n",
      "El cliente  Escritura con probabilidad [0.01011858 0.98988142]\n",
      "El cliente  Escritura con probabilidad [0.13770661 0.86229339]\n",
      "El cliente  Escritura con probabilidad [0.06113434 0.93886566]\n",
      "El cliente  Escritura con probabilidad [0.44061355 0.55938645]\n",
      "El cliente  Escritura con probabilidad [0.1733197 0.8266803]\n",
      "El cliente  Escritura con probabilidad [7.76865539e-09 9.99999992e-01]\n",
      "El cliente  Escritura con probabilidad [0.17161968 0.82838032]\n",
      "El cliente  Escritura con probabilidad [0.27180738 0.72819262]\n",
      "El cliente  Escritura con probabilidad [0.06518865 0.93481135]\n",
      "El cliente  Escritura con probabilidad [0.07835722 0.92164278]\n",
      "El cliente  Escritura con probabilidad [2.19347417e-05 9.99978065e-01]\n",
      "El cliente  Escritura con probabilidad [0.46273788 0.53726212]\n",
      "El cliente  Escritura con probabilidad [0.17209712 0.82790288]\n",
      "El cliente  Escritura con probabilidad [0.02395117 0.97604883]\n",
      "El cliente  Escritura con probabilidad [0.30439274 0.69560726]\n",
      "El cliente  Escritura con probabilidad [0.01336208 0.98663792]\n",
      "El cliente  Escritura con probabilidad [0.13871669 0.86128331]\n",
      "El cliente  Escritura con probabilidad [0.08320558 0.91679442]\n",
      "El cliente  Escritura con probabilidad [8.25248758e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.41516343e-08 9.99999986e-01]\n",
      "El cliente  Escritura con probabilidad [5.41821684e-05 9.99945818e-01]\n",
      "El cliente  Escritura con probabilidad [3.06672084e-05 9.99969333e-01]\n",
      "El cliente  Escritura con probabilidad [0.06457618 0.93542382]\n",
      "El cliente  Escritura con probabilidad [0.0639566 0.9360434]\n",
      "El cliente  Escritura con probabilidad [0.16964194 0.83035806]\n",
      "El cliente  Escritura con probabilidad [0.21586125 0.78413875]\n",
      "El cliente  Escritura con probabilidad [0.17283677 0.82716323]\n",
      "El cliente  Escritura con probabilidad [0.02667329 0.97332671]\n",
      "El cliente  Escritura con probabilidad [0.42147553 0.57852447]\n",
      "El cliente  Escritura con probabilidad [0.03044945 0.96955055]\n",
      "El cliente  Escritura con probabilidad [0.1972868 0.8027132]\n",
      "El cliente  Escritura con probabilidad [0.1539139 0.8460861]\n",
      "El cliente  Escritura con probabilidad [0.04626024 0.95373976]\n",
      "El cliente  Escritura con probabilidad [0.08150811 0.91849189]\n",
      "El cliente  Escritura con probabilidad [5.6977865e-07 9.9999943e-01]\n",
      "El cliente  Escritura con probabilidad [0.03956696 0.96043304]\n",
      "El cliente  Escritura con probabilidad [7.70919284e-05 9.99922908e-01]\n",
      "El cliente  Escritura con probabilidad [0.01254962 0.98745038]\n",
      "El cliente  Escritura con probabilidad [0.19984757 0.80015243]\n",
      "El cliente  Escritura con probabilidad [0.20664573 0.79335427]\n",
      "El cliente  Escritura con probabilidad [1.77814662e-08 9.99999982e-01]\n",
      "El cliente  Escritura con probabilidad [0.2548984 0.7451016]\n",
      "El cliente  Escritura con probabilidad [0.22305663 0.77694337]\n",
      "El cliente  Escritura con probabilidad [0.02326008 0.97673992]\n",
      "El cliente  Escritura con probabilidad [0.06785909 0.93214091]\n",
      "El cliente  Escritura con probabilidad [0.18572952 0.81427048]\n",
      "El cliente  Escritura con probabilidad [0.19787496 0.80212504]\n",
      "El cliente  Escritura con probabilidad [0.25987325 0.74012675]\n",
      "El cliente  Escritura con probabilidad [0.13513008 0.86486992]\n",
      "El cliente  Escritura con probabilidad [0.1021762 0.8978238]\n",
      "El cliente  Escritura con probabilidad [5.39415059e-07 9.99999461e-01]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.00277674 0.99722326]\n",
      "El cliente  Escritura con probabilidad [0.05442537 0.94557463]\n",
      "El cliente  Escritura con probabilidad [0.45631711 0.54368289]\n",
      "El cliente  Escritura con probabilidad [0.0030656 0.9969344]\n",
      "El cliente  Escritura con probabilidad [0.21203404 0.78796596]\n",
      "El cliente  Escritura con probabilidad [0.00145636 0.99854364]\n",
      "El cliente  Escritura con probabilidad [0.127849 0.872151]\n",
      "El cliente  Escritura con probabilidad [0.14410369 0.85589631]\n",
      "El cliente  Escritura con probabilidad [2.22938623e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.10380499 0.89619501]\n",
      "El cliente  Escritura con probabilidad [0.06459453 0.93540547]\n",
      "El cliente  Escritura con probabilidad [0.19238289 0.80761711]\n",
      "El cliente  Escritura con probabilidad [0.18788846 0.81211154]\n",
      "El cliente  Escritura con probabilidad [0.19078414 0.80921586]\n",
      "El cliente  Escritura con probabilidad [0.00448341 0.99551659]\n",
      "El cliente  Escritura con probabilidad [0.13001185 0.86998815]\n",
      "El cliente  Escritura con probabilidad [0.11380012 0.88619988]\n",
      "El cliente  Escritura con probabilidad [0.4419042 0.5580958]\n",
      "El cliente  Escritura con probabilidad [5.90904746e-04 9.99409095e-01]\n",
      "El cliente  Escritura con probabilidad [2.43136562e-06 9.99997569e-01]\n",
      "El cliente  Escritura con probabilidad [0.05387234 0.94612766]\n",
      "El cliente  Escritura con probabilidad [0.01177842 0.98822158]\n",
      "El cliente  Escritura con probabilidad [0.22423643 0.77576357]\n",
      "El cliente  Escritura con probabilidad [0.06112581 0.93887419]\n",
      "El cliente  Escritura con probabilidad [5.59580937e-08 9.99999944e-01]\n",
      "El cliente  Escritura con probabilidad [0.00386931 0.99613069]\n",
      "El cliente  Escritura con probabilidad [0.00753048 0.99246952]\n",
      "El cliente  Escritura con probabilidad [0.09035493 0.90964507]\n",
      "El cliente  Escritura con probabilidad [0.19403202 0.80596798]\n",
      "El cliente  Escritura con probabilidad [8.81644355e-04 9.99118356e-01]\n",
      "El cliente  Escritura con probabilidad [0.26809571 0.73190429]\n",
      "El cliente  Escritura con probabilidad [0.06435827 0.93564173]\n",
      "El cliente  Escritura con probabilidad [1.03793533e-05 9.99989621e-01]\n",
      "El cliente  Escritura con probabilidad [9.82933288e-05 9.99901707e-01]\n",
      "El cliente  Escritura con probabilidad [0.16660743 0.83339257]\n",
      "El cliente  Escritura con probabilidad [0.01241529 0.98758471]\n",
      "El cliente  Escritura con probabilidad [0.04252078 0.95747922]\n",
      "El cliente  Escritura con probabilidad [0.17324127 0.82675873]\n",
      "El cliente  Escritura con probabilidad [8.59059891e-06 9.99991409e-01]\n",
      "El cliente  Escritura con probabilidad [0.28698037 0.71301963]\n",
      "El cliente  Escritura con probabilidad [0.09035087 0.90964913]\n",
      "El cliente  Escritura con probabilidad [0.25340211 0.74659789]\n",
      "El cliente  Escritura con probabilidad [0.01666832 0.98333168]\n",
      "El cliente  Escritura con probabilidad [0.24899775 0.75100225]\n",
      "El cliente  Escritura con probabilidad [0.21335333 0.78664667]\n",
      "El cliente  Escritura con probabilidad [0.00292814 0.99707186]\n",
      "El cliente  Escritura con probabilidad [0.2791224 0.7208776]\n",
      "El cliente  Escritura con probabilidad [0.28460665 0.71539335]\n",
      "El cliente  Escritura con probabilidad [6.33765729e-08 9.99999937e-01]\n",
      "El cliente  Escritura con probabilidad [0.0863293 0.9136707]\n",
      "El cliente  Escritura con probabilidad [0.25057632 0.74942368]\n",
      "El cliente  Escritura con probabilidad [1.30375747e-07 9.99999870e-01]\n",
      "El cliente  Escritura con probabilidad [0.42165058 0.57834942]\n",
      "El cliente  Escritura con probabilidad [0.2474302 0.7525698]\n",
      "El cliente  Escritura con probabilidad [0.21974434 0.78025566]\n",
      "El cliente  Escritura con probabilidad [4.87075378e-06 9.99995129e-01]\n",
      "El cliente  Escritura con probabilidad [0.0716847 0.9283153]\n",
      "El cliente  Escritura con probabilidad [0.10886238 0.89113762]\n",
      "El cliente  Escritura con probabilidad [0.06948688 0.93051312]\n",
      "El cliente  Escritura con probabilidad [0.17811024 0.82188976]\n",
      "El cliente  Escritura con probabilidad [4.60192132e-05 9.99953981e-01]\n",
      "El cliente  Escritura con probabilidad [1.11647781e-08 9.99999989e-01]\n",
      "El cliente  Escritura con probabilidad [0.25865907 0.74134093]\n",
      "El cliente  Escritura con probabilidad [0.19351416 0.80648584]\n",
      "El cliente  Escritura con probabilidad [0.22368412 0.77631588]\n",
      "El cliente  Escritura con probabilidad [3.99880361e-06 9.99996001e-01]\n",
      "El cliente  Escritura con probabilidad [2.84015317e-04 9.99715985e-01]\n",
      "El cliente  Escritura con probabilidad [0.12934477 0.87065523]\n",
      "El cliente  Escritura con probabilidad [0.0677063 0.9322937]\n",
      "El cliente  Escritura con probabilidad [0.04105837 0.95894163]\n",
      "El cliente  Escritura con probabilidad [0.08959188 0.91040812]\n",
      "El cliente  Escritura con probabilidad [8.27328319e-08 9.99999917e-01]\n",
      "El cliente  Escritura con probabilidad [0.20186626 0.79813374]\n",
      "El cliente  Escritura con probabilidad [0.02159785 0.97840215]\n",
      "El cliente  Escritura con probabilidad [0.01737525 0.98262475]\n",
      "El cliente  Escritura con probabilidad [0.06560697 0.93439303]\n",
      "El cliente  Escritura con probabilidad [2.43247470e-08 9.99999976e-01]\n",
      "El cliente  Escritura con probabilidad [6.22223039e-04 9.99377777e-01]\n",
      "El cliente  Escritura con probabilidad [1.56367945e-08 9.99999984e-01]\n",
      "El cliente  Escritura con probabilidad [0.01815273 0.98184727]\n",
      "El cliente  Escritura con probabilidad [0.11455842 0.88544158]\n",
      "El cliente  Escritura con probabilidad [0.02622241 0.97377759]\n",
      "El cliente  Escritura con probabilidad [0.0942757 0.9057243]\n",
      "El cliente  Escritura con probabilidad [2.99121816e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.0580829 0.9419171]\n",
      "El cliente  Escritura con probabilidad [0.01620548 0.98379452]\n",
      "El cliente  Escritura con probabilidad [1.63860586e-04 9.99836139e-01]\n",
      "El cliente  Escritura con probabilidad [0.01515136 0.98484864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.47380258 0.52619742]\n",
      "El cliente  Escritura con probabilidad [0.22066175 0.77933825]\n",
      "El cliente  Escritura con probabilidad [0.0933558 0.9066442]\n",
      "El cliente  Escritura con probabilidad [0.1123765 0.8876235]\n",
      "El cliente  Escritura con probabilidad [0.15536239 0.84463761]\n",
      "El cliente  Escritura con probabilidad [0.21262171 0.78737829]\n",
      "El cliente  Escritura con probabilidad [0.03006923 0.96993077]\n",
      "El cliente  Escritura con probabilidad [8.88487794e-04 9.99111512e-01]\n",
      "El cliente  Escritura con probabilidad [0.01570771 0.98429229]\n",
      "El cliente  Escritura con probabilidad [0.08433371 0.91566629]\n",
      "El cliente  Escritura con probabilidad [1.67941554e-04 9.99832058e-01]\n",
      "El cliente  Escritura con probabilidad [0.02297847 0.97702153]\n",
      "El cliente  Escritura con probabilidad [0.21680691 0.78319309]\n",
      "El cliente  Escritura con probabilidad [0.21109469 0.78890531]\n",
      "El cliente  Escritura con probabilidad [0.20723063 0.79276937]\n",
      "El cliente  Escritura con probabilidad [0.24006297 0.75993703]\n",
      "El cliente  Escritura con probabilidad [0.1041096 0.8958904]\n",
      "El cliente  Escritura con probabilidad [0.02321509 0.97678491]\n",
      "El cliente  Escritura con probabilidad [0.22475618 0.77524382]\n",
      "El cliente  Escritura con probabilidad [0.22108839 0.77891161]\n",
      "El cliente  Escritura con probabilidad [0.1741247 0.8258753]\n",
      "El cliente  Escritura con probabilidad [0.19688542 0.80311458]\n",
      "El cliente  Escritura con probabilidad [0.05774564 0.94225436]\n",
      "El cliente  Escritura con probabilidad [0.20301302 0.79698698]\n",
      "El cliente  Escritura con probabilidad [0.17193218 0.82806782]\n",
      "El cliente  Escritura con probabilidad [0.18950697 0.81049303]\n",
      "El cliente  Escritura con probabilidad [0.19483695 0.80516305]\n",
      "El cliente  Escritura con probabilidad [0.1872645 0.8127355]\n",
      "El cliente  Escritura con probabilidad [0.02423392 0.97576608]\n",
      "El cliente  Escritura con probabilidad [0.30347015 0.69652985]\n",
      "El cliente  Escritura con probabilidad [0.15885105 0.84114895]\n",
      "El cliente  Escritura con probabilidad [0.17301705 0.82698295]\n",
      "El cliente  Escritura con probabilidad [0.31720552 0.68279448]\n",
      "El cliente  Escritura con probabilidad [0.26374035 0.73625965]\n",
      "El cliente  Escritura con probabilidad [4.00045868e-04 9.99599954e-01]\n",
      "El cliente  Escritura con probabilidad [4.01397135e-04 9.99598603e-01]\n",
      "El cliente  Escritura con probabilidad [0.02625239 0.97374761]\n",
      "El cliente  Escritura con probabilidad [0.18059228 0.81940772]\n",
      "El cliente  Escritura con probabilidad [0.28526744 0.71473256]\n",
      "El cliente  Escritura con probabilidad [0.01847647 0.98152353]\n",
      "El cliente  Escritura con probabilidad [0.27016438 0.72983562]\n",
      "El cliente  Escritura con probabilidad [0.03627067 0.96372933]\n",
      "El cliente  Escritura con probabilidad [0.19856839 0.80143161]\n",
      "El cliente  Escritura con probabilidad [0.14302271 0.85697729]\n",
      "El cliente  Escritura con probabilidad [0.00772739 0.99227261]\n",
      "El cliente  Escritura con probabilidad [1.49257673e-08 9.99999985e-01]\n",
      "El cliente  Escritura con probabilidad [0.15893206 0.84106794]\n",
      "El cliente  Escritura con probabilidad [0.44568746 0.55431254]\n",
      "El cliente  Escritura con probabilidad [0.21124216 0.78875784]\n",
      "El cliente  Escritura con probabilidad [0.09521546 0.90478454]\n",
      "El cliente  Escritura con probabilidad [0.01931859 0.98068141]\n",
      "El cliente  Escritura con probabilidad [0.03595995 0.96404005]\n",
      "El cliente  Escritura con probabilidad [0.15943064 0.84056936]\n",
      "El cliente  Escritura con probabilidad [0.05916341 0.94083659]\n",
      "El cliente  Escritura con probabilidad [0.43256445 0.56743555]\n",
      "El cliente  Escritura con probabilidad [0.22327007 0.77672993]\n",
      "El cliente  Escritura con probabilidad [0.17328865 0.82671135]\n",
      "El cliente  Escritura con probabilidad [0.18934867 0.81065133]\n",
      "El cliente  Escritura con probabilidad [0.01582776 0.98417224]\n",
      "El cliente  Escritura con probabilidad [0.24476688 0.75523312]\n",
      "El cliente  Escritura con probabilidad [0.14931259 0.85068741]\n",
      "El cliente  Escritura con probabilidad [0.06638501 0.93361499]\n",
      "El cliente  Escritura con probabilidad [0.21952758 0.78047242]\n",
      "El cliente  Escritura con probabilidad [1.37473063e-05 9.99986253e-01]\n",
      "El cliente  Escritura con probabilidad [0.04934151 0.95065849]\n",
      "El cliente  Escritura con probabilidad [0.03813551 0.96186449]\n",
      "El cliente  Escritura con probabilidad [0.18507515 0.81492485]\n",
      "El cliente  Escritura con probabilidad [0.05993501 0.94006499]\n",
      "El cliente  Escritura con probabilidad [0.02963754 0.97036246]\n",
      "El cliente  Escritura con probabilidad [0.23332541 0.76667459]\n",
      "El cliente  Escritura con probabilidad [0.06728723 0.93271277]\n",
      "El cliente  Escritura con probabilidad [0.00839979 0.99160021]\n",
      "El cliente  Escritura con probabilidad [4.37713743e-07 9.99999562e-01]\n",
      "El cliente  Escritura con probabilidad [0.17129486 0.82870514]\n",
      "El cliente  Escritura con probabilidad [3.12140668e-04 9.99687859e-01]\n",
      "El cliente  Escritura con probabilidad [0.22993838 0.77006162]\n",
      "El cliente  Escritura con probabilidad [0.00249715 0.99750285]\n",
      "El cliente  Escritura con probabilidad [1.85037264e-04 9.99814963e-01]\n",
      "El cliente  Escritura con probabilidad [0.10097939 0.89902061]\n",
      "El cliente  Escritura con probabilidad [0.10894524 0.89105476]\n",
      "El cliente  Escritura con probabilidad [6.85405078e-07 9.99999315e-01]\n",
      "El cliente  Escritura con probabilidad [4.78820717e-05 9.99952118e-01]\n",
      "El cliente  Escritura con probabilidad [1.18052340e-05 9.99988195e-01]\n",
      "El cliente  Escritura con probabilidad [0.06787476 0.93212524]\n",
      "El cliente  Escritura con probabilidad [0.00272482 0.99727518]\n",
      "El cliente  Escritura con probabilidad [0.07737792 0.92262208]\n",
      "El cliente  Escritura con probabilidad [0.04020596 0.95979404]\n",
      "El cliente  Escritura con probabilidad [0.16335101 0.83664899]\n",
      "El cliente  Escritura con probabilidad [0.00439904 0.99560096]\n",
      "El cliente  Escritura con probabilidad [0.2073212 0.7926788]\n",
      "El cliente  Escritura con probabilidad [0.15651786 0.84348214]\n",
      "El cliente  Escritura con probabilidad [0.12786385 0.87213615]\n",
      "El cliente  Escritura con probabilidad [6.26884441e-05 9.99937312e-01]\n",
      "El cliente  Escritura con probabilidad [0.08573364 0.91426636]\n",
      "El cliente  Escritura con probabilidad [0.09900092 0.90099908]\n",
      "El cliente  Escritura con probabilidad [1.76503256e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17387225 0.82612775]\n",
      "El cliente  Escritura con probabilidad [0.01363301 0.98636699]\n",
      "El cliente  Escritura con probabilidad [0.07620531 0.92379469]\n",
      "El cliente  Escritura con probabilidad [0.00448194 0.99551806]\n",
      "El cliente  Escritura con probabilidad [0.08572257 0.91427743]\n",
      "El cliente  Escritura con probabilidad [0.32306821 0.67693179]\n",
      "El cliente  Escritura con probabilidad [0.39164107 0.60835893]\n",
      "El cliente  Escritura con probabilidad [0.02409349 0.97590651]\n",
      "El cliente  Escritura con probabilidad [0.30764223 0.69235777]\n",
      "El cliente  Escritura con probabilidad [0.0925918 0.9074082]\n",
      "El cliente  Escritura con probabilidad [0.36954244 0.63045756]\n",
      "El cliente  Escritura con probabilidad [0.36169541 0.63830459]\n",
      "El cliente  Escritura con probabilidad [0.11976663 0.88023337]\n",
      "El cliente  Escritura con probabilidad [0.05602191 0.94397809]\n",
      "El cliente  Escritura con probabilidad [0.04206444 0.95793556]\n",
      "El cliente  Escritura con probabilidad [0.13546146 0.86453854]\n",
      "El cliente  Escritura con probabilidad [0.20110806 0.79889194]\n",
      "El cliente  Escritura con probabilidad [0.08125177 0.91874823]\n",
      "El cliente  Escritura con probabilidad [0.08960682 0.91039318]\n",
      "El cliente  Escritura con probabilidad [0.02326189 0.97673811]\n",
      "El cliente  Escritura con probabilidad [0.18156677 0.81843323]\n",
      "El cliente  Escritura con probabilidad [0.2384487 0.7615513]\n",
      "El cliente  Escritura con probabilidad [0.18162003 0.81837997]\n",
      "El cliente  Escritura con probabilidad [1.52058204e-04 9.99847942e-01]\n",
      "El cliente  Escritura con probabilidad [5.67765834e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.10901744 0.89098256]\n",
      "El cliente  Escritura con probabilidad [0.28579337 0.71420663]\n",
      "El cliente  Escritura con probabilidad [0.01296033 0.98703967]\n",
      "El cliente  Escritura con probabilidad [0.0572212 0.9427788]\n",
      "El cliente  Escritura con probabilidad [6.58206822e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.24980453 0.75019547]\n",
      "El cliente  Escritura con probabilidad [0.22256447 0.77743553]\n",
      "El cliente  Escritura con probabilidad [0.20853023 0.79146977]\n",
      "El cliente  Escritura con probabilidad [0.08395225 0.91604775]\n",
      "El cliente  Escritura con probabilidad [0.44524165 0.55475835]\n",
      "El cliente  Escritura con probabilidad [0.11228998 0.88771002]\n",
      "El cliente  Escritura con probabilidad [0.29614765 0.70385235]\n",
      "El cliente  Escritura con probabilidad [0.0533467 0.9466533]\n",
      "El cliente  Escritura con probabilidad [0.07313131 0.92686869]\n",
      "El cliente  Escritura con probabilidad [0.20268329 0.79731671]\n",
      "El cliente  Escritura con probabilidad [1.31495411e-07 9.99999869e-01]\n",
      "El cliente  Escritura con probabilidad [0.100559 0.899441]\n",
      "El cliente  Escritura con probabilidad [0.08015828 0.91984172]\n",
      "El cliente  Escritura con probabilidad [0.1723446 0.8276554]\n",
      "El cliente  Escritura con probabilidad [0.00122773 0.99877227]\n",
      "El cliente  Escritura con probabilidad [0.25495837 0.74504163]\n",
      "El cliente  Escritura con probabilidad [0.27824631 0.72175369]\n",
      "El cliente  Escritura con probabilidad [0.24352847 0.75647153]\n",
      "El cliente  Escritura con probabilidad [0.02832285 0.97167715]\n",
      "El cliente  Escritura con probabilidad [0.00868677 0.99131323]\n",
      "El cliente  Escritura con probabilidad [0.0067101 0.9932899]\n",
      "El cliente  Escritura con probabilidad [0.26284184 0.73715816]\n",
      "El cliente  Escritura con probabilidad [0.24239519 0.75760481]\n",
      "El cliente  Escritura con probabilidad [0.21257686 0.78742314]\n",
      "El cliente  Escritura con probabilidad [0.07326646 0.92673354]\n",
      "El cliente  Escritura con probabilidad [0.02137401 0.97862599]\n",
      "El cliente  Escritura con probabilidad [0.16966047 0.83033953]\n",
      "El cliente  Escritura con probabilidad [0.25434229 0.74565771]\n",
      "El cliente  Escritura con probabilidad [0.29156637 0.70843363]\n",
      "El cliente  Escritura con probabilidad [2.91320602e-04 9.99708679e-01]\n",
      "El cliente  Escritura con probabilidad [1.14257074e-06 9.99998857e-01]\n",
      "El cliente  Escritura con probabilidad [0.22720472 0.77279528]\n",
      "El cliente  Escritura con probabilidad [0.15987347 0.84012653]\n",
      "El cliente  Escritura con probabilidad [0.25166576 0.74833424]\n",
      "El cliente  Escritura con probabilidad [0.02247242 0.97752758]\n",
      "El cliente  Escritura con probabilidad [0.25221334 0.74778666]\n",
      "El cliente  Escritura con probabilidad [0.28631339 0.71368661]\n",
      "El cliente  Escritura con probabilidad [0.28316468 0.71683532]\n",
      "El cliente  Escritura con probabilidad [0.08865386 0.91134614]\n",
      "El cliente  Escritura con probabilidad [0.11463894 0.88536106]\n",
      "El cliente  Escritura con probabilidad [0.18538432 0.81461568]\n",
      "El cliente  Escritura con probabilidad [0.07902949 0.92097051]\n",
      "El cliente  Escritura con probabilidad [0.02266436 0.97733564]\n",
      "El cliente  Escritura con probabilidad [0.03763916 0.96236084]\n",
      "El cliente  Escritura con probabilidad [0.27533042 0.72466958]\n",
      "El cliente  Escritura con probabilidad [0.13523199 0.86476801]\n",
      "El cliente  Escritura con probabilidad [2.32112033e-06 9.99997679e-01]\n",
      "El cliente  Escritura con probabilidad [0.23058639 0.76941361]\n",
      "El cliente  Escritura con probabilidad [0.00477915 0.99522085]\n",
      "El cliente  Escritura con probabilidad [0.20805693 0.79194307]\n",
      "El cliente  Escritura con probabilidad [0.01567624 0.98432376]\n",
      "El cliente  Escritura con probabilidad [0.01751692 0.98248308]\n",
      "El cliente  Escritura con probabilidad [0.20435991 0.79564009]\n",
      "El cliente  Escritura con probabilidad [0.16887766 0.83112234]\n",
      "El cliente  Escritura con probabilidad [0.04988574 0.95011426]\n",
      "El cliente  Escritura con probabilidad [0.12906547 0.87093453]\n",
      "El cliente  Escritura con probabilidad [0.06195673 0.93804327]\n",
      "El cliente  Escritura con probabilidad [0.03019408 0.96980592]\n",
      "El cliente  Escritura con probabilidad [0.06370809 0.93629191]\n",
      "El cliente  Escritura con probabilidad [0.19180119 0.80819881]\n",
      "El cliente  Escritura con probabilidad [0.1211716 0.8788284]\n",
      "El cliente  Escritura con probabilidad [0.01848251 0.98151749]\n",
      "El cliente  Escritura con probabilidad [0.31494983 0.68505017]\n",
      "El cliente  Escritura con probabilidad [0.09185142 0.90814858]\n",
      "El cliente  Escritura con probabilidad [0.10802836 0.89197164]\n",
      "El cliente  Escritura con probabilidad [0.03709732 0.96290268]\n",
      "El cliente  Escritura con probabilidad [0.44006256 0.55993744]\n",
      "El cliente  Escritura con probabilidad [0.06675751 0.93324249]\n",
      "El cliente  Escritura con probabilidad [3.42528817e-05 9.99965747e-01]\n",
      "El cliente  Escritura con probabilidad [0.23210008 0.76789992]\n",
      "El cliente  Escritura con probabilidad [0.11941965 0.88058035]\n",
      "El cliente  Escritura con probabilidad [0.04749266 0.95250734]\n",
      "El cliente  Escritura con probabilidad [1.52243536e-08 9.99999985e-01]\n",
      "El cliente  Escritura con probabilidad [0.09148238 0.90851762]\n",
      "El cliente  Escritura con probabilidad [0.21480699 0.78519301]\n",
      "El cliente  Escritura con probabilidad [0.08036261 0.91963739]\n",
      "El cliente  Escritura con probabilidad [0.19074163 0.80925837]\n",
      "El cliente  Escritura con probabilidad [0.17532329 0.82467671]\n",
      "El cliente  Escritura con probabilidad [0.00457954 0.99542046]\n",
      "El cliente  Escritura con probabilidad [0.14239612 0.85760388]\n",
      "El cliente  Escritura con probabilidad [0.00573976 0.99426024]\n",
      "El cliente  Escritura con probabilidad [5.76220384e-04 9.99423780e-01]\n",
      "El cliente  Escritura con probabilidad [0.21882841 0.78117159]\n",
      "El cliente  Escritura con probabilidad [0.27991198 0.72008802]\n",
      "El cliente  Escritura con probabilidad [1.12286928e-06 9.99998877e-01]\n",
      "El cliente  Escritura con probabilidad [1.20858844e-08 9.99999988e-01]\n",
      "El cliente  Escritura con probabilidad [0.16046991 0.83953009]\n",
      "El cliente  Escritura con probabilidad [0.28093249 0.71906751]\n",
      "El cliente  Escritura con probabilidad [0.05110047 0.94889953]\n",
      "El cliente  Escritura con probabilidad [0.08881731 0.91118269]\n",
      "El cliente  Escritura con probabilidad [0.31491897 0.68508103]\n",
      "El cliente  Escritura con probabilidad [0.04416699 0.95583301]\n",
      "El cliente  Escritura con probabilidad [0.13987457 0.86012543]\n",
      "El cliente  Escritura con probabilidad [0.05479547 0.94520453]\n",
      "El cliente  Escritura con probabilidad [0.07014402 0.92985598]\n",
      "El cliente  Escritura con probabilidad [0.15335785 0.84664215]\n",
      "El cliente  Escritura con probabilidad [0.10118462 0.89881538]\n",
      "El cliente  Escritura con probabilidad [0.02605597 0.97394403]\n",
      "El cliente  Escritura con probabilidad [0.1517361 0.8482639]\n",
      "El cliente  Escritura con probabilidad [0.15744859 0.84255141]\n",
      "El cliente  Escritura con probabilidad [0.11466557 0.88533443]\n",
      "El cliente  Escritura con probabilidad [3.88021633e-08 9.99999961e-01]\n",
      "El cliente  Escritura con probabilidad [0.44482641 0.55517359]\n",
      "El cliente  Escritura con probabilidad [0.15962109 0.84037891]\n",
      "El cliente  Escritura con probabilidad [0.1622453 0.8377547]\n",
      "El cliente  Escritura con probabilidad [0.13841041 0.86158959]\n",
      "El cliente  Escritura con probabilidad [0.44360032 0.55639968]\n",
      "El cliente  Escritura con probabilidad [0.01951428 0.98048572]\n",
      "El cliente  Escritura con probabilidad [2.26010322e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.24337088 0.75662912]\n",
      "El cliente  Escritura con probabilidad [0.1005012 0.8994988]\n",
      "El cliente  Escritura con probabilidad [0.13542277 0.86457723]\n",
      "El cliente  Escritura con probabilidad [0.01575933 0.98424067]\n",
      "El cliente  Escritura con probabilidad [0.01336756 0.98663244]\n",
      "El cliente  Escritura con probabilidad [0.23640579 0.76359421]\n",
      "El cliente  Escritura con probabilidad [0.00877179 0.99122821]\n",
      "El cliente  Escritura con probabilidad [0.141576 0.858424]\n",
      "El cliente  Escritura con probabilidad [0.08567843 0.91432157]\n",
      "El cliente  Escritura con probabilidad [0.16469916 0.83530084]\n",
      "El cliente  Escritura con probabilidad [0.10176001 0.89823999]\n",
      "El cliente  Escritura con probabilidad [0.09288381 0.90711619]\n",
      "El cliente  Escritura con probabilidad [0.20875749 0.79124251]\n",
      "El cliente  Escritura con probabilidad [0.10455066 0.89544934]\n",
      "El cliente  Escritura con probabilidad [0.07092853 0.92907147]\n",
      "El cliente  Escritura con probabilidad [0.06289569 0.93710431]\n",
      "El cliente  Escritura con probabilidad [0.20850523 0.79149477]\n",
      "El cliente  Escritura con probabilidad [0.20150518 0.79849482]\n",
      "El cliente  Escritura con probabilidad [0.07940706 0.92059294]\n",
      "El cliente  Escritura con probabilidad [0.02208671 0.97791329]\n",
      "El cliente  Escritura con probabilidad [0.02184267 0.97815733]\n",
      "El cliente  Escritura con probabilidad [0.1995492 0.8004508]\n",
      "El cliente  Escritura con probabilidad [3.47919022e-07 9.99999652e-01]\n",
      "El cliente  Escritura con probabilidad [0.11198268 0.88801732]\n",
      "El cliente  Escritura con probabilidad [0.01873005 0.98126995]\n",
      "El cliente  Escritura con probabilidad [0.03513347 0.96486653]\n",
      "El cliente  Escritura con probabilidad [0.07892816 0.92107184]\n",
      "El cliente  Escritura con probabilidad [1.48823498e-05 9.99985118e-01]\n",
      "El cliente  Escritura con probabilidad [0.08119334 0.91880666]\n",
      "El cliente  Escritura con probabilidad [0.06326473 0.93673527]\n",
      "El cliente  Escritura con probabilidad [0.14774076 0.85225924]\n",
      "El cliente  Escritura con probabilidad [0.23712269 0.76287731]\n",
      "El cliente  Escritura con probabilidad [0.01841151 0.98158849]\n",
      "El cliente  Escritura con probabilidad [0.18515842 0.81484158]\n",
      "El cliente  Escritura con probabilidad [0.26221372 0.73778628]\n",
      "El cliente  Escritura con probabilidad [3.58878061e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [4.22702659e-06 9.99995773e-01]\n",
      "El cliente  Escritura con probabilidad [0.22214612 0.77785388]\n",
      "El cliente  Escritura con probabilidad [0.14899475 0.85100525]\n",
      "El cliente  Escritura con probabilidad [6.18158502e-09 9.99999994e-01]\n",
      "El cliente  Escritura con probabilidad [0.02392114 0.97607886]\n",
      "El cliente  Escritura con probabilidad [0.21770181 0.78229819]\n",
      "El cliente  Escritura con probabilidad [0.11421738 0.88578262]\n",
      "El cliente  Escritura con probabilidad [2.54523112e-04 9.99745477e-01]\n",
      "El cliente  Escritura con probabilidad [7.16479849e-05 9.99928352e-01]\n",
      "El cliente  Escritura con probabilidad [0.15890808 0.84109192]\n",
      "El cliente  Escritura con probabilidad [0.16819723 0.83180277]\n",
      "El cliente  Escritura con probabilidad [7.27218952e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.17004356 0.82995644]\n",
      "El cliente  Escritura con probabilidad [0.02451061 0.97548939]\n",
      "El cliente  Escritura con probabilidad [0.04717641 0.95282359]\n",
      "El cliente  Escritura con probabilidad [0.12931595 0.87068405]\n",
      "El cliente  Escritura con probabilidad [1.46673251e-04 9.99853327e-01]\n",
      "El cliente  Escritura con probabilidad [0.24988708 0.75011292]\n",
      "El cliente  Escritura con probabilidad [0.02717157 0.97282843]\n",
      "El cliente  Escritura con probabilidad [2.26009913e-04 9.99773990e-01]\n",
      "El cliente  Escritura con probabilidad [0.23412251 0.76587749]\n",
      "El cliente  Escritura con probabilidad [0.1433888 0.8566112]\n",
      "El cliente  Escritura con probabilidad [0.12931586 0.87068414]\n",
      "El cliente  Escritura con probabilidad [0.18416437 0.81583563]\n",
      "El cliente  Escritura con probabilidad [0.22482331 0.77517669]\n",
      "El cliente  Escritura con probabilidad [0.01888964 0.98111036]\n",
      "El cliente  Escritura con probabilidad [0.17353037 0.82646963]\n",
      "El cliente  Escritura con probabilidad [0.09297181 0.90702819]\n",
      "El cliente  Escritura con probabilidad [2.23659091e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.08857227 0.91142773]\n",
      "El cliente  Escritura con probabilidad [0.08492084 0.91507916]\n",
      "El cliente  Escritura con probabilidad [1.94827180e-08 9.99999981e-01]\n",
      "El cliente  Escritura con probabilidad [0.22793282 0.77206718]\n",
      "El cliente  Escritura con probabilidad [0.10799119 0.89200881]\n",
      "El cliente  Escritura con probabilidad [3.92179917e-08 9.99999961e-01]\n",
      "El cliente  Escritura con probabilidad [0.21857339 0.78142661]\n",
      "El cliente  Escritura con probabilidad [1.01721372e-05 9.99989828e-01]\n",
      "El cliente  Escritura con probabilidad [0.1030943 0.8969057]\n",
      "El cliente  Escritura con probabilidad [3.99354527e-06 9.99996006e-01]\n",
      "El cliente  Escritura con probabilidad [0.24868039 0.75131961]\n",
      "El cliente  Escritura con probabilidad [0.12598647 0.87401353]\n",
      "El cliente  Escritura con probabilidad [0.14849212 0.85150788]\n",
      "El cliente  Escritura con probabilidad [0.23324456 0.76675544]\n",
      "El cliente  Escritura con probabilidad [8.52398496e-09 9.99999991e-01]\n",
      "El cliente  Escritura con probabilidad [4.44201651e-08 9.99999956e-01]\n",
      "El cliente  Escritura con probabilidad [0.13849633 0.86150367]\n",
      "El cliente  Escritura con probabilidad [9.64624600e-04 9.99035375e-01]\n",
      "El cliente  Escritura con probabilidad [3.53130310e-05 9.99964687e-01]\n",
      "El cliente  Escritura con probabilidad [0.0543785 0.9456215]\n",
      "El cliente  Escritura con probabilidad [0.21693264 0.78306736]\n",
      "El cliente  Escritura con probabilidad [0.20947611 0.79052389]\n",
      "El cliente  Escritura con probabilidad [0.19800033 0.80199967]\n",
      "El cliente  Escritura con probabilidad [0.14698411 0.85301589]\n",
      "El cliente  Escritura con probabilidad [2.21925040e-06 9.99997781e-01]\n",
      "El cliente  Escritura con probabilidad [0.439007 0.560993]\n",
      "El cliente  Escritura con probabilidad [0.10386727 0.89613273]\n",
      "El cliente  Escritura con probabilidad [0.09594255 0.90405745]\n",
      "El cliente  Escritura con probabilidad [8.18112527e-04 9.99181887e-01]\n",
      "El cliente  Escritura con probabilidad [0.20595086 0.79404914]\n",
      "El cliente  Escritura con probabilidad [0.09500727 0.90499273]\n",
      "El cliente  Escritura con probabilidad [0.1456978 0.8543022]\n",
      "El cliente  Escritura con probabilidad [0.01677592 0.98322408]\n",
      "El cliente  Escritura con probabilidad [0.14186458 0.85813542]\n",
      "El cliente  Escritura con probabilidad [0.04428021 0.95571979]\n",
      "El cliente  Escritura con probabilidad [0.28898915 0.71101085]\n",
      "El cliente  Escritura con probabilidad [0.20996741 0.79003259]\n",
      "El cliente  Escritura con probabilidad [0.14651551 0.85348449]\n",
      "El cliente  Escritura con probabilidad [0.10017085 0.89982915]\n",
      "El cliente  Escritura con probabilidad [4.74869657e-06 9.99995251e-01]\n",
      "El cliente  Escritura con probabilidad [0.06921433 0.93078567]\n",
      "El cliente  Escritura con probabilidad [0.06591098 0.93408902]\n",
      "El cliente  Escritura con probabilidad [5.10368237e-07 9.99999490e-01]\n",
      "El cliente  Escritura con probabilidad [0.30305287 0.69694713]\n",
      "El cliente  Escritura con probabilidad [0.15780188 0.84219812]\n",
      "El cliente  Escritura con probabilidad [0.19970077 0.80029923]\n",
      "El cliente  Escritura con probabilidad [0.20316601 0.79683399]\n",
      "El cliente  Escritura con probabilidad [0.10282066 0.89717934]\n",
      "El cliente  Escritura con probabilidad [0.0102303 0.9897697]\n",
      "El cliente  Escritura con probabilidad [0.21922243 0.78077757]\n",
      "El cliente  Escritura con probabilidad [0.14967769 0.85032231]\n",
      "El cliente  Escritura con probabilidad [0.17576461 0.82423539]\n",
      "El cliente  Escritura con probabilidad [0.0454609 0.9545391]\n",
      "El cliente  Escritura con probabilidad [0.05112304 0.94887696]\n",
      "El cliente  Escritura con probabilidad [0.03945335 0.96054665]\n",
      "El cliente  Escritura con probabilidad [0.06504885 0.93495115]\n",
      "El cliente  Escritura con probabilidad [0.20300514 0.79699486]\n",
      "El cliente  Escritura con probabilidad [0.06802809 0.93197191]\n",
      "El cliente  Escritura con probabilidad [1.19980448e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.01732421 0.98267579]\n",
      "El cliente  Escritura con probabilidad [0.16831765 0.83168235]\n",
      "El cliente  Escritura con probabilidad [0.0012577 0.9987423]\n",
      "El cliente  Escritura con probabilidad [0.27283711 0.72716289]\n",
      "El cliente  Escritura con probabilidad [0.1389118 0.8610882]\n",
      "El cliente  Escritura con probabilidad [0.01579956 0.98420044]\n",
      "El cliente  Escritura con probabilidad [0.02402849 0.97597151]\n",
      "El cliente  Escritura con probabilidad [0.04838297 0.95161703]\n",
      "El cliente  Escritura con probabilidad [5.57789952e-04 9.99442210e-01]\n",
      "El cliente  Escritura con probabilidad [0.24988293 0.75011707]\n",
      "El cliente  Escritura con probabilidad [0.02955022 0.97044978]\n",
      "El cliente  Escritura con probabilidad [0.21687244 0.78312756]\n",
      "El cliente  Escritura con probabilidad [0.22682449 0.77317551]\n",
      "El cliente  Escritura con probabilidad [0.1327927 0.8672073]\n",
      "El cliente  Escritura con probabilidad [4.11081704e-05 9.99958892e-01]\n",
      "El cliente  Escritura con probabilidad [0.10579 0.89421]\n",
      "El cliente  Escritura con probabilidad [0.04566142 0.95433858]\n",
      "El cliente  Escritura con probabilidad [0.01791399 0.98208601]\n",
      "El cliente  Escritura con probabilidad [0.31802159 0.68197841]\n",
      "El cliente  Escritura con probabilidad [0.06762519 0.93237481]\n",
      "El cliente  Escritura con probabilidad [0.15813344 0.84186656]\n",
      "El cliente  Escritura con probabilidad [0.01453919 0.98546081]\n",
      "El cliente  Escritura con probabilidad [0.26961744 0.73038256]\n",
      "El cliente  Escritura con probabilidad [0.21504892 0.78495108]\n",
      "El cliente  Escritura con probabilidad [0.02559335 0.97440665]\n",
      "El cliente  Escritura con probabilidad [0.00301918 0.99698082]\n",
      "El cliente  Escritura con probabilidad [0.10368739 0.89631261]\n",
      "El cliente  Escritura con probabilidad [0.13967427 0.86032573]\n",
      "El cliente  Escritura con probabilidad [0.25969965 0.74030035]\n",
      "El cliente  Escritura con probabilidad [0.00190217 0.99809783]\n",
      "El cliente  Escritura con probabilidad [0.17930692 0.82069308]\n",
      "El cliente  Escritura con probabilidad [8.22028231e-04 9.99177972e-01]\n",
      "El cliente  Escritura con probabilidad [0.09275674 0.90724326]\n",
      "El cliente  Escritura con probabilidad [0.16603712 0.83396288]\n",
      "El cliente  Escritura con probabilidad [0.19565829 0.80434171]\n",
      "El cliente  Escritura con probabilidad [0.30370634 0.69629366]\n",
      "El cliente  Escritura con probabilidad [0.1291929 0.8708071]\n",
      "El cliente  Escritura con probabilidad [0.06051479 0.93948521]\n",
      "El cliente  Escritura con probabilidad [0.03241028 0.96758972]\n",
      "El cliente  Escritura con probabilidad [0.09218802 0.90781198]\n",
      "El cliente  Escritura con probabilidad [2.95602953e-08 9.99999970e-01]\n",
      "El cliente  Escritura con probabilidad [0.19203371 0.80796629]\n",
      "El cliente  Escritura con probabilidad [0.01646082 0.98353918]\n",
      "El cliente  Escritura con probabilidad [0.08568971 0.91431029]\n",
      "El cliente  Escritura con probabilidad [4.18981139e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [4.12025969e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.25914522 0.74085478]\n",
      "El cliente  Escritura con probabilidad [0.05851233 0.94148767]\n",
      "El cliente  Escritura con probabilidad [0.13496281 0.86503719]\n",
      "El cliente  Escritura con probabilidad [0.2338331 0.7661669]\n",
      "El cliente  Escritura con probabilidad [0.2608416 0.7391584]\n",
      "El cliente  Escritura con probabilidad [0.03102559 0.96897441]\n",
      "El cliente  Escritura con probabilidad [0.1925705 0.8074295]\n",
      "El cliente  Escritura con probabilidad [9.38770189e-06 9.99990612e-01]\n",
      "El cliente  Escritura con probabilidad [0.22406329 0.77593671]\n",
      "El cliente  Escritura con probabilidad [0.25724533 0.74275467]\n",
      "El cliente  Escritura con probabilidad [0.00377059 0.99622941]\n",
      "El cliente  Escritura con probabilidad [0.20516442 0.79483558]\n",
      "El cliente  Escritura con probabilidad [0.05784439 0.94215561]\n",
      "El cliente  Escritura con probabilidad [0.01440482 0.98559518]\n",
      "El cliente  Escritura con probabilidad [0.04384435 0.95615565]\n",
      "El cliente  Escritura con probabilidad [0.27259127 0.72740873]\n",
      "El cliente  Escritura con probabilidad [0.14010875 0.85989125]\n",
      "El cliente  Escritura con probabilidad [0.04283801 0.95716199]\n",
      "El cliente  Escritura con probabilidad [3.53981833e-05 9.99964602e-01]\n",
      "El cliente  Escritura con probabilidad [0.03102728 0.96897272]\n",
      "El cliente  Escritura con probabilidad [0.05034199 0.94965801]\n",
      "El cliente  Escritura con probabilidad [0.23665491 0.76334509]\n",
      "El cliente  Escritura con probabilidad [0.23443224 0.76556776]\n",
      "El cliente  Escritura con probabilidad [0.27140941 0.72859059]\n",
      "El cliente  Escritura con probabilidad [0.26907154 0.73092846]\n",
      "El cliente  Escritura con probabilidad [0.1913575 0.8086425]\n",
      "El cliente  Escritura con probabilidad [0.05802413 0.94197587]\n",
      "El cliente  Escritura con probabilidad [0.09102652 0.90897348]\n",
      "El cliente  Escritura con probabilidad [1.52273464e-07 9.99999848e-01]\n",
      "El cliente  Escritura con probabilidad [0.01974745 0.98025255]\n",
      "El cliente  Escritura con probabilidad [3.04803946e-04 9.99695196e-01]\n",
      "El cliente  Escritura con probabilidad [4.76338674e-06 9.99995237e-01]\n",
      "El cliente  Escritura con probabilidad [6.37819301e-04 9.99362181e-01]\n",
      "El cliente  Escritura con probabilidad [0.13851244 0.86148756]\n",
      "El cliente  Escritura con probabilidad [0.04684475 0.95315525]\n",
      "El cliente  Escritura con probabilidad [0.08598036 0.91401964]\n",
      "El cliente  Escritura con probabilidad [0.0505032 0.9494968]\n",
      "El cliente  Escritura con probabilidad [0.10778552 0.89221448]\n",
      "El cliente  Escritura con probabilidad [0.27395839 0.72604161]\n",
      "El cliente  Escritura con probabilidad [0.17683662 0.82316338]\n",
      "El cliente  Escritura con probabilidad [0.20697762 0.79302238]\n",
      "El cliente  Escritura con probabilidad [0.13599477 0.86400523]\n",
      "El cliente  Escritura con probabilidad [0.08081766 0.91918234]\n",
      "El cliente  Escritura con probabilidad [3.45461193e-07 9.99999655e-01]\n",
      "El cliente  Escritura con probabilidad [2.09443799e-04 9.99790556e-01]\n",
      "El cliente  Escritura con probabilidad [0.17201738 0.82798262]\n",
      "El cliente  Escritura con probabilidad [0.02001524 0.97998476]\n",
      "El cliente  Escritura con probabilidad [0.17334404 0.82665596]\n",
      "El cliente  Escritura con probabilidad [2.09858551e-07 9.99999790e-01]\n",
      "El cliente  Escritura con probabilidad [0.22278081 0.77721919]\n",
      "El cliente  Escritura con probabilidad [0.18840233 0.81159767]\n",
      "El cliente  Escritura con probabilidad [0.15107717 0.84892283]\n",
      "El cliente  Escritura con probabilidad [0.05976015 0.94023985]\n",
      "El cliente  Escritura con probabilidad [2.52893915e-05 9.99974711e-01]\n",
      "El cliente  Escritura con probabilidad [0.14347779 0.85652221]\n",
      "El cliente  Escritura con probabilidad [0.14599336 0.85400664]\n",
      "El cliente  Escritura con probabilidad [0.09664633 0.90335367]\n",
      "El cliente  Escritura con probabilidad [0.23326224 0.76673776]\n",
      "El cliente  Escritura con probabilidad [0.07418985 0.92581015]\n",
      "El cliente  Escritura con probabilidad [0.19990273 0.80009727]\n",
      "El cliente  Escritura con probabilidad [0.22530861 0.77469139]\n",
      "El cliente  Escritura con probabilidad [0.15246095 0.84753905]\n",
      "El cliente  Escritura con probabilidad [0.02816681 0.97183319]\n",
      "El cliente  Escritura con probabilidad [0.06658573 0.93341427]\n",
      "El cliente  Escritura con probabilidad [0.15169445 0.84830555]\n",
      "El cliente  Escritura con probabilidad [0.07820966 0.92179034]\n",
      "El cliente  Escritura con probabilidad [0.01972835 0.98027165]\n",
      "El cliente  Escritura con probabilidad [0.29181984 0.70818016]\n",
      "El cliente  Escritura con probabilidad [0.12347366 0.87652634]\n",
      "El cliente  Escritura con probabilidad [0.02367784 0.97632216]\n",
      "El cliente  Escritura con probabilidad [0.01298563 0.98701437]\n",
      "El cliente  Escritura con probabilidad [0.0249674 0.9750326]\n",
      "El cliente  Escritura con probabilidad [0.17075601 0.82924399]\n",
      "El cliente  Escritura con probabilidad [0.01320403 0.98679597]\n",
      "El cliente  Escritura con probabilidad [0.45886885 0.54113115]\n",
      "El cliente  Escritura con probabilidad [0.08042313 0.91957687]\n",
      "El cliente  Escritura con probabilidad [0.11572811 0.88427189]\n",
      "El cliente  Escritura con probabilidad [0.16692346 0.83307654]\n",
      "El cliente  Escritura con probabilidad [0.02925354 0.97074646]\n",
      "El cliente  Escritura con probabilidad [0.16824788 0.83175212]\n",
      "El cliente  Escritura con probabilidad [6.87842742e-08 9.99999931e-01]\n",
      "El cliente  Escritura con probabilidad [0.00104613 0.99895387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.22384329 0.77615671]\n",
      "El cliente  Escritura con probabilidad [0.11995552 0.88004448]\n",
      "El cliente  Escritura con probabilidad [0.16682333 0.83317667]\n",
      "El cliente  Escritura con probabilidad [0.08855105 0.91144895]\n",
      "El cliente  Escritura con probabilidad [0.27589406 0.72410594]\n",
      "El cliente  Escritura con probabilidad [0.03307079 0.96692921]\n",
      "El cliente  Escritura con probabilidad [0.155453 0.844547]\n",
      "El cliente  Escritura con probabilidad [0.22899792 0.77100208]\n",
      "El cliente  Escritura con probabilidad [0.10690855 0.89309145]\n",
      "El cliente  Escritura con probabilidad [0.16493329 0.83506671]\n",
      "El cliente  Escritura con probabilidad [0.2663769 0.7336231]\n",
      "El cliente  Escritura con probabilidad [1.36779477e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.15224858 0.84775142]\n",
      "El cliente  Escritura con probabilidad [0.2079232 0.7920768]\n",
      "El cliente  Escritura con probabilidad [4.70433516e-04 9.99529566e-01]\n",
      "El cliente  Escritura con probabilidad [5.56007803e-07 9.99999444e-01]\n",
      "El cliente  Escritura con probabilidad [0.19698434 0.80301566]\n",
      "El cliente  Escritura con probabilidad [0.13881246 0.86118754]\n",
      "El cliente  Escritura con probabilidad [0.06795436 0.93204564]\n",
      "El cliente  Escritura con probabilidad [0.29015518 0.70984482]\n",
      "El cliente  Escritura con probabilidad [1.05575322e-04 9.99894425e-01]\n",
      "El cliente  Escritura con probabilidad [0.12812778 0.87187222]\n",
      "El cliente  Escritura con probabilidad [0.09678852 0.90321148]\n",
      "El cliente  Escritura con probabilidad [0.09914073 0.90085927]\n",
      "El cliente  Escritura con probabilidad [3.05709357e-04 9.99694291e-01]\n",
      "El cliente  Escritura con probabilidad [0.20216183 0.79783817]\n",
      "El cliente  Escritura con probabilidad [0.0182555 0.9817445]\n",
      "El cliente  Escritura con probabilidad [0.22402484 0.77597516]\n",
      "El cliente  Escritura con probabilidad [0.06557126 0.93442874]\n",
      "El cliente  Escritura con probabilidad [0.03486853 0.96513147]\n",
      "El cliente  Escritura con probabilidad [0.00866961 0.99133039]\n",
      "El cliente  Escritura con probabilidad [7.76418501e-05 9.99922358e-01]\n",
      "El cliente  Escritura con probabilidad [0.17696984 0.82303016]\n",
      "El cliente  Escritura con probabilidad [0.18510855 0.81489145]\n",
      "El cliente  Escritura con probabilidad [0.19028689 0.80971311]\n",
      "El cliente  Escritura con probabilidad [0.14328084 0.85671916]\n",
      "El cliente  Escritura con probabilidad [0.02130229 0.97869771]\n",
      "El cliente  Escritura con probabilidad [0.02479514 0.97520486]\n",
      "El cliente  Escritura con probabilidad [0.1753978 0.8246022]\n",
      "El cliente  Escritura con probabilidad [1.29563027e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.16574008 0.83425992]\n",
      "El cliente  Escritura con probabilidad [0.01731639 0.98268361]\n",
      "El cliente  Escritura con probabilidad [0.1759496 0.8240504]\n",
      "El cliente  Escritura con probabilidad [1.55119981e-08 9.99999984e-01]\n",
      "El cliente  Escritura con probabilidad [0.06940567 0.93059433]\n",
      "El cliente  Escritura con probabilidad [0.28463583 0.71536417]\n",
      "El cliente  Escritura con probabilidad [0.13609021 0.86390979]\n",
      "El cliente  Escritura con probabilidad [4.40681876e-05 9.99955932e-01]\n",
      "El cliente  Escritura con probabilidad [0.22691023 0.77308977]\n",
      "El cliente  Escritura con probabilidad [0.01348282 0.98651718]\n",
      "El cliente  Escritura con probabilidad [0.23214952 0.76785048]\n",
      "El cliente  Escritura con probabilidad [0.39387008 0.60612992]\n",
      "El cliente  Escritura con probabilidad [0.00124827 0.99875173]\n",
      "El cliente  Escritura con probabilidad [0.17102573 0.82897427]\n",
      "El cliente  Escritura con probabilidad [0.0319344 0.9680656]\n",
      "El cliente  Escritura con probabilidad [2.52127887e-07 9.99999748e-01]\n",
      "El cliente  Escritura con probabilidad [3.02643090e-06 9.99996974e-01]\n",
      "El cliente  Escritura con probabilidad [0.08132301 0.91867699]\n",
      "El cliente  Escritura con probabilidad [1.88158820e-05 9.99981184e-01]\n",
      "El cliente  Escritura con probabilidad [0.01818925 0.98181075]\n",
      "El cliente  Escritura con probabilidad [0.01957134 0.98042866]\n",
      "El cliente  Escritura con probabilidad [0.11346146 0.88653854]\n",
      "El cliente  Escritura con probabilidad [0.00346188 0.99653812]\n",
      "El cliente  Escritura con probabilidad [0.09083499 0.90916501]\n",
      "El cliente  Escritura con probabilidad [0.42502822 0.57497178]\n",
      "El cliente  Escritura con probabilidad [0.11352003 0.88647997]\n",
      "El cliente  Escritura con probabilidad [2.23181797e-07 9.99999777e-01]\n",
      "El cliente  Escritura con probabilidad [0.05501335 0.94498665]\n",
      "El cliente  Escritura con probabilidad [0.10726342 0.89273658]\n",
      "El cliente  Escritura con probabilidad [0.01821307 0.98178693]\n",
      "El cliente  Escritura con probabilidad [1.22781699e-04 9.99877218e-01]\n",
      "El cliente  Escritura con probabilidad [0.02266305 0.97733695]\n",
      "El cliente  Escritura con probabilidad [0.25190389 0.74809611]\n",
      "El cliente  Escritura con probabilidad [0.16100124 0.83899876]\n",
      "El cliente  Escritura con probabilidad [0.17115298 0.82884702]\n",
      "El cliente  Escritura con probabilidad [2.04610247e-06 9.99997954e-01]\n",
      "El cliente  Escritura con probabilidad [8.25435276e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.18479902 0.81520098]\n",
      "El cliente  Escritura con probabilidad [0.25029612 0.74970388]\n",
      "El cliente  Escritura con probabilidad [0.18910763 0.81089237]\n",
      "El cliente  Escritura con probabilidad [0.15925697 0.84074303]\n",
      "El cliente  Escritura con probabilidad [2.78966949e-05 9.99972103e-01]\n",
      "El cliente  Escritura con probabilidad [0.17898472 0.82101528]\n",
      "El cliente  Escritura con probabilidad [0.14173087 0.85826913]\n",
      "El cliente  Escritura con probabilidad [0.26123768 0.73876232]\n",
      "El cliente  Escritura con probabilidad [0.11354204 0.88645796]\n",
      "El cliente  Escritura con probabilidad [0.06787482 0.93212518]\n",
      "El cliente  Escritura con probabilidad [0.18067197 0.81932803]\n",
      "El cliente  Escritura con probabilidad [0.00633335 0.99366665]\n",
      "El cliente  Escritura con probabilidad [0.13904775 0.86095225]\n",
      "El cliente  Escritura con probabilidad [0.28712307 0.71287693]\n",
      "El cliente  Escritura con probabilidad [0.25483271 0.74516729]\n",
      "El cliente  Escritura con probabilidad [0.02764961 0.97235039]\n",
      "El cliente  Escritura con probabilidad [0.02256754 0.97743246]\n",
      "El cliente  Escritura con probabilidad [2.41756754e-04 9.99758243e-01]\n",
      "El cliente  Escritura con probabilidad [2.47386011e-07 9.99999753e-01]\n",
      "El cliente  Escritura con probabilidad [0.1129259 0.8870741]\n",
      "El cliente  Escritura con probabilidad [2.30704345e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.15401561 0.84598439]\n",
      "El cliente  Escritura con probabilidad [0.2047071 0.7952929]\n",
      "El cliente  Escritura con probabilidad [0.11729142 0.88270858]\n",
      "El cliente  Escritura con probabilidad [0.18693241 0.81306759]\n",
      "El cliente  Escritura con probabilidad [0.2408255 0.7591745]\n",
      "El cliente  Escritura con probabilidad [0.13321543 0.86678457]\n",
      "El cliente  Escritura con probabilidad [0.08804892 0.91195108]\n",
      "El cliente  Escritura con probabilidad [0.11114745 0.88885255]\n",
      "El cliente  Escritura con probabilidad [0.18198915 0.81801085]\n",
      "El cliente  Escritura con probabilidad [0.01570033 0.98429967]\n",
      "El cliente  Escritura con probabilidad [6.6404549e-06 9.9999336e-01]\n",
      "El cliente  Escritura con probabilidad [0.1377513 0.8622487]\n",
      "El cliente  Escritura con probabilidad [0.1421148 0.8578852]\n",
      "El cliente  Escritura con probabilidad [0.22398455 0.77601545]\n",
      "El cliente  Escritura con probabilidad [0.24887574 0.75112426]\n",
      "El cliente  Escritura con probabilidad [0.22636952 0.77363048]\n",
      "El cliente  Escritura con probabilidad [0.24901433 0.75098567]\n",
      "El cliente  Escritura con probabilidad [0.09548573 0.90451427]\n",
      "El cliente  Escritura con probabilidad [7.06489275e-05 9.99929351e-01]\n",
      "El cliente  Escritura con probabilidad [0.30231866 0.69768134]\n",
      "El cliente  Escritura con probabilidad [0.20068923 0.79931077]\n",
      "El cliente  Escritura con probabilidad [0.1986831 0.8013169]\n",
      "El cliente  Escritura con probabilidad [0.13657971 0.86342029]\n",
      "El cliente  Escritura con probabilidad [3.00008932e-06 9.99997000e-01]\n",
      "El cliente  Escritura con probabilidad [0.10991984 0.89008016]\n",
      "El cliente  Escritura con probabilidad [0.0209552 0.9790448]\n",
      "El cliente  Escritura con probabilidad [2.84217094e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.08099151 0.91900849]\n",
      "El cliente  Escritura con probabilidad [0.01500547 0.98499453]\n",
      "El cliente  Escritura con probabilidad [0.2931448 0.7068552]\n",
      "El cliente  Escritura con probabilidad [4.35659397e-07 9.99999564e-01]\n",
      "El cliente  Escritura con probabilidad [0.05591917 0.94408083]\n",
      "El cliente  Escritura con probabilidad [0.22900151 0.77099849]\n",
      "El cliente  Escritura con probabilidad [0.08089543 0.91910457]\n",
      "El cliente  Escritura con probabilidad [0.06155177 0.93844823]\n",
      "El cliente  Escritura con probabilidad [0.00196796 0.99803204]\n",
      "El cliente  Escritura con probabilidad [0.02631254 0.97368746]\n",
      "El cliente  Escritura con probabilidad [0.23750862 0.76249138]\n",
      "El cliente  Escritura con probabilidad [0.11691653 0.88308347]\n",
      "El cliente  Escritura con probabilidad [0.06887074 0.93112926]\n",
      "El cliente  Escritura con probabilidad [0.17614007 0.82385993]\n",
      "El cliente  Escritura con probabilidad [0.26811867 0.73188133]\n",
      "El cliente  Escritura con probabilidad [0.00585448 0.99414552]\n",
      "El cliente  Escritura con probabilidad [0.14301408 0.85698592]\n",
      "El cliente  Escritura con probabilidad [0.05761097 0.94238903]\n",
      "El cliente  Escritura con probabilidad [0.13583998 0.86416002]\n",
      "El cliente  Escritura con probabilidad [0.16422866 0.83577134]\n",
      "El cliente  Escritura con probabilidad [4.4408921e-16 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.23792392 0.76207608]\n",
      "El cliente  Escritura con probabilidad [0.11350663 0.88649337]\n",
      "El cliente  Escritura con probabilidad [0.17915094 0.82084906]\n",
      "El cliente  Escritura con probabilidad [0.05289687 0.94710313]\n",
      "El cliente  Escritura con probabilidad [2.01021456e-06 9.99997990e-01]\n",
      "El cliente  Escritura con probabilidad [0.01605221 0.98394779]\n",
      "El cliente  Escritura con probabilidad [0.03867931 0.96132069]\n",
      "El cliente  Escritura con probabilidad [0.06265348 0.93734652]\n",
      "El cliente  Escritura con probabilidad [0.00529208 0.99470792]\n",
      "El cliente  Escritura con probabilidad [0.18860799 0.81139201]\n",
      "El cliente  Escritura con probabilidad [1.57483804e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17887679 0.82112321]\n",
      "El cliente  Escritura con probabilidad [0.01057607 0.98942393]\n",
      "El cliente  Escritura con probabilidad [0.05785486 0.94214514]\n",
      "El cliente  Escritura con probabilidad [0.00268919 0.99731081]\n",
      "El cliente  Escritura con probabilidad [0.16502085 0.83497915]\n",
      "El cliente  Escritura con probabilidad [0.0403129 0.9596871]\n",
      "El cliente  Escritura con probabilidad [0.18912312 0.81087688]\n",
      "El cliente  Escritura con probabilidad [2.16545010e-06 9.99997835e-01]\n",
      "El cliente  Escritura con probabilidad [0.17748718 0.82251282]\n",
      "El cliente  Escritura con probabilidad [0.16404335 0.83595665]\n",
      "El cliente  Escritura con probabilidad [0.18921266 0.81078734]\n",
      "El cliente  Escritura con probabilidad [0.17909347 0.82090653]\n",
      "El cliente  Escritura con probabilidad [0.07796891 0.92203109]\n",
      "El cliente  Escritura con probabilidad [0.24292194 0.75707806]\n",
      "El cliente  Escritura con probabilidad [0.04421527 0.95578473]\n",
      "El cliente  Escritura con probabilidad [2.35341821e-05 9.99976466e-01]\n",
      "El cliente  Escritura con probabilidad [0.1924197 0.8075803]\n",
      "El cliente  Escritura con probabilidad [3.50574326e-08 9.99999965e-01]\n",
      "El cliente  Escritura con probabilidad [0.15896061 0.84103939]\n",
      "El cliente  Escritura con probabilidad [1.87574851e-07 9.99999812e-01]\n",
      "El cliente  Escritura con probabilidad [0.02942124 0.97057876]\n",
      "El cliente  Escritura con probabilidad [0.03220411 0.96779589]\n",
      "El cliente  Escritura con probabilidad [0.30590321 0.69409679]\n",
      "El cliente  Escritura con probabilidad [0.04671149 0.95328851]\n",
      "El cliente  Escritura con probabilidad [0.15216454 0.84783546]\n",
      "El cliente  Escritura con probabilidad [0.06888523 0.93111477]\n",
      "El cliente  Escritura con probabilidad [0.18346872 0.81653128]\n",
      "El cliente  Escritura con probabilidad [3.19349669e-06 9.99996807e-01]\n",
      "El cliente  Escritura con probabilidad [0.43099635 0.56900365]\n",
      "El cliente  Escritura con probabilidad [0.16197388 0.83802612]\n",
      "El cliente  Escritura con probabilidad [0.09415907 0.90584093]\n",
      "El cliente  Escritura con probabilidad [4.30491145e-05 9.99956951e-01]\n",
      "El cliente  Escritura con probabilidad [0.02162368 0.97837632]\n",
      "El cliente  Escritura con probabilidad [0.14318309 0.85681691]\n",
      "El cliente  Escritura con probabilidad [0.05984315 0.94015685]\n",
      "El cliente  Escritura con probabilidad [0.10253631 0.89746369]\n",
      "El cliente  Escritura con probabilidad [0.20125326 0.79874674]\n",
      "El cliente  Escritura con probabilidad [0.13594083 0.86405917]\n",
      "El cliente  Escritura con probabilidad [0.13013413 0.86986587]\n",
      "El cliente  Escritura con probabilidad [0.16809911 0.83190089]\n",
      "El cliente  Escritura con probabilidad [0.14261382 0.85738618]\n",
      "El cliente  Escritura con probabilidad [9.55711118e-04 9.99044289e-01]\n",
      "El cliente  Escritura con probabilidad [0.42190565 0.57809435]\n",
      "El cliente  Escritura con probabilidad [0.19786399 0.80213601]\n",
      "El cliente  Escritura con probabilidad [0.18513579 0.81486421]\n",
      "El cliente  Escritura con probabilidad [0.29472736 0.70527264]\n",
      "El cliente  Escritura con probabilidad [0.26104957 0.73895043]\n",
      "El cliente  Escritura con probabilidad [5.17496604e-06 9.99994825e-01]\n",
      "El cliente  Escritura con probabilidad [0.20939312 0.79060688]\n",
      "El cliente  Escritura con probabilidad [0.0472185 0.9527815]\n",
      "El cliente  Escritura con probabilidad [0.07462311 0.92537689]\n",
      "El cliente  Escritura con probabilidad [6.72350398e-10 9.99999999e-01]\n",
      "El cliente  Desiste con probabilidad [0.68413228 0.31586772]\n",
      "El cliente  Escritura con probabilidad [0.02414655 0.97585345]\n",
      "El cliente  Escritura con probabilidad [0.21747301 0.78252699]\n",
      "El cliente  Escritura con probabilidad [0.2661636 0.7338364]\n",
      "El cliente  Escritura con probabilidad [0.21014501 0.78985499]\n",
      "El cliente  Escritura con probabilidad [0.20618978 0.79381022]\n",
      "El cliente  Escritura con probabilidad [0.0120819 0.9879181]\n",
      "El cliente  Escritura con probabilidad [0.06503809 0.93496191]\n",
      "El cliente  Escritura con probabilidad [0.29176241 0.70823759]\n",
      "El cliente  Escritura con probabilidad [0.03432838 0.96567162]\n",
      "El cliente  Escritura con probabilidad [0.03139389 0.96860611]\n",
      "El cliente  Escritura con probabilidad [3.91475510e-04 9.99608524e-01]\n",
      "El cliente  Escritura con probabilidad [0.15298404 0.84701596]\n",
      "El cliente  Escritura con probabilidad [1.77225701e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.4366362 0.5633638]\n",
      "El cliente  Escritura con probabilidad [1.75587884e-06 9.99998244e-01]\n",
      "El cliente  Escritura con probabilidad [0.14325586 0.85674414]\n",
      "El cliente  Escritura con probabilidad [0.2134225 0.7865775]\n",
      "El cliente  Escritura con probabilidad [0.14117732 0.85882268]\n",
      "El cliente  Escritura con probabilidad [0.00576537 0.99423463]\n",
      "El cliente  Escritura con probabilidad [0.00776448 0.99223552]\n",
      "El cliente  Escritura con probabilidad [0.14560606 0.85439394]\n",
      "El cliente  Escritura con probabilidad [0.14183837 0.85816163]\n",
      "El cliente  Escritura con probabilidad [0.144898 0.855102]\n",
      "El cliente  Escritura con probabilidad [0.2452252 0.7547748]\n",
      "El cliente  Escritura con probabilidad [0.01953959 0.98046041]\n",
      "El cliente  Escritura con probabilidad [0.02938222 0.97061778]\n",
      "El cliente  Escritura con probabilidad [0.00129998 0.99870002]\n",
      "El cliente  Escritura con probabilidad [0.00994448 0.99005552]\n",
      "El cliente  Escritura con probabilidad [0.02502184 0.97497816]\n",
      "El cliente  Escritura con probabilidad [0.43050904 0.56949096]\n",
      "El cliente  Escritura con probabilidad [0.20483498 0.79516502]\n",
      "El cliente  Escritura con probabilidad [0.10926486 0.89073514]\n",
      "El cliente  Escritura con probabilidad [0.20512082 0.79487918]\n",
      "El cliente  Escritura con probabilidad [0.121314 0.878686]\n",
      "El cliente  Escritura con probabilidad [0.21757099 0.78242901]\n",
      "El cliente  Escritura con probabilidad [0.35492941 0.64507059]\n",
      "El cliente  Escritura con probabilidad [0.27855132 0.72144868]\n",
      "El cliente  Escritura con probabilidad [0.06145858 0.93854142]\n",
      "El cliente  Escritura con probabilidad [0.07309228 0.92690772]\n",
      "El cliente  Escritura con probabilidad [0.05262629 0.94737371]\n",
      "El cliente  Escritura con probabilidad [0.09619932 0.90380068]\n",
      "El cliente  Escritura con probabilidad [0.13957027 0.86042973]\n",
      "El cliente  Escritura con probabilidad [0.1069186 0.8930814]\n",
      "El cliente  Escritura con probabilidad [1.4189645e-05 9.9998581e-01]\n",
      "El cliente  Escritura con probabilidad [0.13118777 0.86881223]\n",
      "El cliente  Escritura con probabilidad [0.05908513 0.94091487]\n",
      "El cliente  Escritura con probabilidad [0.24890718 0.75109282]\n",
      "El cliente  Escritura con probabilidad [3.40220547e-08 9.99999966e-01]\n",
      "El cliente  Escritura con probabilidad [0.01898791 0.98101209]\n",
      "El cliente  Escritura con probabilidad [6.01445521e-04 9.99398554e-01]\n",
      "El cliente  Escritura con probabilidad [0.06642889 0.93357111]\n",
      "El cliente  Escritura con probabilidad [2.42509649e-04 9.99757490e-01]\n",
      "El cliente  Escritura con probabilidad [0.17484701 0.82515299]\n",
      "El cliente  Escritura con probabilidad [0.1348435 0.8651565]\n",
      "El cliente  Escritura con probabilidad [0.34913537 0.65086463]\n",
      "El cliente  Escritura con probabilidad [0.22140663 0.77859337]\n",
      "El cliente  Escritura con probabilidad [0.08578425 0.91421575]\n",
      "El cliente  Escritura con probabilidad [0.10357725 0.89642275]\n",
      "El cliente  Escritura con probabilidad [0.14107111 0.85892889]\n",
      "El cliente  Escritura con probabilidad [0.01229273 0.98770727]\n",
      "El cliente  Escritura con probabilidad [0.26750921 0.73249079]\n",
      "El cliente  Escritura con probabilidad [0.00167993 0.99832007]\n",
      "El cliente  Escritura con probabilidad [0.15069465 0.84930535]\n",
      "El cliente  Escritura con probabilidad [0.00731266 0.99268734]\n",
      "El cliente  Escritura con probabilidad [0.00815331 0.99184669]\n",
      "El cliente  Escritura con probabilidad [9.08413689e-09 9.99999991e-01]\n",
      "El cliente  Escritura con probabilidad [0.28182263 0.71817737]\n",
      "El cliente  Escritura con probabilidad [0.18485697 0.81514303]\n",
      "El cliente  Escritura con probabilidad [0.05305671 0.94694329]\n",
      "El cliente  Escritura con probabilidad [0.16441806 0.83558194]\n",
      "El cliente  Escritura con probabilidad [0.022336 0.977664]\n",
      "El cliente  Escritura con probabilidad [0.0654785 0.9345215]\n",
      "El cliente  Escritura con probabilidad [4.48465460e-07 9.99999552e-01]\n",
      "El cliente  Escritura con probabilidad [0.15751501 0.84248499]\n",
      "El cliente  Escritura con probabilidad [0.18529971 0.81470029]\n",
      "El cliente  Escritura con probabilidad [0.11625418 0.88374582]\n",
      "El cliente  Escritura con probabilidad [0.42113327 0.57886673]\n",
      "El cliente  Escritura con probabilidad [0.12931989 0.87068011]\n",
      "El cliente  Escritura con probabilidad [0.15158381 0.84841619]\n",
      "El cliente  Escritura con probabilidad [0.16277512 0.83722488]\n",
      "El cliente  Escritura con probabilidad [0.25076811 0.74923189]\n",
      "El cliente  Escritura con probabilidad [1.10936817e-05 9.99988906e-01]\n",
      "El cliente  Escritura con probabilidad [5.12934147e-06 9.99994871e-01]\n",
      "El cliente  Escritura con probabilidad [0.07926666 0.92073334]\n",
      "El cliente  Escritura con probabilidad [0.14612293 0.85387707]\n",
      "El cliente  Escritura con probabilidad [0.13301196 0.86698804]\n",
      "El cliente  Escritura con probabilidad [1.52255925e-04 9.99847744e-01]\n",
      "El cliente  Escritura con probabilidad [0.29844182 0.70155818]\n",
      "El cliente  Escritura con probabilidad [0.18750882 0.81249118]\n",
      "El cliente  Escritura con probabilidad [7.34018091e-07 9.99999266e-01]\n",
      "El cliente  Escritura con probabilidad [0.15521726 0.84478274]\n",
      "El cliente  Escritura con probabilidad [0.31201841 0.68798159]\n",
      "El cliente  Escritura con probabilidad [0.21013282 0.78986718]\n",
      "El cliente  Escritura con probabilidad [0.09343301 0.90656699]\n",
      "El cliente  Escritura con probabilidad [2.41373056e-05 9.99975863e-01]\n",
      "El cliente  Escritura con probabilidad [0.02237591 0.97762409]\n",
      "El cliente  Escritura con probabilidad [0.06601573 0.93398427]\n",
      "El cliente  Escritura con probabilidad [0.01024149 0.98975851]\n",
      "El cliente  Escritura con probabilidad [0.15605736 0.84394264]\n",
      "El cliente  Escritura con probabilidad [0.05803852 0.94196148]\n",
      "El cliente  Escritura con probabilidad [0.24171733 0.75828267]\n",
      "El cliente  Escritura con probabilidad [4.20996571e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [5.15143483e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.00124208 0.99875792]\n",
      "El cliente  Escritura con probabilidad [0.09722946 0.90277054]\n",
      "El cliente  Escritura con probabilidad [0.03196198 0.96803802]\n",
      "El cliente  Escritura con probabilidad [0.09622472 0.90377528]\n",
      "El cliente  Escritura con probabilidad [0.20099344 0.79900656]\n",
      "El cliente  Escritura con probabilidad [0.18342963 0.81657037]\n",
      "El cliente  Escritura con probabilidad [0.22742061 0.77257939]\n",
      "El cliente  Escritura con probabilidad [0.37399814 0.62600186]\n",
      "El cliente  Escritura con probabilidad [0.04034957 0.95965043]\n",
      "El cliente  Escritura con probabilidad [0.0089352 0.9910648]\n",
      "El cliente  Escritura con probabilidad [0.21949343 0.78050657]\n",
      "El cliente  Escritura con probabilidad [0.28200511 0.71799489]\n",
      "El cliente  Escritura con probabilidad [0.05797903 0.94202097]\n",
      "El cliente  Escritura con probabilidad [0.13052626 0.86947374]\n",
      "El cliente  Escritura con probabilidad [0.22803833 0.77196167]\n",
      "El cliente  Escritura con probabilidad [0.12464427 0.87535573]\n",
      "El cliente  Escritura con probabilidad [0.02989469 0.97010531]\n",
      "El cliente  Escritura con probabilidad [0.07871229 0.92128771]\n",
      "El cliente  Escritura con probabilidad [0.29892863 0.70107137]\n",
      "El cliente  Escritura con probabilidad [0.13798729 0.86201271]\n",
      "El cliente  Escritura con probabilidad [6.90078409e-04 9.99309922e-01]\n",
      "El cliente  Escritura con probabilidad [0.22949254 0.77050746]\n",
      "El cliente  Escritura con probabilidad [0.08401724 0.91598276]\n",
      "El cliente  Escritura con probabilidad [0.21217946 0.78782054]\n",
      "El cliente  Escritura con probabilidad [0.02102345 0.97897655]\n",
      "El cliente  Escritura con probabilidad [0.03754451 0.96245549]\n",
      "El cliente  Escritura con probabilidad [0.27815044 0.72184956]\n",
      "El cliente  Escritura con probabilidad [0.17040633 0.82959367]\n",
      "El cliente  Escritura con probabilidad [0.02161508 0.97838492]\n",
      "El cliente  Escritura con probabilidad [0.05375981 0.94624019]\n",
      "El cliente  Escritura con probabilidad [0.07447045 0.92552955]\n",
      "El cliente  Escritura con probabilidad [0.06396847 0.93603153]\n",
      "El cliente  Escritura con probabilidad [1.12668319e-05 9.99988733e-01]\n",
      "El cliente  Escritura con probabilidad [0.07510983 0.92489017]\n",
      "El cliente  Escritura con probabilidad [0.1581504 0.8418496]\n",
      "El cliente  Escritura con probabilidad [0.19842538 0.80157462]\n",
      "El cliente  Escritura con probabilidad [0.00426131 0.99573869]\n",
      "El cliente  Escritura con probabilidad [0.26986921 0.73013079]\n",
      "El cliente  Escritura con probabilidad [0.17399756 0.82600244]\n",
      "El cliente  Escritura con probabilidad [0.18284468 0.81715532]\n",
      "El cliente  Escritura con probabilidad [0.22286901 0.77713099]\n",
      "El cliente  Escritura con probabilidad [0.10406784 0.89593216]\n",
      "El cliente  Escritura con probabilidad [0.15394384 0.84605616]\n",
      "El cliente  Escritura con probabilidad [0.03544248 0.96455752]\n",
      "El cliente  Escritura con probabilidad [0.22404264 0.77595736]\n",
      "El cliente  Escritura con probabilidad [1.00467672e-07 9.99999900e-01]\n",
      "El cliente  Escritura con probabilidad [5.05211428e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.43183788 0.56816212]\n",
      "El cliente  Escritura con probabilidad [0.21766172 0.78233828]\n",
      "El cliente  Escritura con probabilidad [0.19684668 0.80315332]\n",
      "El cliente  Escritura con probabilidad [4.93437948e-07 9.99999507e-01]\n",
      "El cliente  Escritura con probabilidad [0.01812863 0.98187137]\n",
      "El cliente  Escritura con probabilidad [5.84301496e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.08198356 0.91801644]\n",
      "El cliente  Escritura con probabilidad [0.14343169 0.85656831]\n",
      "El cliente  Escritura con probabilidad [0.22394103 0.77605897]\n",
      "El cliente  Escritura con probabilidad [0.00440516 0.99559484]\n",
      "El cliente  Escritura con probabilidad [0.17959599 0.82040401]\n",
      "El cliente  Escritura con probabilidad [0.02721574 0.97278426]\n",
      "El cliente  Escritura con probabilidad [0.30480059 0.69519941]\n",
      "El cliente  Escritura con probabilidad [5.45104015e-07 9.99999455e-01]\n",
      "El cliente  Escritura con probabilidad [1.13049761e-06 9.99998870e-01]\n",
      "El cliente  Escritura con probabilidad [0.1405733 0.8594267]\n",
      "El cliente  Escritura con probabilidad [6.66133815e-16 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.23842761 0.76157239]\n",
      "El cliente  Desiste con probabilidad [0.79216876 0.20783124]\n",
      "El cliente  Escritura con probabilidad [0.10127213 0.89872787]\n",
      "El cliente  Escritura con probabilidad [0.17678346 0.82321654]\n",
      "El cliente  Escritura con probabilidad [7.69535990e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.13915554 0.86084446]\n",
      "El cliente  Escritura con probabilidad [0.12447858 0.87552142]\n",
      "El cliente  Escritura con probabilidad [0.09712102 0.90287898]\n",
      "El cliente  Escritura con probabilidad [0.28333319 0.71666681]\n",
      "El cliente  Escritura con probabilidad [0.05020801 0.94979199]\n",
      "El cliente  Escritura con probabilidad [1.11632925e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.24333565 0.75666435]\n",
      "El cliente  Escritura con probabilidad [0.20298501 0.79701499]\n",
      "El cliente  Escritura con probabilidad [0.25559526 0.74440474]\n",
      "El cliente  Escritura con probabilidad [0.10795956 0.89204044]\n",
      "El cliente  Escritura con probabilidad [8.02790249e-05 9.99919721e-01]\n",
      "El cliente  Escritura con probabilidad [0.21307944 0.78692056]\n",
      "El cliente  Escritura con probabilidad [0.06070251 0.93929749]\n",
      "El cliente  Escritura con probabilidad [0.0936689 0.9063311]\n",
      "El cliente  Escritura con probabilidad [5.98890715e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [2.91776248e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.23154777 0.76845223]\n",
      "El cliente  Escritura con probabilidad [0.41988766 0.58011234]\n",
      "El cliente  Escritura con probabilidad [0.18546025 0.81453975]\n",
      "El cliente  Escritura con probabilidad [6.09432171e-09 9.99999994e-01]\n",
      "El cliente  Escritura con probabilidad [0.02166956 0.97833044]\n",
      "El cliente  Escritura con probabilidad [0.24633199 0.75366801]\n",
      "El cliente  Escritura con probabilidad [0.04219057 0.95780943]\n",
      "El cliente  Escritura con probabilidad [0.01749038 0.98250962]\n",
      "El cliente  Escritura con probabilidad [0.22043378 0.77956622]\n",
      "El cliente  Escritura con probabilidad [0.27194514 0.72805486]\n",
      "El cliente  Escritura con probabilidad [0.23964866 0.76035134]\n",
      "El cliente  Escritura con probabilidad [0.00431439 0.99568561]\n",
      "El cliente  Escritura con probabilidad [0.00880523 0.99119477]\n",
      "El cliente  Escritura con probabilidad [0.098285 0.901715]\n",
      "El cliente  Escritura con probabilidad [0.02016072 0.97983928]\n",
      "El cliente  Escritura con probabilidad [0.22365078 0.77634922]\n",
      "El cliente  Escritura con probabilidad [0.21938509 0.78061491]\n",
      "El cliente  Escritura con probabilidad [0.25069914 0.74930086]\n",
      "El cliente  Escritura con probabilidad [0.11401769 0.88598231]\n",
      "El cliente  Escritura con probabilidad [0.22175534 0.77824466]\n",
      "El cliente  Escritura con probabilidad [8.93568400e-04 9.99106432e-01]\n",
      "El cliente  Escritura con probabilidad [0.09319925 0.90680075]\n",
      "El cliente  Escritura con probabilidad [0.05055284 0.94944716]\n",
      "El cliente  Escritura con probabilidad [0.25243587 0.74756413]\n",
      "El cliente  Escritura con probabilidad [0.08085269 0.91914731]\n",
      "El cliente  Escritura con probabilidad [0.00438892 0.99561108]\n",
      "El cliente  Escritura con probabilidad [0.21616407 0.78383593]\n",
      "El cliente  Escritura con probabilidad [4.33582503e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.07216747 0.92783253]\n",
      "El cliente  Escritura con probabilidad [0.00935758 0.99064242]\n",
      "El cliente  Escritura con probabilidad [0.08891406 0.91108594]\n",
      "El cliente  Escritura con probabilidad [0.02127487 0.97872513]\n",
      "El cliente  Escritura con probabilidad [0.12456763 0.87543237]\n",
      "El cliente  Escritura con probabilidad [0.2558352 0.7441648]\n",
      "El cliente  Escritura con probabilidad [0.10685964 0.89314036]\n",
      "El cliente  Escritura con probabilidad [0.17213734 0.82786266]\n",
      "El cliente  Escritura con probabilidad [0.18911112 0.81088888]\n",
      "El cliente  Escritura con probabilidad [0.02527101 0.97472899]\n",
      "El cliente  Escritura con probabilidad [3.39934637e-04 9.99660065e-01]\n",
      "El cliente  Escritura con probabilidad [0.20441906 0.79558094]\n",
      "El cliente  Escritura con probabilidad [0.42107622 0.57892378]\n",
      "El cliente  Escritura con probabilidad [0.22348172 0.77651828]\n",
      "El cliente  Escritura con probabilidad [0.09080695 0.90919305]\n",
      "El cliente  Escritura con probabilidad [0.07604897 0.92395103]\n",
      "El cliente  Escritura con probabilidad [3.03798764e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.02273408 0.97726592]\n",
      "El cliente  Escritura con probabilidad [1.05641986e-06 9.99998944e-01]\n",
      "El cliente  Escritura con probabilidad [0.01396744 0.98603256]\n",
      "El cliente  Escritura con probabilidad [0.03950967 0.96049033]\n",
      "El cliente  Escritura con probabilidad [0.13944631 0.86055369]\n",
      "El cliente  Escritura con probabilidad [0.01542315 0.98457685]\n",
      "El cliente  Escritura con probabilidad [0.0952508 0.9047492]\n",
      "El cliente  Escritura con probabilidad [2.01482521e-07 9.99999799e-01]\n",
      "El cliente  Escritura con probabilidad [3.35808229e-06 9.99996642e-01]\n",
      "El cliente  Escritura con probabilidad [0.15287315 0.84712685]\n",
      "El cliente  Escritura con probabilidad [0.27347588 0.72652412]\n",
      "El cliente  Escritura con probabilidad [0.06219504 0.93780496]\n",
      "El cliente  Escritura con probabilidad [0.02616655 0.97383345]\n",
      "El cliente  Escritura con probabilidad [0.23683351 0.76316649]\n",
      "El cliente  Escritura con probabilidad [0.22093689 0.77906311]\n",
      "El cliente  Escritura con probabilidad [0.09246993 0.90753007]\n",
      "El cliente  Escritura con probabilidad [0.13672886 0.86327114]\n",
      "El cliente  Escritura con probabilidad [0.01889823 0.98110177]\n",
      "El cliente  Escritura con probabilidad [0.20432285 0.79567715]\n",
      "El cliente  Escritura con probabilidad [0.10992527 0.89007473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [1.31796796e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.82227972e-05 9.99981777e-01]\n",
      "El cliente  Escritura con probabilidad [5.46835774e-06 9.99994532e-01]\n",
      "El cliente  Escritura con probabilidad [0.30173222 0.69826778]\n",
      "El cliente  Escritura con probabilidad [0.00749486 0.99250514]\n",
      "El cliente  Escritura con probabilidad [0.19782753 0.80217247]\n",
      "El cliente  Escritura con probabilidad [0.01136916 0.98863084]\n",
      "El cliente  Escritura con probabilidad [0.21578527 0.78421473]\n",
      "El cliente  Escritura con probabilidad [0.28546777 0.71453223]\n",
      "El cliente  Escritura con probabilidad [0.20446773 0.79553227]\n",
      "El cliente  Escritura con probabilidad [0.03732589 0.96267411]\n",
      "El cliente  Escritura con probabilidad [0.10926738 0.89073262]\n",
      "El cliente  Escritura con probabilidad [0.19739956 0.80260044]\n",
      "El cliente  Escritura con probabilidad [0.17591758 0.82408242]\n",
      "El cliente  Escritura con probabilidad [2.05216602e-07 9.99999795e-01]\n",
      "El cliente  Escritura con probabilidad [0.27386781 0.72613219]\n",
      "El cliente  Escritura con probabilidad [0.20491675 0.79508325]\n",
      "El cliente  Escritura con probabilidad [0.13656379 0.86343621]\n",
      "El cliente  Escritura con probabilidad [0.01555757 0.98444243]\n",
      "El cliente  Escritura con probabilidad [0.2300937 0.7699063]\n",
      "El cliente  Escritura con probabilidad [0.10230934 0.89769066]\n",
      "El cliente  Escritura con probabilidad [4.64055732e-05 9.99953594e-01]\n",
      "El cliente  Escritura con probabilidad [0.15115489 0.84884511]\n",
      "El cliente  Escritura con probabilidad [0.19015593 0.80984407]\n",
      "El cliente  Escritura con probabilidad [0.29277759 0.70722241]\n",
      "El cliente  Escritura con probabilidad [0.26953537 0.73046463]\n",
      "El cliente  Escritura con probabilidad [0.19698233 0.80301767]\n",
      "El cliente  Escritura con probabilidad [3.98378362e-04 9.99601622e-01]\n",
      "El cliente  Escritura con probabilidad [0.15326938 0.84673062]\n",
      "El cliente  Escritura con probabilidad [0.06816223 0.93183777]\n",
      "El cliente  Escritura con probabilidad [0.03208467 0.96791533]\n",
      "El cliente  Escritura con probabilidad [0.10024882 0.89975118]\n",
      "El cliente  Escritura con probabilidad [0.224742 0.775258]\n",
      "El cliente  Escritura con probabilidad [6.43283744e-04 9.99356716e-01]\n",
      "El cliente  Escritura con probabilidad [0.1045054 0.8954946]\n",
      "El cliente  Escritura con probabilidad [0.14923832 0.85076168]\n",
      "El cliente  Escritura con probabilidad [0.04414439 0.95585561]\n",
      "El cliente  Escritura con probabilidad [0.23888272 0.76111728]\n",
      "El cliente  Escritura con probabilidad [0.21668464 0.78331536]\n",
      "El cliente  Escritura con probabilidad [0.08516897 0.91483103]\n",
      "El cliente  Escritura con probabilidad [0.25597354 0.74402646]\n",
      "El cliente  Escritura con probabilidad [3.21786594e-04 9.99678213e-01]\n",
      "El cliente  Escritura con probabilidad [1.71600106e-07 9.99999828e-01]\n",
      "El cliente  Escritura con probabilidad [0.24477759 0.75522241]\n",
      "El cliente  Escritura con probabilidad [0.21764545 0.78235455]\n",
      "El cliente  Escritura con probabilidad [0.17062602 0.82937398]\n",
      "El cliente  Escritura con probabilidad [0.2639997 0.7360003]\n",
      "El cliente  Escritura con probabilidad [3.95533382e-08 9.99999960e-01]\n",
      "El cliente  Escritura con probabilidad [0.01583835 0.98416165]\n",
      "El cliente  Escritura con probabilidad [0.18495611 0.81504389]\n",
      "El cliente  Escritura con probabilidad [0.06853145 0.93146855]\n",
      "El cliente  Escritura con probabilidad [0.23671753 0.76328247]\n",
      "El cliente  Escritura con probabilidad [0.0838249 0.9161751]\n",
      "El cliente  Escritura con probabilidad [0.13144687 0.86855313]\n",
      "El cliente  Escritura con probabilidad [1.78771366e-06 9.99998212e-01]\n",
      "El cliente  Escritura con probabilidad [0.2026965 0.7973035]\n",
      "El cliente  Escritura con probabilidad [0.0337647 0.9662353]\n",
      "El cliente  Escritura con probabilidad [0.01594529 0.98405471]\n",
      "El cliente  Escritura con probabilidad [0.18264506 0.81735494]\n",
      "El cliente  Escritura con probabilidad [4.40691239e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.00497726 0.99502274]\n",
      "El cliente  Escritura con probabilidad [0.45019558 0.54980442]\n",
      "El cliente  Escritura con probabilidad [0.2125978 0.7874022]\n",
      "El cliente  Escritura con probabilidad [0.2088996 0.7911004]\n",
      "El cliente  Escritura con probabilidad [0.04728116 0.95271884]\n",
      "El cliente  Escritura con probabilidad [0.17727241 0.82272759]\n",
      "El cliente  Escritura con probabilidad [0.00141215 0.99858785]\n",
      "El cliente  Escritura con probabilidad [0.1402754 0.8597246]\n",
      "El cliente  Escritura con probabilidad [0.01715463 0.98284537]\n",
      "El cliente  Escritura con probabilidad [0.00958925 0.99041075]\n",
      "El cliente  Escritura con probabilidad [0.10377951 0.89622049]\n",
      "El cliente  Escritura con probabilidad [0.14978083 0.85021917]\n",
      "El cliente  Escritura con probabilidad [0.02703523 0.97296477]\n",
      "El cliente  Escritura con probabilidad [0.20480601 0.79519399]\n",
      "El cliente  Escritura con probabilidad [0.13353929 0.86646071]\n",
      "El cliente  Escritura con probabilidad [0.21062194 0.78937806]\n",
      "El cliente  Escritura con probabilidad [0.02254665 0.97745335]\n",
      "El cliente  Escritura con probabilidad [1.51412891e-04 9.99848587e-01]\n",
      "El cliente  Escritura con probabilidad [0.14553251 0.85446749]\n",
      "El cliente  Escritura con probabilidad [0.06298919 0.93701081]\n",
      "El cliente  Escritura con probabilidad [0.24910174 0.75089826]\n",
      "El cliente  Escritura con probabilidad [0.04965393 0.95034607]\n",
      "El cliente  Escritura con probabilidad [0.07871565 0.92128435]\n",
      "El cliente  Escritura con probabilidad [0.06152788 0.93847212]\n",
      "El cliente  Escritura con probabilidad [0.36320155 0.63679845]\n",
      "El cliente  Escritura con probabilidad [1.11330178e-06 9.99998887e-01]\n",
      "El cliente  Escritura con probabilidad [0.14198583 0.85801417]\n",
      "El cliente  Escritura con probabilidad [0.04350425 0.95649575]\n",
      "El cliente  Escritura con probabilidad [0.33720981 0.66279019]\n",
      "El cliente  Escritura con probabilidad [0.22648805 0.77351195]\n",
      "El cliente  Escritura con probabilidad [0.29417065 0.70582935]\n",
      "El cliente  Escritura con probabilidad [2.63535420e-08 9.99999974e-01]\n",
      "El cliente  Escritura con probabilidad [0.09208303 0.90791697]\n",
      "El cliente  Escritura con probabilidad [0.2788184 0.7211816]\n",
      "El cliente  Escritura con probabilidad [0.10794669 0.89205331]\n",
      "El cliente  Escritura con probabilidad [0.19783729 0.80216271]\n",
      "El cliente  Escritura con probabilidad [0.04326182 0.95673818]\n",
      "El cliente  Escritura con probabilidad [0.11953303 0.88046697]\n",
      "El cliente  Escritura con probabilidad [0.19008048 0.80991952]\n",
      "El cliente  Escritura con probabilidad [0.26363755 0.73636245]\n",
      "El cliente  Escritura con probabilidad [0.06612988 0.93387012]\n",
      "El cliente  Escritura con probabilidad [0.13506916 0.86493084]\n",
      "El cliente  Escritura con probabilidad [0.07667174 0.92332826]\n",
      "El cliente  Escritura con probabilidad [0.04147348 0.95852652]\n",
      "El cliente  Escritura con probabilidad [4.28271233e-06 9.99995717e-01]\n",
      "El cliente  Escritura con probabilidad [0.22324622 0.77675378]\n",
      "El cliente  Escritura con probabilidad [0.06410866 0.93589134]\n",
      "El cliente  Escritura con probabilidad [0.08939705 0.91060295]\n",
      "El cliente  Escritura con probabilidad [0.2018952 0.7981048]\n",
      "El cliente  Escritura con probabilidad [0.08974284 0.91025716]\n",
      "El cliente  Escritura con probabilidad [0.18478865 0.81521135]\n",
      "El cliente  Escritura con probabilidad [0.25801955 0.74198045]\n",
      "El cliente  Escritura con probabilidad [0.06995779 0.93004221]\n",
      "El cliente  Escritura con probabilidad [0.12079789 0.87920211]\n",
      "El cliente  Escritura con probabilidad [0.28504419 0.71495581]\n",
      "El cliente  Escritura con probabilidad [6.48043380e-04 9.99351957e-01]\n",
      "El cliente  Escritura con probabilidad [0.12053905 0.87946095]\n",
      "El cliente  Escritura con probabilidad [0.06752833 0.93247167]\n",
      "El cliente  Escritura con probabilidad [0.35271684 0.64728316]\n",
      "El cliente  Escritura con probabilidad [0.18704163 0.81295837]\n",
      "El cliente  Escritura con probabilidad [0.00887785 0.99112215]\n",
      "El cliente  Escritura con probabilidad [0.21050497 0.78949503]\n",
      "El cliente  Escritura con probabilidad [0.07223829 0.92776171]\n",
      "El cliente  Escritura con probabilidad [0.02291934 0.97708066]\n",
      "El cliente  Escritura con probabilidad [0.02570838 0.97429162]\n",
      "El cliente  Escritura con probabilidad [0.15725137 0.84274863]\n",
      "El cliente  Escritura con probabilidad [0.07443643 0.92556357]\n",
      "El cliente  Escritura con probabilidad [0.20891612 0.79108388]\n",
      "El cliente  Escritura con probabilidad [8.27582907e-06 9.99991724e-01]\n",
      "El cliente  Escritura con probabilidad [0.19234461 0.80765539]\n",
      "El cliente  Escritura con probabilidad [0.24000057 0.75999943]\n",
      "El cliente  Escritura con probabilidad [0.01159192 0.98840808]\n",
      "El cliente  Escritura con probabilidad [7.25357895e-08 9.99999927e-01]\n",
      "El cliente  Escritura con probabilidad [0.2231623 0.7768377]\n",
      "El cliente  Escritura con probabilidad [0.11109544 0.88890456]\n",
      "El cliente  Escritura con probabilidad [0.13100476 0.86899524]\n",
      "El cliente  Escritura con probabilidad [0.09028913 0.90971087]\n",
      "El cliente  Escritura con probabilidad [0.06380582 0.93619418]\n",
      "El cliente  Escritura con probabilidad [0.12547028 0.87452972]\n",
      "El cliente  Escritura con probabilidad [2.39586129e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.44129493 0.55870507]\n",
      "El cliente  Escritura con probabilidad [0.12560682 0.87439318]\n",
      "El cliente  Escritura con probabilidad [0.15203692 0.84796308]\n",
      "El cliente  Escritura con probabilidad [0.04292382 0.95707618]\n",
      "El cliente  Escritura con probabilidad [1.16690131e-05 9.99988331e-01]\n",
      "El cliente  Escritura con probabilidad [8.45402044e-08 9.99999915e-01]\n",
      "El cliente  Escritura con probabilidad [9.13625851e-06 9.99990864e-01]\n",
      "El cliente  Escritura con probabilidad [0.00880542 0.99119458]\n",
      "El cliente  Escritura con probabilidad [0.0312781 0.9687219]\n",
      "El cliente  Escritura con probabilidad [0.02327802 0.97672198]\n",
      "El cliente  Escritura con probabilidad [0.01286241 0.98713759]\n",
      "El cliente  Escritura con probabilidad [0.17062919 0.82937081]\n",
      "El cliente  Escritura con probabilidad [1.11094312e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.26287826 0.73712174]\n",
      "El cliente  Escritura con probabilidad [0.11746679 0.88253321]\n",
      "El cliente  Escritura con probabilidad [0.18743897 0.81256103]\n",
      "El cliente  Escritura con probabilidad [8.27113182e-05 9.99917289e-01]\n",
      "El cliente  Escritura con probabilidad [0.36846254 0.63153746]\n",
      "El cliente  Escritura con probabilidad [0.26918648 0.73081352]\n",
      "El cliente  Escritura con probabilidad [0.209323 0.790677]\n",
      "El cliente  Escritura con probabilidad [0.42455448 0.57544552]\n",
      "El cliente  Escritura con probabilidad [0.06199269 0.93800731]\n",
      "El cliente  Escritura con probabilidad [0.12865028 0.87134972]\n",
      "El cliente  Escritura con probabilidad [1.66819235e-04 9.99833181e-01]\n",
      "El cliente  Escritura con probabilidad [0.25445522 0.74554478]\n",
      "El cliente  Escritura con probabilidad [0.15799258 0.84200742]\n",
      "El cliente  Escritura con probabilidad [0.15502963 0.84497037]\n",
      "El cliente  Escritura con probabilidad [0.21839113 0.78160887]\n",
      "El cliente  Escritura con probabilidad [0.18351035 0.81648965]\n",
      "El cliente  Escritura con probabilidad [0.2192705 0.7807295]\n",
      "El cliente  Escritura con probabilidad [0.0254784 0.9745216]\n",
      "El cliente  Escritura con probabilidad [0.4431001 0.5568999]\n",
      "El cliente  Escritura con probabilidad [0.19419885 0.80580115]\n",
      "El cliente  Escritura con probabilidad [0.13103619 0.86896381]\n",
      "El cliente  Escritura con probabilidad [0.24817796 0.75182204]\n",
      "El cliente  Escritura con probabilidad [0.18756881 0.81243119]\n",
      "El cliente  Escritura con probabilidad [0.0476591 0.9523409]\n",
      "El cliente  Escritura con probabilidad [0.09762143 0.90237857]\n",
      "El cliente  Escritura con probabilidad [6.11508664e-08 9.99999939e-01]\n",
      "El cliente  Escritura con probabilidad [0.0135975 0.9864025]\n",
      "El cliente  Escritura con probabilidad [0.17434115 0.82565885]\n",
      "El cliente  Escritura con probabilidad [0.27198761 0.72801239]\n",
      "El cliente  Escritura con probabilidad [0.1428359 0.8571641]\n",
      "El cliente  Escritura con probabilidad [5.59789375e-07 9.99999440e-01]\n",
      "El cliente  Escritura con probabilidad [0.24805201 0.75194799]\n",
      "El cliente  Escritura con probabilidad [0.17296593 0.82703407]\n",
      "El cliente  Escritura con probabilidad [0.19453672 0.80546328]\n",
      "El cliente  Escritura con probabilidad [0.20493484 0.79506516]\n",
      "El cliente  Escritura con probabilidad [0.09500314 0.90499686]\n",
      "El cliente  Escritura con probabilidad [0.01581257 0.98418743]\n",
      "El cliente  Escritura con probabilidad [0.02278068 0.97721932]\n",
      "El cliente  Escritura con probabilidad [0.10354778 0.89645222]\n",
      "El cliente  Escritura con probabilidad [0.05266791 0.94733209]\n",
      "El cliente  Escritura con probabilidad [0.22309469 0.77690531]\n",
      "El cliente  Escritura con probabilidad [0.08602468 0.91397532]\n",
      "El cliente  Escritura con probabilidad [0.00467892 0.99532108]\n",
      "El cliente  Escritura con probabilidad [0.02820182 0.97179818]\n",
      "El cliente  Escritura con probabilidad [0.3023132 0.6976868]\n",
      "El cliente  Escritura con probabilidad [0.17721169 0.82278831]\n",
      "El cliente  Escritura con probabilidad [0.02630987 0.97369013]\n",
      "El cliente  Escritura con probabilidad [3.75528422e-06 9.99996245e-01]\n",
      "El cliente  Escritura con probabilidad [0.0282659 0.9717341]\n",
      "El cliente  Escritura con probabilidad [0.3423436 0.6576564]\n",
      "El cliente  Escritura con probabilidad [0.14163321 0.85836679]\n",
      "El cliente  Escritura con probabilidad [1.19302762e-05 9.99988070e-01]\n",
      "El cliente  Escritura con probabilidad [0.1422201 0.8577799]\n",
      "El cliente  Escritura con probabilidad [0.04619077 0.95380923]\n",
      "El cliente  Escritura con probabilidad [0.21493284 0.78506716]\n",
      "El cliente  Escritura con probabilidad [0.25340817 0.74659183]\n",
      "El cliente  Escritura con probabilidad [0.22562547 0.77437453]\n",
      "El cliente  Escritura con probabilidad [0.22225076 0.77774924]\n",
      "El cliente  Escritura con probabilidad [0.07770739 0.92229261]\n",
      "El cliente  Escritura con probabilidad [0.08121635 0.91878365]\n",
      "El cliente  Escritura con probabilidad [0.1113488 0.8886512]\n",
      "El cliente  Escritura con probabilidad [2.95434344e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.12905245 0.87094755]\n",
      "El cliente  Escritura con probabilidad [0.07015945 0.92984055]\n",
      "El cliente  Escritura con probabilidad [0.1708481 0.8291519]\n",
      "El cliente  Escritura con probabilidad [2.30477726e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [2.86151103e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.21882632 0.78117368]\n",
      "El cliente  Escritura con probabilidad [0.17168176 0.82831824]\n",
      "El cliente  Escritura con probabilidad [2.35108830e-04 9.99764891e-01]\n",
      "El cliente  Escritura con probabilidad [0.24617774 0.75382226]\n",
      "El cliente  Escritura con probabilidad [0.02810112 0.97189888]\n",
      "El cliente  Escritura con probabilidad [0.11847331 0.88152669]\n",
      "El cliente  Escritura con probabilidad [0.01719495 0.98280505]\n",
      "El cliente  Escritura con probabilidad [6.55031585e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [3.27531999e-07 9.99999672e-01]\n",
      "El cliente  Escritura con probabilidad [0.09355755 0.90644245]\n",
      "El cliente  Escritura con probabilidad [0.04290469 0.95709531]\n",
      "El cliente  Escritura con probabilidad [0.00567201 0.99432799]\n",
      "El cliente  Escritura con probabilidad [0.21649278 0.78350722]\n",
      "El cliente  Escritura con probabilidad [1.60576306e-05 9.99983942e-01]\n",
      "El cliente  Escritura con probabilidad [3.83274466e-07 9.99999617e-01]\n",
      "El cliente  Escritura con probabilidad [0.25223412 0.74776588]\n",
      "El cliente  Escritura con probabilidad [0.03929573 0.96070427]\n",
      "El cliente  Escritura con probabilidad [0.23004675 0.76995325]\n",
      "El cliente  Escritura con probabilidad [0.18741203 0.81258797]\n",
      "El cliente  Escritura con probabilidad [0.15234365 0.84765635]\n",
      "El cliente  Escritura con probabilidad [0.19292444 0.80707556]\n",
      "El cliente  Escritura con probabilidad [0.12941691 0.87058309]\n",
      "El cliente  Escritura con probabilidad [0.11340755 0.88659245]\n",
      "El cliente  Escritura con probabilidad [0.06593923 0.93406077]\n",
      "El cliente  Escritura con probabilidad [0.12654273 0.87345727]\n",
      "El cliente  Escritura con probabilidad [0.00859755 0.99140245]\n",
      "El cliente  Escritura con probabilidad [0.13951788 0.86048212]\n",
      "El cliente  Escritura con probabilidad [0.15436129 0.84563871]\n",
      "El cliente  Escritura con probabilidad [0.13042408 0.86957592]\n",
      "El cliente  Escritura con probabilidad [0.19826598 0.80173402]\n",
      "El cliente  Escritura con probabilidad [0.24643528 0.75356472]\n",
      "El cliente  Escritura con probabilidad [0.05997221 0.94002779]\n",
      "El cliente  Escritura con probabilidad [0.208526 0.791474]\n",
      "El cliente  Escritura con probabilidad [0.16178852 0.83821148]\n",
      "El cliente  Escritura con probabilidad [1.16361916e-05 9.99988364e-01]\n",
      "El cliente  Escritura con probabilidad [0.22474499 0.77525501]\n",
      "El cliente  Escritura con probabilidad [0.15038249 0.84961751]\n",
      "El cliente  Escritura con probabilidad [5.71628188e-07 9.99999428e-01]\n",
      "El cliente  Escritura con probabilidad [0.13422843 0.86577157]\n",
      "El cliente  Escritura con probabilidad [0.28459772 0.71540228]\n",
      "El cliente  Escritura con probabilidad [0.13568623 0.86431377]\n",
      "El cliente  Escritura con probabilidad [0.01915331 0.98084669]\n",
      "El cliente  Escritura con probabilidad [0.21711926 0.78288074]\n",
      "El cliente  Escritura con probabilidad [0.28787075 0.71212925]\n",
      "El cliente  Escritura con probabilidad [0.1774623 0.8225377]\n",
      "El cliente  Escritura con probabilidad [0.17437041 0.82562959]\n",
      "El cliente  Escritura con probabilidad [4.37356730e-05 9.99956264e-01]\n",
      "El cliente  Escritura con probabilidad [0.21135947 0.78864053]\n",
      "El cliente  Escritura con probabilidad [1.88685937e-06 9.99998113e-01]\n",
      "El cliente  Escritura con probabilidad [0.19098992 0.80901008]\n",
      "El cliente  Escritura con probabilidad [0.05026361 0.94973639]\n",
      "El cliente  Escritura con probabilidad [0.30522166 0.69477834]\n",
      "El cliente  Escritura con probabilidad [4.17141172e-05 9.99958286e-01]\n",
      "El cliente  Escritura con probabilidad [6.39544527e-07 9.99999360e-01]\n",
      "El cliente  Escritura con probabilidad [0.11532117 0.88467883]\n",
      "El cliente  Escritura con probabilidad [0.17639433 0.82360567]\n",
      "El cliente  Escritura con probabilidad [0.07037332 0.92962668]\n",
      "El cliente  Escritura con probabilidad [0.10011299 0.89988701]\n",
      "El cliente  Escritura con probabilidad [0.17427384 0.82572616]\n",
      "El cliente  Escritura con probabilidad [0.19157969 0.80842031]\n",
      "El cliente  Escritura con probabilidad [0.17903169 0.82096831]\n",
      "El cliente  Escritura con probabilidad [0.06090334 0.93909666]\n",
      "El cliente  Escritura con probabilidad [1.49893301e-05 9.99985011e-01]\n",
      "El cliente  Escritura con probabilidad [0.1886153 0.8113847]\n",
      "El cliente  Escritura con probabilidad [3.48660813e-07 9.99999651e-01]\n",
      "El cliente  Escritura con probabilidad [4.90450943e-05 9.99950955e-01]\n",
      "El cliente  Escritura con probabilidad [1.16164466e-08 9.99999988e-01]\n",
      "El cliente  Escritura con probabilidad [0.0155076 0.9844924]\n",
      "El cliente  Escritura con probabilidad [0.11039286 0.88960714]\n",
      "El cliente  Escritura con probabilidad [2.19552354e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.08605939 0.91394061]\n",
      "El cliente  Escritura con probabilidad [0.18014161 0.81985839]\n",
      "El cliente  Escritura con probabilidad [1.0337675e-08 9.9999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.04705683 0.95294317]\n",
      "El cliente  Escritura con probabilidad [0.02492114 0.97507886]\n",
      "El cliente  Escritura con probabilidad [0.01127188 0.98872812]\n",
      "El cliente  Escritura con probabilidad [7.72402999e-04 9.99227597e-01]\n",
      "El cliente  Escritura con probabilidad [0.19952817 0.80047183]\n",
      "El cliente  Escritura con probabilidad [0.16441721 0.83558279]\n",
      "El cliente  Escritura con probabilidad [2.41218157e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.28137975 0.71862025]\n",
      "El cliente  Escritura con probabilidad [0.00412885 0.99587115]\n",
      "El cliente  Escritura con probabilidad [0.00803361 0.99196639]\n",
      "El cliente  Escritura con probabilidad [0.2800275 0.7199725]\n",
      "El cliente  Escritura con probabilidad [0.08964192 0.91035808]\n",
      "El cliente  Escritura con probabilidad [0.09095095 0.90904905]\n",
      "El cliente  Escritura con probabilidad [0.24479525 0.75520475]\n",
      "El cliente  Escritura con probabilidad [0.23745966 0.76254034]\n",
      "El cliente  Escritura con probabilidad [6.68803626e-08 9.99999933e-01]\n",
      "El cliente  Escritura con probabilidad [0.01331007 0.98668993]\n",
      "El cliente  Escritura con probabilidad [0.12948138 0.87051862]\n",
      "El cliente  Escritura con probabilidad [0.34959823 0.65040177]\n",
      "El cliente  Escritura con probabilidad [0.43374017 0.56625983]\n",
      "El cliente  Escritura con probabilidad [1.04757314e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.12159986 0.87840014]\n",
      "El cliente  Escritura con probabilidad [7.01153018e-04 9.99298847e-01]\n",
      "El cliente  Escritura con probabilidad [0.13462924 0.86537076]\n",
      "El cliente  Escritura con probabilidad [0.16706193 0.83293807]\n",
      "El cliente  Escritura con probabilidad [0.06491218 0.93508782]\n",
      "El cliente  Escritura con probabilidad [0.16505 0.83495]\n",
      "El cliente  Escritura con probabilidad [0.20394079 0.79605921]\n",
      "El cliente  Escritura con probabilidad [2.29390471e-04 9.99770610e-01]\n",
      "El cliente  Escritura con probabilidad [7.10398768e-06 9.99992896e-01]\n",
      "El cliente  Escritura con probabilidad [0.17607257 0.82392743]\n",
      "El cliente  Escritura con probabilidad [0.20667171 0.79332829]\n",
      "El cliente  Escritura con probabilidad [1.86291447e-04 9.99813709e-01]\n",
      "El cliente  Escritura con probabilidad [0.36285343 0.63714657]\n",
      "El cliente  Escritura con probabilidad [0.22472931 0.77527069]\n",
      "El cliente  Escritura con probabilidad [0.2239386 0.7760614]\n",
      "El cliente  Escritura con probabilidad [0.05182394 0.94817606]\n",
      "El cliente  Escritura con probabilidad [0.06491123 0.93508877]\n",
      "El cliente  Escritura con probabilidad [0.21663132 0.78336868]\n",
      "El cliente  Escritura con probabilidad [0.30467017 0.69532983]\n",
      "El cliente  Escritura con probabilidad [0.16760711 0.83239289]\n",
      "El cliente  Escritura con probabilidad [0.1291561 0.8708439]\n",
      "El cliente  Escritura con probabilidad [0.25132529 0.74867471]\n",
      "El cliente  Escritura con probabilidad [0.36258762 0.63741238]\n",
      "El cliente  Escritura con probabilidad [0.04911787 0.95088213]\n",
      "El cliente  Escritura con probabilidad [0.02244966 0.97755034]\n",
      "El cliente  Escritura con probabilidad [1.49609336e-04 9.99850391e-01]\n",
      "El cliente  Escritura con probabilidad [0.14071548 0.85928452]\n",
      "El cliente  Escritura con probabilidad [0.11495417 0.88504583]\n",
      "El cliente  Escritura con probabilidad [9.42286613e-04 9.99057713e-01]\n",
      "El cliente  Escritura con probabilidad [0.00143107 0.99856893]\n",
      "El cliente  Escritura con probabilidad [0.17405792 0.82594208]\n",
      "El cliente  Escritura con probabilidad [0.15403754 0.84596246]\n",
      "El cliente  Escritura con probabilidad [0.07798897 0.92201103]\n",
      "El cliente  Escritura con probabilidad [0.15088625 0.84911375]\n",
      "El cliente  Escritura con probabilidad [0.02140368 0.97859632]\n",
      "El cliente  Escritura con probabilidad [5.56529045e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.27132991 0.72867009]\n",
      "El cliente  Escritura con probabilidad [0.19950145 0.80049855]\n",
      "El cliente  Escritura con probabilidad [0.13901168 0.86098832]\n",
      "El cliente  Escritura con probabilidad [0.09873466 0.90126534]\n",
      "El cliente  Escritura con probabilidad [0.01830992 0.98169008]\n",
      "El cliente  Escritura con probabilidad [0.18366964 0.81633036]\n",
      "El cliente  Escritura con probabilidad [0.01332388 0.98667612]\n",
      "El cliente  Escritura con probabilidad [0.46254607 0.53745393]\n",
      "El cliente  Escritura con probabilidad [0.19590172 0.80409828]\n",
      "El cliente  Escritura con probabilidad [0.07395233 0.92604767]\n",
      "El cliente  Escritura con probabilidad [0.12857363 0.87142637]\n",
      "El cliente  Escritura con probabilidad [0.0911081 0.9088919]\n",
      "El cliente  Escritura con probabilidad [0.07449579 0.92550421]\n",
      "El cliente  Escritura con probabilidad [0.17430974 0.82569026]\n",
      "El cliente  Escritura con probabilidad [0.14332425 0.85667575]\n",
      "El cliente  Escritura con probabilidad [6.16181595e-09 9.99999994e-01]\n",
      "El cliente  Escritura con probabilidad [0.22992789 0.77007211]\n",
      "El cliente  Escritura con probabilidad [0.16743825 0.83256175]\n",
      "El cliente  Escritura con probabilidad [0.09008575 0.90991425]\n",
      "El cliente  Escritura con probabilidad [0.04414832 0.95585168]\n",
      "El cliente  Escritura con probabilidad [0.02274591 0.97725409]\n",
      "El cliente  Escritura con probabilidad [0.20070469 0.79929531]\n",
      "El cliente  Escritura con probabilidad [0.07798656 0.92201344]\n",
      "El cliente  Escritura con probabilidad [0.18134339 0.81865661]\n",
      "El cliente  Escritura con probabilidad [0.15542039 0.84457961]\n",
      "El cliente  Escritura con probabilidad [2.57553763e-05 9.99974245e-01]\n",
      "El cliente  Escritura con probabilidad [0.01701033 0.98298967]\n",
      "El cliente  Escritura con probabilidad [8.76299922e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.11286159 0.88713841]\n",
      "El cliente  Escritura con probabilidad [0.17420545 0.82579455]\n",
      "El cliente  Escritura con probabilidad [0.03499445 0.96500555]\n",
      "El cliente  Escritura con probabilidad [0.30775659 0.69224341]\n",
      "El cliente  Escritura con probabilidad [0.22450448 0.77549552]\n",
      "El cliente  Escritura con probabilidad [0.08140509 0.91859491]\n",
      "El cliente  Escritura con probabilidad [0.15755967 0.84244033]\n",
      "El cliente  Escritura con probabilidad [0.10660425 0.89339575]\n",
      "El cliente  Escritura con probabilidad [0.09912686 0.90087314]\n",
      "El cliente  Escritura con probabilidad [0.1825493 0.8174507]\n",
      "El cliente  Escritura con probabilidad [0.2059585 0.7940415]\n",
      "El cliente  Escritura con probabilidad [0.06179838 0.93820162]\n",
      "El cliente  Escritura con probabilidad [8.62159364e-05 9.99913784e-01]\n",
      "El cliente  Escritura con probabilidad [0.05377142 0.94622858]\n",
      "El cliente  Escritura con probabilidad [0.37969081 0.62030919]\n",
      "El cliente  Escritura con probabilidad [0.16296382 0.83703618]\n",
      "El cliente  Escritura con probabilidad [0.25362571 0.74637429]\n",
      "El cliente  Escritura con probabilidad [0.18637656 0.81362344]\n",
      "El cliente  Escritura con probabilidad [0.19559105 0.80440895]\n",
      "El cliente  Escritura con probabilidad [0.1573094 0.8426906]\n",
      "El cliente  Escritura con probabilidad [0.10646369 0.89353631]\n",
      "El cliente  Escritura con probabilidad [0.02103796 0.97896204]\n",
      "El cliente  Escritura con probabilidad [3.89264877e-07 9.99999611e-01]\n",
      "El cliente  Escritura con probabilidad [0.17127727 0.82872273]\n",
      "El cliente  Escritura con probabilidad [0.07906373 0.92093627]\n",
      "El cliente  Escritura con probabilidad [0.11082176 0.88917824]\n",
      "El cliente  Escritura con probabilidad [0.10923159 0.89076841]\n",
      "El cliente  Escritura con probabilidad [0.01188578 0.98811422]\n",
      "El cliente  Escritura con probabilidad [0.17155223 0.82844777]\n",
      "El cliente  Escritura con probabilidad [0.08123709 0.91876291]\n",
      "El cliente  Escritura con probabilidad [0.17764156 0.82235844]\n",
      "El cliente  Escritura con probabilidad [0.12981465 0.87018535]\n",
      "El cliente  Escritura con probabilidad [0.02319048 0.97680952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.04393329 0.95606671]\n",
      "El cliente  Escritura con probabilidad [0.04028495 0.95971505]\n",
      "El cliente  Escritura con probabilidad [2.42455252e-04 9.99757545e-01]\n",
      "El cliente  Escritura con probabilidad [0.41724503 0.58275497]\n",
      "El cliente  Escritura con probabilidad [0.04251312 0.95748688]\n",
      "El cliente  Escritura con probabilidad [0.15384183 0.84615817]\n",
      "El cliente  Escritura con probabilidad [0.16980314 0.83019686]\n",
      "El cliente  Escritura con probabilidad [0.09243794 0.90756206]\n",
      "El cliente  Escritura con probabilidad [0.01205889 0.98794111]\n",
      "El cliente  Escritura con probabilidad [0.17319235 0.82680765]\n",
      "El cliente  Escritura con probabilidad [8.63276117e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.1200947 0.8799053]\n",
      "El cliente  Escritura con probabilidad [1.13176334e-08 9.99999989e-01]\n",
      "El cliente  Escritura con probabilidad [0.08609037 0.91390963]\n",
      "El cliente  Escritura con probabilidad [0.08350252 0.91649748]\n",
      "El cliente  Escritura con probabilidad [0.21757841 0.78242159]\n",
      "El cliente  Escritura con probabilidad [0.27322713 0.72677287]\n",
      "El cliente  Escritura con probabilidad [0.1638799 0.8361201]\n",
      "El cliente  Escritura con probabilidad [1.17585041e-04 9.99882415e-01]\n",
      "El cliente  Escritura con probabilidad [0.26294422 0.73705578]\n",
      "El cliente  Escritura con probabilidad [0.08941368 0.91058632]\n",
      "El cliente  Escritura con probabilidad [2.05688148e-05 9.99979431e-01]\n",
      "El cliente  Escritura con probabilidad [0.22310546 0.77689454]\n",
      "El cliente  Escritura con probabilidad [0.28421637 0.71578363]\n",
      "El cliente  Escritura con probabilidad [2.05438178e-05 9.99979456e-01]\n",
      "El cliente  Escritura con probabilidad [0.11745823 0.88254177]\n",
      "El cliente  Escritura con probabilidad [0.21085011 0.78914989]\n",
      "El cliente  Escritura con probabilidad [0.21727689 0.78272311]\n",
      "El cliente  Escritura con probabilidad [0.20037219 0.79962781]\n",
      "El cliente  Escritura con probabilidad [0.01808492 0.98191508]\n",
      "El cliente  Escritura con probabilidad [0.11020874 0.88979126]\n",
      "El cliente  Escritura con probabilidad [0.16813583 0.83186417]\n",
      "El cliente  Escritura con probabilidad [0.25983088 0.74016912]\n",
      "El cliente  Escritura con probabilidad [0.09791358 0.90208642]\n",
      "El cliente  Escritura con probabilidad [0.21610997 0.78389003]\n",
      "El cliente  Escritura con probabilidad [2.3897595e-10 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.14328738 0.85671262]\n",
      "El cliente  Escritura con probabilidad [0.07457521 0.92542479]\n",
      "El cliente  Escritura con probabilidad [0.20126462 0.79873538]\n",
      "El cliente  Escritura con probabilidad [0.1743805 0.8256195]\n",
      "El cliente  Escritura con probabilidad [0.13807354 0.86192646]\n",
      "El cliente  Escritura con probabilidad [0.12092726 0.87907274]\n",
      "El cliente  Escritura con probabilidad [0.1193511 0.8806489]\n",
      "El cliente  Escritura con probabilidad [0.24042088 0.75957912]\n",
      "El cliente  Escritura con probabilidad [0.14693613 0.85306387]\n",
      "El cliente  Escritura con probabilidad [0.08191029 0.91808971]\n",
      "El cliente  Escritura con probabilidad [0.10866038 0.89133962]\n",
      "El cliente  Escritura con probabilidad [0.01328334 0.98671666]\n",
      "El cliente  Escritura con probabilidad [0.12864211 0.87135789]\n",
      "El cliente  Escritura con probabilidad [0.21009983 0.78990017]\n",
      "El cliente  Escritura con probabilidad [0.0349898 0.9650102]\n",
      "El cliente  Escritura con probabilidad [0.18801344 0.81198656]\n",
      "El cliente  Escritura con probabilidad [0.12577214 0.87422786]\n",
      "El cliente  Escritura con probabilidad [0.11494259 0.88505741]\n",
      "El cliente  Escritura con probabilidad [0.29265397 0.70734603]\n",
      "El cliente  Escritura con probabilidad [0.28437588 0.71562412]\n",
      "El cliente  Escritura con probabilidad [0.06519253 0.93480747]\n",
      "El cliente  Escritura con probabilidad [0.0325306 0.9674694]\n",
      "El cliente  Escritura con probabilidad [1.61299749e-08 9.99999984e-01]\n",
      "El cliente  Escritura con probabilidad [0.01433654 0.98566346]\n",
      "El cliente  Escritura con probabilidad [0.08569705 0.91430295]\n",
      "El cliente  Escritura con probabilidad [0.23698187 0.76301813]\n",
      "El cliente  Escritura con probabilidad [0.44918793 0.55081207]\n",
      "El cliente  Escritura con probabilidad [0.07150538 0.92849462]\n",
      "El cliente  Escritura con probabilidad [0.22897925 0.77102075]\n",
      "El cliente  Escritura con probabilidad [0.00460438 0.99539562]\n",
      "El cliente  Escritura con probabilidad [0.18906568 0.81093432]\n",
      "El cliente  Escritura con probabilidad [0.21848054 0.78151946]\n",
      "El cliente  Escritura con probabilidad [0.04994687 0.95005313]\n",
      "El cliente  Escritura con probabilidad [0.29181241 0.70818759]\n",
      "El cliente  Escritura con probabilidad [0.02219781 0.97780219]\n",
      "El cliente  Escritura con probabilidad [5.15183896e-05 9.99948482e-01]\n",
      "El cliente  Escritura con probabilidad [0.01893703 0.98106297]\n",
      "El cliente  Escritura con probabilidad [0.27436612 0.72563388]\n",
      "El cliente  Escritura con probabilidad [0.19057782 0.80942218]\n",
      "El cliente  Escritura con probabilidad [0.15225825 0.84774175]\n",
      "El cliente  Escritura con probabilidad [0.0129237 0.9870763]\n",
      "El cliente  Escritura con probabilidad [0.28117811 0.71882189]\n",
      "El cliente  Escritura con probabilidad [0.17186838 0.82813162]\n",
      "El cliente  Escritura con probabilidad [4.02355219e-06 9.99995976e-01]\n",
      "El cliente  Escritura con probabilidad [0.15089878 0.84910122]\n",
      "El cliente  Escritura con probabilidad [0.18021464 0.81978536]\n",
      "El cliente  Escritura con probabilidad [2.32288411e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.16151997 0.83848003]\n",
      "El cliente  Escritura con probabilidad [0.10661337 0.89338663]\n",
      "El cliente  Escritura con probabilidad [0.24716149 0.75283851]\n",
      "El cliente  Escritura con probabilidad [0.0921564 0.9078436]\n",
      "El cliente  Escritura con probabilidad [0.05266538 0.94733462]\n",
      "El cliente  Escritura con probabilidad [0.02231166 0.97768834]\n",
      "El cliente  Escritura con probabilidad [0.19131707 0.80868293]\n",
      "El cliente  Escritura con probabilidad [0.06885098 0.93114902]\n",
      "El cliente  Escritura con probabilidad [4.09841935e-08 9.99999959e-01]\n",
      "El cliente  Escritura con probabilidad [0.20640957 0.79359043]\n",
      "El cliente  Escritura con probabilidad [0.22621039 0.77378961]\n",
      "El cliente  Escritura con probabilidad [0.05401977 0.94598023]\n",
      "El cliente  Escritura con probabilidad [0.06706566 0.93293434]\n",
      "El cliente  Escritura con probabilidad [0.01627985 0.98372015]\n",
      "El cliente  Escritura con probabilidad [0.16012712 0.83987288]\n",
      "El cliente  Escritura con probabilidad [0.0467126 0.9532874]\n",
      "El cliente  Escritura con probabilidad [0.21685859 0.78314141]\n",
      "El cliente  Escritura con probabilidad [0.08906959 0.91093041]\n",
      "El cliente  Escritura con probabilidad [0.03402235 0.96597765]\n",
      "El cliente  Escritura con probabilidad [5.89607208e-04 9.99410393e-01]\n",
      "El cliente  Escritura con probabilidad [0.02028005 0.97971995]\n",
      "El cliente  Escritura con probabilidad [0.02380129 0.97619871]\n",
      "El cliente  Escritura con probabilidad [0.20449414 0.79550586]\n",
      "El cliente  Escritura con probabilidad [0.01604855 0.98395145]\n",
      "El cliente  Escritura con probabilidad [0.15193241 0.84806759]\n",
      "El cliente  Escritura con probabilidad [0.02052319 0.97947681]\n",
      "El cliente  Escritura con probabilidad [0.12698841 0.87301159]\n",
      "El cliente  Escritura con probabilidad [0.021643 0.978357]\n",
      "El cliente  Escritura con probabilidad [0.13908082 0.86091918]\n",
      "El cliente  Escritura con probabilidad [0.1659095 0.8340905]\n",
      "El cliente  Escritura con probabilidad [0.03407287 0.96592713]\n",
      "El cliente  Escritura con probabilidad [3.69762572e-04 9.99630237e-01]\n",
      "El cliente  Escritura con probabilidad [0.27482593 0.72517407]\n",
      "El cliente  Escritura con probabilidad [0.21308575 0.78691425]\n",
      "El cliente  Escritura con probabilidad [0.03952453 0.96047547]\n",
      "El cliente  Escritura con probabilidad [8.41455545e-04 9.99158544e-01]\n",
      "El cliente  Escritura con probabilidad [0.11056982 0.88943018]\n",
      "El cliente  Escritura con probabilidad [0.09778635 0.90221365]\n",
      "El cliente  Escritura con probabilidad [0.01672967 0.98327033]\n",
      "El cliente  Escritura con probabilidad [0.15803739 0.84196261]\n",
      "El cliente  Escritura con probabilidad [0.15313961 0.84686039]\n",
      "El cliente  Escritura con probabilidad [0.00294041 0.99705959]\n",
      "El cliente  Escritura con probabilidad [0.09729419 0.90270581]\n",
      "El cliente  Escritura con probabilidad [0.2564437 0.7435563]\n",
      "El cliente  Escritura con probabilidad [0.17880139 0.82119861]\n",
      "El cliente  Escritura con probabilidad [0.2462284 0.7537716]\n",
      "El cliente  Escritura con probabilidad [0.2241479 0.7758521]\n",
      "El cliente  Escritura con probabilidad [0.28067056 0.71932944]\n",
      "El cliente  Escritura con probabilidad [0.09297724 0.90702276]\n",
      "El cliente  Escritura con probabilidad [2.85652075e-07 9.99999714e-01]\n",
      "El cliente  Escritura con probabilidad [2.32480701e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.99358715e-06 9.99998006e-01]\n",
      "El cliente  Escritura con probabilidad [3.94780668e-07 9.99999605e-01]\n",
      "El cliente  Escritura con probabilidad [0.14646904 0.85353096]\n",
      "El cliente  Escritura con probabilidad [0.28220864 0.71779136]\n",
      "El cliente  Escritura con probabilidad [0.0825799 0.9174201]\n",
      "El cliente  Escritura con probabilidad [0.08153715 0.91846285]\n",
      "El cliente  Escritura con probabilidad [0.08016046 0.91983954]\n",
      "El cliente  Escritura con probabilidad [0.20802268 0.79197732]\n",
      "El cliente  Escritura con probabilidad [0.22661564 0.77338436]\n",
      "El cliente  Escritura con probabilidad [1.08781428e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.1085418 0.8914582]\n",
      "El cliente  Escritura con probabilidad [0.26026606 0.73973394]\n",
      "El cliente  Escritura con probabilidad [0.22514415 0.77485585]\n",
      "El cliente  Escritura con probabilidad [0.13723803 0.86276197]\n",
      "El cliente  Escritura con probabilidad [0.12185919 0.87814081]\n",
      "El cliente  Escritura con probabilidad [0.17857539 0.82142461]\n",
      "El cliente  Escritura con probabilidad [7.24087457e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.00241256 0.99758744]\n",
      "El cliente  Escritura con probabilidad [0.11293856 0.88706144]\n",
      "El cliente  Escritura con probabilidad [0.18928875 0.81071125]\n",
      "El cliente  Escritura con probabilidad [0.09066809 0.90933191]\n",
      "El cliente  Escritura con probabilidad [8.60828693e-04 9.99139171e-01]\n",
      "El cliente  Escritura con probabilidad [0.03691462 0.96308538]\n",
      "El cliente  Escritura con probabilidad [0.24868996 0.75131004]\n",
      "El cliente  Escritura con probabilidad [0.14229273 0.85770727]\n",
      "El cliente  Escritura con probabilidad [0.20670466 0.79329534]\n",
      "El cliente  Escritura con probabilidad [0.02264335 0.97735665]\n",
      "El cliente  Escritura con probabilidad [0.04912739 0.95087261]\n",
      "El cliente  Escritura con probabilidad [5.56272806e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.42216729 0.57783271]\n",
      "El cliente  Escritura con probabilidad [0.23566829 0.76433171]\n",
      "El cliente  Escritura con probabilidad [0.1522502 0.8477498]\n",
      "El cliente  Escritura con probabilidad [0.08272611 0.91727389]\n",
      "El cliente  Escritura con probabilidad [0.06072886 0.93927114]\n",
      "El cliente  Escritura con probabilidad [5.60822259e-07 9.99999439e-01]\n",
      "El cliente  Escritura con probabilidad [0.23884243 0.76115757]\n",
      "El cliente  Escritura con probabilidad [0.21386465 0.78613535]\n",
      "El cliente  Escritura con probabilidad [0.0497865 0.9502135]\n",
      "El cliente  Escritura con probabilidad [0.05847042 0.94152958]\n",
      "El cliente  Escritura con probabilidad [0.06748633 0.93251367]\n",
      "El cliente  Escritura con probabilidad [0.12307724 0.87692276]\n",
      "El cliente  Escritura con probabilidad [0.39032682 0.60967318]\n",
      "El cliente  Escritura con probabilidad [2.49445638e-04 9.99750554e-01]\n",
      "El cliente  Escritura con probabilidad [3.00307851e-05 9.99969969e-01]\n",
      "El cliente  Escritura con probabilidad [0.20697737 0.79302263]\n",
      "El cliente  Escritura con probabilidad [0.20842443 0.79157557]\n",
      "El cliente  Escritura con probabilidad [4.48854349e-04 9.99551146e-01]\n",
      "El cliente  Escritura con probabilidad [0.43372616 0.56627384]\n",
      "El cliente  Escritura con probabilidad [0.27267864 0.72732136]\n",
      "El cliente  Escritura con probabilidad [0.1238248 0.8761752]\n",
      "El cliente  Escritura con probabilidad [0.07621725 0.92378275]\n",
      "El cliente  Escritura con probabilidad [1.91335784e-08 9.99999981e-01]\n",
      "El cliente  Escritura con probabilidad [1.90932287e-08 9.99999981e-01]\n",
      "El cliente  Escritura con probabilidad [0.09630061 0.90369939]\n",
      "El cliente  Escritura con probabilidad [0.21513332 0.78486668]\n",
      "El cliente  Escritura con probabilidad [0.07267078 0.92732922]\n",
      "El cliente  Escritura con probabilidad [0.01633581 0.98366419]\n",
      "El cliente  Escritura con probabilidad [0.28463262 0.71536738]\n",
      "El cliente  Escritura con probabilidad [0.14648107 0.85351893]\n",
      "El cliente  Escritura con probabilidad [0.19046669 0.80953331]\n",
      "El cliente  Escritura con probabilidad [0.14245495 0.85754505]\n",
      "El cliente  Escritura con probabilidad [0.11857138 0.88142862]\n",
      "El cliente  Escritura con probabilidad [0.00646782 0.99353218]\n",
      "El cliente  Escritura con probabilidad [0.17319469 0.82680531]\n",
      "El cliente  Escritura con probabilidad [0.10001426 0.89998574]\n",
      "El cliente  Escritura con probabilidad [0.00921764 0.99078236]\n",
      "El cliente  Escritura con probabilidad [0.10507842 0.89492158]\n",
      "El cliente  Escritura con probabilidad [0.22432194 0.77567806]\n",
      "El cliente  Escritura con probabilidad [0.12461064 0.87538936]\n",
      "El cliente  Escritura con probabilidad [3.31086486e-07 9.99999669e-01]\n",
      "El cliente  Escritura con probabilidad [0.10851414 0.89148586]\n",
      "El cliente  Escritura con probabilidad [0.32874124 0.67125876]\n",
      "El cliente  Escritura con probabilidad [4.45997344e-08 9.99999955e-01]\n",
      "El cliente  Escritura con probabilidad [0.06024252 0.93975748]\n",
      "El cliente  Escritura con probabilidad [0.10344897 0.89655103]\n",
      "El cliente  Escritura con probabilidad [0.07693856 0.92306144]\n",
      "El cliente  Escritura con probabilidad [8.86052099e-04 9.99113948e-01]\n",
      "El cliente  Escritura con probabilidad [0.22329744 0.77670256]\n",
      "El cliente  Escritura con probabilidad [0.03796809 0.96203191]\n",
      "El cliente  Escritura con probabilidad [3.19717070e-05 9.99968028e-01]\n",
      "El cliente  Escritura con probabilidad [2.39860004e-08 9.99999976e-01]\n",
      "El cliente  Escritura con probabilidad [0.02428181 0.97571819]\n",
      "El cliente  Escritura con probabilidad [0.24533748 0.75466252]\n",
      "El cliente  Escritura con probabilidad [1.45568152e-06 9.99998544e-01]\n",
      "El cliente  Escritura con probabilidad [0.04051895 0.95948105]\n",
      "El cliente  Escritura con probabilidad [0.00119913 0.99880087]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.28603311 0.71396689]\n",
      "El cliente  Escritura con probabilidad [4.63434269e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [0.11183703 0.88816297]\n",
      "El cliente  Escritura con probabilidad [0.1854265 0.8145735]\n",
      "El cliente  Escritura con probabilidad [0.08492742 0.91507258]\n",
      "El cliente  Escritura con probabilidad [2.36958157e-06 9.99997630e-01]\n",
      "El cliente  Escritura con probabilidad [0.19661839 0.80338161]\n",
      "El cliente  Escritura con probabilidad [0.05578715 0.94421285]\n",
      "El cliente  Escritura con probabilidad [0.02227833 0.97772167]\n",
      "El cliente  Escritura con probabilidad [0.06125053 0.93874947]\n",
      "El cliente  Escritura con probabilidad [0.10125572 0.89874428]\n",
      "El cliente  Escritura con probabilidad [3.47247181e-07 9.99999653e-01]\n",
      "El cliente  Escritura con probabilidad [0.04445757 0.95554243]\n",
      "El cliente  Escritura con probabilidad [1.36536879e-05 9.99986346e-01]\n",
      "El cliente  Escritura con probabilidad [0.06198344 0.93801656]\n",
      "El cliente  Escritura con probabilidad [0.22446695 0.77553305]\n",
      "El cliente  Escritura con probabilidad [0.19283144 0.80716856]\n",
      "El cliente  Escritura con probabilidad [0.04129645 0.95870355]\n",
      "El cliente  Escritura con probabilidad [0.11483529 0.88516471]\n",
      "El cliente  Escritura con probabilidad [0.00756423 0.99243577]\n",
      "El cliente  Escritura con probabilidad [0.17859159 0.82140841]\n",
      "El cliente  Escritura con probabilidad [2.72906452e-08 9.99999973e-01]\n",
      "El cliente  Escritura con probabilidad [0.24702673 0.75297327]\n",
      "El cliente  Escritura con probabilidad [0.15204387 0.84795613]\n",
      "El cliente  Escritura con probabilidad [0.02834845 0.97165155]\n",
      "El cliente  Escritura con probabilidad [3.59964017e-07 9.99999640e-01]\n",
      "El cliente  Escritura con probabilidad [0.05383801 0.94616199]\n",
      "El cliente  Escritura con probabilidad [0.22765427 0.77234573]\n",
      "El cliente  Escritura con probabilidad [0.26445145 0.73554855]\n",
      "El cliente  Escritura con probabilidad [3.24462152e-05 9.99967554e-01]\n",
      "El cliente  Escritura con probabilidad [0.21773215 0.78226785]\n",
      "El cliente  Escritura con probabilidad [0.22108678 0.77891322]\n",
      "El cliente  Escritura con probabilidad [0.07098081 0.92901919]\n",
      "El cliente  Escritura con probabilidad [0.08048698 0.91951302]\n",
      "El cliente  Escritura con probabilidad [0.07251854 0.92748146]\n",
      "El cliente  Escritura con probabilidad [0.34022416 0.65977584]\n",
      "El cliente  Escritura con probabilidad [1.37691414e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.13713964 0.86286036]\n",
      "El cliente  Escritura con probabilidad [0.00753519 0.99246481]\n",
      "El cliente  Escritura con probabilidad [0.43305793 0.56694207]\n",
      "El cliente  Escritura con probabilidad [6.90559809e-05 9.99930944e-01]\n",
      "El cliente  Escritura con probabilidad [0.13800509 0.86199491]\n",
      "El cliente  Escritura con probabilidad [0.03356262 0.96643738]\n",
      "El cliente  Escritura con probabilidad [0.21946813 0.78053187]\n",
      "El cliente  Escritura con probabilidad [0.42856188 0.57143812]\n",
      "El cliente  Escritura con probabilidad [0.1733371 0.8266629]\n",
      "El cliente  Escritura con probabilidad [8.03001016e-04 9.99196999e-01]\n",
      "El cliente  Escritura con probabilidad [0.03001149 0.96998851]\n",
      "El cliente  Escritura con probabilidad [0.14181473 0.85818527]\n",
      "El cliente  Escritura con probabilidad [0.01030972 0.98969028]\n",
      "El cliente  Escritura con probabilidad [0.02624071 0.97375929]\n",
      "El cliente  Escritura con probabilidad [0.2463923 0.7536077]\n",
      "El cliente  Escritura con probabilidad [1.21211026e-05 9.99987879e-01]\n",
      "El cliente  Escritura con probabilidad [0.03027067 0.96972933]\n",
      "El cliente  Escritura con probabilidad [2.19158025e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.11966408 0.88033592]\n",
      "El cliente  Escritura con probabilidad [0.09817019 0.90182981]\n",
      "El cliente  Escritura con probabilidad [0.00723711 0.99276289]\n",
      "El cliente  Escritura con probabilidad [0.2873323 0.7126677]\n",
      "El cliente  Escritura con probabilidad [0.0830036 0.9169964]\n",
      "El cliente  Escritura con probabilidad [0.07449666 0.92550334]\n",
      "El cliente  Escritura con probabilidad [0.01808729 0.98191271]\n",
      "El cliente  Escritura con probabilidad [0.20467373 0.79532627]\n",
      "El cliente  Escritura con probabilidad [4.79182720e-04 9.99520817e-01]\n",
      "El cliente  Escritura con probabilidad [0.09374467 0.90625533]\n",
      "El cliente  Escritura con probabilidad [0.44226791 0.55773209]\n",
      "El cliente  Escritura con probabilidad [0.20922571 0.79077429]\n",
      "El cliente  Escritura con probabilidad [0.31781166 0.68218834]\n",
      "El cliente  Escritura con probabilidad [0.17940923 0.82059077]\n",
      "El cliente  Escritura con probabilidad [0.26365735 0.73634265]\n",
      "El cliente  Escritura con probabilidad [0.08271367 0.91728633]\n",
      "El cliente  Escritura con probabilidad [0.04006783 0.95993217]\n",
      "El cliente  Escritura con probabilidad [0.21499102 0.78500898]\n",
      "El cliente  Escritura con probabilidad [0.02046032 0.97953968]\n",
      "El cliente  Escritura con probabilidad [0.06572712 0.93427288]\n",
      "El cliente  Escritura con probabilidad [0.2322402 0.7677598]\n",
      "El cliente  Escritura con probabilidad [0.03469666 0.96530334]\n",
      "El cliente  Escritura con probabilidad [0.22260985 0.77739015]\n",
      "El cliente  Escritura con probabilidad [0.17550632 0.82449368]\n",
      "El cliente  Escritura con probabilidad [0.04624669 0.95375331]\n",
      "El cliente  Escritura con probabilidad [0.12993087 0.87006913]\n",
      "El cliente  Escritura con probabilidad [0.26302841 0.73697159]\n",
      "El cliente  Escritura con probabilidad [0.2387995 0.7612005]\n",
      "El cliente  Escritura con probabilidad [4.88315123e-04 9.99511685e-01]\n",
      "El cliente  Escritura con probabilidad [0.34180892 0.65819108]\n",
      "El cliente  Escritura con probabilidad [0.19784585 0.80215415]\n",
      "El cliente  Escritura con probabilidad [0.39711402 0.60288598]\n",
      "El cliente  Escritura con probabilidad [7.48155438e-09 9.99999993e-01]\n",
      "El cliente  Escritura con probabilidad [0.13677713 0.86322287]\n",
      "El cliente  Escritura con probabilidad [6.45324835e-04 9.99354675e-01]\n",
      "El cliente  Escritura con probabilidad [0.20055172 0.79944828]\n",
      "El cliente  Escritura con probabilidad [0.18778812 0.81221188]\n",
      "El cliente  Escritura con probabilidad [0.0247613 0.9752387]\n",
      "El cliente  Escritura con probabilidad [0.26551051 0.73448949]\n",
      "El cliente  Escritura con probabilidad [0.15298297 0.84701703]\n",
      "El cliente  Escritura con probabilidad [0.1341057 0.8658943]\n",
      "El cliente  Escritura con probabilidad [0.00268826 0.99731174]\n",
      "El cliente  Escritura con probabilidad [0.23050915 0.76949085]\n",
      "El cliente  Escritura con probabilidad [0.01636233 0.98363767]\n",
      "El cliente  Escritura con probabilidad [0.09864722 0.90135278]\n",
      "El cliente  Escritura con probabilidad [0.03047598 0.96952402]\n",
      "El cliente  Escritura con probabilidad [1.7239099e-10 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.26756462 0.73243538]\n",
      "El cliente  Escritura con probabilidad [7.83659641e-08 9.99999922e-01]\n",
      "El cliente  Escritura con probabilidad [0.13989226 0.86010774]\n",
      "El cliente  Escritura con probabilidad [0.08801112 0.91198888]\n",
      "El cliente  Escritura con probabilidad [1.71979863e-08 9.99999983e-01]\n",
      "El cliente  Escritura con probabilidad [1.81012183e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.21470177 0.78529823]\n",
      "El cliente  Escritura con probabilidad [0.09155224 0.90844776]\n",
      "El cliente  Escritura con probabilidad [6.17912388e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.22589564 0.77410436]\n",
      "El cliente  Escritura con probabilidad [0.29002582 0.70997418]\n",
      "El cliente  Escritura con probabilidad [0.3063762 0.6936238]\n",
      "El cliente  Escritura con probabilidad [0.05904907 0.94095093]\n",
      "El cliente  Escritura con probabilidad [0.219949 0.780051]\n",
      "El cliente  Escritura con probabilidad [0.08905153 0.91094847]\n",
      "El cliente  Escritura con probabilidad [7.53191461e-07 9.99999247e-01]\n",
      "El cliente  Escritura con probabilidad [9.43505274e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.03486629 0.96513371]\n",
      "El cliente  Escritura con probabilidad [0.07167341 0.92832659]\n",
      "El cliente  Escritura con probabilidad [0.00751309 0.99248691]\n",
      "El cliente  Escritura con probabilidad [0.19537027 0.80462973]\n",
      "El cliente  Escritura con probabilidad [0.167402 0.832598]\n",
      "El cliente  Escritura con probabilidad [0.10323486 0.89676514]\n",
      "El cliente  Escritura con probabilidad [0.01945979 0.98054021]\n",
      "El cliente  Escritura con probabilidad [0.03898339 0.96101661]\n",
      "El cliente  Escritura con probabilidad [0.22024616 0.77975384]\n",
      "El cliente  Escritura con probabilidad [0.0014541 0.9985459]\n",
      "El cliente  Escritura con probabilidad [1.22586067e-08 9.99999988e-01]\n",
      "El cliente  Escritura con probabilidad [0.10136098 0.89863902]\n",
      "El cliente  Escritura con probabilidad [0.2183396 0.7816604]\n",
      "El cliente  Escritura con probabilidad [0.24046218 0.75953782]\n",
      "El cliente  Escritura con probabilidad [0.21304204 0.78695796]\n",
      "El cliente  Escritura con probabilidad [2.24168923e-07 9.99999776e-01]\n",
      "El cliente  Escritura con probabilidad [0.26432054 0.73567946]\n",
      "El cliente  Escritura con probabilidad [0.03070907 0.96929093]\n",
      "El cliente  Escritura con probabilidad [0.18449554 0.81550446]\n",
      "El cliente  Escritura con probabilidad [0.24874785 0.75125215]\n",
      "El cliente  Escritura con probabilidad [0.24287285 0.75712715]\n",
      "El cliente  Escritura con probabilidad [0.1682475 0.8317525]\n",
      "El cliente  Escritura con probabilidad [0.24384845 0.75615155]\n",
      "El cliente  Escritura con probabilidad [1.93652200e-06 9.99998063e-01]\n",
      "El cliente  Escritura con probabilidad [0.31874042 0.68125958]\n",
      "El cliente  Escritura con probabilidad [0.21148861 0.78851139]\n",
      "El cliente  Escritura con probabilidad [6.83874268e-09 9.99999993e-01]\n",
      "El cliente  Escritura con probabilidad [0.06928943 0.93071057]\n",
      "El cliente  Escritura con probabilidad [0.0078544 0.9921456]\n",
      "El cliente  Escritura con probabilidad [4.86853002e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.43843266e-04 9.99856157e-01]\n",
      "El cliente  Escritura con probabilidad [0.16666669 0.83333331]\n",
      "El cliente  Escritura con probabilidad [0.07831254 0.92168746]\n",
      "El cliente  Escritura con probabilidad [2.30403330e-04 9.99769597e-01]\n",
      "El cliente  Escritura con probabilidad [0.38473492 0.61526508]\n",
      "El cliente  Escritura con probabilidad [0.01756898 0.98243102]\n",
      "El cliente  Escritura con probabilidad [0.18983295 0.81016705]\n",
      "El cliente  Escritura con probabilidad [0.19749051 0.80250949]\n",
      "El cliente  Escritura con probabilidad [5.56737705e-07 9.99999443e-01]\n",
      "El cliente  Escritura con probabilidad [0.07799819 0.92200181]\n",
      "El cliente  Escritura con probabilidad [0.13896073 0.86103927]\n",
      "El cliente  Escritura con probabilidad [1.80315108e-04 9.99819685e-01]\n",
      "El cliente  Escritura con probabilidad [0.17955902 0.82044098]\n",
      "El cliente  Escritura con probabilidad [0.31423614 0.68576386]\n",
      "El cliente  Escritura con probabilidad [0.18602763 0.81397237]\n",
      "El cliente  Escritura con probabilidad [0.1592055 0.8407945]\n",
      "El cliente  Escritura con probabilidad [0.12536438 0.87463562]\n",
      "El cliente  Escritura con probabilidad [0.09418352 0.90581648]\n",
      "El cliente  Escritura con probabilidad [0.09932316 0.90067684]\n",
      "El cliente  Escritura con probabilidad [0.18276739 0.81723261]\n",
      "El cliente  Escritura con probabilidad [0.16444597 0.83555403]\n",
      "El cliente  Escritura con probabilidad [6.19631235e-04 9.99380369e-01]\n",
      "El cliente  Escritura con probabilidad [0.03663257 0.96336743]\n",
      "El cliente  Escritura con probabilidad [0.12222967 0.87777033]\n",
      "El cliente  Escritura con probabilidad [3.28224546e-07 9.99999672e-01]\n",
      "El cliente  Escritura con probabilidad [0.04873023 0.95126977]\n",
      "El cliente  Escritura con probabilidad [0.18601611 0.81398389]\n",
      "El cliente  Escritura con probabilidad [0.19287128 0.80712872]\n",
      "El cliente  Escritura con probabilidad [0.4383428 0.5616572]\n",
      "El cliente  Escritura con probabilidad [0.02630087 0.97369913]\n",
      "El cliente  Escritura con probabilidad [0.2436665 0.7563335]\n",
      "El cliente  Escritura con probabilidad [0.21274728 0.78725272]\n",
      "El cliente  Escritura con probabilidad [0.25161024 0.74838976]\n",
      "El cliente  Escritura con probabilidad [0.06509316 0.93490684]\n",
      "El cliente  Escritura con probabilidad [0.17896246 0.82103754]\n",
      "El cliente  Escritura con probabilidad [0.00875634 0.99124366]\n",
      "El cliente  Escritura con probabilidad [0.00101909 0.99898091]\n",
      "El cliente  Escritura con probabilidad [0.16213175 0.83786825]\n",
      "El cliente  Escritura con probabilidad [0.0712803 0.9287197]\n",
      "El cliente  Escritura con probabilidad [0.042834 0.957166]\n",
      "El cliente  Escritura con probabilidad [2.32705830e-04 9.99767294e-01]\n",
      "El cliente  Escritura con probabilidad [2.26656346e-07 9.99999773e-01]\n",
      "El cliente  Escritura con probabilidad [0.41999138 0.58000862]\n",
      "El cliente  Escritura con probabilidad [1.49368027e-08 9.99999985e-01]\n",
      "El cliente  Escritura con probabilidad [0.05409447 0.94590553]\n",
      "El cliente  Escritura con probabilidad [1.44328993e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.12754438 0.87245562]\n",
      "El cliente  Escritura con probabilidad [0.1820143 0.8179857]\n",
      "El cliente  Escritura con probabilidad [0.26392539 0.73607461]\n",
      "El cliente  Escritura con probabilidad [0.1379988 0.8620012]\n",
      "El cliente  Escritura con probabilidad [0.04422432 0.95577568]\n",
      "El cliente  Escritura con probabilidad [0.14003798 0.85996202]\n",
      "El cliente  Escritura con probabilidad [5.05719278e-06 9.99994943e-01]\n",
      "El cliente  Escritura con probabilidad [0.4420337 0.5579663]\n",
      "El cliente  Escritura con probabilidad [0.13936072 0.86063928]\n",
      "El cliente  Escritura con probabilidad [8.60756150e-07 9.99999139e-01]\n",
      "El cliente  Escritura con probabilidad [0.18120591 0.81879409]\n",
      "El cliente  Escritura con probabilidad [0.06143173 0.93856827]\n",
      "El cliente  Escritura con probabilidad [0.22113497 0.77886503]\n",
      "El cliente  Escritura con probabilidad [0.19437716 0.80562284]\n",
      "El cliente  Escritura con probabilidad [0.01535192 0.98464808]\n",
      "El cliente  Escritura con probabilidad [0.18415929 0.81584071]\n",
      "El cliente  Escritura con probabilidad [1.23710845e-05 9.99987629e-01]\n",
      "El cliente  Escritura con probabilidad [0.2571442 0.7428558]\n",
      "El cliente  Escritura con probabilidad [0.32815458 0.67184542]\n",
      "El cliente  Escritura con probabilidad [0.01048344 0.98951656]\n",
      "El cliente  Escritura con probabilidad [0.26137175 0.73862825]\n",
      "El cliente  Escritura con probabilidad [0.2435836 0.7564164]\n",
      "El cliente  Escritura con probabilidad [6.02971836e-05 9.99939703e-01]\n",
      "El cliente  Escritura con probabilidad [5.83732848e-06 9.99994163e-01]\n",
      "El cliente  Escritura con probabilidad [0.02559506 0.97440494]\n",
      "El cliente  Escritura con probabilidad [0.0595228 0.9404772]\n",
      "El cliente  Escritura con probabilidad [0.11811764 0.88188236]\n",
      "El cliente  Escritura con probabilidad [0.21512884 0.78487116]\n",
      "El cliente  Escritura con probabilidad [2.62723177e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.13155372 0.86844628]\n",
      "El cliente  Escritura con probabilidad [3.00457193e-07 9.99999700e-01]\n",
      "El cliente  Escritura con probabilidad [0.12143354 0.87856646]\n",
      "El cliente  Escritura con probabilidad [0.2524127 0.7475873]\n",
      "El cliente  Escritura con probabilidad [0.12908571 0.87091429]\n",
      "El cliente  Escritura con probabilidad [0.12048116 0.87951884]\n",
      "El cliente  Escritura con probabilidad [0.27444401 0.72555599]\n",
      "El cliente  Escritura con probabilidad [0.23934437 0.76065563]\n",
      "El cliente  Escritura con probabilidad [4.54776152e-07 9.99999545e-01]\n",
      "El cliente  Escritura con probabilidad [0.14167103 0.85832897]\n",
      "El cliente  Escritura con probabilidad [0.30775426 0.69224574]\n",
      "El cliente  Escritura con probabilidad [0.23772515 0.76227485]\n",
      "El cliente  Escritura con probabilidad [0.24595599 0.75404401]\n",
      "El cliente  Escritura con probabilidad [0.09270503 0.90729497]\n",
      "El cliente  Escritura con probabilidad [0.15364384 0.84635616]\n",
      "El cliente  Escritura con probabilidad [0.05683723 0.94316277]\n",
      "El cliente  Escritura con probabilidad [2.07717248e-08 9.99999979e-01]\n",
      "El cliente  Escritura con probabilidad [3.22809440e-08 9.99999968e-01]\n",
      "El cliente  Escritura con probabilidad [0.05248912 0.94751088]\n",
      "El cliente  Escritura con probabilidad [0.27379544 0.72620456]\n",
      "El cliente  Escritura con probabilidad [0.10985593 0.89014407]\n",
      "El cliente  Escritura con probabilidad [0.00201051 0.99798949]\n",
      "El cliente  Escritura con probabilidad [0.19840825 0.80159175]\n",
      "El cliente  Escritura con probabilidad [0.21004971 0.78995029]\n",
      "El cliente  Escritura con probabilidad [0.08160246 0.91839754]\n",
      "El cliente  Escritura con probabilidad [0.23121248 0.76878752]\n",
      "El cliente  Escritura con probabilidad [0.15090349 0.84909651]\n",
      "El cliente  Escritura con probabilidad [0.04304453 0.95695547]\n",
      "El cliente  Escritura con probabilidad [4.26780115e-06 9.99995732e-01]\n",
      "El cliente  Escritura con probabilidad [0.07828338 0.92171662]\n",
      "El cliente  Escritura con probabilidad [0.04280937 0.95719063]\n",
      "El cliente  Escritura con probabilidad [0.01424197 0.98575803]\n",
      "El cliente  Escritura con probabilidad [0.18358843 0.81641157]\n",
      "El cliente  Escritura con probabilidad [0.00950792 0.99049208]\n",
      "El cliente  Escritura con probabilidad [0.44236195 0.55763805]\n",
      "El cliente  Escritura con probabilidad [2.33702606e-05 9.99976630e-01]\n",
      "El cliente  Escritura con probabilidad [0.20337021 0.79662979]\n",
      "El cliente  Escritura con probabilidad [0.27604436 0.72395564]\n",
      "El cliente  Escritura con probabilidad [0.02030423 0.97969577]\n",
      "El cliente  Escritura con probabilidad [0.19713061 0.80286939]\n",
      "El cliente  Escritura con probabilidad [0.01645922 0.98354078]\n",
      "El cliente  Escritura con probabilidad [0.2699669 0.7300331]\n",
      "El cliente  Escritura con probabilidad [0.15757526 0.84242474]\n",
      "El cliente  Escritura con probabilidad [0.05005994 0.94994006]\n",
      "El cliente  Escritura con probabilidad [0.18930229 0.81069771]\n",
      "El cliente  Escritura con probabilidad [0.2177795 0.7822205]\n",
      "El cliente  Escritura con probabilidad [0.44584963 0.55415037]\n",
      "El cliente  Escritura con probabilidad [0.05044933 0.94955067]\n",
      "El cliente  Escritura con probabilidad [0.12788045 0.87211955]\n",
      "El cliente  Escritura con probabilidad [1.03646874e-05 9.99989635e-01]\n",
      "El cliente  Escritura con probabilidad [0.25393602 0.74606398]\n",
      "El cliente  Escritura con probabilidad [0.18899937 0.81100063]\n",
      "El cliente  Escritura con probabilidad [0.19960145 0.80039855]\n",
      "El cliente  Escritura con probabilidad [0.06459679 0.93540321]\n",
      "El cliente  Escritura con probabilidad [0.0656989 0.9343011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.2241479 0.7758521]\n",
      "El cliente  Escritura con probabilidad [0.08170997 0.91829003]\n",
      "El cliente  Escritura con probabilidad [0.1444617 0.8555383]\n",
      "El cliente  Escritura con probabilidad [1.37945031e-06 9.99998621e-01]\n",
      "El cliente  Escritura con probabilidad [0.14111157 0.85888843]\n",
      "El cliente  Escritura con probabilidad [0.08959884 0.91040116]\n",
      "El cliente  Escritura con probabilidad [0.04062696 0.95937304]\n",
      "El cliente  Escritura con probabilidad [0.24688861 0.75311139]\n",
      "El cliente  Escritura con probabilidad [0.02074037 0.97925963]\n",
      "El cliente  Escritura con probabilidad [0.06880591 0.93119409]\n",
      "El cliente  Escritura con probabilidad [1.34031162e-08 9.99999987e-01]\n",
      "El cliente  Escritura con probabilidad [0.01646401 0.98353599]\n",
      "El cliente  Escritura con probabilidad [0.02103246 0.97896754]\n",
      "El cliente  Escritura con probabilidad [4.52847801e-08 9.99999955e-01]\n",
      "El cliente  Escritura con probabilidad [0.11105762 0.88894238]\n",
      "El cliente  Escritura con probabilidad [0.01114786 0.98885214]\n",
      "El cliente  Escritura con probabilidad [0.02146215 0.97853785]\n",
      "El cliente  Escritura con probabilidad [0.00222158 0.99777842]\n",
      "El cliente  Escritura con probabilidad [5.07823291e-05 9.99949218e-01]\n",
      "El cliente  Escritura con probabilidad [0.02592711 0.97407289]\n",
      "El cliente  Escritura con probabilidad [0.09834377 0.90165623]\n",
      "El cliente  Escritura con probabilidad [0.15131779 0.84868221]\n",
      "El cliente  Escritura con probabilidad [0.08303368 0.91696632]\n",
      "El cliente  Escritura con probabilidad [0.18807276 0.81192724]\n",
      "El cliente  Escritura con probabilidad [0.07156375 0.92843625]\n",
      "El cliente  Escritura con probabilidad [0.30160685 0.69839315]\n",
      "El cliente  Escritura con probabilidad [0.13114227 0.86885773]\n",
      "El cliente  Escritura con probabilidad [0.04516664 0.95483336]\n",
      "El cliente  Escritura con probabilidad [0.00776837 0.99223163]\n",
      "El cliente  Escritura con probabilidad [0.14348061 0.85651939]\n",
      "El cliente  Escritura con probabilidad [0.12382663 0.87617337]\n",
      "El cliente  Escritura con probabilidad [0.16790509 0.83209491]\n",
      "El cliente  Escritura con probabilidad [0.23551616 0.76448384]\n",
      "El cliente  Escritura con probabilidad [0.1302048 0.8697952]\n",
      "El cliente  Escritura con probabilidad [0.12041511 0.87958489]\n",
      "El cliente  Escritura con probabilidad [0.20794897 0.79205103]\n",
      "El cliente  Escritura con probabilidad [0.23254651 0.76745349]\n",
      "El cliente  Escritura con probabilidad [0.16365723 0.83634277]\n",
      "El cliente  Escritura con probabilidad [0.02402095 0.97597905]\n",
      "El cliente  Escritura con probabilidad [0.17848044 0.82151956]\n",
      "El cliente  Escritura con probabilidad [3.27693472e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.11485705 0.88514295]\n",
      "El cliente  Escritura con probabilidad [0.12661163 0.87338837]\n",
      "El cliente  Escritura con probabilidad [0.02863834 0.97136166]\n",
      "El cliente  Escritura con probabilidad [0.07547807 0.92452193]\n",
      "El cliente  Escritura con probabilidad [0.30157577 0.69842423]\n",
      "El cliente  Escritura con probabilidad [0.10412109 0.89587891]\n",
      "El cliente  Escritura con probabilidad [0.14976865 0.85023135]\n",
      "El cliente  Escritura con probabilidad [3.12739911e-07 9.99999687e-01]\n",
      "El cliente  Escritura con probabilidad [0.00708055 0.99291945]\n",
      "El cliente  Escritura con probabilidad [0.01756867 0.98243133]\n",
      "El cliente  Escritura con probabilidad [0.15681076 0.84318924]\n",
      "El cliente  Escritura con probabilidad [0.02282664 0.97717336]\n",
      "El cliente  Escritura con probabilidad [0.08480537 0.91519463]\n",
      "El cliente  Escritura con probabilidad [0.18204179 0.81795821]\n",
      "El cliente  Escritura con probabilidad [0.19870313 0.80129687]\n",
      "El cliente  Escritura con probabilidad [0.21089688 0.78910312]\n",
      "El cliente  Escritura con probabilidad [0.1014487 0.8985513]\n",
      "El cliente  Escritura con probabilidad [0.09726998 0.90273002]\n",
      "El cliente  Escritura con probabilidad [0.26151834 0.73848166]\n",
      "El cliente  Escritura con probabilidad [3.97725210e-04 9.99602275e-01]\n",
      "El cliente  Escritura con probabilidad [0.01899932 0.98100068]\n",
      "El cliente  Escritura con probabilidad [0.10282727 0.89717273]\n",
      "El cliente  Escritura con probabilidad [0.00103367 0.99896633]\n",
      "El cliente  Escritura con probabilidad [0.165748 0.834252]\n",
      "El cliente  Escritura con probabilidad [1.47047865e-08 9.99999985e-01]\n",
      "El cliente  Escritura con probabilidad [0.25489348 0.74510652]\n",
      "El cliente  Escritura con probabilidad [0.01518912 0.98481088]\n",
      "El cliente  Escritura con probabilidad [0.09104515 0.90895485]\n",
      "El cliente  Escritura con probabilidad [0.4020013 0.5979987]\n",
      "El cliente  Escritura con probabilidad [0.16499962 0.83500038]\n",
      "El cliente  Escritura con probabilidad [0.03852547 0.96147453]\n",
      "El cliente  Escritura con probabilidad [0.15973796 0.84026204]\n",
      "El cliente  Escritura con probabilidad [0.2544042 0.7455958]\n",
      "El cliente  Escritura con probabilidad [9.20179373e-08 9.99999908e-01]\n",
      "El cliente  Escritura con probabilidad [0.14893322 0.85106678]\n",
      "El cliente  Escritura con probabilidad [2.49442636e-07 9.99999751e-01]\n",
      "El cliente  Escritura con probabilidad [0.02171545 0.97828455]\n",
      "El cliente  Escritura con probabilidad [0.21911816 0.78088184]\n",
      "El cliente  Escritura con probabilidad [0.21097333 0.78902667]\n",
      "El cliente  Escritura con probabilidad [0.1236748 0.8763252]\n",
      "El cliente  Escritura con probabilidad [0.18010309 0.81989691]\n",
      "El cliente  Escritura con probabilidad [0.16979601 0.83020399]\n",
      "El cliente  Escritura con probabilidad [0.07046104 0.92953896]\n",
      "El cliente  Escritura con probabilidad [1.25993171e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.11046142 0.88953858]\n",
      "El cliente  Escritura con probabilidad [8.14562002e-07 9.99999185e-01]\n",
      "El cliente  Escritura con probabilidad [6.34735390e-05 9.99936526e-01]\n",
      "El cliente  Escritura con probabilidad [0.03984531 0.96015469]\n",
      "El cliente  Escritura con probabilidad [0.01822864 0.98177136]\n",
      "El cliente  Escritura con probabilidad [0.24668112 0.75331888]\n",
      "El cliente  Escritura con probabilidad [0.17113956 0.82886044]\n",
      "El cliente  Escritura con probabilidad [0.02283636 0.97716364]\n",
      "El cliente  Escritura con probabilidad [0.17284111 0.82715889]\n",
      "El cliente  Escritura con probabilidad [0.17131606 0.82868394]\n",
      "El cliente  Escritura con probabilidad [0.09969934 0.90030066]\n",
      "El cliente  Escritura con probabilidad [0.17770671 0.82229329]\n",
      "El cliente  Escritura con probabilidad [0.00407614 0.99592386]\n",
      "El cliente  Escritura con probabilidad [0.22183612 0.77816388]\n",
      "El cliente  Escritura con probabilidad [0.2178159 0.7821841]\n",
      "El cliente  Escritura con probabilidad [0.1216801 0.8783199]\n",
      "El cliente  Escritura con probabilidad [2.18892904e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.15577159 0.84422841]\n",
      "El cliente  Escritura con probabilidad [0.08155365 0.91844635]\n",
      "El cliente  Escritura con probabilidad [0.0067414 0.9932586]\n",
      "El cliente  Escritura con probabilidad [3.25187854e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.22114798 0.77885202]\n",
      "El cliente  Escritura con probabilidad [0.03245045 0.96754955]\n",
      "El cliente  Escritura con probabilidad [1.43620907e-06 9.99998564e-01]\n",
      "El cliente  Escritura con probabilidad [0.03137184 0.96862816]\n",
      "El cliente  Escritura con probabilidad [0.19962152 0.80037848]\n",
      "El cliente  Escritura con probabilidad [0.03389913 0.96610087]\n",
      "El cliente  Escritura con probabilidad [1.96965777e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.25940206 0.74059794]\n",
      "El cliente  Escritura con probabilidad [2.03777759e-06 9.99997962e-01]\n",
      "El cliente  Escritura con probabilidad [0.25358788 0.74641212]\n",
      "El cliente  Escritura con probabilidad [0.2140692 0.7859308]\n",
      "El cliente  Escritura con probabilidad [0.25237713 0.74762287]\n",
      "El cliente  Escritura con probabilidad [0.22134633 0.77865367]\n",
      "El cliente  Escritura con probabilidad [0.03357575 0.96642425]\n",
      "El cliente  Escritura con probabilidad [0.08575945 0.91424055]\n",
      "El cliente  Escritura con probabilidad [0.13663097 0.86336903]\n",
      "El cliente  Escritura con probabilidad [0.00272352 0.99727648]\n",
      "El cliente  Escritura con probabilidad [0.01982059 0.98017941]\n",
      "El cliente  Escritura con probabilidad [0.19236317 0.80763683]\n",
      "El cliente  Escritura con probabilidad [0.24725841 0.75274159]\n",
      "El cliente  Escritura con probabilidad [0.02472689 0.97527311]\n",
      "El cliente  Escritura con probabilidad [0.10574676 0.89425324]\n",
      "El cliente  Escritura con probabilidad [0.16620862 0.83379138]\n",
      "El cliente  Escritura con probabilidad [0.16098449 0.83901551]\n",
      "El cliente  Escritura con probabilidad [0.16025834 0.83974166]\n",
      "El cliente  Escritura con probabilidad [4.26615005e-07 9.99999573e-01]\n",
      "El cliente  Escritura con probabilidad [4.41811158e-07 9.99999558e-01]\n",
      "El cliente  Escritura con probabilidad [0.15362642 0.84637358]\n",
      "El cliente  Escritura con probabilidad [0.05305238 0.94694762]\n",
      "El cliente  Escritura con probabilidad [0.01928502 0.98071498]\n",
      "El cliente  Escritura con probabilidad [1.69778226e-06 9.99998302e-01]\n",
      "El cliente  Escritura con probabilidad [0.01429476 0.98570524]\n",
      "El cliente  Escritura con probabilidad [0.18153277 0.81846723]\n",
      "El cliente  Escritura con probabilidad [0.05221898 0.94778102]\n",
      "El cliente  Escritura con probabilidad [0.19401546 0.80598454]\n",
      "El cliente  Escritura con probabilidad [0.20374274 0.79625726]\n",
      "El cliente  Escritura con probabilidad [0.28596758 0.71403242]\n",
      "El cliente  Escritura con probabilidad [0.22651591 0.77348409]\n",
      "El cliente  Escritura con probabilidad [0.03208158 0.96791842]\n",
      "El cliente  Escritura con probabilidad [0.23056695 0.76943305]\n",
      "El cliente  Escritura con probabilidad [0.01811377 0.98188623]\n",
      "El cliente  Escritura con probabilidad [9.14850970e-06 9.99990851e-01]\n",
      "El cliente  Escritura con probabilidad [0.03112696 0.96887304]\n",
      "El cliente  Escritura con probabilidad [0.09326698 0.90673302]\n",
      "El cliente  Escritura con probabilidad [3.79298718e-08 9.99999962e-01]\n",
      "El cliente  Escritura con probabilidad [0.01696581 0.98303419]\n",
      "El cliente  Escritura con probabilidad [0.18032588 0.81967412]\n",
      "El cliente  Escritura con probabilidad [2.46630304e-07 9.99999753e-01]\n",
      "El cliente  Escritura con probabilidad [0.00122961 0.99877039]\n",
      "El cliente  Escritura con probabilidad [2.36371405e-07 9.99999764e-01]\n",
      "El cliente  Escritura con probabilidad [0.12091799 0.87908201]\n",
      "El cliente  Escritura con probabilidad [0.40016307 0.59983693]\n",
      "El cliente  Escritura con probabilidad [0.08315067 0.91684933]\n",
      "El cliente  Escritura con probabilidad [0.27072286 0.72927714]\n",
      "El cliente  Escritura con probabilidad [0.02396798 0.97603202]\n",
      "El cliente  Escritura con probabilidad [0.18362398 0.81637602]\n",
      "El cliente  Escritura con probabilidad [0.03478265 0.96521735]\n",
      "El cliente  Escritura con probabilidad [0.3276264 0.6723736]\n",
      "El cliente  Escritura con probabilidad [0.12508266 0.87491734]\n",
      "El cliente  Escritura con probabilidad [0.1172519 0.8827481]\n",
      "El cliente  Escritura con probabilidad [0.00112803 0.99887197]\n",
      "El cliente  Escritura con probabilidad [9.88676863e-09 9.99999990e-01]\n",
      "El cliente  Escritura con probabilidad [8.38304798e-08 9.99999916e-01]\n",
      "El cliente  Escritura con probabilidad [0.12931989 0.87068011]\n",
      "El cliente  Escritura con probabilidad [0.12201886 0.87798114]\n",
      "El cliente  Escritura con probabilidad [0.12324002 0.87675998]\n",
      "El cliente  Escritura con probabilidad [0.27281421 0.72718579]\n",
      "El cliente  Escritura con probabilidad [0.12901264 0.87098736]\n",
      "El cliente  Escritura con probabilidad [4.19013980e-04 9.99580986e-01]\n",
      "El cliente  Escritura con probabilidad [0.12169115 0.87830885]\n",
      "El cliente  Escritura con probabilidad [0.16195386 0.83804614]\n",
      "El cliente  Escritura con probabilidad [0.12105667 0.87894333]\n",
      "El cliente  Escritura con probabilidad [6.52616984e-07 9.99999347e-01]\n",
      "El cliente  Escritura con probabilidad [0.02151211 0.97848789]\n",
      "El cliente  Escritura con probabilidad [0.02473505 0.97526495]\n",
      "El cliente  Escritura con probabilidad [0.39653152 0.60346848]\n",
      "El cliente  Escritura con probabilidad [0.01603071 0.98396929]\n",
      "El cliente  Escritura con probabilidad [0.22963028 0.77036972]\n",
      "El cliente  Escritura con probabilidad [0.10510641 0.89489359]\n",
      "El cliente  Escritura con probabilidad [0.06628972 0.93371028]\n",
      "El cliente  Escritura con probabilidad [0.20684735 0.79315265]\n",
      "El cliente  Escritura con probabilidad [0.21478698 0.78521302]\n",
      "El cliente  Escritura con probabilidad [0.05048774 0.94951226]\n",
      "El cliente  Escritura con probabilidad [0.1581159 0.8418841]\n",
      "El cliente  Escritura con probabilidad [0.27592946 0.72407054]\n",
      "El cliente  Escritura con probabilidad [7.50725968e-07 9.99999249e-01]\n",
      "El cliente  Escritura con probabilidad [1.25904519e-04 9.99874095e-01]\n",
      "El cliente  Escritura con probabilidad [0.06528961 0.93471039]\n",
      "El cliente  Escritura con probabilidad [0.11475154 0.88524846]\n",
      "El cliente  Escritura con probabilidad [0.13332891 0.86667109]\n",
      "El cliente  Escritura con probabilidad [0.08933471 0.91066529]\n",
      "El cliente  Escritura con probabilidad [7.56383622e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.23967987 0.76032013]\n",
      "El cliente  Escritura con probabilidad [0.21685456 0.78314544]\n",
      "El cliente  Escritura con probabilidad [0.43587966 0.56412034]\n",
      "El cliente  Escritura con probabilidad [0.17327547 0.82672453]\n",
      "El cliente  Escritura con probabilidad [0.03988261 0.96011739]\n",
      "El cliente  Escritura con probabilidad [0.06793392 0.93206608]\n",
      "El cliente  Escritura con probabilidad [2.47027044e-07 9.99999753e-01]\n",
      "El cliente  Escritura con probabilidad [0.01612683 0.98387317]\n",
      "El cliente  Escritura con probabilidad [9.31411406e-05 9.99906859e-01]\n",
      "El cliente  Escritura con probabilidad [0.08090193 0.91909807]\n",
      "El cliente  Escritura con probabilidad [1.75667360e-08 9.99999982e-01]\n",
      "El cliente  Escritura con probabilidad [0.22085861 0.77914139]\n",
      "El cliente  Escritura con probabilidad [0.17826645 0.82173355]\n",
      "El cliente  Escritura con probabilidad [1.70930121e-05 9.99982907e-01]\n",
      "El cliente  Escritura con probabilidad [0.01239332 0.98760668]\n",
      "El cliente  Escritura con probabilidad [0.19410372 0.80589628]\n",
      "El cliente  Escritura con probabilidad [0.00232764 0.99767236]\n",
      "El cliente  Escritura con probabilidad [0.21584634 0.78415366]\n",
      "El cliente  Escritura con probabilidad [0.13488361 0.86511639]\n",
      "El cliente  Escritura con probabilidad [0.39418418 0.60581582]\n",
      "El cliente  Escritura con probabilidad [2.01698727e-05 9.99979830e-01]\n",
      "El cliente  Escritura con probabilidad [0.03427429 0.96572571]\n",
      "El cliente  Escritura con probabilidad [0.14821081 0.85178919]\n",
      "El cliente  Escritura con probabilidad [0.21086961 0.78913039]\n",
      "El cliente  Escritura con probabilidad [1.39364076e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.18356065 0.81643935]\n",
      "El cliente  Escritura con probabilidad [0.12555816 0.87444184]\n",
      "El cliente  Escritura con probabilidad [0.10889428 0.89110572]\n",
      "El cliente  Escritura con probabilidad [0.16183818 0.83816182]\n",
      "El cliente  Escritura con probabilidad [0.08722735 0.91277265]\n",
      "El cliente  Escritura con probabilidad [1.67643677e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.07554726 0.92445274]\n",
      "El cliente  Escritura con probabilidad [0.37857091 0.62142909]\n",
      "El cliente  Escritura con probabilidad [0.13793161 0.86206839]\n",
      "El cliente  Escritura con probabilidad [0.16310956 0.83689044]\n",
      "El cliente  Escritura con probabilidad [0.21758565 0.78241435]\n",
      "El cliente  Escritura con probabilidad [0.15784476 0.84215524]\n",
      "El cliente  Escritura con probabilidad [0.14275113 0.85724887]\n",
      "El cliente  Escritura con probabilidad [0.00332174 0.99667826]\n",
      "El cliente  Escritura con probabilidad [5.11794370e-04 9.99488206e-01]\n",
      "El cliente  Escritura con probabilidad [0.09912903 0.90087097]\n",
      "El cliente  Escritura con probabilidad [0.03854228 0.96145772]\n",
      "El cliente  Escritura con probabilidad [0.06228191 0.93771809]\n",
      "El cliente  Escritura con probabilidad [0.19413073 0.80586927]\n",
      "El cliente  Escritura con probabilidad [0.20407161 0.79592839]\n",
      "El cliente  Escritura con probabilidad [0.15660622 0.84339378]\n",
      "El cliente  Escritura con probabilidad [0.01074275 0.98925725]\n",
      "El cliente  Escritura con probabilidad [0.08229235 0.91770765]\n",
      "El cliente  Escritura con probabilidad [0.27136478 0.72863522]\n",
      "El cliente  Escritura con probabilidad [0.11646485 0.88353515]\n",
      "El cliente  Escritura con probabilidad [2.48630113e-07 9.99999751e-01]\n",
      "El cliente  Escritura con probabilidad [0.01431719 0.98568281]\n",
      "El cliente  Escritura con probabilidad [0.01663133 0.98336867]\n",
      "El cliente  Escritura con probabilidad [0.03354023 0.96645977]\n",
      "El cliente  Escritura con probabilidad [0.21514548 0.78485452]\n",
      "El cliente  Escritura con probabilidad [8.85478037e-06 9.99991145e-01]\n",
      "El cliente  Escritura con probabilidad [0.1941591 0.8058409]\n",
      "El cliente  Escritura con probabilidad [0.24714486 0.75285514]\n",
      "El cliente  Escritura con probabilidad [1.20673534e-04 9.99879326e-01]\n",
      "El cliente  Escritura con probabilidad [0.24712738 0.75287262]\n",
      "El cliente  Escritura con probabilidad [0.21369859 0.78630141]\n",
      "El cliente  Escritura con probabilidad [0.15282018 0.84717982]\n",
      "El cliente  Escritura con probabilidad [0.25134591 0.74865409]\n",
      "El cliente  Escritura con probabilidad [0.16953548 0.83046452]\n",
      "El cliente  Escritura con probabilidad [0.08216691 0.91783309]\n",
      "El cliente  Escritura con probabilidad [0.06201293 0.93798707]\n",
      "El cliente  Escritura con probabilidad [2.71924292e-05 9.99972808e-01]\n",
      "El cliente  Escritura con probabilidad [0.20537912 0.79462088]\n",
      "El cliente  Escritura con probabilidad [0.13846152 0.86153848]\n",
      "El cliente  Escritura con probabilidad [0.2100298 0.7899702]\n",
      "El cliente  Escritura con probabilidad [0.00126083 0.99873917]\n",
      "El cliente  Escritura con probabilidad [9.70160829e-09 9.99999990e-01]\n",
      "El cliente  Escritura con probabilidad [0.14493075 0.85506925]\n",
      "El cliente  Escritura con probabilidad [1.70748653e-07 9.99999829e-01]\n",
      "El cliente  Escritura con probabilidad [0.00310091 0.99689909]\n",
      "El cliente  Escritura con probabilidad [0.00173688 0.99826312]\n",
      "El cliente  Escritura con probabilidad [0.12472947 0.87527053]\n",
      "El cliente  Escritura con probabilidad [0.178767 0.821233]\n",
      "El cliente  Escritura con probabilidad [0.1622413 0.8377587]\n",
      "El cliente  Escritura con probabilidad [0.13723818 0.86276182]\n",
      "El cliente  Escritura con probabilidad [0.0382618 0.9617382]\n",
      "El cliente  Escritura con probabilidad [1.08889904e-05 9.99989111e-01]\n",
      "El cliente  Escritura con probabilidad [0.06588545 0.93411455]\n",
      "El cliente  Escritura con probabilidad [0.11686977 0.88313023]\n",
      "El cliente  Escritura con probabilidad [0.18801101 0.81198899]\n",
      "El cliente  Escritura con probabilidad [1.08375938e-04 9.99891624e-01]\n",
      "El cliente  Escritura con probabilidad [7.14860128e-05 9.99928514e-01]\n",
      "El cliente  Escritura con probabilidad [0.24398455 0.75601545]\n",
      "El cliente  Escritura con probabilidad [0.08866937 0.91133063]\n",
      "El cliente  Escritura con probabilidad [0.08091771 0.91908229]\n",
      "El cliente  Escritura con probabilidad [0.14251143 0.85748857]\n",
      "El cliente  Escritura con probabilidad [0.24180913 0.75819087]\n",
      "El cliente  Escritura con probabilidad [0.00102716 0.99897284]\n",
      "El cliente  Escritura con probabilidad [0.0353257 0.9646743]\n",
      "El cliente  Escritura con probabilidad [0.05544391 0.94455609]\n",
      "El cliente  Escritura con probabilidad [8.98855912e-09 9.99999991e-01]\n",
      "El cliente  Escritura con probabilidad [2.30735929e-08 9.99999977e-01]\n",
      "El cliente  Escritura con probabilidad [0.22933053 0.77066947]\n",
      "El cliente  Escritura con probabilidad [0.20510905 0.79489095]\n",
      "El cliente  Escritura con probabilidad [0.28527095 0.71472905]\n",
      "El cliente  Escritura con probabilidad [0.06465077 0.93534923]\n",
      "El cliente  Escritura con probabilidad [0.13808661 0.86191339]\n",
      "El cliente  Escritura con probabilidad [0.33564332 0.66435668]\n",
      "El cliente  Escritura con probabilidad [0.13294401 0.86705599]\n",
      "El cliente  Escritura con probabilidad [0.01289518 0.98710482]\n",
      "El cliente  Escritura con probabilidad [0.22680894 0.77319106]\n",
      "El cliente  Escritura con probabilidad [0.10808033 0.89191967]\n",
      "El cliente  Escritura con probabilidad [0.23590435 0.76409565]\n",
      "El cliente  Escritura con probabilidad [0.19980597 0.80019403]\n",
      "El cliente  Escritura con probabilidad [5.12847786e-08 9.99999949e-01]\n",
      "El cliente  Escritura con probabilidad [0.11323034 0.88676966]\n",
      "El cliente  Escritura con probabilidad [7.58151678e-07 9.99999242e-01]\n",
      "El cliente  Escritura con probabilidad [0.15480467 0.84519533]\n",
      "El cliente  Escritura con probabilidad [5.59260137e-04 9.99440740e-01]\n",
      "El cliente  Escritura con probabilidad [0.19453773 0.80546227]\n",
      "El cliente  Escritura con probabilidad [0.15764587 0.84235413]\n",
      "El cliente  Escritura con probabilidad [0.0415301 0.9584699]\n",
      "El cliente  Escritura con probabilidad [0.18540075 0.81459925]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.21159545 0.78840455]\n",
      "El cliente  Escritura con probabilidad [0.14830005 0.85169995]\n",
      "El cliente  Escritura con probabilidad [5.78030757e-04 9.99421969e-01]\n",
      "El cliente  Escritura con probabilidad [3.21204071e-04 9.99678796e-01]\n",
      "El cliente  Escritura con probabilidad [0.05218525 0.94781475]\n",
      "El cliente  Escritura con probabilidad [0.12383864 0.87616136]\n",
      "El cliente  Escritura con probabilidad [0.08662328 0.91337672]\n",
      "El cliente  Escritura con probabilidad [0.22731685 0.77268315]\n",
      "El cliente  Escritura con probabilidad [0.04489303 0.95510697]\n",
      "El cliente  Escritura con probabilidad [1.05431153e-06 9.99998946e-01]\n",
      "El cliente  Escritura con probabilidad [5.29020592e-06 9.99994710e-01]\n",
      "El cliente  Escritura con probabilidad [0.22358871 0.77641129]\n",
      "El cliente  Escritura con probabilidad [0.02919548 0.97080452]\n",
      "El cliente  Escritura con probabilidad [0.42712906 0.57287094]\n",
      "El cliente  Escritura con probabilidad [0.09310037 0.90689963]\n",
      "El cliente  Escritura con probabilidad [2.03135286e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.22504392 0.77495608]\n",
      "El cliente  Escritura con probabilidad [0.01424765 0.98575235]\n",
      "El cliente  Escritura con probabilidad [0.01020569 0.98979431]\n",
      "El cliente  Escritura con probabilidad [0.02328347 0.97671653]\n",
      "El cliente  Escritura con probabilidad [0.19017345 0.80982655]\n",
      "El cliente  Escritura con probabilidad [2.21837542e-04 9.99778162e-01]\n",
      "El cliente  Escritura con probabilidad [0.24687676 0.75312324]\n",
      "El cliente  Escritura con probabilidad [0.10670023 0.89329977]\n",
      "El cliente  Escritura con probabilidad [0.13038569 0.86961431]\n",
      "El cliente  Escritura con probabilidad [7.61276011e-05 9.99923872e-01]\n",
      "El cliente  Escritura con probabilidad [0.0191978 0.9808022]\n",
      "El cliente  Escritura con probabilidad [0.09006534 0.90993466]\n",
      "El cliente  Escritura con probabilidad [0.17316221 0.82683779]\n",
      "El cliente  Escritura con probabilidad [0.17893174 0.82106826]\n",
      "El cliente  Escritura con probabilidad [0.11015151 0.88984849]\n",
      "El cliente  Escritura con probabilidad [0.07876607 0.92123393]\n",
      "El cliente  Escritura con probabilidad [1.17566583e-06 9.99998824e-01]\n",
      "El cliente  Escritura con probabilidad [0.13440148 0.86559852]\n",
      "El cliente  Escritura con probabilidad [0.14823034 0.85176966]\n",
      "El cliente  Escritura con probabilidad [0.19538765 0.80461235]\n",
      "El cliente  Escritura con probabilidad [0.0441355 0.9558645]\n",
      "El cliente  Escritura con probabilidad [0.05782509 0.94217491]\n",
      "El cliente  Escritura con probabilidad [0.02152239 0.97847761]\n",
      "El cliente  Escritura con probabilidad [0.12530127 0.87469873]\n",
      "El cliente  Escritura con probabilidad [0.0624531 0.9375469]\n",
      "El cliente  Escritura con probabilidad [0.25135959 0.74864041]\n",
      "El cliente  Escritura con probabilidad [0.0203257 0.9796743]\n",
      "El cliente  Escritura con probabilidad [0.08132325 0.91867675]\n",
      "El cliente  Escritura con probabilidad [0.06847924 0.93152076]\n",
      "El cliente  Escritura con probabilidad [0.12869312 0.87130688]\n",
      "El cliente  Escritura con probabilidad [0.01392923 0.98607077]\n",
      "El cliente  Escritura con probabilidad [0.23135686 0.76864314]\n",
      "El cliente  Escritura con probabilidad [0.22121706 0.77878294]\n",
      "El cliente  Escritura con probabilidad [0.12921084 0.87078916]\n",
      "El cliente  Escritura con probabilidad [0.19895375 0.80104625]\n",
      "El cliente  Escritura con probabilidad [0.10641715 0.89358285]\n",
      "El cliente  Escritura con probabilidad [1.36611662e-04 9.99863388e-01]\n",
      "El cliente  Escritura con probabilidad [0.10050494 0.89949506]\n",
      "El cliente  Escritura con probabilidad [0.12156667 0.87843333]\n",
      "El cliente  Escritura con probabilidad [0.2818438 0.7181562]\n",
      "El cliente  Escritura con probabilidad [8.66248617e-07 9.99999134e-01]\n",
      "El cliente  Escritura con probabilidad [0.03650661 0.96349339]\n",
      "El cliente  Escritura con probabilidad [2.44306879e-06 9.99997557e-01]\n",
      "El cliente  Escritura con probabilidad [0.106097 0.893903]\n",
      "El cliente  Escritura con probabilidad [4.2432724e-13 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.20513339 0.79486661]\n",
      "El cliente  Escritura con probabilidad [0.08000913 0.91999087]\n",
      "El cliente  Escritura con probabilidad [0.27386009 0.72613991]\n",
      "El cliente  Escritura con probabilidad [0.13004778 0.86995222]\n",
      "El cliente  Escritura con probabilidad [0.14832506 0.85167494]\n",
      "El cliente  Escritura con probabilidad [0.0762356 0.9237644]\n",
      "El cliente  Escritura con probabilidad [0.20123879 0.79876121]\n",
      "El cliente  Escritura con probabilidad [0.05128567 0.94871433]\n",
      "El cliente  Escritura con probabilidad [0.11919555 0.88080445]\n",
      "El cliente  Escritura con probabilidad [0.31065846 0.68934154]\n",
      "El cliente  Escritura con probabilidad [1.1772805e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.16591343 0.83408657]\n",
      "El cliente  Escritura con probabilidad [0.08885877 0.91114123]\n",
      "El cliente  Escritura con probabilidad [0.31015332 0.68984668]\n",
      "El cliente  Escritura con probabilidad [3.12561088e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [6.12190298e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.1383841 0.8616159]\n",
      "El cliente  Escritura con probabilidad [0.11363114 0.88636886]\n",
      "El cliente  Escritura con probabilidad [0.13879453 0.86120547]\n",
      "El cliente  Escritura con probabilidad [2.15236481e-04 9.99784764e-01]\n",
      "El cliente  Escritura con probabilidad [1.81528957e-04 9.99818471e-01]\n",
      "El cliente  Escritura con probabilidad [1.16875034e-07 9.99999883e-01]\n",
      "El cliente  Escritura con probabilidad [0.0402213 0.9597787]\n",
      "El cliente  Escritura con probabilidad [0.23024021 0.76975979]\n",
      "El cliente  Escritura con probabilidad [0.06262481 0.93737519]\n",
      "El cliente  Escritura con probabilidad [0.14867471 0.85132529]\n",
      "El cliente  Escritura con probabilidad [2.44822504e-06 9.99997552e-01]\n",
      "El cliente  Escritura con probabilidad [0.21781749 0.78218251]\n",
      "El cliente  Escritura con probabilidad [0.22554597 0.77445403]\n",
      "El cliente  Escritura con probabilidad [4.67020551e-08 9.99999953e-01]\n",
      "El cliente  Escritura con probabilidad [0.21717411 0.78282589]\n",
      "El cliente  Escritura con probabilidad [0.0408181 0.9591819]\n",
      "El cliente  Escritura con probabilidad [0.23889337 0.76110663]\n",
      "El cliente  Escritura con probabilidad [0.11207478 0.88792522]\n",
      "El cliente  Escritura con probabilidad [0.06924116 0.93075884]\n",
      "El cliente  Escritura con probabilidad [0.13507264 0.86492736]\n",
      "El cliente  Escritura con probabilidad [0.19927408 0.80072592]\n",
      "El cliente  Escritura con probabilidad [0.29525384 0.70474616]\n",
      "El cliente  Escritura con probabilidad [1.34900334e-04 9.99865100e-01]\n",
      "El cliente  Escritura con probabilidad [0.25961233 0.74038767]\n",
      "El cliente  Escritura con probabilidad [0.20748211 0.79251789]\n",
      "El cliente  Escritura con probabilidad [0.12805974 0.87194026]\n",
      "El cliente  Escritura con probabilidad [0.1152464 0.8847536]\n",
      "El cliente  Escritura con probabilidad [0.18830539 0.81169461]\n",
      "El cliente  Escritura con probabilidad [0.43922925 0.56077075]\n",
      "El cliente  Escritura con probabilidad [0.12259549 0.87740451]\n",
      "El cliente  Escritura con probabilidad [0.20480031 0.79519969]\n",
      "El cliente  Escritura con probabilidad [0.15363973 0.84636027]\n",
      "El cliente  Escritura con probabilidad [0.08046696 0.91953304]\n",
      "El cliente  Escritura con probabilidad [0.23164942 0.76835058]\n",
      "El cliente  Escritura con probabilidad [0.16678263 0.83321737]\n",
      "El cliente  Escritura con probabilidad [0.08242507 0.91757493]\n",
      "El cliente  Escritura con probabilidad [0.12242505 0.87757495]\n",
      "El cliente  Escritura con probabilidad [0.0685528 0.9314472]\n",
      "El cliente  Escritura con probabilidad [1.85657613e-08 9.99999981e-01]\n",
      "El cliente  Escritura con probabilidad [0.00123151 0.99876849]\n",
      "El cliente  Escritura con probabilidad [0.26727797 0.73272203]\n",
      "El cliente  Escritura con probabilidad [0.09040953 0.90959047]\n",
      "El cliente  Escritura con probabilidad [0.07346824 0.92653176]\n",
      "El cliente  Escritura con probabilidad [0.04808171 0.95191829]\n",
      "El cliente  Escritura con probabilidad [0.07139358 0.92860642]\n",
      "El cliente  Escritura con probabilidad [0.09244705 0.90755295]\n",
      "El cliente  Escritura con probabilidad [0.1653831 0.8346169]\n",
      "El cliente  Escritura con probabilidad [0.02224876 0.97775124]\n",
      "El cliente  Escritura con probabilidad [0.20693177 0.79306823]\n",
      "El cliente  Escritura con probabilidad [0.1425405 0.8574595]\n",
      "El cliente  Escritura con probabilidad [0.04492204 0.95507796]\n",
      "El cliente  Escritura con probabilidad [0.25410319 0.74589681]\n",
      "El cliente  Escritura con probabilidad [0.14864788 0.85135212]\n",
      "El cliente  Escritura con probabilidad [0.09453785 0.90546215]\n",
      "El cliente  Escritura con probabilidad [0.19031202 0.80968798]\n",
      "El cliente  Escritura con probabilidad [0.26985401 0.73014599]\n",
      "El cliente  Escritura con probabilidad [0.04241214 0.95758786]\n",
      "El cliente  Escritura con probabilidad [0.11699666 0.88300334]\n",
      "El cliente  Escritura con probabilidad [0.05954504 0.94045496]\n",
      "El cliente  Escritura con probabilidad [0.06340031 0.93659969]\n",
      "El cliente  Escritura con probabilidad [0.27502619 0.72497381]\n",
      "El cliente  Escritura con probabilidad [0.11754753 0.88245247]\n",
      "El cliente  Escritura con probabilidad [0.12910202 0.87089798]\n",
      "El cliente  Escritura con probabilidad [2.65873560e-06 9.99997341e-01]\n",
      "El cliente  Escritura con probabilidad [0.24860083 0.75139917]\n",
      "El cliente  Escritura con probabilidad [0.03428906 0.96571094]\n",
      "El cliente  Escritura con probabilidad [0.11982016 0.88017984]\n",
      "El cliente  Escritura con probabilidad [0.0276011 0.9723989]\n",
      "El cliente  Escritura con probabilidad [0.44368846 0.55631154]\n",
      "El cliente  Escritura con probabilidad [2.04974177e-06 9.99997950e-01]\n",
      "El cliente  Escritura con probabilidad [0.1648983 0.8351017]\n",
      "El cliente  Escritura con probabilidad [0.20657209 0.79342791]\n",
      "El cliente  Escritura con probabilidad [0.08453742 0.91546258]\n",
      "El cliente  Escritura con probabilidad [0.2307363 0.7692637]\n",
      "El cliente  Escritura con probabilidad [0.09345777 0.90654223]\n",
      "El cliente  Escritura con probabilidad [0.19901051 0.80098949]\n",
      "El cliente  Escritura con probabilidad [0.13994067 0.86005933]\n",
      "El cliente  Escritura con probabilidad [0.06499716 0.93500284]\n",
      "El cliente  Escritura con probabilidad [0.22623871 0.77376129]\n",
      "El cliente  Escritura con probabilidad [0.07971434 0.92028566]\n",
      "El cliente  Escritura con probabilidad [0.13127789 0.86872211]\n",
      "El cliente  Escritura con probabilidad [0.23720407 0.76279593]\n",
      "El cliente  Escritura con probabilidad [0.23006817 0.76993183]\n",
      "El cliente  Escritura con probabilidad [2.58713977e-06 9.99997413e-01]\n",
      "El cliente  Escritura con probabilidad [0.23396145 0.76603855]\n",
      "El cliente  Escritura con probabilidad [0.19290456 0.80709544]\n",
      "El cliente  Escritura con probabilidad [0.16470053 0.83529947]\n",
      "El cliente  Escritura con probabilidad [0.1938899 0.8061101]\n",
      "El cliente  Escritura con probabilidad [0.19530454 0.80469546]\n",
      "El cliente  Escritura con probabilidad [0.27789621 0.72210379]\n",
      "El cliente  Escritura con probabilidad [6.58588545e-07 9.99999341e-01]\n",
      "El cliente  Escritura con probabilidad [0.25939131 0.74060869]\n",
      "El cliente  Escritura con probabilidad [0.0155826 0.9844174]\n",
      "El cliente  Escritura con probabilidad [0.04307735 0.95692265]\n",
      "El cliente  Escritura con probabilidad [0.00275439 0.99724561]\n",
      "El cliente  Escritura con probabilidad [0.1614313 0.8385687]\n",
      "El cliente  Escritura con probabilidad [0.23793329 0.76206671]\n",
      "El cliente  Escritura con probabilidad [0.16113713 0.83886287]\n",
      "El cliente  Escritura con probabilidad [0.45558887 0.54441113]\n",
      "El cliente  Escritura con probabilidad [2.05820405e-08 9.99999979e-01]\n",
      "El cliente  Escritura con probabilidad [0.25485462 0.74514538]\n",
      "El cliente  Escritura con probabilidad [0.23283896 0.76716104]\n",
      "El cliente  Escritura con probabilidad [0.16422866 0.83577134]\n",
      "El cliente  Escritura con probabilidad [0.10218412 0.89781588]\n",
      "El cliente  Escritura con probabilidad [0.27163845 0.72836155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.09863544 0.90136456]\n",
      "El cliente  Escritura con probabilidad [0.06458146 0.93541854]\n",
      "El cliente  Escritura con probabilidad [0.08243877 0.91756123]\n",
      "El cliente  Escritura con probabilidad [0.04596265 0.95403735]\n",
      "El cliente  Escritura con probabilidad [0.24842089 0.75157911]\n",
      "El cliente  Escritura con probabilidad [0.02214301 0.97785699]\n",
      "El cliente  Escritura con probabilidad [2.13979350e-07 9.99999786e-01]\n",
      "El cliente  Escritura con probabilidad [0.01909306 0.98090694]\n",
      "El cliente  Escritura con probabilidad [0.17201525 0.82798475]\n",
      "El cliente  Escritura con probabilidad [0.21484182 0.78515818]\n",
      "El cliente  Escritura con probabilidad [0.15086527 0.84913473]\n",
      "El cliente  Escritura con probabilidad [0.14408337 0.85591663]\n",
      "El cliente  Escritura con probabilidad [0.29298162 0.70701838]\n",
      "El cliente  Escritura con probabilidad [0.17964968 0.82035032]\n",
      "El cliente  Escritura con probabilidad [0.03989594 0.96010406]\n",
      "El cliente  Escritura con probabilidad [4.49832274e-05 9.99955017e-01]\n",
      "El cliente  Escritura con probabilidad [0.03803864 0.96196136]\n",
      "El cliente  Escritura con probabilidad [0.20278832 0.79721168]\n",
      "El cliente  Escritura con probabilidad [0.01507881 0.98492119]\n",
      "El cliente  Escritura con probabilidad [0.0376527 0.9623473]\n",
      "El cliente  Escritura con probabilidad [0.45533953 0.54466047]\n",
      "El cliente  Escritura con probabilidad [6.94158749e-06 9.99993058e-01]\n",
      "El cliente  Escritura con probabilidad [0.06659886 0.93340114]\n",
      "El cliente  Escritura con probabilidad [0.0986445 0.9013555]\n",
      "El cliente  Escritura con probabilidad [0.3048619 0.6951381]\n",
      "El cliente  Escritura con probabilidad [0.13030142 0.86969858]\n",
      "El cliente  Escritura con probabilidad [0.03771756 0.96228244]\n",
      "El cliente  Escritura con probabilidad [0.12298627 0.87701373]\n",
      "El cliente  Escritura con probabilidad [0.06396158 0.93603842]\n",
      "El cliente  Escritura con probabilidad [2.79898377e-05 9.99972010e-01]\n",
      "El cliente  Escritura con probabilidad [0.25991304 0.74008696]\n",
      "El cliente  Escritura con probabilidad [0.04847645 0.95152355]\n",
      "El cliente  Escritura con probabilidad [0.22176176 0.77823824]\n",
      "El cliente  Escritura con probabilidad [0.43176583 0.56823417]\n",
      "El cliente  Escritura con probabilidad [0.03971738 0.96028262]\n",
      "El cliente  Escritura con probabilidad [0.0554593 0.9445407]\n",
      "El cliente  Escritura con probabilidad [0.11788979 0.88211021]\n",
      "El cliente  Escritura con probabilidad [2.99962277e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.68364471e-04 9.99831636e-01]\n",
      "El cliente  Escritura con probabilidad [0.02310374 0.97689626]\n",
      "El cliente  Escritura con probabilidad [0.01391612 0.98608388]\n",
      "El cliente  Escritura con probabilidad [0.16010178 0.83989822]\n",
      "El cliente  Escritura con probabilidad [0.15593068 0.84406932]\n",
      "El cliente  Escritura con probabilidad [1.64434181e-05 9.99983557e-01]\n",
      "El cliente  Escritura con probabilidad [0.02614743 0.97385257]\n",
      "El cliente  Escritura con probabilidad [0.07221216 0.92778784]\n",
      "El cliente  Escritura con probabilidad [0.28002562 0.71997438]\n",
      "El cliente  Escritura con probabilidad [0.08718666 0.91281334]\n",
      "El cliente  Escritura con probabilidad [0.18229933 0.81770067]\n",
      "El cliente  Escritura con probabilidad [0.2180903 0.7819097]\n",
      "El cliente  Escritura con probabilidad [0.11847558 0.88152442]\n",
      "El cliente  Escritura con probabilidad [0.04056691 0.95943309]\n",
      "El cliente  Escritura con probabilidad [0.03020021 0.96979979]\n",
      "El cliente  Escritura con probabilidad [0.14887812 0.85112188]\n",
      "El cliente  Escritura con probabilidad [1.42419119e-08 9.99999986e-01]\n",
      "El cliente  Escritura con probabilidad [0.40200223 0.59799777]\n",
      "El cliente  Escritura con probabilidad [0.14970032 0.85029968]\n",
      "El cliente  Escritura con probabilidad [0.15117669 0.84882331]\n",
      "El cliente  Escritura con probabilidad [0.19496815 0.80503185]\n",
      "El cliente  Escritura con probabilidad [2.22044605e-16 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17526473 0.82473527]\n",
      "El cliente  Escritura con probabilidad [2.34959285e-06 9.99997650e-01]\n",
      "El cliente  Escritura con probabilidad [0.02753408 0.97246592]\n",
      "El cliente  Escritura con probabilidad [0.22494292 0.77505708]\n",
      "El cliente  Escritura con probabilidad [0.04682745 0.95317255]\n",
      "El cliente  Escritura con probabilidad [0.00107587 0.99892413]\n",
      "El cliente  Escritura con probabilidad [0.07305611 0.92694389]\n",
      "El cliente  Escritura con probabilidad [0.11919056 0.88080944]\n",
      "El cliente  Escritura con probabilidad [0.01630718 0.98369282]\n",
      "El cliente  Escritura con probabilidad [0.18116755 0.81883245]\n",
      "El cliente  Escritura con probabilidad [0.05508961 0.94491039]\n",
      "El cliente  Escritura con probabilidad [0.02541721 0.97458279]\n",
      "El cliente  Escritura con probabilidad [0.16041879 0.83958121]\n",
      "El cliente  Escritura con probabilidad [0.21482872 0.78517128]\n",
      "El cliente  Escritura con probabilidad [0.11326865 0.88673135]\n",
      "El cliente  Escritura con probabilidad [0.1760338 0.8239662]\n",
      "El cliente  Escritura con probabilidad [0.04840578 0.95159422]\n",
      "El cliente  Escritura con probabilidad [0.14135082 0.85864918]\n",
      "El cliente  Escritura con probabilidad [0.00162768 0.99837232]\n",
      "El cliente  Escritura con probabilidad [2.48131715e-06 9.99997519e-01]\n",
      "El cliente  Escritura con probabilidad [0.39934476 0.60065524]\n",
      "El cliente  Escritura con probabilidad [1.73194792e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.28137975 0.71862025]\n",
      "El cliente  Escritura con probabilidad [2.67435520e-04 9.99732564e-01]\n",
      "El cliente  Escritura con probabilidad [0.16305211 0.83694789]\n",
      "El cliente  Escritura con probabilidad [4.70390511e-07 9.99999530e-01]\n",
      "El cliente  Escritura con probabilidad [0.08131481 0.91868519]\n",
      "El cliente  Escritura con probabilidad [0.0475908 0.9524092]\n",
      "El cliente  Escritura con probabilidad [0.06938388 0.93061612]\n",
      "El cliente  Escritura con probabilidad [0.17815225 0.82184775]\n",
      "El cliente  Escritura con probabilidad [0.15782698 0.84217302]\n",
      "El cliente  Escritura con probabilidad [0.00793232 0.99206768]\n",
      "El cliente  Escritura con probabilidad [0.08554787 0.91445213]\n",
      "El cliente  Escritura con probabilidad [0.27708718 0.72291282]\n",
      "El cliente  Escritura con probabilidad [0.16412585 0.83587415]\n",
      "El cliente  Escritura con probabilidad [0.26979541 0.73020459]\n",
      "El cliente  Escritura con probabilidad [0.08461355 0.91538645]\n",
      "El cliente  Escritura con probabilidad [0.10702372 0.89297628]\n",
      "El cliente  Escritura con probabilidad [0.21299093 0.78700907]\n",
      "El cliente  Escritura con probabilidad [0.23228003 0.76771997]\n",
      "El cliente  Escritura con probabilidad [0.17387922 0.82612078]\n",
      "El cliente  Escritura con probabilidad [2.48944234e-06 9.99997511e-01]\n",
      "El cliente  Escritura con probabilidad [1.12953785e-04 9.99887046e-01]\n",
      "El cliente  Escritura con probabilidad [0.03794076 0.96205924]\n",
      "El cliente  Escritura con probabilidad [0.06891523 0.93108477]\n",
      "El cliente  Escritura con probabilidad [0.12714642 0.87285358]\n",
      "El cliente  Escritura con probabilidad [3.22657812e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [3.15914045e-04 9.99684086e-01]\n",
      "El cliente  Escritura con probabilidad [0.01514901 0.98485099]\n",
      "El cliente  Escritura con probabilidad [0.26524168 0.73475832]\n",
      "El cliente  Escritura con probabilidad [0.01113741 0.98886259]\n",
      "El cliente  Escritura con probabilidad [0.24652476 0.75347524]\n",
      "El cliente  Escritura con probabilidad [0.18072507 0.81927493]\n",
      "El cliente  Escritura con probabilidad [0.14016722 0.85983278]\n",
      "El cliente  Escritura con probabilidad [0.04627825 0.95372175]\n",
      "El cliente  Escritura con probabilidad [0.05854076 0.94145924]\n",
      "El cliente  Escritura con probabilidad [0.02581334 0.97418666]\n",
      "El cliente  Escritura con probabilidad [0.08661961 0.91338039]\n",
      "El cliente  Escritura con probabilidad [0.00214583 0.99785417]\n",
      "El cliente  Escritura con probabilidad [0.06302095 0.93697905]\n",
      "El cliente  Escritura con probabilidad [0.02201138 0.97798862]\n",
      "El cliente  Escritura con probabilidad [0.25453877 0.74546123]\n",
      "El cliente  Escritura con probabilidad [3.57819846e-05 9.99964218e-01]\n",
      "El cliente  Escritura con probabilidad [0.12204159 0.87795841]\n",
      "El cliente  Escritura con probabilidad [1.72559059e-05 9.99982744e-01]\n",
      "El cliente  Escritura con probabilidad [0.16907401 0.83092599]\n",
      "El cliente  Escritura con probabilidad [0.13104651 0.86895349]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.14851449 0.85148551]\n",
      "El cliente  Escritura con probabilidad [0.21104684 0.78895316]\n",
      "El cliente  Escritura con probabilidad [3.23008287e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.30339013 0.69660987]\n",
      "El cliente  Escritura con probabilidad [0.14811104 0.85188896]\n",
      "El cliente  Escritura con probabilidad [0.18069456 0.81930544]\n",
      "El cliente  Escritura con probabilidad [0.01427325 0.98572675]\n",
      "El cliente  Escritura con probabilidad [0.15053551 0.84946449]\n",
      "El cliente  Escritura con probabilidad [0.144436 0.855564]\n",
      "El cliente  Escritura con probabilidad [0.25611675 0.74388325]\n",
      "El cliente  Escritura con probabilidad [0.18313718 0.81686282]\n",
      "El cliente  Escritura con probabilidad [0.17995035 0.82004965]\n",
      "El cliente  Escritura con probabilidad [0.02287114 0.97712886]\n",
      "El cliente  Escritura con probabilidad [0.19995512 0.80004488]\n",
      "El cliente  Escritura con probabilidad [0.07994847 0.92005153]\n",
      "El cliente  Escritura con probabilidad [0.2034359 0.7965641]\n",
      "El cliente  Escritura con probabilidad [0.01583133 0.98416867]\n",
      "El cliente  Escritura con probabilidad [0.41114995 0.58885005]\n",
      "El cliente  Escritura con probabilidad [0.20846852 0.79153148]\n",
      "El cliente  Escritura con probabilidad [0.24151068 0.75848932]\n",
      "El cliente  Escritura con probabilidad [0.04509199 0.95490801]\n",
      "El cliente  Escritura con probabilidad [0.27247244 0.72752756]\n",
      "El cliente  Escritura con probabilidad [0.05932322 0.94067678]\n",
      "El cliente  Escritura con probabilidad [0.16545662 0.83454338]\n",
      "El cliente  Escritura con probabilidad [0.05462691 0.94537309]\n",
      "El cliente  Escritura con probabilidad [1.05504564e-04 9.99894495e-01]\n",
      "El cliente  Escritura con probabilidad [0.00649908 0.99350092]\n",
      "El cliente  Escritura con probabilidad [0.08840694 0.91159306]\n",
      "El cliente  Escritura con probabilidad [0.15422223 0.84577777]\n",
      "El cliente  Escritura con probabilidad [0.38295773 0.61704227]\n",
      "El cliente  Escritura con probabilidad [2.66965516e-04 9.99733034e-01]\n",
      "El cliente  Escritura con probabilidad [0.20140902 0.79859098]\n",
      "El cliente  Escritura con probabilidad [3.56212193e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.09696602 0.90303398]\n",
      "El cliente  Escritura con probabilidad [0.02906737 0.97093263]\n",
      "El cliente  Escritura con probabilidad [8.26317404e-06 9.99991737e-01]\n",
      "El cliente  Escritura con probabilidad [0.07459647 0.92540353]\n",
      "El cliente  Escritura con probabilidad [0.18885276 0.81114724]\n",
      "El cliente  Escritura con probabilidad [0.26123513 0.73876487]\n",
      "El cliente  Escritura con probabilidad [0.28251004 0.71748996]\n",
      "El cliente  Escritura con probabilidad [0.0190213 0.9809787]\n",
      "El cliente  Escritura con probabilidad [0.14726697 0.85273303]\n",
      "El cliente  Escritura con probabilidad [9.97597138e-04 9.99002403e-01]\n",
      "El cliente  Escritura con probabilidad [0.24532399 0.75467601]\n",
      "El cliente  Escritura con probabilidad [0.13122157 0.86877843]\n",
      "El cliente  Escritura con probabilidad [0.1269372 0.8730628]\n",
      "El cliente  Escritura con probabilidad [0.07938709 0.92061291]\n",
      "El cliente  Escritura con probabilidad [1.54015400e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.20194492 0.79805508]\n",
      "El cliente  Escritura con probabilidad [0.02187827 0.97812173]\n",
      "El cliente  Escritura con probabilidad [0.008291 0.991709]\n",
      "El cliente  Escritura con probabilidad [0.15546103 0.84453897]\n",
      "El cliente  Escritura con probabilidad [0.09208727 0.90791273]\n",
      "El cliente  Escritura con probabilidad [0.09066324 0.90933676]\n",
      "El cliente  Escritura con probabilidad [0.11236956 0.88763044]\n",
      "El cliente  Escritura con probabilidad [0.12006453 0.87993547]\n",
      "El cliente  Escritura con probabilidad [0.00210018 0.99789982]\n",
      "El cliente  Escritura con probabilidad [0.200162 0.799838]\n",
      "El cliente  Escritura con probabilidad [0.05086571 0.94913429]\n",
      "El cliente  Escritura con probabilidad [0.09839445 0.90160555]\n",
      "El cliente  Escritura con probabilidad [0.03166502 0.96833498]\n",
      "El cliente  Escritura con probabilidad [0.14820985 0.85179015]\n",
      "El cliente  Escritura con probabilidad [0.03982331 0.96017669]\n",
      "El cliente  Escritura con probabilidad [0.18567527 0.81432473]\n",
      "El cliente  Escritura con probabilidad [0.14907682 0.85092318]\n",
      "El cliente  Escritura con probabilidad [0.17891323 0.82108677]\n",
      "El cliente  Escritura con probabilidad [3.52182069e-08 9.99999965e-01]\n",
      "El cliente  Escritura con probabilidad [0.13736299 0.86263701]\n",
      "El cliente  Escritura con probabilidad [0.17164399 0.82835601]\n",
      "El cliente  Escritura con probabilidad [0.01590171 0.98409829]\n",
      "El cliente  Escritura con probabilidad [0.05316386 0.94683614]\n",
      "El cliente  Escritura con probabilidad [0.26841947 0.73158053]\n",
      "El cliente  Escritura con probabilidad [0.23430072 0.76569928]\n",
      "El cliente  Escritura con probabilidad [0.22721074 0.77278926]\n",
      "El cliente  Escritura con probabilidad [0.05397215 0.94602785]\n",
      "El cliente  Escritura con probabilidad [0.27162741 0.72837259]\n",
      "El cliente  Escritura con probabilidad [0.0790629 0.9209371]\n",
      "El cliente  Escritura con probabilidad [1.57304367e-08 9.99999984e-01]\n",
      "El cliente  Escritura con probabilidad [2.55591104e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.10273643 0.89726357]\n",
      "El cliente  Escritura con probabilidad [0.05655961 0.94344039]\n",
      "El cliente  Escritura con probabilidad [0.09960328 0.90039672]\n",
      "El cliente  Escritura con probabilidad [2.91181663e-07 9.99999709e-01]\n",
      "El cliente  Escritura con probabilidad [0.25692996 0.74307004]\n",
      "El cliente  Escritura con probabilidad [0.17566793 0.82433207]\n",
      "El cliente  Escritura con probabilidad [2.73072521e-07 9.99999727e-01]\n",
      "El cliente  Escritura con probabilidad [1.30118138e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.21188446 0.78811554]\n",
      "El cliente  Escritura con probabilidad [0.0486312 0.9513688]\n",
      "El cliente  Escritura con probabilidad [0.07188195 0.92811805]\n",
      "El cliente  Escritura con probabilidad [0.06217153 0.93782847]\n",
      "El cliente  Escritura con probabilidad [0.23154022 0.76845978]\n",
      "El cliente  Escritura con probabilidad [0.13550163 0.86449837]\n",
      "El cliente  Escritura con probabilidad [1.87170235e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.42930963 0.57069037]\n",
      "El cliente  Escritura con probabilidad [0.01975548 0.98024452]\n",
      "El cliente  Escritura con probabilidad [0.11000508 0.88999492]\n",
      "El cliente  Escritura con probabilidad [0.25210739 0.74789261]\n",
      "El cliente  Escritura con probabilidad [0.13386339 0.86613661]\n",
      "El cliente  Escritura con probabilidad [0.1797552 0.8202448]\n",
      "El cliente  Escritura con probabilidad [0.23461881 0.76538119]\n",
      "El cliente  Escritura con probabilidad [0.1327533 0.8672467]\n",
      "El cliente  Escritura con probabilidad [0.24064827 0.75935173]\n",
      "El cliente  Escritura con probabilidad [0.07761721 0.92238279]\n",
      "El cliente  Escritura con probabilidad [0.23572449 0.76427551]\n",
      "El cliente  Escritura con probabilidad [0.25885055 0.74114945]\n",
      "El cliente  Escritura con probabilidad [0.02963498 0.97036502]\n",
      "El cliente  Escritura con probabilidad [0.20125827 0.79874173]\n",
      "El cliente  Escritura con probabilidad [0.16054766 0.83945234]\n",
      "El cliente  Escritura con probabilidad [0.26689765 0.73310235]\n",
      "El cliente  Escritura con probabilidad [8.10577788e-04 9.99189422e-01]\n",
      "El cliente  Escritura con probabilidad [0.27643385 0.72356615]\n",
      "El cliente  Escritura con probabilidad [0.03139163 0.96860837]\n",
      "El cliente  Escritura con probabilidad [0.28532326 0.71467674]\n",
      "El cliente  Escritura con probabilidad [0.05143312 0.94856688]\n",
      "El cliente  Escritura con probabilidad [1.43188372e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.08361292 0.91638708]\n",
      "El cliente  Escritura con probabilidad [6.65053697e-07 9.99999335e-01]\n",
      "El cliente  Escritura con probabilidad [0.09200937 0.90799063]\n",
      "El cliente  Escritura con probabilidad [1.00259529e-05 9.99989974e-01]\n",
      "El cliente  Escritura con probabilidad [0.19760499 0.80239501]\n",
      "El cliente  Escritura con probabilidad [0.05626425 0.94373575]\n",
      "El cliente  Escritura con probabilidad [0.09899801 0.90100199]\n",
      "El cliente  Escritura con probabilidad [0.22154465 0.77845535]\n",
      "El cliente  Escritura con probabilidad [0.14363274 0.85636726]\n",
      "El cliente  Escritura con probabilidad [0.29022641 0.70977359]\n",
      "El cliente  Escritura con probabilidad [0.06428712 0.93571288]\n",
      "El cliente  Escritura con probabilidad [0.1442831 0.8557169]\n",
      "El cliente  Escritura con probabilidad [0.2202544 0.7797456]\n",
      "El cliente  Escritura con probabilidad [0.26301479 0.73698521]\n",
      "El cliente  Escritura con probabilidad [5.36473343e-07 9.99999464e-01]\n",
      "El cliente  Escritura con probabilidad [0.13637719 0.86362281]\n",
      "El cliente  Escritura con probabilidad [0.13973005 0.86026995]\n",
      "El cliente  Escritura con probabilidad [0.14314383 0.85685617]\n",
      "El cliente  Escritura con probabilidad [5.36266385e-08 9.99999946e-01]\n",
      "El cliente  Escritura con probabilidad [0.04273351 0.95726649]\n",
      "El cliente  Escritura con probabilidad [0.27673279 0.72326721]\n",
      "El cliente  Escritura con probabilidad [0.2745494 0.7254506]\n",
      "El cliente  Escritura con probabilidad [2.40570803e-06 9.99997594e-01]\n",
      "El cliente  Escritura con probabilidad [4.57884155e-04 9.99542116e-01]\n",
      "El cliente  Escritura con probabilidad [0.0227908 0.9772092]\n",
      "El cliente  Escritura con probabilidad [0.27689876 0.72310124]\n",
      "El cliente  Escritura con probabilidad [0.03607074 0.96392926]\n",
      "El cliente  Escritura con probabilidad [3.90559902e-04 9.99609440e-01]\n",
      "El cliente  Escritura con probabilidad [0.10976672 0.89023328]\n",
      "El cliente  Escritura con probabilidad [1.84341985e-06 9.99998157e-01]\n",
      "El cliente  Escritura con probabilidad [0.19649781 0.80350219]\n",
      "El cliente  Escritura con probabilidad [0.23423314 0.76576686]\n",
      "El cliente  Escritura con probabilidad [0.033477 0.966523]\n",
      "El cliente  Escritura con probabilidad [0.0454384 0.9545616]\n",
      "El cliente  Escritura con probabilidad [0.09424102 0.90575898]\n",
      "El cliente  Escritura con probabilidad [0.06170058 0.93829942]\n",
      "El cliente  Escritura con probabilidad [0.08279371 0.91720629]\n",
      "El cliente  Escritura con probabilidad [0.04624816 0.95375184]\n",
      "El cliente  Escritura con probabilidad [0.09687809 0.90312191]\n",
      "El cliente  Escritura con probabilidad [3.86307830e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [3.47962182e-05 9.99965204e-01]\n",
      "El cliente  Escritura con probabilidad [0.22498501 0.77501499]\n",
      "El cliente  Escritura con probabilidad [8.77194367e-08 9.99999912e-01]\n",
      "El cliente  Escritura con probabilidad [0.15239159 0.84760841]\n",
      "El cliente  Escritura con probabilidad [0.17118096 0.82881904]\n",
      "El cliente  Escritura con probabilidad [0.05286827 0.94713173]\n",
      "El cliente  Escritura con probabilidad [0.16645166 0.83354834]\n",
      "El cliente  Escritura con probabilidad [0.22767241 0.77232759]\n",
      "El cliente  Escritura con probabilidad [4.16890310e-06 9.99995831e-01]\n",
      "El cliente  Escritura con probabilidad [8.89099498e-07 9.99999111e-01]\n",
      "El cliente  Escritura con probabilidad [0.23465874 0.76534126]\n",
      "El cliente  Escritura con probabilidad [4.55545424e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [1.09576188e-05 9.99989042e-01]\n",
      "El cliente  Escritura con probabilidad [0.27598684 0.72401316]\n",
      "El cliente  Escritura con probabilidad [0.2728166 0.7271834]\n",
      "El cliente  Escritura con probabilidad [0.00656218 0.99343782]\n",
      "El cliente  Escritura con probabilidad [0.15594068 0.84405932]\n",
      "El cliente  Escritura con probabilidad [0.02418425 0.97581575]\n",
      "El cliente  Escritura con probabilidad [0.08031876 0.91968124]\n",
      "El cliente  Escritura con probabilidad [0.00101136 0.99898864]\n",
      "El cliente  Escritura con probabilidad [0.20449265 0.79550735]\n",
      "El cliente  Escritura con probabilidad [0.23065321 0.76934679]\n",
      "El cliente  Escritura con probabilidad [0.06531851 0.93468149]\n",
      "El cliente  Escritura con probabilidad [0.02536579 0.97463421]\n",
      "El cliente  Escritura con probabilidad [2.05693978e-04 9.99794306e-01]\n",
      "El cliente  Escritura con probabilidad [0.17281984 0.82718016]\n",
      "El cliente  Escritura con probabilidad [7.17883015e-08 9.99999928e-01]\n",
      "El cliente  Escritura con probabilidad [0.1573666 0.8426334]\n",
      "El cliente  Escritura con probabilidad [0.1771882 0.8228118]\n",
      "El cliente  Escritura con probabilidad [0.16602536 0.83397464]\n",
      "El cliente  Escritura con probabilidad [0.23471953 0.76528047]\n",
      "El cliente  Escritura con probabilidad [0.16667964 0.83332036]\n",
      "El cliente  Escritura con probabilidad [0.04707445 0.95292555]\n",
      "El cliente  Escritura con probabilidad [0.05921317 0.94078683]\n",
      "El cliente  Escritura con probabilidad [2.08721929e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.29634081e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.01046428 0.98953572]\n",
      "El cliente  Escritura con probabilidad [0.27303682 0.72696318]\n",
      "El cliente  Escritura con probabilidad [2.15318928e-06 9.99997847e-01]\n",
      "El cliente  Escritura con probabilidad [0.21801579 0.78198421]\n",
      "El cliente  Escritura con probabilidad [0.11549818 0.88450182]\n",
      "El cliente  Escritura con probabilidad [0.03867213 0.96132787]\n",
      "El cliente  Escritura con probabilidad [0.26496938 0.73503062]\n",
      "El cliente  Escritura con probabilidad [0.02284471 0.97715529]\n",
      "El cliente  Escritura con probabilidad [0.21262234 0.78737766]\n",
      "El cliente  Escritura con probabilidad [0.16368401 0.83631599]\n",
      "El cliente  Escritura con probabilidad [0.07163719 0.92836281]\n",
      "El cliente  Escritura con probabilidad [0.13095073 0.86904927]\n",
      "El cliente  Escritura con probabilidad [0.10896609 0.89103391]\n",
      "El cliente  Escritura con probabilidad [0.20277889 0.79722111]\n",
      "El cliente  Escritura con probabilidad [0.20369976 0.79630024]\n",
      "El cliente  Escritura con probabilidad [0.28137975 0.71862025]\n",
      "El cliente  Escritura con probabilidad [0.23635514 0.76364486]\n",
      "El cliente  Escritura con probabilidad [0.10472956 0.89527044]\n",
      "El cliente  Escritura con probabilidad [0.01846143 0.98153857]\n",
      "El cliente  Escritura con probabilidad [0.13378381 0.86621619]\n",
      "El cliente  Escritura con probabilidad [0.12520982 0.87479018]\n",
      "El cliente  Escritura con probabilidad [9.44181526e-05 9.99905582e-01]\n",
      "El cliente  Escritura con probabilidad [0.11024652 0.88975348]\n",
      "El cliente  Escritura con probabilidad [0.02339273 0.97660727]\n",
      "El cliente  Escritura con probabilidad [0.14855891 0.85144109]\n",
      "El cliente  Escritura con probabilidad [0.06630311 0.93369689]\n",
      "El cliente  Escritura con probabilidad [0.19903967 0.80096033]\n",
      "El cliente  Escritura con probabilidad [0.2168704 0.7831296]\n",
      "El cliente  Escritura con probabilidad [0.26166879 0.73833121]\n",
      "El cliente  Escritura con probabilidad [5.99146209e-05 9.99940085e-01]\n",
      "El cliente  Escritura con probabilidad [6.73379453e-06 9.99993266e-01]\n",
      "El cliente  Escritura con probabilidad [2.54019028e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.02486258 0.97513742]\n",
      "El cliente  Escritura con probabilidad [0.08403626 0.91596374]\n",
      "El cliente  Escritura con probabilidad [0.07531751 0.92468249]\n",
      "El cliente  Escritura con probabilidad [3.87752102e-07 9.99999612e-01]\n",
      "El cliente  Escritura con probabilidad [0.26172993 0.73827007]\n",
      "El cliente  Escritura con probabilidad [0.20180557 0.79819443]\n",
      "El cliente  Escritura con probabilidad [0.20850634 0.79149366]\n",
      "El cliente  Escritura con probabilidad [0.09807475 0.90192525]\n",
      "El cliente  Escritura con probabilidad [0.17569934 0.82430066]\n",
      "El cliente  Escritura con probabilidad [0.16396846 0.83603154]\n",
      "El cliente  Escritura con probabilidad [0.40729851 0.59270149]\n",
      "El cliente  Escritura con probabilidad [0.09231727 0.90768273]\n",
      "El cliente  Escritura con probabilidad [0.16863953 0.83136047]\n",
      "El cliente  Escritura con probabilidad [4.08482460e-06 9.99995915e-01]\n",
      "El cliente  Escritura con probabilidad [9.16108045e-06 9.99990839e-01]\n",
      "El cliente  Escritura con probabilidad [2.35390663e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.00797481 0.99202519]\n",
      "El cliente  Escritura con probabilidad [0.22079831 0.77920169]\n",
      "El cliente  Escritura con probabilidad [0.08917942 0.91082058]\n",
      "El cliente  Escritura con probabilidad [0.10660495 0.89339505]\n",
      "El cliente  Escritura con probabilidad [0.16980498 0.83019502]\n",
      "El cliente  Escritura con probabilidad [0.0594257 0.9405743]\n",
      "El cliente  Escritura con probabilidad [0.19353448 0.80646552]\n",
      "El cliente  Escritura con probabilidad [0.08561426 0.91438574]\n",
      "El cliente  Escritura con probabilidad [0.02200862 0.97799138]\n",
      "El cliente  Escritura con probabilidad [0.06925279 0.93074721]\n",
      "El cliente  Escritura con probabilidad [0.19198849 0.80801151]\n",
      "El cliente  Escritura con probabilidad [0.08514689 0.91485311]\n",
      "El cliente  Escritura con probabilidad [0.20146614 0.79853386]\n",
      "El cliente  Escritura con probabilidad [0.00492927 0.99507073]\n",
      "El cliente  Escritura con probabilidad [0.22085133 0.77914867]\n",
      "El cliente  Escritura con probabilidad [0.1134029 0.8865971]\n",
      "El cliente  Escritura con probabilidad [0.15240565 0.84759435]\n",
      "El cliente  Escritura con probabilidad [0.08155982 0.91844018]\n",
      "El cliente  Escritura con probabilidad [0.14157397 0.85842603]\n",
      "El cliente  Escritura con probabilidad [0.07596506 0.92403494]\n",
      "El cliente  Escritura con probabilidad [0.16218421 0.83781579]\n",
      "El cliente  Escritura con probabilidad [0.4112705 0.5887295]\n",
      "El cliente  Escritura con probabilidad [0.02150409 0.97849591]\n",
      "El cliente  Escritura con probabilidad [0.08999052 0.91000948]\n",
      "El cliente  Escritura con probabilidad [0.14328461 0.85671539]\n",
      "El cliente  Escritura con probabilidad [0.09564301 0.90435699]\n",
      "El cliente  Escritura con probabilidad [0.20061617 0.79938383]\n",
      "El cliente  Escritura con probabilidad [0.11036949 0.88963051]\n",
      "El cliente  Escritura con probabilidad [0.06788205 0.93211795]\n",
      "El cliente  Escritura con probabilidad [0.12611045 0.87388955]\n",
      "El cliente  Escritura con probabilidad [0.09886572 0.90113428]\n",
      "El cliente  Escritura con probabilidad [0.22134031 0.77865969]\n",
      "El cliente  Escritura con probabilidad [3.55468137e-05 9.99964453e-01]\n",
      "El cliente  Escritura con probabilidad [1.16361803e-05 9.99988364e-01]\n",
      "El cliente  Escritura con probabilidad [1.18110447e-04 9.99881890e-01]\n",
      "El cliente  Escritura con probabilidad [0.20116837 0.79883163]\n",
      "El cliente  Escritura con probabilidad [0.0848704 0.9151296]\n",
      "El cliente  Escritura con probabilidad [3.69316067e-06 9.99996307e-01]\n",
      "El cliente  Escritura con probabilidad [0.39145912 0.60854088]\n",
      "El cliente  Escritura con probabilidad [0.2422017 0.7577983]\n",
      "El cliente  Escritura con probabilidad [0.11290721 0.88709279]\n",
      "El cliente  Escritura con probabilidad [0.02636272 0.97363728]\n",
      "El cliente  Escritura con probabilidad [0.11965182 0.88034818]\n",
      "El cliente  Escritura con probabilidad [8.69282313e-08 9.99999913e-01]\n",
      "El cliente  Escritura con probabilidad [0.16133449 0.83866551]\n",
      "El cliente  Escritura con probabilidad [0.16227067 0.83772933]\n",
      "El cliente  Escritura con probabilidad [0.18554196 0.81445804]\n",
      "El cliente  Escritura con probabilidad [0.01219704 0.98780296]\n",
      "El cliente  Escritura con probabilidad [6.40569783e-04 9.99359430e-01]\n",
      "El cliente  Escritura con probabilidad [0.14456738 0.85543262]\n",
      "El cliente  Escritura con probabilidad [0.03677697 0.96322303]\n",
      "El cliente  Escritura con probabilidad [0.09790521 0.90209479]\n",
      "El cliente  Escritura con probabilidad [0.26466095 0.73533905]\n",
      "El cliente  Escritura con probabilidad [0.19024687 0.80975313]\n",
      "El cliente  Escritura con probabilidad [0.02563676 0.97436324]\n",
      "El cliente  Escritura con probabilidad [0.15455217 0.84544783]\n",
      "El cliente  Escritura con probabilidad [0.01671746 0.98328254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [1.13228213e-07 9.99999887e-01]\n",
      "El cliente  Escritura con probabilidad [0.11158281 0.88841719]\n",
      "El cliente  Escritura con probabilidad [0.05234051 0.94765949]\n",
      "El cliente  Escritura con probabilidad [0.16292184 0.83707816]\n",
      "El cliente  Escritura con probabilidad [0.09821979 0.90178021]\n",
      "El cliente  Escritura con probabilidad [1.66291588e-06 9.99998337e-01]\n",
      "El cliente  Escritura con probabilidad [0.15146209 0.84853791]\n",
      "El cliente  Escritura con probabilidad [3.82378573e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.03937365 0.96062635]\n",
      "El cliente  Escritura con probabilidad [0.02246138 0.97753862]\n",
      "El cliente  Escritura con probabilidad [0.29068518 0.70931482]\n",
      "El cliente  Escritura con probabilidad [0.25381134 0.74618866]\n",
      "El cliente  Escritura con probabilidad [0.00472585 0.99527415]\n",
      "El cliente  Escritura con probabilidad [0.00159727 0.99840273]\n",
      "El cliente  Escritura con probabilidad [0.15979219 0.84020781]\n",
      "El cliente  Escritura con probabilidad [0.00363567 0.99636433]\n",
      "El cliente  Escritura con probabilidad [0.22696758 0.77303242]\n",
      "El cliente  Escritura con probabilidad [0.06773907 0.93226093]\n",
      "El cliente  Escritura con probabilidad [0.04675001 0.95324999]\n",
      "El cliente  Escritura con probabilidad [0.15453336 0.84546664]\n",
      "El cliente  Escritura con probabilidad [9.10383290e-08 9.99999909e-01]\n",
      "El cliente  Escritura con probabilidad [0.03352045 0.96647955]\n",
      "El cliente  Escritura con probabilidad [0.31264578 0.68735422]\n",
      "El cliente  Escritura con probabilidad [0.14252474 0.85747526]\n",
      "El cliente  Escritura con probabilidad [0.0441938 0.9558062]\n",
      "El cliente  Escritura con probabilidad [0.15588064 0.84411936]\n",
      "El cliente  Escritura con probabilidad [3.62686919e-05 9.99963731e-01]\n",
      "El cliente  Escritura con probabilidad [0.18463303 0.81536697]\n",
      "El cliente  Escritura con probabilidad [0.09276835 0.90723165]\n",
      "El cliente  Escritura con probabilidad [1.44328993e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.00136408 0.99863592]\n",
      "El cliente  Escritura con probabilidad [0.14523607 0.85476393]\n",
      "El cliente  Escritura con probabilidad [0.03561543 0.96438457]\n",
      "El cliente  Escritura con probabilidad [0.28673437 0.71326563]\n",
      "El cliente  Escritura con probabilidad [0.05275294 0.94724706]\n",
      "El cliente  Escritura con probabilidad [5.13398062e-07 9.99999487e-01]\n",
      "El cliente  Escritura con probabilidad [0.05817517 0.94182483]\n",
      "El cliente  Escritura con probabilidad [0.19553338 0.80446662]\n",
      "El cliente  Escritura con probabilidad [0.15349206 0.84650794]\n",
      "El cliente  Escritura con probabilidad [0.16682394 0.83317606]\n",
      "El cliente  Escritura con probabilidad [0.2839057 0.7160943]\n",
      "El cliente  Escritura con probabilidad [0.30329362 0.69670638]\n",
      "El cliente  Escritura con probabilidad [0.21012137 0.78987863]\n",
      "El cliente  Escritura con probabilidad [0.21765038 0.78234962]\n",
      "El cliente  Escritura con probabilidad [0.05529084 0.94470916]\n",
      "El cliente  Escritura con probabilidad [1.40287314e-05 9.99985971e-01]\n",
      "El cliente  Escritura con probabilidad [2.28202061e-06 9.99997718e-01]\n",
      "El cliente  Escritura con probabilidad [0.19962562 0.80037438]\n",
      "El cliente  Escritura con probabilidad [0.30938449 0.69061551]\n",
      "El cliente  Escritura con probabilidad [0.05681591 0.94318409]\n",
      "El cliente  Escritura con probabilidad [0.2320735 0.7679265]\n",
      "El cliente  Escritura con probabilidad [0.03616306 0.96383694]\n",
      "El cliente  Escritura con probabilidad [0.12235568 0.87764432]\n",
      "El cliente  Escritura con probabilidad [6.88531907e-06 9.99993115e-01]\n",
      "El cliente  Escritura con probabilidad [6.30586694e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.0115896 0.9884104]\n",
      "El cliente  Escritura con probabilidad [0.26224475 0.73775525]\n",
      "El cliente  Escritura con probabilidad [0.13011013 0.86988987]\n",
      "El cliente  Escritura con probabilidad [0.04400183 0.95599817]\n",
      "El cliente  Escritura con probabilidad [0.1430106 0.8569894]\n",
      "El cliente  Escritura con probabilidad [0.20090646 0.79909354]\n",
      "El cliente  Escritura con probabilidad [0.11868287 0.88131713]\n",
      "El cliente  Escritura con probabilidad [0.1371796 0.8628204]\n",
      "El cliente  Escritura con probabilidad [0.07171464 0.92828536]\n",
      "El cliente  Escritura con probabilidad [0.13204718 0.86795282]\n",
      "El cliente  Escritura con probabilidad [0.1055339 0.8944661]\n",
      "El cliente  Escritura con probabilidad [0.12885669 0.87114331]\n",
      "El cliente  Escritura con probabilidad [0.35520133 0.64479867]\n",
      "El cliente  Escritura con probabilidad [0.32891004 0.67108996]\n",
      "El cliente  Escritura con probabilidad [1.24545068e-07 9.99999875e-01]\n",
      "El cliente  Escritura con probabilidad [0.21524966 0.78475034]\n",
      "El cliente  Escritura con probabilidad [0.21495136 0.78504864]\n",
      "El cliente  Escritura con probabilidad [0.26899036 0.73100964]\n",
      "El cliente  Escritura con probabilidad [3.41159576e-06 9.99996588e-01]\n",
      "El cliente  Escritura con probabilidad [0.16283673 0.83716327]\n",
      "El cliente  Escritura con probabilidad [5.56942541e-04 9.99443057e-01]\n",
      "El cliente  Escritura con probabilidad [0.17271689 0.82728311]\n",
      "El cliente  Escritura con probabilidad [0.18395841 0.81604159]\n",
      "El cliente  Escritura con probabilidad [0.14399136 0.85600864]\n",
      "El cliente  Escritura con probabilidad [6.55354360e-05 9.99934465e-01]\n",
      "El cliente  Escritura con probabilidad [0.1997575 0.8002425]\n",
      "El cliente  Escritura con probabilidad [0.11556299 0.88443701]\n",
      "El cliente  Escritura con probabilidad [0.00863757 0.99136243]\n",
      "El cliente  Escritura con probabilidad [0.10340868 0.89659132]\n",
      "El cliente  Escritura con probabilidad [3.09105829e-05 9.99969089e-01]\n",
      "El cliente  Escritura con probabilidad [0.22313381 0.77686619]\n",
      "El cliente  Escritura con probabilidad [0.23300106 0.76699894]\n",
      "El cliente  Escritura con probabilidad [2.00991948e-08 9.99999980e-01]\n",
      "El cliente  Escritura con probabilidad [0.03606089 0.96393911]\n",
      "El cliente  Escritura con probabilidad [0.02693365 0.97306635]\n",
      "El cliente  Escritura con probabilidad [1.01531630e-04 9.99898468e-01]\n",
      "El cliente  Escritura con probabilidad [0.27568049 0.72431951]\n",
      "El cliente  Escritura con probabilidad [5.48013210e-07 9.99999452e-01]\n",
      "El cliente  Escritura con probabilidad [0.00108321 0.99891679]\n",
      "El cliente  Escritura con probabilidad [5.29880584e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.06719624 0.93280376]\n",
      "El cliente  Escritura con probabilidad [0.22766744 0.77233256]\n",
      "El cliente  Escritura con probabilidad [0.14574471 0.85425529]\n",
      "El cliente  Escritura con probabilidad [0.00151319 0.99848681]\n",
      "El cliente  Escritura con probabilidad [0.12907419 0.87092581]\n",
      "El cliente  Escritura con probabilidad [0.13913451 0.86086549]\n",
      "El cliente  Escritura con probabilidad [0.0993342 0.9006658]\n",
      "El cliente  Escritura con probabilidad [0.02320018 0.97679982]\n",
      "El cliente  Escritura con probabilidad [0.01178265 0.98821735]\n",
      "El cliente  Escritura con probabilidad [0.10771787 0.89228213]\n",
      "El cliente  Escritura con probabilidad [0.24233736 0.75766264]\n",
      "El cliente  Escritura con probabilidad [0.10198697 0.89801303]\n",
      "El cliente  Escritura con probabilidad [0.14834312 0.85165688]\n",
      "El cliente  Escritura con probabilidad [0.00247766 0.99752234]\n",
      "El cliente  Escritura con probabilidad [4.05710088e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.10900088 0.89099912]\n",
      "El cliente  Escritura con probabilidad [0.06418236 0.93581764]\n",
      "El cliente  Escritura con probabilidad [0.07444612 0.92555388]\n",
      "El cliente  Escritura con probabilidad [0.1332731 0.8667269]\n",
      "El cliente  Escritura con probabilidad [0.00481679 0.99518321]\n",
      "El cliente  Escritura con probabilidad [0.13273427 0.86726573]\n",
      "El cliente  Escritura con probabilidad [0.12166182 0.87833818]\n",
      "El cliente  Escritura con probabilidad [0.32573414 0.67426586]\n",
      "El cliente  Escritura con probabilidad [0.24738281 0.75261719]\n",
      "El cliente  Escritura con probabilidad [0.14833042 0.85166958]\n",
      "El cliente  Escritura con probabilidad [0.14993966 0.85006034]\n",
      "El cliente  Escritura con probabilidad [0.15344715 0.84655285]\n",
      "El cliente  Escritura con probabilidad [0.23488602 0.76511398]\n",
      "El cliente  Escritura con probabilidad [4.35336176e-04 9.99564664e-01]\n",
      "El cliente  Escritura con probabilidad [0.22848348 0.77151652]\n",
      "El cliente  Escritura con probabilidad [0.04704903 0.95295097]\n",
      "El cliente  Escritura con probabilidad [0.26238594 0.73761406]\n",
      "El cliente  Escritura con probabilidad [0.44226791 0.55773209]\n",
      "El cliente  Escritura con probabilidad [0.13092389 0.86907611]\n",
      "El cliente  Escritura con probabilidad [3.06870307e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.04184875 0.95815125]\n",
      "El cliente  Escritura con probabilidad [0.18111689 0.81888311]\n",
      "El cliente  Escritura con probabilidad [0.22063162 0.77936838]\n",
      "El cliente  Escritura con probabilidad [0.42914149 0.57085851]\n",
      "El cliente  Escritura con probabilidad [2.51161447e-05 9.99974884e-01]\n",
      "El cliente  Escritura con probabilidad [0.04882257 0.95117743]\n",
      "El cliente  Escritura con probabilidad [0.06704496 0.93295504]\n",
      "El cliente  Escritura con probabilidad [0.13532199 0.86467801]\n",
      "El cliente  Escritura con probabilidad [0.05901158 0.94098842]\n",
      "El cliente  Escritura con probabilidad [7.14766348e-04 9.99285234e-01]\n",
      "El cliente  Escritura con probabilidad [0.08125689 0.91874311]\n",
      "El cliente  Escritura con probabilidad [0.15944705 0.84055295]\n",
      "El cliente  Escritura con probabilidad [0.20711875 0.79288125]\n",
      "El cliente  Escritura con probabilidad [0.1963316 0.8036684]\n",
      "El cliente  Escritura con probabilidad [0.12264936 0.87735064]\n",
      "El cliente  Escritura con probabilidad [0.08774981 0.91225019]\n",
      "El cliente  Escritura con probabilidad [2.07939261e-05 9.99979206e-01]\n",
      "El cliente  Escritura con probabilidad [1.34918310e-07 9.99999865e-01]\n",
      "El cliente  Escritura con probabilidad [0.16629685 0.83370315]\n",
      "El cliente  Escritura con probabilidad [0.19169918 0.80830082]\n",
      "El cliente  Escritura con probabilidad [0.08524312 0.91475688]\n",
      "El cliente  Escritura con probabilidad [0.28515726 0.71484274]\n",
      "El cliente  Escritura con probabilidad [0.11420882 0.88579118]\n",
      "El cliente  Escritura con probabilidad [0.43475223 0.56524777]\n",
      "El cliente  Escritura con probabilidad [0.24723063 0.75276937]\n",
      "El cliente  Escritura con probabilidad [0.02896161 0.97103839]\n",
      "El cliente  Escritura con probabilidad [0.07596306 0.92403694]\n",
      "El cliente  Escritura con probabilidad [0.06415955 0.93584045]\n",
      "El cliente  Escritura con probabilidad [9.99200722e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.0898537 0.9101463]\n",
      "El cliente  Escritura con probabilidad [5.72120918e-06 9.99994279e-01]\n",
      "El cliente  Escritura con probabilidad [0.21280901 0.78719099]\n",
      "El cliente  Escritura con probabilidad [0.12518976 0.87481024]\n",
      "El cliente  Escritura con probabilidad [9.62201275e-04 9.99037799e-01]\n",
      "El cliente  Escritura con probabilidad [0.0596192 0.9403808]\n",
      "El cliente  Escritura con probabilidad [0.22102899 0.77897101]\n",
      "El cliente  Escritura con probabilidad [0.4449277 0.5550723]\n",
      "El cliente  Escritura con probabilidad [0.25567904 0.74432096]\n",
      "El cliente  Escritura con probabilidad [1.32421369e-06 9.99998676e-01]\n",
      "El cliente  Escritura con probabilidad [0.06101328 0.93898672]\n",
      "El cliente  Escritura con probabilidad [6.91562664e-04 9.99308437e-01]\n",
      "El cliente  Escritura con probabilidad [0.00433728 0.99566272]\n",
      "El cliente  Escritura con probabilidad [0.08078215 0.91921785]\n",
      "El cliente  Escritura con probabilidad [2.08965956e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [2.25960127e-07 9.99999774e-01]\n",
      "El cliente  Escritura con probabilidad [0.17278184 0.82721816]\n",
      "El cliente  Escritura con probabilidad [0.1345829 0.8654171]\n",
      "El cliente  Escritura con probabilidad [0.02893155 0.97106845]\n",
      "El cliente  Escritura con probabilidad [0.21216021 0.78783979]\n",
      "El cliente  Escritura con probabilidad [0.12557956 0.87442044]\n",
      "El cliente  Escritura con probabilidad [0.16584435 0.83415565]\n",
      "El cliente  Escritura con probabilidad [0.07747776 0.92252224]\n",
      "El cliente  Escritura con probabilidad [0.02553986 0.97446014]\n",
      "El cliente  Escritura con probabilidad [0.09237378 0.90762622]\n",
      "El cliente  Escritura con probabilidad [0.22233607 0.77766393]\n",
      "El cliente  Escritura con probabilidad [0.19231092 0.80768908]\n",
      "El cliente  Escritura con probabilidad [0.29294352 0.70705648]\n",
      "El cliente  Escritura con probabilidad [6.53216370e-09 9.99999993e-01]\n",
      "El cliente  Escritura con probabilidad [0.04549034 0.95450966]\n",
      "El cliente  Escritura con probabilidad [2.17803553e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.0322072 0.9677928]\n",
      "El cliente  Escritura con probabilidad [0.28144986 0.71855014]\n",
      "El cliente  Escritura con probabilidad [0.06455355 0.93544645]\n",
      "El cliente  Escritura con probabilidad [0.22040904 0.77959096]\n",
      "El cliente  Escritura con probabilidad [0.09072716 0.90927284]\n",
      "El cliente  Escritura con probabilidad [0.29079748 0.70920252]\n",
      "El cliente  Escritura con probabilidad [0.11981318 0.88018682]\n",
      "El cliente  Escritura con probabilidad [0.00753371 0.99246629]\n",
      "El cliente  Escritura con probabilidad [0.14233768 0.85766232]\n",
      "El cliente  Escritura con probabilidad [2.11855087e-04 9.99788145e-01]\n",
      "El cliente  Escritura con probabilidad [0.44091996 0.55908004]\n",
      "El cliente  Escritura con probabilidad [1.12670203e-04 9.99887330e-01]\n",
      "El cliente  Escritura con probabilidad [0.00160796 0.99839204]\n",
      "El cliente  Escritura con probabilidad [0.14041853 0.85958147]\n",
      "El cliente  Escritura con probabilidad [0.32219329 0.67780671]\n",
      "El cliente  Escritura con probabilidad [0.16781344 0.83218656]\n",
      "El cliente  Escritura con probabilidad [0.20246939 0.79753061]\n",
      "El cliente  Escritura con probabilidad [0.30748872 0.69251128]\n",
      "El cliente  Escritura con probabilidad [0.22691683 0.77308317]\n",
      "El cliente  Escritura con probabilidad [0.20031136 0.79968864]\n",
      "El cliente  Escritura con probabilidad [0.10914875 0.89085125]\n",
      "El cliente  Escritura con probabilidad [0.17423982 0.82576018]\n",
      "El cliente  Escritura con probabilidad [3.43218682e-04 9.99656781e-01]\n",
      "El cliente  Escritura con probabilidad [0.06529601 0.93470399]\n",
      "El cliente  Escritura con probabilidad [0.22652262 0.77347738]\n",
      "El cliente  Escritura con probabilidad [0.06984601 0.93015399]\n",
      "El cliente  Escritura con probabilidad [1.19854079e-07 9.99999880e-01]\n",
      "El cliente  Escritura con probabilidad [0.12922779 0.87077221]\n",
      "El cliente  Escritura con probabilidad [0.00787426 0.99212574]\n",
      "El cliente  Escritura con probabilidad [0.08682326 0.91317674]\n",
      "El cliente  Escritura con probabilidad [0.06773366 0.93226634]\n",
      "El cliente  Escritura con probabilidad [0.12106804 0.87893196]\n",
      "El cliente  Escritura con probabilidad [0.46064202 0.53935798]\n",
      "El cliente  Escritura con probabilidad [0.18702112 0.81297888]\n",
      "El cliente  Escritura con probabilidad [0.3382195 0.6617805]\n",
      "El cliente  Escritura con probabilidad [0.30365863 0.69634137]\n",
      "El cliente  Escritura con probabilidad [0.20100561 0.79899439]\n",
      "El cliente  Escritura con probabilidad [0.2466526 0.7533474]\n",
      "El cliente  Escritura con probabilidad [0.11980933 0.88019067]\n",
      "El cliente  Escritura con probabilidad [0.20860805 0.79139195]\n",
      "El cliente  Escritura con probabilidad [0.11615093 0.88384907]\n",
      "El cliente  Escritura con probabilidad [5.77193027e-09 9.99999994e-01]\n",
      "El cliente  Escritura con probabilidad [0.10525727 0.89474273]\n",
      "El cliente  Escritura con probabilidad [0.08302381 0.91697619]\n",
      "El cliente  Escritura con probabilidad [0.02024115 0.97975885]\n",
      "El cliente  Escritura con probabilidad [0.01381661 0.98618339]\n",
      "El cliente  Escritura con probabilidad [0.28377758 0.71622242]\n",
      "El cliente  Escritura con probabilidad [0.20148203 0.79851797]\n",
      "El cliente  Escritura con probabilidad [5.33182483e-08 9.99999947e-01]\n",
      "El cliente  Escritura con probabilidad [0.05705603 0.94294397]\n",
      "El cliente  Escritura con probabilidad [0.22368539 0.77631461]\n",
      "El cliente  Escritura con probabilidad [0.06054997 0.93945003]\n",
      "El cliente  Escritura con probabilidad [0.10228964 0.89771036]\n",
      "El cliente  Escritura con probabilidad [0.08461627 0.91538373]\n",
      "El cliente  Escritura con probabilidad [0.33966236 0.66033764]\n",
      "El cliente  Escritura con probabilidad [0.09222903 0.90777097]\n",
      "El cliente  Escritura con probabilidad [0.03006129 0.96993871]\n",
      "El cliente  Escritura con probabilidad [0.14811108 0.85188892]\n",
      "El cliente  Escritura con probabilidad [0.06598605 0.93401395]\n",
      "El cliente  Escritura con probabilidad [0.16176036 0.83823964]\n",
      "El cliente  Escritura con probabilidad [0.03082523 0.96917477]\n",
      "El cliente  Escritura con probabilidad [1.05443300e-04 9.99894557e-01]\n",
      "El cliente  Escritura con probabilidad [0.02165099 0.97834901]\n",
      "El cliente  Escritura con probabilidad [0.03385832 0.96614168]\n",
      "El cliente  Escritura con probabilidad [1.89686042e-06 9.99998103e-01]\n",
      "El cliente  Escritura con probabilidad [1.35207921e-06 9.99998648e-01]\n",
      "El cliente  Escritura con probabilidad [0.21612289 0.78387711]\n",
      "El cliente  Escritura con probabilidad [0.18376758 0.81623242]\n",
      "El cliente  Escritura con probabilidad [0.19487618 0.80512382]\n",
      "El cliente  Escritura con probabilidad [0.2622521 0.7377479]\n",
      "El cliente  Escritura con probabilidad [0.27883481 0.72116519]\n",
      "El cliente  Escritura con probabilidad [0.00152584 0.99847416]\n",
      "El cliente  Escritura con probabilidad [1.07351223e-04 9.99892649e-01]\n",
      "El cliente  Escritura con probabilidad [0.22578158 0.77421842]\n",
      "El cliente  Escritura con probabilidad [0.21066014 0.78933986]\n",
      "El cliente  Escritura con probabilidad [0.14038263 0.85961737]\n",
      "El cliente  Escritura con probabilidad [0.20618569 0.79381431]\n",
      "El cliente  Escritura con probabilidad [3.17437099e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.03741244 0.96258756]\n",
      "El cliente  Escritura con probabilidad [0.23819729 0.76180271]\n",
      "El cliente  Escritura con probabilidad [0.26025727 0.73974273]\n",
      "El cliente  Escritura con probabilidad [0.17513332 0.82486668]\n",
      "El cliente  Escritura con probabilidad [0.00256779 0.99743221]\n",
      "El cliente  Escritura con probabilidad [0.15816997 0.84183003]\n",
      "El cliente  Escritura con probabilidad [0.0104695 0.9895305]\n",
      "El cliente  Escritura con probabilidad [0.06783903 0.93216097]\n",
      "El cliente  Escritura con probabilidad [0.25876846 0.74123154]\n",
      "El cliente  Escritura con probabilidad [0.03153467 0.96846533]\n",
      "El cliente  Escritura con probabilidad [0.12076608 0.87923392]\n",
      "El cliente  Escritura con probabilidad [0.19372385 0.80627615]\n",
      "El cliente  Escritura con probabilidad [0.22230845 0.77769155]\n",
      "El cliente  Escritura con probabilidad [3.26170424e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.24746331 0.75253669]\n",
      "El cliente  Escritura con probabilidad [0.22138542 0.77861458]\n",
      "El cliente  Escritura con probabilidad [0.15846144 0.84153856]\n",
      "El cliente  Escritura con probabilidad [1.58999215e-05 9.99984100e-01]\n",
      "El cliente  Escritura con probabilidad [0.06951559 0.93048441]\n",
      "El cliente  Escritura con probabilidad [0.02068019 0.97931981]\n",
      "El cliente  Escritura con probabilidad [0.24181997 0.75818003]\n",
      "El cliente  Escritura con probabilidad [0.17083747 0.82916253]\n",
      "El cliente  Escritura con probabilidad [0.19686305 0.80313695]\n",
      "El cliente  Escritura con probabilidad [4.50173129e-05 9.99954983e-01]\n",
      "El cliente  Escritura con probabilidad [0.25423495 0.74576505]\n",
      "El cliente  Escritura con probabilidad [6.89990287e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [2.64375654e-06 9.99997356e-01]\n",
      "El cliente  Escritura con probabilidad [0.26197483 0.73802517]\n",
      "El cliente  Escritura con probabilidad [0.28133215 0.71866785]\n",
      "El cliente  Escritura con probabilidad [0.08783615 0.91216385]\n",
      "El cliente  Escritura con probabilidad [0.06366664 0.93633336]\n",
      "El cliente  Escritura con probabilidad [0.42051297 0.57948703]\n",
      "El cliente  Escritura con probabilidad [0.22189721 0.77810279]\n",
      "El cliente  Escritura con probabilidad [0.28193114 0.71806886]\n",
      "El cliente  Escritura con probabilidad [0.16101633 0.83898367]\n",
      "El cliente  Escritura con probabilidad [5.63934214e-08 9.99999944e-01]\n",
      "El cliente  Escritura con probabilidad [0.07720473 0.92279527]\n",
      "El cliente  Escritura con probabilidad [0.4252497 0.5747503]\n",
      "El cliente  Escritura con probabilidad [0.41808782 0.58191218]\n",
      "El cliente  Escritura con probabilidad [0.18685555 0.81314445]\n",
      "El cliente  Escritura con probabilidad [0.05978251 0.94021749]\n",
      "El cliente  Escritura con probabilidad [0.12685408 0.87314592]\n",
      "El cliente  Escritura con probabilidad [0.20111791 0.79888209]\n",
      "El cliente  Escritura con probabilidad [0.13906607 0.86093393]\n",
      "El cliente  Escritura con probabilidad [0.27220379 0.72779621]\n",
      "El cliente  Escritura con probabilidad [0.13722853 0.86277147]\n",
      "El cliente  Escritura con probabilidad [6.2021055e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.13664993 0.86335007]\n",
      "El cliente  Escritura con probabilidad [0.24570133 0.75429867]\n",
      "El cliente  Escritura con probabilidad [0.03389713 0.96610287]\n",
      "El cliente  Escritura con probabilidad [0.0898549 0.9101451]\n",
      "El cliente  Escritura con probabilidad [0.18452613 0.81547387]\n",
      "El cliente  Escritura con probabilidad [0.03044945 0.96955055]\n",
      "El cliente  Escritura con probabilidad [0.1355977 0.8644023]\n",
      "El cliente  Escritura con probabilidad [0.15926618 0.84073382]\n",
      "El cliente  Escritura con probabilidad [0.17801214 0.82198786]\n",
      "El cliente  Escritura con probabilidad [0.14366096 0.85633904]\n",
      "El cliente  Escritura con probabilidad [0.28698444 0.71301556]\n",
      "El cliente  Escritura con probabilidad [0.00122158 0.99877842]\n",
      "El cliente  Escritura con probabilidad [0.17987088 0.82012912]\n",
      "El cliente  Escritura con probabilidad [0.15978987 0.84021013]\n",
      "El cliente  Escritura con probabilidad [0.03820972 0.96179028]\n",
      "El cliente  Escritura con probabilidad [6.49857945e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.41765710e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.19025003 0.80974997]\n",
      "El cliente  Escritura con probabilidad [0.03346549 0.96653451]\n",
      "El cliente  Escritura con probabilidad [1.40462821e-08 9.99999986e-01]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.00353428 0.99646572]\n",
      "El cliente  Escritura con probabilidad [0.44569412 0.55430588]\n",
      "El cliente  Escritura con probabilidad [0.18468613 0.81531387]\n",
      "El cliente  Escritura con probabilidad [0.17866001 0.82133999]\n",
      "El cliente  Escritura con probabilidad [0.28267742 0.71732258]\n",
      "El cliente  Escritura con probabilidad [0.04097088 0.95902912]\n",
      "El cliente  Escritura con probabilidad [3.86420478e-05 9.99961358e-01]\n",
      "El cliente  Escritura con probabilidad [0.0105062 0.9894938]\n",
      "El cliente  Escritura con probabilidad [8.15797698e-04 9.99184202e-01]\n",
      "El cliente  Escritura con probabilidad [0.07691213 0.92308787]\n",
      "El cliente  Escritura con probabilidad [0.09667166 0.90332834]\n",
      "El cliente  Escritura con probabilidad [0.25630111 0.74369889]\n",
      "El cliente  Escritura con probabilidad [0.22971817 0.77028183]\n",
      "El cliente  Escritura con probabilidad [0.07557293 0.92442707]\n",
      "El cliente  Escritura con probabilidad [0.11714816 0.88285184]\n",
      "El cliente  Escritura con probabilidad [0.23121952 0.76878048]\n",
      "El cliente  Escritura con probabilidad [0.05301558 0.94698442]\n",
      "El cliente  Escritura con probabilidad [0.24097937 0.75902063]\n",
      "El cliente  Escritura con probabilidad [0.15518834 0.84481166]\n",
      "El cliente  Escritura con probabilidad [0.22682551 0.77317449]\n",
      "El cliente  Escritura con probabilidad [0.23018519 0.76981481]\n",
      "El cliente  Escritura con probabilidad [0.19470765 0.80529235]\n",
      "El cliente  Escritura con probabilidad [8.56387753e-04 9.99143612e-01]\n",
      "El cliente  Escritura con probabilidad [9.84970094e-09 9.99999990e-01]\n",
      "El cliente  Escritura con probabilidad [1.73710931e-06 9.99998263e-01]\n",
      "El cliente  Escritura con probabilidad [0.23838098 0.76161902]\n",
      "El cliente  Escritura con probabilidad [0.32159332 0.67840668]\n",
      "El cliente  Escritura con probabilidad [0.09797777 0.90202223]\n",
      "El cliente  Escritura con probabilidad [3.78650878e-06 9.99996213e-01]\n",
      "El cliente  Escritura con probabilidad [0.13798454 0.86201546]\n",
      "El cliente  Escritura con probabilidad [0.15147111 0.84852889]\n",
      "El cliente  Escritura con probabilidad [0.05372868 0.94627132]\n",
      "El cliente  Escritura con probabilidad [2.37438535e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [3.32157173e-04 9.99667843e-01]\n",
      "El cliente  Escritura con probabilidad [0.05564236 0.94435764]\n",
      "El cliente  Escritura con probabilidad [0.30447029 0.69552971]\n",
      "El cliente  Escritura con probabilidad [0.22013246 0.77986754]\n",
      "El cliente  Escritura con probabilidad [0.17498212 0.82501788]\n",
      "El cliente  Escritura con probabilidad [0.19040614 0.80959386]\n",
      "El cliente  Escritura con probabilidad [3.13769557e-06 9.99996862e-01]\n",
      "El cliente  Escritura con probabilidad [0.21558858 0.78441142]\n",
      "El cliente  Escritura con probabilidad [0.01457837 0.98542163]\n",
      "El cliente  Escritura con probabilidad [0.18040976 0.81959024]\n",
      "El cliente  Escritura con probabilidad [0.26811954 0.73188046]\n",
      "El cliente  Escritura con probabilidad [0.02730075 0.97269925]\n",
      "El cliente  Escritura con probabilidad [0.04662476 0.95337524]\n",
      "El cliente  Escritura con probabilidad [0.00247913 0.99752087]\n",
      "El cliente  Escritura con probabilidad [0.13957928 0.86042072]\n",
      "El cliente  Escritura con probabilidad [0.00286763 0.99713237]\n",
      "El cliente  Escritura con probabilidad [0.19491511 0.80508489]\n",
      "El cliente  Escritura con probabilidad [0.28812426 0.71187574]\n",
      "El cliente  Escritura con probabilidad [0.44731221 0.55268779]\n",
      "El cliente  Escritura con probabilidad [0.01810236 0.98189764]\n",
      "El cliente  Escritura con probabilidad [3.99680289e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [2.12439399e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.01336253 0.98663747]\n",
      "El cliente  Escritura con probabilidad [0.07149972 0.92850028]\n",
      "El cliente  Escritura con probabilidad [0.2277404 0.7722596]\n",
      "El cliente  Escritura con probabilidad [0.10557524 0.89442476]\n",
      "El cliente  Escritura con probabilidad [9.33822612e-07 9.99999066e-01]\n",
      "El cliente  Escritura con probabilidad [0.14373509 0.85626491]\n",
      "El cliente  Escritura con probabilidad [0.0462336 0.9537664]\n",
      "El cliente  Escritura con probabilidad [0.22057056 0.77942944]\n",
      "El cliente  Escritura con probabilidad [4.79182720e-04 9.99520817e-01]\n",
      "El cliente  Escritura con probabilidad [0.07903162 0.92096838]\n",
      "El cliente  Escritura con probabilidad [0.15011594 0.84988406]\n",
      "El cliente  Escritura con probabilidad [0.01101534 0.98898466]\n",
      "El cliente  Escritura con probabilidad [0.23150579 0.76849421]\n",
      "El cliente  Escritura con probabilidad [0.1797678 0.8202322]\n",
      "El cliente  Escritura con probabilidad [0.13393571 0.86606429]\n",
      "El cliente  Escritura con probabilidad [0.07633292 0.92366708]\n",
      "El cliente  Escritura con probabilidad [0.15633929 0.84366071]\n",
      "El cliente  Escritura con probabilidad [3.18726457e-04 9.99681274e-01]\n",
      "El cliente  Escritura con probabilidad [0.1982451 0.8017549]\n",
      "El cliente  Escritura con probabilidad [0.2830195 0.7169805]\n",
      "El cliente  Escritura con probabilidad [5.52261792e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.14093969 0.85906031]\n",
      "El cliente  Escritura con probabilidad [3.21863333e-07 9.99999678e-01]\n",
      "El cliente  Escritura con probabilidad [0.0477825 0.9522175]\n",
      "El cliente  Escritura con probabilidad [0.00365031 0.99634969]\n",
      "El cliente  Escritura con probabilidad [0.23040515 0.76959485]\n",
      "El cliente  Escritura con probabilidad [0.0085623 0.9914377]\n",
      "El cliente  Escritura con probabilidad [0.23083042 0.76916958]\n",
      "El cliente  Escritura con probabilidad [0.20351835 0.79648165]\n",
      "El cliente  Escritura con probabilidad [0.08763649 0.91236351]\n",
      "El cliente  Escritura con probabilidad [0.06001224 0.93998776]\n",
      "El cliente  Escritura con probabilidad [0.04074962 0.95925038]\n",
      "El cliente  Escritura con probabilidad [0.21331023 0.78668977]\n",
      "El cliente  Escritura con probabilidad [0.13414027 0.86585973]\n",
      "El cliente  Escritura con probabilidad [0.18862078 0.81137922]\n",
      "El cliente  Escritura con probabilidad [0.06352251 0.93647749]\n",
      "El cliente  Escritura con probabilidad [0.10541052 0.89458948]\n",
      "El cliente  Escritura con probabilidad [0.30100964 0.69899036]\n",
      "El cliente  Escritura con probabilidad [0.17552339 0.82447661]\n",
      "El cliente  Escritura con probabilidad [0.20992829 0.79007171]\n",
      "El cliente  Escritura con probabilidad [6.16759008e-07 9.99999383e-01]\n",
      "El cliente  Escritura con probabilidad [4.07348917e-06 9.99995927e-01]\n",
      "El cliente  Escritura con probabilidad [0.20979454 0.79020546]\n",
      "El cliente  Escritura con probabilidad [9.8013105e-07 9.9999902e-01]\n",
      "El cliente  Escritura con probabilidad [0.02938057 0.97061943]\n",
      "El cliente  Escritura con probabilidad [0.05488603 0.94511397]\n",
      "El cliente  Escritura con probabilidad [0.13606635 0.86393365]\n",
      "El cliente  Escritura con probabilidad [0.31476247 0.68523753]\n",
      "El cliente  Escritura con probabilidad [0.11907733 0.88092267]\n",
      "El cliente  Escritura con probabilidad [0.1442667 0.8557333]\n",
      "El cliente  Escritura con probabilidad [0.07853196 0.92146804]\n",
      "El cliente  Escritura con probabilidad [0.02328414 0.97671586]\n",
      "El cliente  Escritura con probabilidad [0.28175769 0.71824231]\n",
      "El cliente  Escritura con probabilidad [0.44675323 0.55324677]\n",
      "El cliente  Escritura con probabilidad [0.13343679 0.86656321]\n",
      "El cliente  Escritura con probabilidad [0.43795392 0.56204608]\n",
      "El cliente  Escritura con probabilidad [0.23907997 0.76092003]\n",
      "El cliente  Escritura con probabilidad [0.18265213 0.81734787]\n",
      "El cliente  Escritura con probabilidad [2.34657058e-06 9.99997653e-01]\n",
      "El cliente  Escritura con probabilidad [0.27286015 0.72713985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.17781929 0.82218071]\n",
      "El cliente  Escritura con probabilidad [0.03027359 0.96972641]\n",
      "El cliente  Escritura con probabilidad [0.12010253 0.87989747]\n",
      "El cliente  Escritura con probabilidad [0.16953118 0.83046882]\n",
      "El cliente  Escritura con probabilidad [7.48616724e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.2196741 0.7803259]\n",
      "El cliente  Escritura con probabilidad [0.23875905 0.76124095]\n",
      "El cliente  Escritura con probabilidad [0.17705349 0.82294651]\n",
      "El cliente  Escritura con probabilidad [0.0801688 0.9198312]\n",
      "El cliente  Escritura con probabilidad [0.21639598 0.78360402]\n",
      "El cliente  Escritura con probabilidad [2.01516143e-07 9.99999798e-01]\n",
      "El cliente  Escritura con probabilidad [0.24492359 0.75507641]\n",
      "El cliente  Escritura con probabilidad [0.43108928 0.56891072]\n",
      "El cliente  Escritura con probabilidad [0.20479362 0.79520638]\n",
      "El cliente  Escritura con probabilidad [0.22160867 0.77839133]\n",
      "El cliente  Escritura con probabilidad [0.0997409 0.9002591]\n",
      "El cliente  Escritura con probabilidad [0.01900705 0.98099295]\n",
      "El cliente  Escritura con probabilidad [2.64389199e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.14796436 0.85203564]\n",
      "El cliente  Escritura con probabilidad [0.34548815 0.65451185]\n",
      "El cliente  Escritura con probabilidad [0.1991953 0.8008047]\n",
      "El cliente  Escritura con probabilidad [0.43882361 0.56117639]\n",
      "El cliente  Escritura con probabilidad [1.83138264e-07 9.99999817e-01]\n",
      "El cliente  Escritura con probabilidad [0.08122388 0.91877612]\n",
      "El cliente  Escritura con probabilidad [0.28295856 0.71704144]\n",
      "El cliente  Escritura con probabilidad [0.15252394 0.84747606]\n",
      "El cliente  Escritura con probabilidad [0.05780606 0.94219394]\n",
      "El cliente  Escritura con probabilidad [3.03996791e-07 9.99999696e-01]\n",
      "El cliente  Escritura con probabilidad [0.13766316 0.86233684]\n",
      "El cliente  Escritura con probabilidad [0.05217781 0.94782219]\n",
      "El cliente  Escritura con probabilidad [6.66133815e-16 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.01786046 0.98213954]\n",
      "El cliente  Escritura con probabilidad [1.58999347e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.05998867 0.94001133]\n",
      "El cliente  Escritura con probabilidad [0.04162354 0.95837646]\n",
      "El cliente  Escritura con probabilidad [0.12302828 0.87697172]\n",
      "El cliente  Escritura con probabilidad [0.13550807 0.86449193]\n",
      "El cliente  Escritura con probabilidad [0.17583346 0.82416654]\n",
      "El cliente  Escritura con probabilidad [0.13710484 0.86289516]\n",
      "El cliente  Escritura con probabilidad [0.27256493 0.72743507]\n",
      "El cliente  Escritura con probabilidad [2.20822979e-07 9.99999779e-01]\n",
      "El cliente  Escritura con probabilidad [0.0831286 0.9168714]\n",
      "El cliente  Escritura con probabilidad [0.13885887 0.86114113]\n",
      "El cliente  Escritura con probabilidad [0.01168422 0.98831578]\n",
      "El cliente  Escritura con probabilidad [0.13732414 0.86267586]\n",
      "El cliente  Escritura con probabilidad [0.12065034 0.87934966]\n",
      "El cliente  Escritura con probabilidad [0.3885207 0.6114793]\n",
      "El cliente  Escritura con probabilidad [0.07003348 0.92996652]\n",
      "El cliente  Escritura con probabilidad [0.18564568 0.81435432]\n",
      "El cliente  Escritura con probabilidad [0.02055333 0.97944667]\n",
      "El cliente  Escritura con probabilidad [0.18802888 0.81197112]\n",
      "El cliente  Escritura con probabilidad [0.00796787 0.99203213]\n",
      "El cliente  Escritura con probabilidad [4.55213286e-06 9.99995448e-01]\n",
      "El cliente  Escritura con probabilidad [0.22368675 0.77631325]\n",
      "El cliente  Escritura con probabilidad [9.14997536e-04 9.99085002e-01]\n",
      "El cliente  Escritura con probabilidad [0.1389658 0.8610342]\n",
      "El cliente  Escritura con probabilidad [0.12780395 0.87219605]\n",
      "El cliente  Escritura con probabilidad [0.19747245 0.80252755]\n",
      "El cliente  Escritura con probabilidad [0.164056 0.835944]\n",
      "El cliente  Escritura con probabilidad [0.07468584 0.92531416]\n",
      "El cliente  Escritura con probabilidad [0.22067896 0.77932104]\n",
      "El cliente  Escritura con probabilidad [0.28518108 0.71481892]\n",
      "El cliente  Escritura con probabilidad [0.21837393 0.78162607]\n",
      "El cliente  Escritura con probabilidad [0.0930511 0.9069489]\n",
      "El cliente  Escritura con probabilidad [2.84137992e-05 9.99971586e-01]\n",
      "El cliente  Escritura con probabilidad [0.1073558 0.8926442]\n",
      "El cliente  Escritura con probabilidad [0.21016834 0.78983166]\n",
      "El cliente  Escritura con probabilidad [0.10342292 0.89657708]\n",
      "El cliente  Escritura con probabilidad [0.19950127 0.80049873]\n",
      "El cliente  Escritura con probabilidad [0.19503313 0.80496687]\n",
      "El cliente  Escritura con probabilidad [5.60203368e-04 9.99439797e-01]\n",
      "El cliente  Escritura con probabilidad [0.17658389 0.82341611]\n",
      "El cliente  Escritura con probabilidad [8.95752428e-06 9.99991042e-01]\n",
      "El cliente  Escritura con probabilidad [0.17733466 0.82266534]\n",
      "El cliente  Escritura con probabilidad [0.1134877 0.8865123]\n",
      "El cliente  Escritura con probabilidad [0.01660851 0.98339149]\n",
      "El cliente  Escritura con probabilidad [0.15826392 0.84173608]\n",
      "El cliente  Escritura con probabilidad [0.15205209 0.84794791]\n",
      "El cliente  Escritura con probabilidad [1.29843469e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.29553889 0.70446111]\n",
      "El cliente  Escritura con probabilidad [0.19859888 0.80140112]\n",
      "El cliente  Escritura con probabilidad [3.57843918e-04 9.99642156e-01]\n",
      "El cliente  Escritura con probabilidad [0.18125719 0.81874281]\n",
      "El cliente  Escritura con probabilidad [0.01693703 0.98306297]\n",
      "El cliente  Escritura con probabilidad [0.00845237 0.99154763]\n",
      "El cliente  Escritura con probabilidad [0.13173502 0.86826498]\n",
      "El cliente  Escritura con probabilidad [0.0302633 0.9697367]\n",
      "El cliente  Escritura con probabilidad [0.18449294 0.81550706]\n",
      "El cliente  Escritura con probabilidad [2.55991348e-08 9.99999974e-01]\n",
      "El cliente  Escritura con probabilidad [0.12639199 0.87360801]\n",
      "El cliente  Escritura con probabilidad [0.02350251 0.97649749]\n",
      "El cliente  Escritura con probabilidad [0.09084197 0.90915803]\n",
      "El cliente  Escritura con probabilidad [0.00127491 0.99872509]\n",
      "El cliente  Escritura con probabilidad [6.87805826e-04 9.99312194e-01]\n",
      "El cliente  Escritura con probabilidad [0.11621148 0.88378852]\n",
      "El cliente  Escritura con probabilidad [1.52296656e-06 9.99998477e-01]\n",
      "El cliente  Escritura con probabilidad [0.10806878 0.89193122]\n",
      "El cliente  Escritura con probabilidad [1.39385144e-06 9.99998606e-01]\n",
      "El cliente  Escritura con probabilidad [0.08595435 0.91404565]\n",
      "El cliente  Escritura con probabilidad [0.21161796 0.78838204]\n",
      "El cliente  Escritura con probabilidad [0.01311159 0.98688841]\n",
      "El cliente  Escritura con probabilidad [0.01092062 0.98907938]\n",
      "El cliente  Escritura con probabilidad [0.171449 0.828551]\n",
      "El cliente  Escritura con probabilidad [0.03058204 0.96941796]\n",
      "El cliente  Escritura con probabilidad [0.30049193 0.69950807]\n",
      "El cliente  Escritura con probabilidad [0.14525214 0.85474786]\n",
      "El cliente  Escritura con probabilidad [0.14747094 0.85252906]\n",
      "El cliente  Escritura con probabilidad [0.05132739 0.94867261]\n",
      "El cliente  Escritura con probabilidad [9.74461745e-05 9.99902554e-01]\n",
      "El cliente  Escritura con probabilidad [0.28330588 0.71669412]\n",
      "El cliente  Escritura con probabilidad [0.02399033 0.97600967]\n",
      "El cliente  Escritura con probabilidad [2.22777417e-06 9.99997772e-01]\n",
      "El cliente  Escritura con probabilidad [0.2428176 0.7571824]\n",
      "El cliente  Escritura con probabilidad [0.18842611 0.81157389]\n",
      "El cliente  Escritura con probabilidad [0.20242172 0.79757828]\n",
      "El cliente  Escritura con probabilidad [0.09768517 0.90231483]\n",
      "El cliente  Escritura con probabilidad [0.00242214 0.99757786]\n",
      "El cliente  Escritura con probabilidad [0.24584198 0.75415802]\n",
      "El cliente  Escritura con probabilidad [0.21006443 0.78993557]\n",
      "El cliente  Escritura con probabilidad [2.07921185e-04 9.99792079e-01]\n",
      "El cliente  Escritura con probabilidad [0.01801742 0.98198258]\n",
      "El cliente  Escritura con probabilidad [0.18768755 0.81231245]\n",
      "El cliente  Escritura con probabilidad [1.30260781e-05 9.99986974e-01]\n",
      "El cliente  Escritura con probabilidad [1.09076596e-06 9.99998909e-01]\n",
      "El cliente  Escritura con probabilidad [0.20591437 0.79408563]\n",
      "El cliente  Escritura con probabilidad [0.11299749 0.88700251]\n",
      "El cliente  Escritura con probabilidad [0.11132442 0.88867558]\n",
      "El cliente  Escritura con probabilidad [0.11474059 0.88525941]\n",
      "El cliente  Escritura con probabilidad [0.15626128 0.84373872]\n",
      "El cliente  Escritura con probabilidad [0.15711787 0.84288213]\n",
      "El cliente  Escritura con probabilidad [0.0865585 0.9134415]\n",
      "El cliente  Escritura con probabilidad [0.02452376 0.97547624]\n",
      "El cliente  Escritura con probabilidad [0.2236481 0.7763519]\n",
      "El cliente  Escritura con probabilidad [0.20065871 0.79934129]\n",
      "El cliente  Escritura con probabilidad [0.00893914 0.99106086]\n",
      "El cliente  Escritura con probabilidad [0.04524567 0.95475433]\n",
      "El cliente  Escritura con probabilidad [0.04384763 0.95615237]\n",
      "El cliente  Escritura con probabilidad [0.26395957 0.73604043]\n",
      "El cliente  Escritura con probabilidad [0.10010873 0.89989127]\n",
      "El cliente  Escritura con probabilidad [0.18747523 0.81252477]\n",
      "El cliente  Escritura con probabilidad [0.18183595 0.81816405]\n",
      "El cliente  Escritura con probabilidad [0.041504 0.958496]\n",
      "El cliente  Escritura con probabilidad [0.06271992 0.93728008]\n",
      "El cliente  Escritura con probabilidad [1.23065206e-05 9.99987693e-01]\n",
      "El cliente  Escritura con probabilidad [0.12890765 0.87109235]\n",
      "El cliente  Escritura con probabilidad [0.20414899 0.79585101]\n",
      "El cliente  Escritura con probabilidad [0.00584961 0.99415039]\n",
      "El cliente  Escritura con probabilidad [0.19822769 0.80177231]\n",
      "El cliente  Escritura con probabilidad [0.08381154 0.91618846]\n",
      "El cliente  Escritura con probabilidad [4.38760805e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.2098091 0.7901909]\n",
      "El cliente  Escritura con probabilidad [0.03826989 0.96173011]\n",
      "El cliente  Escritura con probabilidad [0.11144946 0.88855054]\n",
      "El cliente  Escritura con probabilidad [8.83125317e-06 9.99991169e-01]\n",
      "El cliente  Escritura con probabilidad [0.20071497 0.79928503]\n",
      "El cliente  Escritura con probabilidad [0.09898059 0.90101941]\n",
      "El cliente  Escritura con probabilidad [0.01495615 0.98504385]\n",
      "El cliente  Escritura con probabilidad [0.00149548 0.99850452]\n",
      "El cliente  Escritura con probabilidad [0.20581204 0.79418796]\n",
      "El cliente  Escritura con probabilidad [0.2823187 0.7176813]\n",
      "El cliente  Escritura con probabilidad [2.62081402e-04 9.99737919e-01]\n",
      "El cliente  Escritura con probabilidad [6.32696225e-06 9.99993673e-01]\n",
      "El cliente  Escritura con probabilidad [0.14731396 0.85268604]\n",
      "El cliente  Escritura con probabilidad [0.20040378 0.79959622]\n",
      "El cliente  Escritura con probabilidad [1.11446971e-08 9.99999989e-01]\n",
      "El cliente  Escritura con probabilidad [0.00591388 0.99408612]\n",
      "El cliente  Escritura con probabilidad [0.07763369 0.92236631]\n",
      "El cliente  Escritura con probabilidad [0.21267583 0.78732417]\n",
      "El cliente  Escritura con probabilidad [1.48990067e-07 9.99999851e-01]\n",
      "El cliente  Escritura con probabilidad [0.23868278 0.76131722]\n",
      "El cliente  Escritura con probabilidad [0.09440805 0.90559195]\n",
      "El cliente  Escritura con probabilidad [6.73158287e-07 9.99999327e-01]\n",
      "El cliente  Escritura con probabilidad [1.96485143e-04 9.99803515e-01]\n",
      "El cliente  Escritura con probabilidad [0.04446562 0.95553438]\n",
      "El cliente  Escritura con probabilidad [0.04880162 0.95119838]\n",
      "El cliente  Escritura con probabilidad [0.25292711 0.74707289]\n",
      "El cliente  Escritura con probabilidad [5.86473387e-07 9.99999414e-01]\n",
      "El cliente  Escritura con probabilidad [0.24136344 0.75863656]\n",
      "El cliente  Escritura con probabilidad [0.21948537 0.78051463]\n",
      "El cliente  Escritura con probabilidad [0.29578115 0.70421885]\n",
      "El cliente  Escritura con probabilidad [0.02940686 0.97059314]\n",
      "El cliente  Escritura con probabilidad [0.06295244 0.93704756]\n",
      "El cliente  Escritura con probabilidad [0.19380607 0.80619393]\n",
      "El cliente  Escritura con probabilidad [7.63518970e-05 9.99923648e-01]\n",
      "El cliente  Escritura con probabilidad [0.05748949 0.94251051]\n",
      "El cliente  Escritura con probabilidad [0.00369611 0.99630389]\n",
      "El cliente  Escritura con probabilidad [0.02296189 0.97703811]\n",
      "El cliente  Escritura con probabilidad [0.12595736 0.87404264]\n",
      "El cliente  Escritura con probabilidad [0.28302462 0.71697538]\n",
      "El cliente  Escritura con probabilidad [2.49893004e-08 9.99999975e-01]\n",
      "El cliente  Escritura con probabilidad [2.34964763e-07 9.99999765e-01]\n",
      "El cliente  Escritura con probabilidad [0.07040636 0.92959364]\n",
      "El cliente  Escritura con probabilidad [5.42306156e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [0.08121109 0.91878891]\n",
      "El cliente  Escritura con probabilidad [0.00741678 0.99258322]\n",
      "El cliente  Escritura con probabilidad [6.64997568e-09 9.99999993e-01]\n",
      "El cliente  Escritura con probabilidad [0.13297967 0.86702033]\n",
      "El cliente  Escritura con probabilidad [0.01771285 0.98228715]\n",
      "El cliente  Escritura con probabilidad [0.14386903 0.85613097]\n",
      "El cliente  Escritura con probabilidad [0.06251355 0.93748645]\n",
      "El cliente  Escritura con probabilidad [0.06030193 0.93969807]\n",
      "El cliente  Escritura con probabilidad [0.2189088 0.7810912]\n",
      "El cliente  Escritura con probabilidad [0.0155153 0.9844847]\n",
      "El cliente  Escritura con probabilidad [0.00751028 0.99248972]\n",
      "El cliente  Escritura con probabilidad [0.01156207 0.98843793]\n",
      "El cliente  Escritura con probabilidad [0.04156358 0.95843642]\n",
      "El cliente  Escritura con probabilidad [8.04949483e-08 9.99999920e-01]\n",
      "El cliente  Escritura con probabilidad [0.14480777 0.85519223]\n",
      "El cliente  Escritura con probabilidad [0.24838252 0.75161748]\n",
      "El cliente  Escritura con probabilidad [0.27934326 0.72065674]\n",
      "El cliente  Escritura con probabilidad [0.09157609 0.90842391]\n",
      "El cliente  Escritura con probabilidad [0.27703784 0.72296216]\n",
      "El cliente  Escritura con probabilidad [0.08307262 0.91692738]\n",
      "El cliente  Escritura con probabilidad [0.0620168 0.9379832]\n",
      "El cliente  Escritura con probabilidad [0.2051741 0.7948259]\n",
      "El cliente  Escritura con probabilidad [0.02909564 0.97090436]\n",
      "El cliente  Escritura con probabilidad [0.02447929 0.97552071]\n",
      "El cliente  Escritura con probabilidad [0.18514748 0.81485252]\n",
      "El cliente  Escritura con probabilidad [0.08092243 0.91907757]\n",
      "El cliente  Escritura con probabilidad [0.16491824 0.83508176]\n",
      "El cliente  Escritura con probabilidad [0.22071779 0.77928221]\n",
      "El cliente  Escritura con probabilidad [2.54087343e-07 9.99999746e-01]\n",
      "El cliente  Escritura con probabilidad [0.26186283 0.73813717]\n",
      "El cliente  Escritura con probabilidad [0.22036881 0.77963119]\n",
      "El cliente  Escritura con probabilidad [2.65056248e-06 9.99997349e-01]\n",
      "El cliente  Escritura con probabilidad [0.1501266 0.8498734]\n",
      "El cliente  Escritura con probabilidad [0.16381753 0.83618247]\n",
      "El cliente  Escritura con probabilidad [0.10915047 0.89084953]\n",
      "El cliente  Escritura con probabilidad [0.17357203 0.82642797]\n",
      "El cliente  Escritura con probabilidad [0.15447165 0.84552835]\n",
      "El cliente  Escritura con probabilidad [0.41860116 0.58139884]\n",
      "El cliente  Escritura con probabilidad [0.0411141 0.9588859]\n",
      "El cliente  Escritura con probabilidad [0.1065873 0.8934127]\n",
      "El cliente  Escritura con probabilidad [0.07852495 0.92147505]\n",
      "El cliente  Escritura con probabilidad [1.42463706e-04 9.99857536e-01]\n",
      "El cliente  Escritura con probabilidad [0.17952199 0.82047801]\n",
      "El cliente  Escritura con probabilidad [0.353762 0.646238]\n",
      "El cliente  Escritura con probabilidad [0.01812016 0.98187984]\n",
      "El cliente  Escritura con probabilidad [0.11408624 0.88591376]\n",
      "El cliente  Escritura con probabilidad [0.21657988 0.78342012]\n",
      "El cliente  Escritura con probabilidad [0.01871883 0.98128117]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [4.73315184e-07 9.99999527e-01]\n",
      "El cliente  Escritura con probabilidad [0.19849293 0.80150707]\n",
      "El cliente  Escritura con probabilidad [0.15907537 0.84092463]\n",
      "El cliente  Escritura con probabilidad [0.19056501 0.80943499]\n",
      "El cliente  Escritura con probabilidad [0.13076166 0.86923834]\n",
      "El cliente  Escritura con probabilidad [0.08697173 0.91302827]\n",
      "El cliente  Escritura con probabilidad [0.20023378 0.79976622]\n",
      "El cliente  Escritura con probabilidad [0.2710733 0.7289267]\n",
      "El cliente  Escritura con probabilidad [9.52899483e-05 9.99904710e-01]\n",
      "El cliente  Escritura con probabilidad [0.1282863 0.8717137]\n",
      "El cliente  Escritura con probabilidad [0.13822912 0.86177088]\n",
      "El cliente  Escritura con probabilidad [0.0434821 0.9565179]\n",
      "El cliente  Escritura con probabilidad [2.50407954e-05 9.99974959e-01]\n",
      "El cliente  Escritura con probabilidad [1.5142243e-10 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.09398159 0.90601841]\n",
      "El cliente  Escritura con probabilidad [0.20965001 0.79034999]\n",
      "El cliente  Escritura con probabilidad [0.18353913 0.81646087]\n",
      "El cliente  Escritura con probabilidad [0.18924476 0.81075524]\n",
      "El cliente  Escritura con probabilidad [0.01565594 0.98434406]\n",
      "El cliente  Escritura con probabilidad [0.20294888 0.79705112]\n",
      "El cliente  Escritura con probabilidad [2.94481188e-04 9.99705519e-01]\n",
      "El cliente  Escritura con probabilidad [0.01595727 0.98404273]\n",
      "El cliente  Escritura con probabilidad [1.14047466e-04 9.99885953e-01]\n",
      "El cliente  Escritura con probabilidad [0.32264066 0.67735934]\n",
      "El cliente  Escritura con probabilidad [0.21976576 0.78023424]\n",
      "El cliente  Escritura con probabilidad [0.05471738 0.94528262]\n",
      "El cliente  Escritura con probabilidad [0.0154598 0.9845402]\n",
      "El cliente  Escritura con probabilidad [1.92475434e-05 9.99980752e-01]\n",
      "El cliente  Escritura con probabilidad [0.00143034 0.99856966]\n",
      "El cliente  Escritura con probabilidad [0.2396262 0.7603738]\n",
      "El cliente  Escritura con probabilidad [0.1020626 0.8979374]\n",
      "El cliente  Escritura con probabilidad [0.10929251 0.89070749]\n",
      "El cliente  Escritura con probabilidad [0.01366247 0.98633753]\n",
      "El cliente  Escritura con probabilidad [0.17955769 0.82044231]\n",
      "El cliente  Escritura con probabilidad [0.20610743 0.79389257]\n",
      "El cliente  Escritura con probabilidad [0.13622994 0.86377006]\n",
      "El cliente  Escritura con probabilidad [0.23191506 0.76808494]\n",
      "El cliente  Escritura con probabilidad [0.1289233 0.8710767]\n",
      "El cliente  Escritura con probabilidad [0.10088796 0.89911204]\n",
      "El cliente  Escritura con probabilidad [1.57485540e-06 9.99998425e-01]\n",
      "El cliente  Escritura con probabilidad [0.12007056 0.87992944]\n",
      "El cliente  Escritura con probabilidad [0.02223493 0.97776507]\n",
      "El cliente  Escritura con probabilidad [6.45261877e-04 9.99354738e-01]\n",
      "El cliente  Escritura con probabilidad [0.4244108 0.5755892]\n",
      "El cliente  Escritura con probabilidad [0.06455588 0.93544412]\n",
      "El cliente  Escritura con probabilidad [0.11204257 0.88795743]\n",
      "El cliente  Escritura con probabilidad [0.29138541 0.70861459]\n",
      "El cliente  Escritura con probabilidad [0.00614922 0.99385078]\n",
      "El cliente  Escritura con probabilidad [0.03524786 0.96475214]\n",
      "El cliente  Escritura con probabilidad [0.08663234 0.91336766]\n",
      "El cliente  Escritura con probabilidad [0.21559986 0.78440014]\n",
      "El cliente  Escritura con probabilidad [0.42366974 0.57633026]\n",
      "El cliente  Escritura con probabilidad [1.11022302e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.03627009 0.96372991]\n",
      "El cliente  Escritura con probabilidad [0.01844615 0.98155385]\n",
      "El cliente  Escritura con probabilidad [0.21063568 0.78936432]\n",
      "El cliente  Escritura con probabilidad [0.02363404 0.97636596]\n",
      "El cliente  Escritura con probabilidad [0.23029658 0.76970342]\n",
      "El cliente  Escritura con probabilidad [0.22029969 0.77970031]\n",
      "El cliente  Escritura con probabilidad [0.43455966 0.56544034]\n",
      "El cliente  Escritura con probabilidad [0.04373926 0.95626074]\n",
      "El cliente  Escritura con probabilidad [0.15768424 0.84231576]\n",
      "El cliente  Escritura con probabilidad [0.07336818 0.92663182]\n",
      "El cliente  Escritura con probabilidad [0.00197988 0.99802012]\n",
      "El cliente  Escritura con probabilidad [0.24250228 0.75749772]\n",
      "El cliente  Escritura con probabilidad [0.00878031 0.99121969]\n",
      "El cliente  Escritura con probabilidad [0.05634078 0.94365922]\n",
      "El cliente  Escritura con probabilidad [0.01217812 0.98782188]\n",
      "El cliente  Escritura con probabilidad [0.21641841 0.78358159]\n",
      "El cliente  Escritura con probabilidad [0.19563829 0.80436171]\n",
      "El cliente  Escritura con probabilidad [0.09333417 0.90666583]\n",
      "El cliente  Escritura con probabilidad [0.11095726 0.88904274]\n",
      "El cliente  Escritura con probabilidad [0.34955191 0.65044809]\n",
      "El cliente  Escritura con probabilidad [0.01547689 0.98452311]\n",
      "El cliente  Escritura con probabilidad [0.21794371 0.78205629]\n",
      "El cliente  Escritura con probabilidad [0.00437467 0.99562533]\n",
      "El cliente  Escritura con probabilidad [6.96274535e-05 9.99930373e-01]\n",
      "El cliente  Escritura con probabilidad [0.15038233 0.84961767]\n",
      "El cliente  Escritura con probabilidad [6.21724894e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.19756185 0.80243815]\n",
      "El cliente  Escritura con probabilidad [0.06361861 0.93638139]\n",
      "El cliente  Escritura con probabilidad [0.20329968 0.79670032]\n",
      "El cliente  Escritura con probabilidad [2.90580893e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.44757299 0.55242701]\n",
      "El cliente  Escritura con probabilidad [0.12222022 0.87777978]\n",
      "El cliente  Escritura con probabilidad [0.11473938 0.88526062]\n",
      "El cliente  Escritura con probabilidad [0.23297379 0.76702621]\n",
      "El cliente  Escritura con probabilidad [0.00573659 0.99426341]\n",
      "El cliente  Escritura con probabilidad [0.07823232 0.92176768]\n",
      "El cliente  Escritura con probabilidad [0.32822437 0.67177563]\n",
      "El cliente  Escritura con probabilidad [0.16728173 0.83271827]\n",
      "El cliente  Escritura con probabilidad [6.19791089e-04 9.99380209e-01]\n",
      "El cliente  Escritura con probabilidad [0.01663762 0.98336238]\n",
      "El cliente  Escritura con probabilidad [0.18374634 0.81625366]\n",
      "El cliente  Escritura con probabilidad [0.11508334 0.88491666]\n",
      "El cliente  Escritura con probabilidad [2.18559760e-05 9.99978144e-01]\n",
      "El cliente  Escritura con probabilidad [0.08219311 0.91780689]\n",
      "El cliente  Escritura con probabilidad [0.19107589 0.80892411]\n",
      "El cliente  Escritura con probabilidad [0.32583588 0.67416412]\n",
      "El cliente  Escritura con probabilidad [0.05860949 0.94139051]\n",
      "El cliente  Escritura con probabilidad [0.30165082 0.69834918]\n",
      "El cliente  Escritura con probabilidad [8.01204936e-09 9.99999992e-01]\n",
      "El cliente  Escritura con probabilidad [0.08982588 0.91017412]\n",
      "El cliente  Escritura con probabilidad [0.01436379 0.98563621]\n",
      "El cliente  Escritura con probabilidad [0.1046754 0.8953246]\n",
      "El cliente  Escritura con probabilidad [5.21965196e-04 9.99478035e-01]\n",
      "El cliente  Escritura con probabilidad [0.03024163 0.96975837]\n",
      "El cliente  Escritura con probabilidad [0.16686065 0.83313935]\n",
      "El cliente  Escritura con probabilidad [0.02804623 0.97195377]\n",
      "El cliente  Escritura con probabilidad [0.19192063 0.80807937]\n",
      "El cliente  Escritura con probabilidad [0.09445364 0.90554636]\n",
      "El cliente  Escritura con probabilidad [0.00444453 0.99555547]\n",
      "El cliente  Escritura con probabilidad [0.02826392 0.97173608]\n",
      "El cliente  Escritura con probabilidad [0.14047919 0.85952081]\n",
      "El cliente  Escritura con probabilidad [0.282708 0.717292]\n",
      "El cliente  Escritura con probabilidad [3.90946265e-04 9.99609054e-01]\n",
      "El cliente  Escritura con probabilidad [0.31386683 0.68613317]\n",
      "El cliente  Escritura con probabilidad [0.43301374 0.56698626]\n",
      "El cliente  Escritura con probabilidad [0.13605322 0.86394678]\n",
      "El cliente  Escritura con probabilidad [0.22523677 0.77476323]\n",
      "El cliente  Escritura con probabilidad [0.08278109 0.91721891]\n",
      "El cliente  Escritura con probabilidad [0.09507744 0.90492256]\n",
      "El cliente  Escritura con probabilidad [0.11117376 0.88882624]\n",
      "El cliente  Escritura con probabilidad [0.02105065 0.97894935]\n",
      "El cliente  Escritura con probabilidad [0.09381858 0.90618142]\n",
      "El cliente  Escritura con probabilidad [0.08423228 0.91576772]\n",
      "El cliente  Escritura con probabilidad [0.02681234 0.97318766]\n",
      "El cliente  Escritura con probabilidad [0.09158993 0.90841007]\n",
      "El cliente  Escritura con probabilidad [0.12377569 0.87622431]\n",
      "El cliente  Escritura con probabilidad [0.16125414 0.83874586]\n",
      "El cliente  Escritura con probabilidad [0.00781171 0.99218829]\n",
      "El cliente  Escritura con probabilidad [0.17420916 0.82579084]\n",
      "El cliente  Escritura con probabilidad [0.04111931 0.95888069]\n",
      "El cliente  Escritura con probabilidad [1.93748375e-07 9.99999806e-01]\n",
      "El cliente  Escritura con probabilidad [0.25580809 0.74419191]\n",
      "El cliente  Escritura con probabilidad [0.06339936 0.93660064]\n",
      "El cliente  Escritura con probabilidad [0.10618901 0.89381099]\n",
      "El cliente  Escritura con probabilidad [0.36095569 0.63904431]\n",
      "El cliente  Escritura con probabilidad [0.01901412 0.98098588]\n",
      "El cliente  Escritura con probabilidad [6.50341204e-04 9.99349659e-01]\n",
      "El cliente  Escritura con probabilidad [3.96216393e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [5.21158337e-04 9.99478842e-01]\n",
      "El cliente  Escritura con probabilidad [2.94481188e-04 9.99705519e-01]\n",
      "El cliente  Escritura con probabilidad [6.12156303e-09 9.99999994e-01]\n",
      "El cliente  Escritura con probabilidad [0.19511917 0.80488083]\n",
      "El cliente  Escritura con probabilidad [0.08177837 0.91822163]\n",
      "El cliente  Escritura con probabilidad [2.57260213e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.25363736 0.74636264]\n",
      "El cliente  Escritura con probabilidad [0.1684212 0.8315788]\n",
      "El cliente  Escritura con probabilidad [0.45955698 0.54044302]\n",
      "El cliente  Escritura con probabilidad [0.15493796 0.84506204]\n",
      "El cliente  Escritura con probabilidad [0.01582981 0.98417019]\n",
      "El cliente  Escritura con probabilidad [0.04258727 0.95741273]\n",
      "El cliente  Escritura con probabilidad [0.25227619 0.74772381]\n",
      "El cliente  Escritura con probabilidad [0.01519833 0.98480167]\n",
      "El cliente  Escritura con probabilidad [0.00617507 0.99382493]\n",
      "El cliente  Escritura con probabilidad [0.2267405 0.7732595]\n",
      "El cliente  Escritura con probabilidad [0.1537664 0.8462336]\n",
      "El cliente  Escritura con probabilidad [0.13816546 0.86183454]\n",
      "El cliente  Escritura con probabilidad [0.17858795 0.82141205]\n",
      "El cliente  Escritura con probabilidad [3.69987984e-07 9.99999630e-01]\n",
      "El cliente  Escritura con probabilidad [0.03227102 0.96772898]\n",
      "El cliente  Escritura con probabilidad [0.01324834 0.98675166]\n",
      "El cliente  Escritura con probabilidad [0.22735823 0.77264177]\n",
      "El cliente  Escritura con probabilidad [0.06502315 0.93497685]\n",
      "El cliente  Escritura con probabilidad [0.17978857 0.82021143]\n",
      "El cliente  Escritura con probabilidad [0.02443455 0.97556545]\n",
      "El cliente  Escritura con probabilidad [0.36347803 0.63652197]\n",
      "El cliente  Escritura con probabilidad [0.00213591 0.99786409]\n",
      "El cliente  Escritura con probabilidad [0.45870616 0.54129384]\n",
      "El cliente  Escritura con probabilidad [0.29027294 0.70972706]\n",
      "El cliente  Escritura con probabilidad [0.39169481 0.60830519]\n",
      "El cliente  Escritura con probabilidad [0.23719073 0.76280927]\n",
      "El cliente  Escritura con probabilidad [0.28315272 0.71684728]\n",
      "El cliente  Escritura con probabilidad [0.28335365 0.71664635]\n",
      "El cliente  Escritura con probabilidad [0.14603171 0.85396829]\n",
      "El cliente  Escritura con probabilidad [0.24471175 0.75528825]\n",
      "El cliente  Escritura con probabilidad [0.02716028 0.97283972]\n",
      "El cliente  Escritura con probabilidad [0.13727771 0.86272229]\n",
      "El cliente  Escritura con probabilidad [0.18217995 0.81782005]\n",
      "El cliente  Escritura con probabilidad [0.11949434 0.88050566]\n",
      "El cliente  Escritura con probabilidad [2.92601811e-05 9.99970740e-01]\n",
      "El cliente  Escritura con probabilidad [0.02112224 0.97887776]\n",
      "El cliente  Escritura con probabilidad [0.00688122 0.99311878]\n",
      "El cliente  Escritura con probabilidad [0.14569736 0.85430264]\n",
      "El cliente  Escritura con probabilidad [2.22005857e-05 9.99977799e-01]\n",
      "El cliente  Escritura con probabilidad [0.2501583 0.7498417]\n",
      "El cliente  Escritura con probabilidad [0.16416686 0.83583314]\n",
      "El cliente  Escritura con probabilidad [0.38143079 0.61856921]\n",
      "El cliente  Escritura con probabilidad [0.09259916 0.90740084]\n",
      "El cliente  Escritura con probabilidad [0.00417348 0.99582652]\n",
      "El cliente  Escritura con probabilidad [0.22771044 0.77228956]\n",
      "El cliente  Escritura con probabilidad [0.00269124 0.99730876]\n",
      "El cliente  Escritura con probabilidad [0.45015401 0.54984599]\n",
      "El cliente  Escritura con probabilidad [0.10375518 0.89624482]\n",
      "El cliente  Escritura con probabilidad [1.77507982e-05 9.99982249e-01]\n",
      "El cliente  Escritura con probabilidad [0.03926041 0.96073959]\n",
      "El cliente  Escritura con probabilidad [0.09326462 0.90673538]\n",
      "El cliente  Escritura con probabilidad [0.21475511 0.78524489]\n",
      "El cliente  Escritura con probabilidad [0.1928373 0.8071627]\n",
      "El cliente  Escritura con probabilidad [0.11324648 0.88675352]\n",
      "El cliente  Escritura con probabilidad [0.06703928 0.93296072]\n",
      "El cliente  Escritura con probabilidad [0.16056255 0.83943745]\n",
      "El cliente  Escritura con probabilidad [0.02653291 0.97346709]\n",
      "El cliente  Escritura con probabilidad [0.0973947 0.9026053]\n",
      "El cliente  Escritura con probabilidad [0.1152208 0.8847792]\n",
      "El cliente  Escritura con probabilidad [2.36683931e-06 9.99997633e-01]\n",
      "El cliente  Escritura con probabilidad [0.09743047 0.90256953]\n",
      "El cliente  Escritura con probabilidad [0.19569701 0.80430299]\n",
      "El cliente  Escritura con probabilidad [0.28462301 0.71537699]\n",
      "El cliente  Escritura con probabilidad [0.00243886 0.99756114]\n",
      "El cliente  Escritura con probabilidad [0.06342034 0.93657966]\n",
      "El cliente  Escritura con probabilidad [0.02485473 0.97514527]\n",
      "El cliente  Escritura con probabilidad [0.28348094 0.71651906]\n",
      "El cliente  Escritura con probabilidad [0.22491299 0.77508701]\n",
      "El cliente  Escritura con probabilidad [0.23298679 0.76701321]\n",
      "El cliente  Escritura con probabilidad [1.60470139e-05 9.99983953e-01]\n",
      "El cliente  Escritura con probabilidad [0.13782536 0.86217464]\n",
      "El cliente  Escritura con probabilidad [0.09748384 0.90251616]\n",
      "El cliente  Escritura con probabilidad [0.08947771 0.91052229]\n",
      "El cliente  Escritura con probabilidad [0.22522578 0.77477422]\n",
      "El cliente  Escritura con probabilidad [0.14725015 0.85274985]\n",
      "El cliente  Escritura con probabilidad [0.25259186 0.74740814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.26916153 0.73083847]\n",
      "El cliente  Escritura con probabilidad [0.23880343 0.76119657]\n",
      "El cliente  Escritura con probabilidad [0.0228091 0.9771909]\n",
      "El cliente  Escritura con probabilidad [0.01870415 0.98129585]\n",
      "El cliente  Escritura con probabilidad [0.08337105 0.91662895]\n",
      "El cliente  Escritura con probabilidad [0.06870079 0.93129921]\n",
      "El cliente  Escritura con probabilidad [0.08815 0.91185]\n",
      "El cliente  Escritura con probabilidad [0.09104333 0.90895667]\n",
      "El cliente  Escritura con probabilidad [1.70549201e-05 9.99982945e-01]\n",
      "El cliente  Escritura con probabilidad [0.04825824 0.95174176]\n",
      "El cliente  Escritura con probabilidad [0.01857054 0.98142946]\n",
      "El cliente  Escritura con probabilidad [0.02274809 0.97725191]\n",
      "El cliente  Escritura con probabilidad [0.20229342 0.79770658]\n",
      "El cliente  Escritura con probabilidad [0.17458789 0.82541211]\n",
      "El cliente  Escritura con probabilidad [0.07342695 0.92657305]\n",
      "El cliente  Escritura con probabilidad [0.22311948 0.77688052]\n",
      "El cliente  Escritura con probabilidad [0.05660661 0.94339339]\n",
      "El cliente  Escritura con probabilidad [0.0201139 0.9798861]\n",
      "El cliente  Escritura con probabilidad [5.58527981e-07 9.99999441e-01]\n",
      "El cliente  Escritura con probabilidad [0.28294578 0.71705422]\n",
      "El cliente  Escritura con probabilidad [0.07889958 0.92110042]\n",
      "El cliente  Escritura con probabilidad [1.05956012e-04 9.99894044e-01]\n",
      "El cliente  Escritura con probabilidad [0.10218754 0.89781246]\n",
      "El cliente  Escritura con probabilidad [0.12076983 0.87923017]\n",
      "El cliente  Escritura con probabilidad [0.14127045 0.85872955]\n",
      "El cliente  Escritura con probabilidad [5.81761528e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.27269233 0.72730767]\n",
      "El cliente  Escritura con probabilidad [0.03797136 0.96202864]\n",
      "El cliente  Escritura con probabilidad [0.13427157 0.86572843]\n",
      "El cliente  Escritura con probabilidad [0.1843799 0.8156201]\n",
      "El cliente  Escritura con probabilidad [0.19210457 0.80789543]\n",
      "El cliente  Escritura con probabilidad [0.01837862 0.98162138]\n",
      "El cliente  Escritura con probabilidad [0.01733267 0.98266733]\n",
      "El cliente  Escritura con probabilidad [0.04380147 0.95619853]\n",
      "El cliente  Escritura con probabilidad [0.13228356 0.86771644]\n",
      "El cliente  Escritura con probabilidad [0.01595478 0.98404522]\n",
      "El cliente  Escritura con probabilidad [0.20919706 0.79080294]\n",
      "El cliente  Escritura con probabilidad [0.22329744 0.77670256]\n",
      "El cliente  Escritura con probabilidad [2.17899408e-04 9.99782101e-01]\n",
      "El cliente  Escritura con probabilidad [0.21114441 0.78885559]\n",
      "El cliente  Escritura con probabilidad [0.1013947 0.8986053]\n",
      "El cliente  Escritura con probabilidad [0.01492196 0.98507804]\n",
      "El cliente  Escritura con probabilidad [0.06011669 0.93988331]\n",
      "El cliente  Escritura con probabilidad [0.25429319 0.74570681]\n",
      "El cliente  Escritura con probabilidad [0.43221203 0.56778797]\n",
      "El cliente  Escritura con probabilidad [0.0249917 0.9750083]\n",
      "El cliente  Escritura con probabilidad [0.27570186 0.72429814]\n",
      "El cliente  Escritura con probabilidad [0.02287902 0.97712098]\n",
      "El cliente  Escritura con probabilidad [0.29777141 0.70222859]\n",
      "El cliente  Escritura con probabilidad [0.21186712 0.78813288]\n",
      "El cliente  Escritura con probabilidad [0.12391477 0.87608523]\n",
      "El cliente  Escritura con probabilidad [2.39980525e-08 9.99999976e-01]\n",
      "El cliente  Escritura con probabilidad [0.20354196 0.79645804]\n",
      "El cliente  Escritura con probabilidad [0.17645573 0.82354427]\n",
      "El cliente  Escritura con probabilidad [0.16964326 0.83035674]\n",
      "El cliente  Escritura con probabilidad [0.03670774 0.96329226]\n",
      "El cliente  Escritura con probabilidad [0.09257661 0.90742339]\n",
      "El cliente  Escritura con probabilidad [0.13434309 0.86565691]\n",
      "El cliente  Escritura con probabilidad [0.2706011 0.7293989]\n",
      "El cliente  Escritura con probabilidad [0.33019604 0.66980396]\n",
      "El cliente  Escritura con probabilidad [8.56940821e-06 9.99991431e-01]\n",
      "El cliente  Escritura con probabilidad [0.04798332 0.95201668]\n",
      "El cliente  Escritura con probabilidad [0.12389709 0.87610291]\n",
      "El cliente  Escritura con probabilidad [0.20907629 0.79092371]\n",
      "El cliente  Escritura con probabilidad [0.0052591 0.9947409]\n",
      "El cliente  Escritura con probabilidad [0.16631047 0.83368953]\n",
      "El cliente  Escritura con probabilidad [0.15456623 0.84543377]\n",
      "El cliente  Escritura con probabilidad [0.03599221 0.96400779]\n",
      "El cliente  Escritura con probabilidad [3.99680289e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.30728891 0.69271109]\n",
      "El cliente  Escritura con probabilidad [0.0935182 0.9064818]\n",
      "El cliente  Escritura con probabilidad [0.27953734 0.72046266]\n",
      "El cliente  Escritura con probabilidad [1.00431971e-08 9.99999990e-01]\n",
      "El cliente  Escritura con probabilidad [0.04756713 0.95243287]\n",
      "El cliente  Escritura con probabilidad [4.46062665e-08 9.99999955e-01]\n",
      "El cliente  Escritura con probabilidad [0.04909877 0.95090123]\n",
      "El cliente  Escritura con probabilidad [0.22145143 0.77854857]\n",
      "El cliente  Escritura con probabilidad [0.10596487 0.89403513]\n",
      "El cliente  Escritura con probabilidad [0.43006256 0.56993744]\n",
      "El cliente  Escritura con probabilidad [4.02471249e-04 9.99597529e-01]\n",
      "El cliente  Escritura con probabilidad [0.0719822 0.9280178]\n",
      "El cliente  Escritura con probabilidad [0.05305636 0.94694364]\n",
      "El cliente  Escritura con probabilidad [0.03792283 0.96207717]\n",
      "El cliente  Escritura con probabilidad [1.16004983e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.1300699 0.8699301]\n",
      "El cliente  Escritura con probabilidad [0.25373243 0.74626757]\n",
      "El cliente  Escritura con probabilidad [0.2765643 0.7234357]\n",
      "El cliente  Escritura con probabilidad [0.1858942 0.8141058]\n",
      "El cliente  Escritura con probabilidad [0.0030351 0.9969649]\n",
      "El cliente  Escritura con probabilidad [0.27028919 0.72971081]\n",
      "El cliente  Escritura con probabilidad [0.2170943 0.7829057]\n",
      "El cliente  Escritura con probabilidad [0.07604484 0.92395516]\n",
      "El cliente  Escritura con probabilidad [0.23789465 0.76210535]\n",
      "El cliente  Escritura con probabilidad [0.08372405 0.91627595]\n",
      "El cliente  Escritura con probabilidad [3.66552765e-05 9.99963345e-01]\n",
      "El cliente  Escritura con probabilidad [0.11108556 0.88891444]\n",
      "El cliente  Escritura con probabilidad [0.17215482 0.82784518]\n",
      "El cliente  Escritura con probabilidad [0.02223778 0.97776222]\n",
      "El cliente  Escritura con probabilidad [0.18352573 0.81647427]\n",
      "El cliente  Escritura con probabilidad [2.05018225e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.42911657 0.57088343]\n",
      "El cliente  Escritura con probabilidad [0.03102213 0.96897787]\n",
      "El cliente  Escritura con probabilidad [0.02445726 0.97554274]\n",
      "El cliente  Escritura con probabilidad [2.05174506e-08 9.99999979e-01]\n",
      "El cliente  Escritura con probabilidad [0.25881754 0.74118246]\n",
      "El cliente  Escritura con probabilidad [0.23124208 0.76875792]\n",
      "El cliente  Escritura con probabilidad [0.21840983 0.78159017]\n",
      "El cliente  Escritura con probabilidad [0.22319515 0.77680485]\n",
      "El cliente  Escritura con probabilidad [0.19527625 0.80472375]\n",
      "El cliente  Escritura con probabilidad [0.11121321 0.88878679]\n",
      "El cliente  Escritura con probabilidad [0.19462941 0.80537059]\n",
      "El cliente  Escritura con probabilidad [0.00342839 0.99657161]\n",
      "El cliente  Escritura con probabilidad [0.03982718 0.96017282]\n",
      "El cliente  Escritura con probabilidad [0.21977932 0.78022068]\n",
      "El cliente  Escritura con probabilidad [0.12918779 0.87081221]\n",
      "El cliente  Escritura con probabilidad [0.14696186 0.85303814]\n",
      "El cliente  Escritura con probabilidad [0.17735799 0.82264201]\n",
      "El cliente  Escritura con probabilidad [0.22600015 0.77399985]\n",
      "El cliente  Escritura con probabilidad [0.20242426 0.79757574]\n",
      "El cliente  Escritura con probabilidad [1.96761000e-06 9.99998032e-01]\n",
      "El cliente  Escritura con probabilidad [0.17322241 0.82677759]\n",
      "El cliente  Escritura con probabilidad [0.02490082 0.97509918]\n",
      "El cliente  Escritura con probabilidad [0.08013079 0.91986921]\n",
      "El cliente  Escritura con probabilidad [0.17589265 0.82410735]\n",
      "El cliente  Escritura con probabilidad [1.07172604e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [2.28300729e-07 9.99999772e-01]\n",
      "El cliente  Escritura con probabilidad [6.84838321e-04 9.99315162e-01]\n",
      "El cliente  Escritura con probabilidad [0.21977018 0.78022982]\n",
      "El cliente  Escritura con probabilidad [5.80153226e-07 9.99999420e-01]\n",
      "El cliente  Escritura con probabilidad [0.18493706 0.81506294]\n",
      "El cliente  Escritura con probabilidad [4.03637568e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [3.46294802e-05 9.99965371e-01]\n",
      "El cliente  Escritura con probabilidad [0.2324279 0.7675721]\n",
      "El cliente  Escritura con probabilidad [0.07081435 0.92918565]\n",
      "El cliente  Escritura con probabilidad [0.17597423 0.82402577]\n",
      "El cliente  Escritura con probabilidad [1.73819604e-04 9.99826180e-01]\n",
      "El cliente  Escritura con probabilidad [0.23815305 0.76184695]\n",
      "El cliente  Escritura con probabilidad [0.01530943 0.98469057]\n",
      "El cliente  Escritura con probabilidad [0.06884956 0.93115044]\n",
      "El cliente  Escritura con probabilidad [0.08849601 0.91150399]\n",
      "El cliente  Escritura con probabilidad [0.07316524 0.92683476]\n",
      "El cliente  Escritura con probabilidad [0.42239423 0.57760577]\n",
      "El cliente  Escritura con probabilidad [7.31614624e-05 9.99926839e-01]\n",
      "El cliente  Escritura con probabilidad [0.24133404 0.75866596]\n",
      "El cliente  Escritura con probabilidad [0.28118579 0.71881421]\n",
      "El cliente  Escritura con probabilidad [0.0913101 0.9086899]\n",
      "El cliente  Escritura con probabilidad [0.07572523 0.92427477]\n",
      "El cliente  Escritura con probabilidad [0.08263919 0.91736081]\n",
      "El cliente  Escritura con probabilidad [0.02650354 0.97349646]\n",
      "El cliente  Escritura con probabilidad [9.06331003e-07 9.99999094e-01]\n",
      "El cliente  Escritura con probabilidad [0.0253041 0.9746959]\n",
      "El cliente  Escritura con probabilidad [0.00134342 0.99865658]\n",
      "El cliente  Escritura con probabilidad [1.88305111e-05 9.99981169e-01]\n",
      "El cliente  Escritura con probabilidad [0.19410833 0.80589167]\n",
      "El cliente  Escritura con probabilidad [0.06355747 0.93644253]\n",
      "El cliente  Escritura con probabilidad [1.1504353e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.04010998 0.95989002]\n",
      "El cliente  Escritura con probabilidad [0.05749304 0.94250696]\n",
      "El cliente  Escritura con probabilidad [0.03503495 0.96496505]\n",
      "El cliente  Escritura con probabilidad [0.25639573 0.74360427]\n",
      "El cliente  Escritura con probabilidad [0.11507038 0.88492962]\n",
      "El cliente  Escritura con probabilidad [0.16353001 0.83646999]\n",
      "El cliente  Escritura con probabilidad [0.29461529 0.70538471]\n",
      "El cliente  Escritura con probabilidad [2.02071937e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.05302141 0.94697859]\n",
      "El cliente  Escritura con probabilidad [0.09862384 0.90137616]\n",
      "El cliente  Escritura con probabilidad [0.07262568 0.92737432]\n",
      "El cliente  Escritura con probabilidad [0.0144505 0.9855495]\n",
      "El cliente  Escritura con probabilidad [0.21025953 0.78974047]\n",
      "El cliente  Escritura con probabilidad [1.76424789e-08 9.99999982e-01]\n",
      "El cliente  Escritura con probabilidad [2.41797443e-06 9.99997582e-01]\n",
      "El cliente  Escritura con probabilidad [0.12215769 0.87784231]\n",
      "El cliente  Escritura con probabilidad [0.14709271 0.85290729]\n",
      "El cliente  Escritura con probabilidad [0.02381252 0.97618748]\n",
      "El cliente  Escritura con probabilidad [0.00819767 0.99180233]\n",
      "El cliente  Escritura con probabilidad [0.12844846 0.87155154]\n",
      "El cliente  Desiste con probabilidad [0.61248067 0.38751933]\n",
      "El cliente  Escritura con probabilidad [0.11795595 0.88204405]\n",
      "El cliente  Escritura con probabilidad [0.13271413 0.86728587]\n",
      "El cliente  Escritura con probabilidad [0.0575242 0.9424758]\n",
      "El cliente  Escritura con probabilidad [0.15007868 0.84992132]\n",
      "El cliente  Escritura con probabilidad [0.21494145 0.78505855]\n",
      "El cliente  Escritura con probabilidad [0.25921791 0.74078209]\n",
      "El cliente  Escritura con probabilidad [0.06714789 0.93285211]\n",
      "El cliente  Escritura con probabilidad [2.31959145e-04 9.99768041e-01]\n",
      "El cliente  Escritura con probabilidad [0.12122052 0.87877948]\n",
      "El cliente  Escritura con probabilidad [0.10739046 0.89260954]\n",
      "El cliente  Escritura con probabilidad [0.12602469 0.87397531]\n",
      "El cliente  Escritura con probabilidad [0.17616188 0.82383812]\n",
      "El cliente  Escritura con probabilidad [0.0703785 0.9296215]\n",
      "El cliente  Escritura con probabilidad [0.0749393 0.9250607]\n",
      "El cliente  Escritura con probabilidad [2.12907801e-05 9.99978709e-01]\n",
      "El cliente  Escritura con probabilidad [4.21884749e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17486744 0.82513256]\n",
      "El cliente  Escritura con probabilidad [3.59687264e-05 9.99964031e-01]\n",
      "El cliente  Escritura con probabilidad [0.08441198 0.91558802]\n",
      "El cliente  Escritura con probabilidad [0.15034836 0.84965164]\n",
      "El cliente  Escritura con probabilidad [0.1729996 0.8270004]\n",
      "El cliente  Escritura con probabilidad [0.15449743 0.84550257]\n",
      "El cliente  Escritura con probabilidad [0.09140089 0.90859911]\n",
      "El cliente  Escritura con probabilidad [5.64104636e-04 9.99435895e-01]\n",
      "El cliente  Escritura con probabilidad [0.07131241 0.92868759]\n",
      "El cliente  Escritura con probabilidad [0.13984611 0.86015389]\n",
      "El cliente  Escritura con probabilidad [2.13618245e-04 9.99786382e-01]\n",
      "El cliente  Escritura con probabilidad [0.03167544 0.96832456]\n",
      "El cliente  Escritura con probabilidad [0.28311689 0.71688311]\n",
      "El cliente  Escritura con probabilidad [5.55404833e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.01801037 0.98198963]\n",
      "El cliente  Escritura con probabilidad [0.1522509 0.8477491]\n",
      "El cliente  Escritura con probabilidad [0.09023624 0.90976376]\n",
      "El cliente  Escritura con probabilidad [0.09655346 0.90344654]\n",
      "El cliente  Escritura con probabilidad [0.1017097 0.8982903]\n",
      "El cliente  Escritura con probabilidad [0.01041103 0.98958897]\n",
      "El cliente  Escritura con probabilidad [0.17018895 0.82981105]\n",
      "El cliente  Escritura con probabilidad [0.21314151 0.78685849]\n",
      "El cliente  Escritura con probabilidad [0.08226502 0.91773498]\n",
      "El cliente  Escritura con probabilidad [0.42886279 0.57113721]\n",
      "El cliente  Escritura con probabilidad [0.1061797 0.8938203]\n",
      "El cliente  Escritura con probabilidad [0.06076128 0.93923872]\n",
      "El cliente  Escritura con probabilidad [0.00442409 0.99557591]\n",
      "El cliente  Escritura con probabilidad [0.1754448 0.8245552]\n",
      "El cliente  Escritura con probabilidad [1.13490612e-05 9.99988651e-01]\n",
      "El cliente  Escritura con probabilidad [0.00730856 0.99269144]\n",
      "El cliente  Escritura con probabilidad [0.07959253 0.92040747]\n",
      "El cliente  Escritura con probabilidad [0.18382531 0.81617469]\n",
      "El cliente  Escritura con probabilidad [0.09675156 0.90324844]\n",
      "El cliente  Escritura con probabilidad [0.10312453 0.89687547]\n",
      "El cliente  Escritura con probabilidad [0.2312614 0.7687386]\n",
      "El cliente  Escritura con probabilidad [0.16730349 0.83269651]\n",
      "El cliente  Escritura con probabilidad [0.02757727 0.97242273]\n",
      "El cliente  Escritura con probabilidad [0.1234921 0.8765079]\n",
      "El cliente  Escritura con probabilidad [0.22301928 0.77698072]\n",
      "El cliente  Escritura con probabilidad [0.04039367 0.95960633]\n",
      "El cliente  Escritura con probabilidad [8.12787826e-09 9.99999992e-01]\n",
      "El cliente  Escritura con probabilidad [0.01150294 0.98849706]\n",
      "El cliente  Escritura con probabilidad [7.85603756e-05 9.99921440e-01]\n",
      "El cliente  Escritura con probabilidad [0.05404893 0.94595107]\n",
      "El cliente  Escritura con probabilidad [0.24021146 0.75978854]\n",
      "El cliente  Escritura con probabilidad [0.04750894 0.95249106]\n",
      "El cliente  Escritura con probabilidad [0.08068808 0.91931192]\n",
      "El cliente  Escritura con probabilidad [0.21648176 0.78351824]\n",
      "El cliente  Escritura con probabilidad [0.08342962 0.91657038]\n",
      "El cliente  Escritura con probabilidad [0.07386544 0.92613456]\n",
      "El cliente  Escritura con probabilidad [0.0492049 0.9507951]\n",
      "El cliente  Escritura con probabilidad [0.09546763 0.90453237]\n",
      "El cliente  Escritura con probabilidad [0.15205715 0.84794285]\n",
      "El cliente  Escritura con probabilidad [0.06410988 0.93589012]\n",
      "El cliente  Escritura con probabilidad [0.20597519 0.79402481]\n",
      "El cliente  Escritura con probabilidad [0.19505161 0.80494839]\n",
      "El cliente  Escritura con probabilidad [0.03279338 0.96720662]\n",
      "El cliente  Escritura con probabilidad [0.27484803 0.72515197]\n",
      "El cliente  Escritura con probabilidad [0.29091944 0.70908056]\n",
      "El cliente  Escritura con probabilidad [0.24987579 0.75012421]\n",
      "El cliente  Escritura con probabilidad [0.03082612 0.96917388]\n",
      "El cliente  Escritura con probabilidad [0.03914363 0.96085637]\n",
      "El cliente  Escritura con probabilidad [0.13457362 0.86542638]\n",
      "El cliente  Escritura con probabilidad [0.04749441 0.95250559]\n",
      "El cliente  Escritura con probabilidad [0.25164804 0.74835196]\n",
      "El cliente  Escritura con probabilidad [0.21804043 0.78195957]\n",
      "El cliente  Escritura con probabilidad [0.26296443 0.73703557]\n",
      "El cliente  Escritura con probabilidad [0.14873513 0.85126487]\n",
      "El cliente  Escritura con probabilidad [5.18864951e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.22411796 0.77588204]\n",
      "El cliente  Escritura con probabilidad [0.13112215 0.86887785]\n",
      "El cliente  Escritura con probabilidad [0.15199012 0.84800988]\n",
      "El cliente  Escritura con probabilidad [0.02553568 0.97446432]\n",
      "El cliente  Escritura con probabilidad [0.06172612 0.93827388]\n",
      "El cliente  Escritura con probabilidad [3.94507333e-07 9.99999605e-01]\n",
      "El cliente  Escritura con probabilidad [2.04656762e-06 9.99997953e-01]\n",
      "El cliente  Escritura con probabilidad [0.28133404 0.71866596]\n",
      "El cliente  Escritura con probabilidad [1.35447209e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.11322567 0.88677433]\n",
      "El cliente  Escritura con probabilidad [0.02090094 0.97909906]\n",
      "El cliente  Escritura con probabilidad [0.04985076 0.95014924]\n",
      "El cliente  Escritura con probabilidad [0.21613856 0.78386144]\n",
      "El cliente  Escritura con probabilidad [0.1078004 0.8921996]\n",
      "El cliente  Escritura con probabilidad [3.25049187e-05 9.99967495e-01]\n",
      "El cliente  Escritura con probabilidad [0.22849861 0.77150139]\n",
      "El cliente  Escritura con probabilidad [0.23008531 0.76991469]\n",
      "El cliente  Escritura con probabilidad [0.09002341 0.90997659]\n",
      "El cliente  Escritura con probabilidad [0.12955735 0.87044265]\n",
      "El cliente  Escritura con probabilidad [0.016976 0.983024]\n",
      "El cliente  Escritura con probabilidad [0.22831242 0.77168758]\n",
      "El cliente  Escritura con probabilidad [2.66453526e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [9.32647937e-04 9.99067352e-01]\n",
      "El cliente  Escritura con probabilidad [0.16147026 0.83852974]\n",
      "El cliente  Escritura con probabilidad [0.08331326 0.91668674]\n",
      "El cliente  Escritura con probabilidad [0.04176004 0.95823996]\n",
      "El cliente  Escritura con probabilidad [0.32870089 0.67129911]\n",
      "El cliente  Escritura con probabilidad [0.07207564 0.92792436]\n",
      "El cliente  Escritura con probabilidad [0.02090902 0.97909098]\n",
      "El cliente  Escritura con probabilidad [0.1415544 0.8584456]\n",
      "El cliente  Escritura con probabilidad [0.07757902 0.92242098]\n",
      "El cliente  Escritura con probabilidad [0.23781758 0.76218242]\n",
      "El cliente  Escritura con probabilidad [0.23971801 0.76028199]\n",
      "El cliente  Escritura con probabilidad [0.26162821 0.73837179]\n",
      "El cliente  Escritura con probabilidad [3.30162714e-06 9.99996698e-01]\n",
      "El cliente  Escritura con probabilidad [0.0397477 0.9602523]\n",
      "El cliente  Escritura con probabilidad [1.83217714e-06 9.99998168e-01]\n",
      "El cliente  Escritura con probabilidad [0.29454119 0.70545881]\n",
      "El cliente  Escritura con probabilidad [0.43066296 0.56933704]\n",
      "El cliente  Escritura con probabilidad [0.23207809 0.76792191]\n",
      "El cliente  Escritura con probabilidad [0.19017345 0.80982655]\n",
      "El cliente  Escritura con probabilidad [0.1229225 0.8770775]\n",
      "El cliente  Escritura con probabilidad [7.95020853e-07 9.99999205e-01]\n",
      "El cliente  Escritura con probabilidad [4.82676121e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17869027 0.82130973]\n",
      "El cliente  Escritura con probabilidad [9.82903127e-04 9.99017097e-01]\n",
      "El cliente  Escritura con probabilidad [0.27465369 0.72534631]\n",
      "El cliente  Escritura con probabilidad [0.08869951 0.91130049]\n",
      "El cliente  Escritura con probabilidad [0.22163872 0.77836128]\n",
      "El cliente  Escritura con probabilidad [0.14220015 0.85779985]\n",
      "El cliente  Escritura con probabilidad [0.20946473 0.79053527]\n",
      "El cliente  Escritura con probabilidad [0.07015781 0.92984219]\n",
      "El cliente  Escritura con probabilidad [1.37948777e-07 9.99999862e-01]\n",
      "El cliente  Escritura con probabilidad [0.04397661 0.95602339]\n",
      "El cliente  Escritura con probabilidad [1.58769455e-04 9.99841231e-01]\n",
      "El cliente  Escritura con probabilidad [0.07132863 0.92867137]\n",
      "El cliente  Escritura con probabilidad [0.11786684 0.88213316]\n",
      "El cliente  Escritura con probabilidad [0.43099463 0.56900537]\n",
      "El cliente  Escritura con probabilidad [3.52150508e-08 9.99999965e-01]\n",
      "El cliente  Escritura con probabilidad [0.18130399 0.81869601]\n",
      "El cliente  Escritura con probabilidad [0.18742635 0.81257365]\n",
      "El cliente  Escritura con probabilidad [0.26230081 0.73769919]\n",
      "El cliente  Escritura con probabilidad [0.05609599 0.94390401]\n",
      "El cliente  Escritura con probabilidad [1.18185461e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.14254193 0.85745807]\n",
      "El cliente  Escritura con probabilidad [0.00506099 0.99493901]\n",
      "El cliente  Escritura con probabilidad [0.04812493 0.95187507]\n",
      "El cliente  Escritura con probabilidad [0.26734862 0.73265138]\n",
      "El cliente  Escritura con probabilidad [0.09500991 0.90499009]\n",
      "El cliente  Escritura con probabilidad [0.01026117 0.98973883]\n",
      "El cliente  Escritura con probabilidad [0.00211371 0.99788629]\n",
      "El cliente  Escritura con probabilidad [0.21155941 0.78844059]\n",
      "El cliente  Escritura con probabilidad [0.30985586 0.69014414]\n",
      "El cliente  Escritura con probabilidad [0.09924686 0.90075314]\n",
      "El cliente  Escritura con probabilidad [0.19266127 0.80733873]\n",
      "El cliente  Escritura con probabilidad [0.23251046 0.76748954]\n",
      "El cliente  Escritura con probabilidad [0.11492968 0.88507032]\n",
      "El cliente  Escritura con probabilidad [0.01479688 0.98520312]\n",
      "El cliente  Escritura con probabilidad [0.0029534 0.9970466]\n",
      "El cliente  Escritura con probabilidad [0.24891624 0.75108376]\n",
      "El cliente  Escritura con probabilidad [0.06309627 0.93690373]\n",
      "El cliente  Escritura con probabilidad [0.02624898 0.97375102]\n",
      "El cliente  Escritura con probabilidad [0.08150299 0.91849701]\n",
      "El cliente  Escritura con probabilidad [0.30731981 0.69268019]\n",
      "El cliente  Escritura con probabilidad [5.49582516e-07 9.99999450e-01]\n",
      "El cliente  Escritura con probabilidad [0.24572226 0.75427774]\n",
      "El cliente  Escritura con probabilidad [0.123971 0.876029]\n",
      "El cliente  Escritura con probabilidad [3.53374204e-05 9.99964663e-01]\n",
      "El cliente  Escritura con probabilidad [0.01564721 0.98435279]\n",
      "El cliente  Escritura con probabilidad [0.17749649 0.82250351]\n",
      "El cliente  Escritura con probabilidad [0.20025409 0.79974591]\n",
      "El cliente  Escritura con probabilidad [0.30712477 0.69287523]\n",
      "El cliente  Escritura con probabilidad [0.31272431 0.68727569]\n",
      "El cliente  Escritura con probabilidad [3.28948913e-07 9.99999671e-01]\n",
      "El cliente  Escritura con probabilidad [6.32791597e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.01712551 0.98287449]\n",
      "El cliente  Escritura con probabilidad [0.04684287 0.95315713]\n",
      "El cliente  Escritura con probabilidad [0.04268432 0.95731568]\n",
      "El cliente  Escritura con probabilidad [0.26521637 0.73478363]\n",
      "El cliente  Escritura con probabilidad [6.66133815e-16 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.18666239 0.81333761]\n",
      "El cliente  Escritura con probabilidad [0.00532904 0.99467096]\n",
      "El cliente  Escritura con probabilidad [3.14469517e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.21727807 0.78272193]\n",
      "El cliente  Escritura con probabilidad [0.15997287 0.84002713]\n",
      "El cliente  Escritura con probabilidad [0.12663862 0.87336138]\n",
      "El cliente  Escritura con probabilidad [0.09827749 0.90172251]\n",
      "El cliente  Escritura con probabilidad [0.2860635 0.7139365]\n",
      "El cliente  Escritura con probabilidad [1.48410974e-06 9.99998516e-01]\n",
      "El cliente  Escritura con probabilidad [0.13762736 0.86237264]\n",
      "El cliente  Escritura con probabilidad [0.45308017 0.54691983]\n",
      "El cliente  Escritura con probabilidad [0.26533662 0.73466338]\n",
      "El cliente  Escritura con probabilidad [0.16477664 0.83522336]\n",
      "El cliente  Escritura con probabilidad [0.0947612 0.9052388]\n",
      "El cliente  Escritura con probabilidad [0.2564032 0.7435968]\n",
      "El cliente  Escritura con probabilidad [0.20176312 0.79823688]\n",
      "El cliente  Escritura con probabilidad [0.18877613 0.81122387]\n",
      "El cliente  Escritura con probabilidad [0.02419406 0.97580594]\n",
      "El cliente  Escritura con probabilidad [0.2972091 0.7027909]\n",
      "El cliente  Escritura con probabilidad [0.19128461 0.80871539]\n",
      "El cliente  Escritura con probabilidad [0.21238449 0.78761551]\n",
      "El cliente  Escritura con probabilidad [0.21912714 0.78087286]\n",
      "El cliente  Escritura con probabilidad [0.18374676 0.81625324]\n",
      "El cliente  Escritura con probabilidad [0.10830497 0.89169503]\n",
      "El cliente  Escritura con probabilidad [0.20741375 0.79258625]\n",
      "El cliente  Escritura con probabilidad [0.02384158 0.97615842]\n",
      "El cliente  Escritura con probabilidad [0.26044763 0.73955237]\n",
      "El cliente  Escritura con probabilidad [0.26484887 0.73515113]\n",
      "El cliente  Escritura con probabilidad [0.29027294 0.70972706]\n",
      "El cliente  Escritura con probabilidad [0.2074839 0.7925161]\n",
      "El cliente  Escritura con probabilidad [0.32000471 0.67999529]\n",
      "El cliente  Escritura con probabilidad [0.01651814 0.98348186]\n",
      "El cliente  Escritura con probabilidad [0.03945032 0.96054968]\n",
      "El cliente  Escritura con probabilidad [0.02712482 0.97287518]\n",
      "El cliente  Escritura con probabilidad [0.18004825 0.81995175]\n",
      "El cliente  Escritura con probabilidad [0.16545001 0.83454999]\n",
      "El cliente  Escritura con probabilidad [0.20815667 0.79184333]\n",
      "El cliente  Escritura con probabilidad [0.21222886 0.78777114]\n",
      "El cliente  Escritura con probabilidad [0.42617857 0.57382143]\n",
      "El cliente  Escritura con probabilidad [0.18827463 0.81172537]\n",
      "El cliente  Escritura con probabilidad [0.23468577 0.76531423]\n",
      "El cliente  Escritura con probabilidad [0.21602439 0.78397561]\n",
      "El cliente  Escritura con probabilidad [0.1217006 0.8782994]\n",
      "El cliente  Escritura con probabilidad [0.12958039 0.87041961]\n",
      "El cliente  Escritura con probabilidad [0.21385164 0.78614836]\n",
      "El cliente  Escritura con probabilidad [0.18962825 0.81037175]\n",
      "El cliente  Escritura con probabilidad [0.23996749 0.76003251]\n",
      "El cliente  Escritura con probabilidad [1.15530608e-06 9.99998845e-01]\n",
      "El cliente  Escritura con probabilidad [0.12512795 0.87487205]\n",
      "El cliente  Escritura con probabilidad [0.09558358 0.90441642]\n",
      "El cliente  Escritura con probabilidad [0.42462332 0.57537668]\n",
      "El cliente  Escritura con probabilidad [0.05044821 0.94955179]\n",
      "El cliente  Escritura con probabilidad [0.17128953 0.82871047]\n",
      "El cliente  Escritura con probabilidad [0.04686484 0.95313516]\n",
      "El cliente  Escritura con probabilidad [0.04815721 0.95184279]\n",
      "El cliente  Escritura con probabilidad [0.17442269 0.82557731]\n",
      "El cliente  Escritura con probabilidad [0.03809895 0.96190105]\n",
      "El cliente  Escritura con probabilidad [0.11939212 0.88060788]\n",
      "El cliente  Escritura con probabilidad [9.70334924e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.2303733 0.7696267]\n",
      "El cliente  Escritura con probabilidad [0.02448154 0.97551846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.13738878 0.86261122]\n",
      "El cliente  Escritura con probabilidad [0.29680129 0.70319871]\n",
      "El cliente  Escritura con probabilidad [0.18213268 0.81786732]\n",
      "El cliente  Escritura con probabilidad [0.03456005 0.96543995]\n",
      "El cliente  Escritura con probabilidad [0.22038241 0.77961759]\n",
      "El cliente  Escritura con probabilidad [0.07435639 0.92564361]\n",
      "El cliente  Escritura con probabilidad [0.2000245 0.7999755]\n",
      "El cliente  Escritura con probabilidad [0.2323399 0.7676601]\n",
      "El cliente  Escritura con probabilidad [0.3890381 0.6109619]\n",
      "El cliente  Escritura con probabilidad [0.13956003 0.86043997]\n",
      "El cliente  Escritura con probabilidad [0.03146924 0.96853076]\n",
      "El cliente  Escritura con probabilidad [0.17125985 0.82874015]\n",
      "El cliente  Escritura con probabilidad [0.21487411 0.78512589]\n",
      "El cliente  Escritura con probabilidad [0.10263155 0.89736845]\n",
      "El cliente  Escritura con probabilidad [0.25983133 0.74016867]\n",
      "El cliente  Escritura con probabilidad [0.27126596 0.72873404]\n",
      "El cliente  Escritura con probabilidad [0.02126455 0.97873545]\n",
      "El cliente  Escritura con probabilidad [0.23433544 0.76566456]\n",
      "El cliente  Escritura con probabilidad [0.19030838 0.80969162]\n",
      "El cliente  Escritura con probabilidad [0.05235483 0.94764517]\n",
      "El cliente  Escritura con probabilidad [0.1826984 0.8173016]\n",
      "El cliente  Escritura con probabilidad [0.1763493 0.8236507]\n",
      "El cliente  Escritura con probabilidad [0.18145129 0.81854871]\n",
      "El cliente  Escritura con probabilidad [0.00345663 0.99654337]\n",
      "El cliente  Escritura con probabilidad [1.85702125e-05 9.99981430e-01]\n",
      "El cliente  Escritura con probabilidad [0.28018989 0.71981011]\n",
      "El cliente  Escritura con probabilidad [0.34699647 0.65300353]\n",
      "El cliente  Escritura con probabilidad [0.09378532 0.90621468]\n",
      "El cliente  Escritura con probabilidad [0.04767448 0.95232552]\n",
      "El cliente  Escritura con probabilidad [0.00882853 0.99117147]\n",
      "El cliente  Escritura con probabilidad [0.25247406 0.74752594]\n",
      "El cliente  Escritura con probabilidad [0.1115339 0.8884661]\n",
      "El cliente  Escritura con probabilidad [0.26605701 0.73394299]\n",
      "El cliente  Escritura con probabilidad [0.1879421 0.8120579]\n",
      "El cliente  Escritura con probabilidad [1.79973835e-04 9.99820026e-01]\n",
      "El cliente  Escritura con probabilidad [0.02424123 0.97575877]\n",
      "El cliente  Escritura con probabilidad [0.12605061 0.87394939]\n",
      "El cliente  Escritura con probabilidad [0.28457015 0.71542985]\n",
      "El cliente  Escritura con probabilidad [0.23294302 0.76705698]\n",
      "El cliente  Escritura con probabilidad [0.18163275 0.81836725]\n",
      "El cliente  Escritura con probabilidad [0.04802977 0.95197023]\n",
      "El cliente  Escritura con probabilidad [0.08366955 0.91633045]\n",
      "El cliente  Escritura con probabilidad [0.13072233 0.86927767]\n",
      "El cliente  Escritura con probabilidad [0.09197306 0.90802694]\n",
      "El cliente  Escritura con probabilidad [0.15520435 0.84479565]\n",
      "El cliente  Escritura con probabilidad [0.10882296 0.89117704]\n",
      "El cliente  Escritura con probabilidad [1.17619005e-06 9.99998824e-01]\n",
      "El cliente  Escritura con probabilidad [0.00984389 0.99015611]\n",
      "El cliente  Escritura con probabilidad [0.20221141 0.79778859]\n",
      "El cliente  Escritura con probabilidad [0.16987426 0.83012574]\n",
      "El cliente  Escritura con probabilidad [0.02907698 0.97092302]\n",
      "El cliente  Escritura con probabilidad [0.09197288 0.90802712]\n",
      "El cliente  Escritura con probabilidad [3.61425923e-05 9.99963857e-01]\n",
      "El cliente  Escritura con probabilidad [0.11843969 0.88156031]\n",
      "El cliente  Escritura con probabilidad [0.03658318 0.96341682]\n",
      "El cliente  Escritura con probabilidad [0.30345808 0.69654192]\n",
      "El cliente  Escritura con probabilidad [0.2193141 0.7806859]\n",
      "El cliente  Escritura con probabilidad [1.69986411e-07 9.99999830e-01]\n",
      "El cliente  Escritura con probabilidad [0.0967911 0.9032089]\n",
      "El cliente  Escritura con probabilidad [0.22916281 0.77083719]\n",
      "El cliente  Escritura con probabilidad [0.07946357 0.92053643]\n",
      "El cliente  Escritura con probabilidad [0.0446049 0.9553951]\n",
      "El cliente  Escritura con probabilidad [1.39638877e-07 9.99999860e-01]\n",
      "El cliente  Escritura con probabilidad [0.18799061 0.81200939]\n",
      "El cliente  Escritura con probabilidad [0.20416578 0.79583422]\n",
      "El cliente  Escritura con probabilidad [0.18589136 0.81410864]\n",
      "El cliente  Escritura con probabilidad [4.88306418e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [0.02834085 0.97165915]\n",
      "El cliente  Escritura con probabilidad [0.10660325 0.89339675]\n",
      "El cliente  Escritura con probabilidad [0.03000224 0.96999776]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.01791014 0.98208986]\n",
      "El cliente  Escritura con probabilidad [0.26759906 0.73240094]\n",
      "El cliente  Escritura con probabilidad [0.15165837 0.84834163]\n",
      "El cliente  Escritura con probabilidad [0.01062177 0.98937823]\n",
      "El cliente  Escritura con probabilidad [0.00717647 0.99282353]\n",
      "El cliente  Escritura con probabilidad [0.10716771 0.89283229]\n",
      "El cliente  Escritura con probabilidad [0.08582837 0.91417163]\n",
      "El cliente  Escritura con probabilidad [0.15320228 0.84679772]\n",
      "El cliente  Escritura con probabilidad [0.02024004 0.97975996]\n",
      "El cliente  Escritura con probabilidad [0.14482982 0.85517018]\n",
      "El cliente  Escritura con probabilidad [0.21774707 0.78225293]\n",
      "El cliente  Escritura con probabilidad [0.25035109 0.74964891]\n",
      "El cliente  Escritura con probabilidad [0.14648501 0.85351499]\n",
      "El cliente  Escritura con probabilidad [0.11268442 0.88731558]\n",
      "El cliente  Escritura con probabilidad [2.38607762e-05 9.99976139e-01]\n",
      "El cliente  Escritura con probabilidad [4.49706024e-07 9.99999550e-01]\n",
      "El cliente  Escritura con probabilidad [0.29070615 0.70929385]\n",
      "El cliente  Escritura con probabilidad [0.27379745 0.72620255]\n",
      "El cliente  Escritura con probabilidad [0.27997108 0.72002892]\n",
      "El cliente  Escritura con probabilidad [0.06428602 0.93571398]\n",
      "El cliente  Escritura con probabilidad [0.04755439 0.95244561]\n",
      "El cliente  Escritura con probabilidad [0.17591091 0.82408909]\n",
      "El cliente  Escritura con probabilidad [0.0080575 0.9919425]\n",
      "El cliente  Escritura con probabilidad [0.04638962 0.95361038]\n",
      "El cliente  Escritura con probabilidad [0.06976521 0.93023479]\n",
      "El cliente  Escritura con probabilidad [0.04383332 0.95616668]\n",
      "El cliente  Escritura con probabilidad [0.08816908 0.91183092]\n",
      "El cliente  Escritura con probabilidad [0.03067822 0.96932178]\n",
      "El cliente  Escritura con probabilidad [0.05094842 0.94905158]\n",
      "El cliente  Escritura con probabilidad [0.16559015 0.83440985]\n",
      "El cliente  Escritura con probabilidad [0.16786381 0.83213619]\n",
      "El cliente  Escritura con probabilidad [0.05057841 0.94942159]\n",
      "El cliente  Escritura con probabilidad [9.73814951e-09 9.99999990e-01]\n",
      "El cliente  Escritura con probabilidad [0.15080995 0.84919005]\n",
      "El cliente  Escritura con probabilidad [0.05873508 0.94126492]\n",
      "El cliente  Escritura con probabilidad [0.01888151 0.98111849]\n",
      "El cliente  Escritura con probabilidad [0.09200754 0.90799246]\n",
      "El cliente  Escritura con probabilidad [0.07808013 0.92191987]\n",
      "El cliente  Escritura con probabilidad [0.15679336 0.84320664]\n",
      "El cliente  Escritura con probabilidad [0.28081884 0.71918116]\n",
      "El cliente  Escritura con probabilidad [0.08002191 0.91997809]\n",
      "El cliente  Escritura con probabilidad [0.21835224 0.78164776]\n",
      "El cliente  Escritura con probabilidad [0.0847376 0.9152624]\n",
      "El cliente  Escritura con probabilidad [0.10059209 0.89940791]\n",
      "El cliente  Escritura con probabilidad [0.002531 0.997469]\n",
      "El cliente  Escritura con probabilidad [0.09258983 0.90741017]\n",
      "El cliente  Escritura con probabilidad [0.07756421 0.92243579]\n",
      "El cliente  Escritura con probabilidad [0.08351945 0.91648055]\n",
      "El cliente  Escritura con probabilidad [0.20967322 0.79032678]\n",
      "El cliente  Escritura con probabilidad [4.08075994e-05 9.99959192e-01]\n",
      "El cliente  Escritura con probabilidad [0.02467997 0.97532003]\n",
      "El cliente  Escritura con probabilidad [0.01712393 0.98287607]\n",
      "El cliente  Escritura con probabilidad [1.59463871e-04 9.99840536e-01]\n",
      "El cliente  Escritura con probabilidad [4.57165500e-04 9.99542835e-01]\n",
      "El cliente  Escritura con probabilidad [0.00173208 0.99826792]\n",
      "El cliente  Escritura con probabilidad [0.41116623 0.58883377]\n",
      "El cliente  Escritura con probabilidad [0.05206237 0.94793763]\n",
      "El cliente  Escritura con probabilidad [0.16010459 0.83989541]\n",
      "El cliente  Escritura con probabilidad [0.16897074 0.83102926]\n",
      "El cliente  Escritura con probabilidad [0.1297675 0.8702325]\n",
      "El cliente  Escritura con probabilidad [0.1965557 0.8034443]\n",
      "El cliente  Escritura con probabilidad [0.0068959 0.9931041]\n",
      "El cliente  Escritura con probabilidad [0.12723941 0.87276059]\n",
      "El cliente  Escritura con probabilidad [0.28937124 0.71062876]\n",
      "El cliente  Escritura con probabilidad [0.11914897 0.88085103]\n",
      "El cliente  Escritura con probabilidad [0.22597699 0.77402301]\n",
      "El cliente  Escritura con probabilidad [0.26588062 0.73411938]\n",
      "El cliente  Escritura con probabilidad [0.13072116 0.86927884]\n",
      "El cliente  Escritura con probabilidad [0.03400969 0.96599031]\n",
      "El cliente  Escritura con probabilidad [2.98091312e-07 9.99999702e-01]\n",
      "El cliente  Escritura con probabilidad [0.10778563 0.89221437]\n",
      "El cliente  Escritura con probabilidad [0.07092726 0.92907274]\n",
      "El cliente  Escritura con probabilidad [0.19039155 0.80960845]\n",
      "El cliente  Escritura con probabilidad [0.09887107 0.90112893]\n",
      "El cliente  Escritura con probabilidad [0.02282202 0.97717798]\n",
      "El cliente  Escritura con probabilidad [0.16491293 0.83508707]\n",
      "El cliente  Escritura con probabilidad [0.22121495 0.77878505]\n",
      "El cliente  Escritura con probabilidad [4.80800227e-06 9.99995192e-01]\n",
      "El cliente  Escritura con probabilidad [0.25759915 0.74240085]\n",
      "El cliente  Escritura con probabilidad [0.06975139 0.93024861]\n",
      "El cliente  Escritura con probabilidad [0.23273059 0.76726941]\n",
      "El cliente  Escritura con probabilidad [0.06691719 0.93308281]\n",
      "El cliente  Desiste con probabilidad [0.50686817 0.49313183]\n",
      "El cliente  Escritura con probabilidad [8.89814428e-06 9.99991102e-01]\n",
      "El cliente  Escritura con probabilidad [0.23991425 0.76008575]\n",
      "El cliente  Escritura con probabilidad [0.11477341 0.88522659]\n",
      "El cliente  Escritura con probabilidad [0.0514733 0.9485267]\n",
      "El cliente  Escritura con probabilidad [0.24078128 0.75921872]\n",
      "El cliente  Escritura con probabilidad [0.0372782 0.9627218]\n",
      "El cliente  Escritura con probabilidad [0.0777587 0.9222413]\n",
      "El cliente  Escritura con probabilidad [0.02171545 0.97828455]\n",
      "El cliente  Escritura con probabilidad [9.90753510e-08 9.99999901e-01]\n",
      "El cliente  Escritura con probabilidad [0.00371354 0.99628646]\n",
      "El cliente  Escritura con probabilidad [0.24265162 0.75734838]\n",
      "El cliente  Escritura con probabilidad [0.43542585 0.56457415]\n",
      "El cliente  Escritura con probabilidad [0.05885676 0.94114324]\n",
      "El cliente  Escritura con probabilidad [0.1754967 0.8245033]\n",
      "El cliente  Escritura con probabilidad [3.26001515e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.00331484 0.99668516]\n",
      "El cliente  Escritura con probabilidad [0.11371371 0.88628629]\n",
      "El cliente  Escritura con probabilidad [3.51309877e-05 9.99964869e-01]\n",
      "El cliente  Escritura con probabilidad [0.01677699 0.98322301]\n",
      "El cliente  Escritura con probabilidad [0.00539019 0.99460981]\n",
      "El cliente  Escritura con probabilidad [0.14850559 0.85149441]\n",
      "El cliente  Escritura con probabilidad [0.22092269 0.77907731]\n",
      "El cliente  Escritura con probabilidad [0.4201835 0.5798165]\n",
      "El cliente  Escritura con probabilidad [4.6629367e-15 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.11248539 0.88751461]\n",
      "El cliente  Escritura con probabilidad [0.21480861 0.78519139]\n",
      "El cliente  Escritura con probabilidad [0.06610048 0.93389952]\n",
      "El cliente  Escritura con probabilidad [0.21912146 0.78087854]\n",
      "El cliente  Escritura con probabilidad [0.07922928 0.92077072]\n",
      "El cliente  Escritura con probabilidad [0.10250889 0.89749111]\n",
      "El cliente  Escritura con probabilidad [0.23555926 0.76444074]\n",
      "El cliente  Escritura con probabilidad [0.0960149 0.9039851]\n",
      "El cliente  Escritura con probabilidad [0.0589731 0.9410269]\n",
      "El cliente  Escritura con probabilidad [0.1926112 0.8073888]\n",
      "El cliente  Escritura con probabilidad [1.20550739e-05 9.99987945e-01]\n",
      "El cliente  Escritura con probabilidad [0.06172279 0.93827721]\n",
      "El cliente  Escritura con probabilidad [0.24667619 0.75332381]\n",
      "El cliente  Escritura con probabilidad [0.28137975 0.71862025]\n",
      "El cliente  Escritura con probabilidad [0.0561336 0.9438664]\n",
      "El cliente  Escritura con probabilidad [0.06389582 0.93610418]\n",
      "El cliente  Escritura con probabilidad [0.1791343 0.8208657]\n",
      "El cliente  Escritura con probabilidad [0.20400884 0.79599116]\n",
      "El cliente  Escritura con probabilidad [0.01650072 0.98349928]\n",
      "El cliente  Escritura con probabilidad [0.2184766 0.7815234]\n",
      "El cliente  Escritura con probabilidad [0.11685857 0.88314143]\n",
      "El cliente  Escritura con probabilidad [0.19769065 0.80230935]\n",
      "El cliente  Escritura con probabilidad [0.12866622 0.87133378]\n",
      "El cliente  Escritura con probabilidad [0.01997892 0.98002108]\n",
      "El cliente  Escritura con probabilidad [0.11165574 0.88834426]\n",
      "El cliente  Escritura con probabilidad [0.01625679 0.98374321]\n",
      "El cliente  Escritura con probabilidad [0.28435674 0.71564326]\n",
      "El cliente  Escritura con probabilidad [0.02491196 0.97508804]\n",
      "El cliente  Escritura con probabilidad [0.08592683 0.91407317]\n",
      "El cliente  Escritura con probabilidad [0.02095233 0.97904767]\n",
      "El cliente  Escritura con probabilidad [0.07125874 0.92874126]\n",
      "El cliente  Escritura con probabilidad [0.0010551 0.9989449]\n",
      "El cliente  Escritura con probabilidad [0.11233075 0.88766925]\n",
      "El cliente  Escritura con probabilidad [0.13639566 0.86360434]\n",
      "El cliente  Escritura con probabilidad [0.2934834 0.7065166]\n",
      "El cliente  Escritura con probabilidad [0.15731676 0.84268324]\n",
      "El cliente  Escritura con probabilidad [0.12455722 0.87544278]\n",
      "El cliente  Escritura con probabilidad [0.26938542 0.73061458]\n",
      "El cliente  Escritura con probabilidad [0.17660248 0.82339752]\n",
      "El cliente  Escritura con probabilidad [0.06546879 0.93453121]\n",
      "El cliente  Escritura con probabilidad [1.40346606e-04 9.99859653e-01]\n",
      "El cliente  Escritura con probabilidad [0.13551954 0.86448046]\n",
      "El cliente  Escritura con probabilidad [9.14937924e-07 9.99999085e-01]\n",
      "El cliente  Escritura con probabilidad [0.12821702 0.87178298]\n",
      "El cliente  Escritura con probabilidad [0.18543074 0.81456926]\n",
      "El cliente  Escritura con probabilidad [0.39084163 0.60915837]\n",
      "El cliente  Escritura con probabilidad [0.21715403 0.78284597]\n",
      "El cliente  Escritura con probabilidad [0.07979213 0.92020787]\n",
      "El cliente  Escritura con probabilidad [0.28393148 0.71606852]\n",
      "El cliente  Escritura con probabilidad [0.17237957 0.82762043]\n",
      "El cliente  Escritura con probabilidad [0.02055951 0.97944049]\n",
      "El cliente  Escritura con probabilidad [0.11614721 0.88385279]\n",
      "El cliente  Escritura con probabilidad [0.00387567 0.99612433]\n",
      "El cliente  Escritura con probabilidad [0.17388932 0.82611068]\n",
      "El cliente  Escritura con probabilidad [0.01519185 0.98480815]\n",
      "El cliente  Escritura con probabilidad [7.37589767e-09 9.99999993e-01]\n",
      "El cliente  Escritura con probabilidad [0.00976459 0.99023541]\n",
      "El cliente  Escritura con probabilidad [0.03832228 0.96167772]\n",
      "El cliente  Escritura con probabilidad [0.03124258 0.96875742]\n",
      "El cliente  Escritura con probabilidad [0.07455208 0.92544792]\n",
      "El cliente  Escritura con probabilidad [0.01838568 0.98161432]\n",
      "El cliente  Escritura con probabilidad [0.03583652 0.96416348]\n",
      "El cliente  Escritura con probabilidad [0.25495546 0.74504454]\n",
      "El cliente  Escritura con probabilidad [0.28016323 0.71983677]\n",
      "El cliente  Escritura con probabilidad [8.83022966e-07 9.99999117e-01]\n",
      "El cliente  Escritura con probabilidad [0.13533883 0.86466117]\n",
      "El cliente  Escritura con probabilidad [0.00243653 0.99756347]\n",
      "El cliente  Escritura con probabilidad [0.18952334 0.81047666]\n",
      "El cliente  Escritura con probabilidad [0.1906421 0.8093579]\n",
      "El cliente  Escritura con probabilidad [0.01009967 0.98990033]\n",
      "El cliente  Escritura con probabilidad [2.72980039e-06 9.99997270e-01]\n",
      "El cliente  Escritura con probabilidad [0.11128102 0.88871898]\n",
      "El cliente  Escritura con probabilidad [0.0663548 0.9336452]\n",
      "El cliente  Escritura con probabilidad [4.78728168e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.15306308 0.84693692]\n",
      "El cliente  Escritura con probabilidad [0.18867315 0.81132685]\n",
      "El cliente  Escritura con probabilidad [4.25570327e-07 9.99999574e-01]\n",
      "El cliente  Escritura con probabilidad [0.26928198 0.73071802]\n",
      "El cliente  Escritura con probabilidad [0.23516081 0.76483919]\n",
      "El cliente  Escritura con probabilidad [3.77933914e-08 9.99999962e-01]\n",
      "El cliente  Escritura con probabilidad [0.03715827 0.96284173]\n",
      "El cliente  Escritura con probabilidad [0.08902096 0.91097904]\n",
      "El cliente  Escritura con probabilidad [0.18410013 0.81589987]\n",
      "El cliente  Escritura con probabilidad [0.22919478 0.77080522]\n",
      "El cliente  Escritura con probabilidad [0.12885385 0.87114615]\n",
      "El cliente  Escritura con probabilidad [0.17808817 0.82191183]\n",
      "El cliente  Escritura con probabilidad [0.00765977 0.99234023]\n",
      "El cliente  Escritura con probabilidad [2.21363211e-05 9.99977864e-01]\n",
      "El cliente  Escritura con probabilidad [0.00856648 0.99143352]\n",
      "El cliente  Escritura con probabilidad [0.01808492 0.98191508]\n",
      "El cliente  Escritura con probabilidad [0.09459936 0.90540064]\n",
      "El cliente  Escritura con probabilidad [0.07389454 0.92610546]\n",
      "El cliente  Escritura con probabilidad [0.06165454 0.93834546]\n",
      "El cliente  Escritura con probabilidad [0.03017862 0.96982138]\n",
      "El cliente  Escritura con probabilidad [0.10427262 0.89572738]\n",
      "El cliente  Escritura con probabilidad [0.22042373 0.77957627]\n",
      "El cliente  Escritura con probabilidad [0.20393104 0.79606896]\n",
      "El cliente  Escritura con probabilidad [1.53409739e-07 9.99999847e-01]\n",
      "El cliente  Escritura con probabilidad [0.17091823 0.82908177]\n",
      "El cliente  Escritura con probabilidad [0.12639052 0.87360948]\n",
      "El cliente  Escritura con probabilidad [0.29021946 0.70978054]\n",
      "El cliente  Escritura con probabilidad [0.17594197 0.82405803]\n",
      "El cliente  Escritura con probabilidad [0.08206198 0.91793802]\n",
      "El cliente  Escritura con probabilidad [0.18779746 0.81220254]\n",
      "El cliente  Escritura con probabilidad [3.78273545e-08 9.99999962e-01]\n",
      "El cliente  Escritura con probabilidad [1.55875313e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.14278108 0.85721892]\n",
      "El cliente  Escritura con probabilidad [0.01589445 0.98410555]\n",
      "El cliente  Escritura con probabilidad [0.00398368 0.99601632]\n",
      "El cliente  Escritura con probabilidad [0.05180562 0.94819438]\n",
      "El cliente  Escritura con probabilidad [0.1165145 0.8834855]\n",
      "El cliente  Escritura con probabilidad [0.2172208 0.7827792]\n",
      "El cliente  Escritura con probabilidad [0.04946472 0.95053528]\n",
      "El cliente  Escritura con probabilidad [0.03148345 0.96851655]\n",
      "El cliente  Escritura con probabilidad [0.05474134 0.94525866]\n",
      "El cliente  Escritura con probabilidad [0.25627261 0.74372739]\n",
      "El cliente  Escritura con probabilidad [0.20962368 0.79037632]\n",
      "El cliente  Escritura con probabilidad [0.19391582 0.80608418]\n",
      "El cliente  Escritura con probabilidad [0.01170081 0.98829919]\n",
      "El cliente  Escritura con probabilidad [0.06091252 0.93908748]\n",
      "El cliente  Escritura con probabilidad [0.08806484 0.91193516]\n",
      "El cliente  Escritura con probabilidad [0.0971932 0.9028068]\n",
      "El cliente  Escritura con probabilidad [0.10149172 0.89850828]\n",
      "El cliente  Escritura con probabilidad [0.21367875 0.78632125]\n",
      "El cliente  Escritura con probabilidad [0.28323183 0.71676817]\n",
      "El cliente  Escritura con probabilidad [0.05583065 0.94416935]\n",
      "El cliente  Escritura con probabilidad [0.03303993 0.96696007]\n",
      "El cliente  Escritura con probabilidad [0.02198902 0.97801098]\n",
      "El cliente  Escritura con probabilidad [0.24133404 0.75866596]\n",
      "El cliente  Escritura con probabilidad [0.2577036 0.7422964]\n",
      "El cliente  Escritura con probabilidad [0.36213148 0.63786852]\n",
      "El cliente  Escritura con probabilidad [0.1444135 0.8555865]\n",
      "El cliente  Escritura con probabilidad [0.12958724 0.87041276]\n",
      "El cliente  Escritura con probabilidad [0.18935243 0.81064757]\n",
      "El cliente  Escritura con probabilidad [0.02081791 0.97918209]\n",
      "El cliente  Escritura con probabilidad [0.11966408 0.88033592]\n",
      "El cliente  Escritura con probabilidad [0.09318613 0.90681387]\n",
      "El cliente  Escritura con probabilidad [0.13715177 0.86284823]\n",
      "El cliente  Escritura con probabilidad [0.27480614 0.72519386]\n",
      "El cliente  Escritura con probabilidad [0.26859396 0.73140604]\n",
      "El cliente  Escritura con probabilidad [0.01861826 0.98138174]\n",
      "El cliente  Escritura con probabilidad [0.01990158 0.98009842]\n",
      "El cliente  Escritura con probabilidad [0.09675056 0.90324944]\n",
      "El cliente  Escritura con probabilidad [0.07140577 0.92859423]\n",
      "El cliente  Escritura con probabilidad [0.28255025 0.71744975]\n",
      "El cliente  Escritura con probabilidad [0.16144822 0.83855178]\n",
      "El cliente  Escritura con probabilidad [0.03598258 0.96401742]\n",
      "El cliente  Escritura con probabilidad [0.20505754 0.79494246]\n",
      "El cliente  Escritura con probabilidad [0.01600619 0.98399381]\n",
      "El cliente  Escritura con probabilidad [0.14924766 0.85075234]\n",
      "El cliente  Escritura con probabilidad [0.11428844 0.88571156]\n",
      "El cliente  Escritura con probabilidad [0.26204651 0.73795349]\n",
      "El cliente  Escritura con probabilidad [0.25926399 0.74073601]\n",
      "El cliente  Escritura con probabilidad [0.02038062 0.97961938]\n",
      "El cliente  Escritura con probabilidad [0.22100954 0.77899046]\n",
      "El cliente  Escritura con probabilidad [0.08247263 0.91752737]\n",
      "El cliente  Escritura con probabilidad [0.04641182 0.95358818]\n",
      "El cliente  Escritura con probabilidad [0.10607782 0.89392218]\n",
      "El cliente  Escritura con probabilidad [0.25672638 0.74327362]\n",
      "El cliente  Escritura con probabilidad [0.2865897 0.7134103]\n",
      "El cliente  Escritura con probabilidad [0.06167035 0.93832965]\n",
      "El cliente  Escritura con probabilidad [0.16984005 0.83015995]\n",
      "El cliente  Escritura con probabilidad [0.04651476 0.95348524]\n",
      "El cliente  Escritura con probabilidad [0.13810706 0.86189294]\n",
      "El cliente  Escritura con probabilidad [0.20433802 0.79566198]\n",
      "El cliente  Escritura con probabilidad [0.19091536 0.80908464]\n",
      "El cliente  Escritura con probabilidad [6.9839325e-08 9.9999993e-01]\n",
      "El cliente  Escritura con probabilidad [0.44459222 0.55540778]\n",
      "El cliente  Escritura con probabilidad [0.25499553 0.74500447]\n",
      "El cliente  Escritura con probabilidad [0.27362795 0.72637205]\n",
      "El cliente  Escritura con probabilidad [0.06214476 0.93785524]\n",
      "El cliente  Escritura con probabilidad [0.24907708 0.75092292]\n",
      "El cliente  Escritura con probabilidad [0.17747223 0.82252777]\n",
      "El cliente  Escritura con probabilidad [0.21442026 0.78557974]\n",
      "El cliente  Escritura con probabilidad [0.01272506 0.98727494]\n",
      "El cliente  Escritura con probabilidad [0.27600916 0.72399084]\n",
      "El cliente  Escritura con probabilidad [0.12954418 0.87045582]\n",
      "El cliente  Escritura con probabilidad [0.21888922 0.78111078]\n",
      "El cliente  Escritura con probabilidad [0.19285892 0.80714108]\n",
      "El cliente  Escritura con probabilidad [0.19079752 0.80920248]\n",
      "El cliente  Escritura con probabilidad [1.33522968e-08 9.99999987e-01]\n",
      "El cliente  Escritura con probabilidad [0.20013857 0.79986143]\n",
      "El cliente  Escritura con probabilidad [0.17018899 0.82981101]\n",
      "El cliente  Escritura con probabilidad [2.86355954e-07 9.99999714e-01]\n",
      "El cliente  Escritura con probabilidad [1.83798700e-04 9.99816201e-01]\n",
      "El cliente  Escritura con probabilidad [0.02984575 0.97015425]\n",
      "El cliente  Escritura con probabilidad [0.19312531 0.80687469]\n",
      "El cliente  Escritura con probabilidad [6.92642493e-07 9.99999307e-01]\n",
      "El cliente  Escritura con probabilidad [0.07390097 0.92609903]\n",
      "El cliente  Escritura con probabilidad [0.23382962 0.76617038]\n",
      "El cliente  Escritura con probabilidad [0.18883077 0.81116923]\n",
      "El cliente  Escritura con probabilidad [1.21519710e-05 9.99987848e-01]\n",
      "El cliente  Escritura con probabilidad [0.05599795 0.94400205]\n",
      "El cliente  Escritura con probabilidad [0.11160704 0.88839296]\n",
      "El cliente  Escritura con probabilidad [0.00304065 0.99695935]\n",
      "El cliente  Escritura con probabilidad [0.04031979 0.95968021]\n",
      "El cliente  Escritura con probabilidad [9.47534495e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.01589445 0.98410555]\n",
      "El cliente  Escritura con probabilidad [0.07344075 0.92655925]\n",
      "El cliente  Escritura con probabilidad [0.13762652 0.86237348]\n",
      "El cliente  Escritura con probabilidad [0.00737781 0.99262219]\n",
      "El cliente  Escritura con probabilidad [0.04996337 0.95003663]\n",
      "El cliente  Escritura con probabilidad [0.10601423 0.89398577]\n",
      "El cliente  Escritura con probabilidad [0.06623019 0.93376981]\n",
      "El cliente  Escritura con probabilidad [0.10570296 0.89429704]\n",
      "El cliente  Escritura con probabilidad [0.00461567 0.99538433]\n",
      "El cliente  Escritura con probabilidad [0.00212233 0.99787767]\n",
      "El cliente  Escritura con probabilidad [0.21935456 0.78064544]\n",
      "El cliente  Escritura con probabilidad [0.21944337 0.78055663]\n",
      "El cliente  Escritura con probabilidad [0.1699959 0.8300041]\n",
      "El cliente  Escritura con probabilidad [0.03687912 0.96312088]\n",
      "El cliente  Escritura con probabilidad [0.46180097 0.53819903]\n",
      "El cliente  Escritura con probabilidad [0.0430094 0.9569906]\n",
      "El cliente  Escritura con probabilidad [0.29926668 0.70073332]\n",
      "El cliente  Escritura con probabilidad [0.26434562 0.73565438]\n",
      "El cliente  Escritura con probabilidad [0.19778797 0.80221203]\n",
      "El cliente  Escritura con probabilidad [0.32593007 0.67406993]\n",
      "El cliente  Escritura con probabilidad [0.28363153 0.71636847]\n",
      "El cliente  Escritura con probabilidad [0.13869274 0.86130726]\n",
      "El cliente  Escritura con probabilidad [5.36191464e-06 9.99994638e-01]\n",
      "El cliente  Escritura con probabilidad [0.0916526 0.9083474]\n",
      "El cliente  Escritura con probabilidad [0.1976467 0.8023533]\n",
      "El cliente  Escritura con probabilidad [0.1941675 0.8058325]\n",
      "El cliente  Escritura con probabilidad [0.21058199 0.78941801]\n",
      "El cliente  Escritura con probabilidad [0.26926528 0.73073472]\n",
      "El cliente  Escritura con probabilidad [0.05371525 0.94628475]\n",
      "El cliente  Escritura con probabilidad [2.11852758e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [2.97794167e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.21480861 0.78519139]\n",
      "El cliente  Escritura con probabilidad [0.22021604 0.77978396]\n",
      "El cliente  Escritura con probabilidad [1.83199566e-06 9.99998168e-01]\n",
      "El cliente  Escritura con probabilidad [3.09124802e-05 9.99969088e-01]\n",
      "El cliente  Escritura con probabilidad [0.16885155 0.83114845]\n",
      "El cliente  Escritura con probabilidad [0.02706402 0.97293598]\n",
      "El cliente  Escritura con probabilidad [0.16222427 0.83777573]\n",
      "El cliente  Escritura con probabilidad [0.2018172 0.7981828]\n",
      "El cliente  Escritura con probabilidad [3.55964154e-07 9.99999644e-01]\n",
      "El cliente  Escritura con probabilidad [0.18554957 0.81445043]\n",
      "El cliente  Escritura con probabilidad [2.14490878e-06 9.99997855e-01]\n",
      "El cliente  Escritura con probabilidad [0.12565864 0.87434136]\n",
      "El cliente  Escritura con probabilidad [1.43326811e-06 9.99998567e-01]\n",
      "El cliente  Escritura con probabilidad [0.17403565 0.82596435]\n",
      "El cliente  Escritura con probabilidad [0.03153749 0.96846251]\n",
      "El cliente  Escritura con probabilidad [0.2218546 0.7781454]\n",
      "El cliente  Escritura con probabilidad [0.10592524 0.89407476]\n",
      "El cliente  Escritura con probabilidad [0.08566386 0.91433614]\n",
      "El cliente  Escritura con probabilidad [0.22800027 0.77199973]\n",
      "El cliente  Escritura con probabilidad [0.35388766 0.64611234]\n",
      "El cliente  Escritura con probabilidad [0.08397838 0.91602162]\n",
      "El cliente  Escritura con probabilidad [0.0140869 0.9859131]\n",
      "El cliente  Escritura con probabilidad [4.61760078e-06 9.99995382e-01]\n",
      "El cliente  Escritura con probabilidad [0.23987688 0.76012312]\n",
      "El cliente  Escritura con probabilidad [0.12354893 0.87645107]\n",
      "El cliente  Escritura con probabilidad [0.07023281 0.92976719]\n",
      "El cliente  Escritura con probabilidad [0.0064663 0.9935337]\n",
      "El cliente  Escritura con probabilidad [0.18626785 0.81373215]\n",
      "El cliente  Escritura con probabilidad [0.22101397 0.77898603]\n",
      "El cliente  Escritura con probabilidad [0.07358495 0.92641505]\n",
      "El cliente  Escritura con probabilidad [0.00546539 0.99453461]\n",
      "El cliente  Escritura con probabilidad [0.3331615 0.6668385]\n",
      "El cliente  Escritura con probabilidad [0.10499512 0.89500488]\n",
      "El cliente  Escritura con probabilidad [0.14851323 0.85148677]\n",
      "El cliente  Escritura con probabilidad [0.17268809 0.82731191]\n",
      "El cliente  Escritura con probabilidad [0.1688628 0.8311372]\n",
      "El cliente  Escritura con probabilidad [0.03681153 0.96318847]\n",
      "El cliente  Escritura con probabilidad [0.03896329 0.96103671]\n",
      "El cliente  Escritura con probabilidad [0.22920723 0.77079277]\n",
      "El cliente  Escritura con probabilidad [0.10808447 0.89191553]\n",
      "El cliente  Escritura con probabilidad [1.13861681e-08 9.99999989e-01]\n",
      "El cliente  Escritura con probabilidad [0.01621577 0.98378423]\n",
      "El cliente  Escritura con probabilidad [0.08171253 0.91828747]\n",
      "El cliente  Escritura con probabilidad [0.15683082 0.84316918]\n",
      "El cliente  Escritura con probabilidad [0.05278609 0.94721391]\n",
      "El cliente  Escritura con probabilidad [0.19274971 0.80725029]\n",
      "El cliente  Escritura con probabilidad [0.13358453 0.86641547]\n",
      "El cliente  Escritura con probabilidad [0.16451866 0.83548134]\n",
      "El cliente  Escritura con probabilidad [0.29866655 0.70133345]\n",
      "El cliente  Escritura con probabilidad [2.84872281e-04 9.99715128e-01]\n",
      "El cliente  Escritura con probabilidad [3.04227540e-07 9.99999696e-01]\n",
      "El cliente  Escritura con probabilidad [0.28473408 0.71526592]\n",
      "El cliente  Escritura con probabilidad [0.16126307 0.83873693]\n",
      "El cliente  Escritura con probabilidad [2.95105831e-06 9.99997049e-01]\n",
      "El cliente  Escritura con probabilidad [0.11339146 0.88660854]\n",
      "El cliente  Escritura con probabilidad [0.10866563 0.89133437]\n",
      "El cliente  Escritura con probabilidad [0.01941402 0.98058598]\n",
      "El cliente  Escritura con probabilidad [0.21006662 0.78993338]\n",
      "El cliente  Escritura con probabilidad [4.78195528e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [0.13138616 0.86861384]\n",
      "El cliente  Escritura con probabilidad [0.144414 0.855586]\n",
      "El cliente  Escritura con probabilidad [0.02978457 0.97021543]\n",
      "El cliente  Escritura con probabilidad [0.2405959 0.7594041]\n",
      "El cliente  Escritura con probabilidad [0.28424662 0.71575338]\n",
      "El cliente  Escritura con probabilidad [0.2640503 0.7359497]\n",
      "El cliente  Escritura con probabilidad [0.16125415 0.83874585]\n",
      "El cliente  Escritura con probabilidad [0.05265732 0.94734268]\n",
      "El cliente  Escritura con probabilidad [0.05720774 0.94279226]\n",
      "El cliente  Escritura con probabilidad [0.30122668 0.69877332]\n",
      "El cliente  Escritura con probabilidad [3.67636089e-04 9.99632364e-01]\n",
      "El cliente  Escritura con probabilidad [4.47821340e-05 9.99955218e-01]\n",
      "El cliente  Escritura con probabilidad [0.03738408 0.96261592]\n",
      "El cliente  Escritura con probabilidad [0.18525158 0.81474842]\n",
      "El cliente  Escritura con probabilidad [0.00811495 0.99188505]\n",
      "El cliente  Escritura con probabilidad [0.17522567 0.82477433]\n",
      "El cliente  Escritura con probabilidad [3.25996119e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.21374177 0.78625823]\n",
      "El cliente  Escritura con probabilidad [0.14709931 0.85290069]\n",
      "El cliente  Escritura con probabilidad [0.23732987 0.76267013]\n",
      "El cliente  Escritura con probabilidad [0.02529659 0.97470341]\n",
      "El cliente  Escritura con probabilidad [0.06060833 0.93939167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.20025384 0.79974616]\n",
      "El cliente  Escritura con probabilidad [0.02927502 0.97072498]\n",
      "El cliente  Escritura con probabilidad [0.17838608 0.82161392]\n",
      "El cliente  Escritura con probabilidad [0.03273233 0.96726767]\n",
      "El cliente  Escritura con probabilidad [3.80559617e-07 9.99999619e-01]\n",
      "El cliente  Escritura con probabilidad [0.19689189 0.80310811]\n",
      "El cliente  Escritura con probabilidad [0.14387143 0.85612857]\n",
      "El cliente  Escritura con probabilidad [0.00295998 0.99704002]\n",
      "El cliente  Escritura con probabilidad [1.81595006e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.28250606 0.71749394]\n",
      "El cliente  Escritura con probabilidad [0.14762855 0.85237145]\n",
      "El cliente  Escritura con probabilidad [0.16297944 0.83702056]\n",
      "El cliente  Escritura con probabilidad [0.28835567 0.71164433]\n",
      "El cliente  Escritura con probabilidad [0.17391944 0.82608056]\n",
      "El cliente  Escritura con probabilidad [0.00495292 0.99504708]\n",
      "El cliente  Escritura con probabilidad [0.22021604 0.77978396]\n",
      "El cliente  Escritura con probabilidad [0.01627654 0.98372346]\n",
      "El cliente  Escritura con probabilidad [4.04262701e-04 9.99595737e-01]\n",
      "El cliente  Escritura con probabilidad [0.11183638 0.88816362]\n",
      "El cliente  Escritura con probabilidad [0.14597112 0.85402888]\n",
      "El cliente  Escritura con probabilidad [0.19686321 0.80313679]\n",
      "El cliente  Escritura con probabilidad [2.55129251e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [6.17486142e-06 9.99993825e-01]\n",
      "El cliente  Escritura con probabilidad [0.0842073 0.9157927]\n",
      "El cliente  Escritura con probabilidad [0.19938373 0.80061627]\n",
      "El cliente  Escritura con probabilidad [2.05980566e-08 9.99999979e-01]\n",
      "El cliente  Escritura con probabilidad [0.24679672 0.75320328]\n",
      "El cliente  Escritura con probabilidad [0.30856079 0.69143921]\n",
      "El cliente  Escritura con probabilidad [0.13934727 0.86065273]\n",
      "El cliente  Escritura con probabilidad [0.26822212 0.73177788]\n",
      "El cliente  Escritura con probabilidad [0.06278024 0.93721976]\n",
      "El cliente  Escritura con probabilidad [0.20915573 0.79084427]\n",
      "El cliente  Escritura con probabilidad [1.32528188e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.07086079 0.92913921]\n",
      "El cliente  Escritura con probabilidad [2.99617664e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.06739926 0.93260074]\n",
      "El cliente  Escritura con probabilidad [0.35476167 0.64523833]\n",
      "El cliente  Escritura con probabilidad [0.22355572 0.77644428]\n",
      "El cliente  Escritura con probabilidad [0.07113754 0.92886246]\n",
      "El cliente  Escritura con probabilidad [0.04737989 0.95262011]\n",
      "El cliente  Escritura con probabilidad [0.25394599 0.74605401]\n",
      "El cliente  Escritura con probabilidad [0.16926912 0.83073088]\n",
      "El cliente  Escritura con probabilidad [1.47303513e-04 9.99852696e-01]\n",
      "El cliente  Escritura con probabilidad [0.26953537 0.73046463]\n",
      "El cliente  Escritura con probabilidad [0.19447159 0.80552841]\n",
      "El cliente  Escritura con probabilidad [0.25752722 0.74247278]\n",
      "El cliente  Escritura con probabilidad [0.08474977 0.91525023]\n",
      "El cliente  Escritura con probabilidad [0.05870109 0.94129891]\n",
      "El cliente  Escritura con probabilidad [0.01554114 0.98445886]\n",
      "El cliente  Escritura con probabilidad [0.18723253 0.81276747]\n",
      "El cliente  Escritura con probabilidad [0.04895881 0.95104119]\n",
      "El cliente  Escritura con probabilidad [0.24862522 0.75137478]\n",
      "El cliente  Escritura con probabilidad [0.1142662 0.8857338]\n",
      "El cliente  Escritura con probabilidad [0.25126651 0.74873349]\n",
      "El cliente  Escritura con probabilidad [0.19466722 0.80533278]\n",
      "El cliente  Escritura con probabilidad [0.33525791 0.66474209]\n",
      "El cliente  Escritura con probabilidad [0.17119819 0.82880181]\n",
      "El cliente  Escritura con probabilidad [0.16128462 0.83871538]\n",
      "El cliente  Escritura con probabilidad [0.02665906 0.97334094]\n",
      "El cliente  Escritura con probabilidad [0.31262205 0.68737795]\n",
      "El cliente  Escritura con probabilidad [2.33452027e-05 9.99976655e-01]\n",
      "El cliente  Escritura con probabilidad [0.02406784 0.97593216]\n",
      "El cliente  Escritura con probabilidad [0.00718863 0.99281137]\n",
      "El cliente  Escritura con probabilidad [1.83973401e-05 9.99981603e-01]\n",
      "El cliente  Escritura con probabilidad [0.31709051 0.68290949]\n",
      "El cliente  Escritura con probabilidad [0.04417447 0.95582553]\n",
      "El cliente  Escritura con probabilidad [0.03964367 0.96035633]\n",
      "El cliente  Escritura con probabilidad [0.23167786 0.76832214]\n",
      "El cliente  Escritura con probabilidad [0.10012597 0.89987403]\n",
      "El cliente  Escritura con probabilidad [0.00333724 0.99666276]\n",
      "El cliente  Escritura con probabilidad [0.22626338 0.77373662]\n",
      "El cliente  Escritura con probabilidad [0.23004204 0.76995796]\n",
      "El cliente  Escritura con probabilidad [2.67791558e-06 9.99997322e-01]\n",
      "El cliente  Escritura con probabilidad [0.09421061 0.90578939]\n",
      "El cliente  Escritura con probabilidad [0.01734167 0.98265833]\n",
      "El cliente  Escritura con probabilidad [0.30051562 0.69948438]\n",
      "El cliente  Escritura con probabilidad [0.19808375 0.80191625]\n",
      "El cliente  Escritura con probabilidad [0.22695796 0.77304204]\n",
      "El cliente  Escritura con probabilidad [1.16787887e-06 9.99998832e-01]\n",
      "El cliente  Escritura con probabilidad [0.04085118 0.95914882]\n",
      "El cliente  Escritura con probabilidad [0.076413 0.923587]\n",
      "El cliente  Escritura con probabilidad [0.39569995 0.60430005]\n",
      "El cliente  Escritura con probabilidad [0.01928708 0.98071292]\n",
      "El cliente  Escritura con probabilidad [0.07426901 0.92573099]\n",
      "El cliente  Escritura con probabilidad [0.25179925 0.74820075]\n",
      "El cliente  Escritura con probabilidad [0.03607127 0.96392873]\n",
      "El cliente  Escritura con probabilidad [0.10538966 0.89461034]\n",
      "El cliente  Escritura con probabilidad [0.21199984 0.78800016]\n",
      "El cliente  Escritura con probabilidad [0.24682761 0.75317239]\n",
      "El cliente  Escritura con probabilidad [0.42041705 0.57958295]\n",
      "El cliente  Escritura con probabilidad [0.43356759 0.56643241]\n",
      "El cliente  Escritura con probabilidad [0.03649683 0.96350317]\n",
      "El cliente  Escritura con probabilidad [0.08425223 0.91574777]\n",
      "El cliente  Escritura con probabilidad [0.08878239 0.91121761]\n",
      "El cliente  Escritura con probabilidad [0.11752238 0.88247762]\n",
      "El cliente  Escritura con probabilidad [0.14813048 0.85186952]\n",
      "El cliente  Escritura con probabilidad [0.01506652 0.98493348]\n",
      "El cliente  Escritura con probabilidad [0.09104914 0.90895086]\n",
      "El cliente  Escritura con probabilidad [0.02219748 0.97780252]\n",
      "El cliente  Escritura con probabilidad [0.19251591 0.80748409]\n",
      "El cliente  Escritura con probabilidad [0.05103207 0.94896793]\n",
      "El cliente  Escritura con probabilidad [0.01810232 0.98189768]\n",
      "El cliente  Escritura con probabilidad [4.68618901e-04 9.99531381e-01]\n",
      "El cliente  Escritura con probabilidad [0.28200322 0.71799678]\n",
      "El cliente  Escritura con probabilidad [2.64894773e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.2056515 0.7943485]\n",
      "El cliente  Escritura con probabilidad [0.02297668 0.97702332]\n",
      "El cliente  Escritura con probabilidad [0.02083469 0.97916531]\n",
      "El cliente  Escritura con probabilidad [0.44952985 0.55047015]\n",
      "El cliente  Escritura con probabilidad [0.05454113 0.94545887]\n",
      "El cliente  Escritura con probabilidad [0.13782383 0.86217617]\n",
      "El cliente  Escritura con probabilidad [0.09217548 0.90782452]\n",
      "El cliente  Escritura con probabilidad [0.30136017 0.69863983]\n",
      "El cliente  Escritura con probabilidad [0.17403565 0.82596435]\n",
      "El cliente  Escritura con probabilidad [0.04727512 0.95272488]\n",
      "El cliente  Escritura con probabilidad [0.16265329 0.83734671]\n",
      "El cliente  Escritura con probabilidad [0.25388469 0.74611531]\n",
      "El cliente  Escritura con probabilidad [0.10290353 0.89709647]\n",
      "El cliente  Escritura con probabilidad [0.23071989 0.76928011]\n",
      "El cliente  Escritura con probabilidad [0.19524211 0.80475789]\n",
      "El cliente  Escritura con probabilidad [0.04222948 0.95777052]\n",
      "El cliente  Escritura con probabilidad [0.2601066 0.7398934]\n",
      "El cliente  Escritura con probabilidad [5.17866749e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [0.15734211 0.84265789]\n",
      "El cliente  Escritura con probabilidad [2.82918210e-06 9.99997171e-01]\n",
      "El cliente  Escritura con probabilidad [0.22874843 0.77125157]\n",
      "El cliente  Escritura con probabilidad [0.06423604 0.93576396]\n",
      "El cliente  Escritura con probabilidad [0.11632873 0.88367127]\n",
      "El cliente  Escritura con probabilidad [8.81576605e-06 9.99991184e-01]\n",
      "El cliente  Escritura con probabilidad [0.01400756 0.98599244]\n",
      "El cliente  Escritura con probabilidad [3.42479461e-07 9.99999658e-01]\n",
      "El cliente  Escritura con probabilidad [0.10016556 0.89983444]\n",
      "El cliente  Escritura con probabilidad [0.26784199 0.73215801]\n",
      "El cliente  Escritura con probabilidad [1.57463105e-04 9.99842537e-01]\n",
      "El cliente  Escritura con probabilidad [0.08943121 0.91056879]\n",
      "El cliente  Escritura con probabilidad [1.22508908e-05 9.99987749e-01]\n",
      "El cliente  Escritura con probabilidad [0.02275783 0.97724217]\n",
      "El cliente  Escritura con probabilidad [0.10853407 0.89146593]\n",
      "El cliente  Escritura con probabilidad [0.04487879 0.95512121]\n",
      "El cliente  Escritura con probabilidad [0.28241437 0.71758563]\n",
      "El cliente  Escritura con probabilidad [0.00526184 0.99473816]\n",
      "El cliente  Escritura con probabilidad [0.17122248 0.82877752]\n",
      "El cliente  Escritura con probabilidad [1.16197274e-10 1.00000000e+00]\n",
      "El cliente  Desiste con probabilidad [0.52587929 0.47412071]\n",
      "El cliente  Escritura con probabilidad [0.2351454 0.7648546]\n",
      "El cliente  Escritura con probabilidad [0.29889495 0.70110505]\n",
      "El cliente  Escritura con probabilidad [0.27527741 0.72472259]\n",
      "El cliente  Escritura con probabilidad [0.06283488 0.93716512]\n",
      "El cliente  Escritura con probabilidad [0.091949 0.908051]\n",
      "El cliente  Escritura con probabilidad [1.50767738e-05 9.99984923e-01]\n",
      "El cliente  Escritura con probabilidad [0.01377129 0.98622871]\n",
      "El cliente  Escritura con probabilidad [0.25081245 0.74918755]\n",
      "El cliente  Escritura con probabilidad [1.49898627e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.13746749 0.86253251]\n",
      "El cliente  Escritura con probabilidad [1.27342489e-04 9.99872658e-01]\n",
      "El cliente  Escritura con probabilidad [0.05324002 0.94675998]\n",
      "El cliente  Escritura con probabilidad [0.1238416 0.8761584]\n",
      "El cliente  Escritura con probabilidad [0.11067888 0.88932112]\n",
      "El cliente  Escritura con probabilidad [0.16226986 0.83773014]\n",
      "El cliente  Escritura con probabilidad [0.13378594 0.86621406]\n",
      "El cliente  Escritura con probabilidad [0.04262802 0.95737198]\n",
      "El cliente  Escritura con probabilidad [0.43372616 0.56627384]\n",
      "El cliente  Escritura con probabilidad [1.55431223e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.09521864 0.90478136]\n",
      "El cliente  Escritura con probabilidad [0.20511793 0.79488207]\n",
      "El cliente  Escritura con probabilidad [4.52177206e-06 9.99995478e-01]\n",
      "El cliente  Escritura con probabilidad [0.22489332 0.77510668]\n",
      "El cliente  Escritura con probabilidad [0.01118048 0.98881952]\n",
      "El cliente  Escritura con probabilidad [0.07895693 0.92104307]\n",
      "El cliente  Escritura con probabilidad [0.00767126 0.99232874]\n",
      "El cliente  Escritura con probabilidad [0.17193984 0.82806016]\n",
      "El cliente  Escritura con probabilidad [0.03463501 0.96536499]\n",
      "El cliente  Escritura con probabilidad [0.1902121 0.8097879]\n",
      "El cliente  Escritura con probabilidad [0.23767994 0.76232006]\n",
      "El cliente  Escritura con probabilidad [0.24680478 0.75319522]\n",
      "El cliente  Escritura con probabilidad [0.22058525 0.77941475]\n",
      "El cliente  Escritura con probabilidad [1.09687162e-04 9.99890313e-01]\n",
      "El cliente  Escritura con probabilidad [1.21933525e-04 9.99878066e-01]\n",
      "El cliente  Escritura con probabilidad [0.08188408 0.91811592]\n",
      "El cliente  Escritura con probabilidad [0.02251956 0.97748044]\n",
      "El cliente  Escritura con probabilidad [0.21392527 0.78607473]\n",
      "El cliente  Escritura con probabilidad [0.09632952 0.90367048]\n",
      "El cliente  Escritura con probabilidad [3.45284610e-04 9.99654715e-01]\n",
      "El cliente  Escritura con probabilidad [0.2019357 0.7980643]\n",
      "El cliente  Escritura con probabilidad [0.09220211 0.90779789]\n",
      "El cliente  Escritura con probabilidad [0.16616946 0.83383054]\n",
      "El cliente  Escritura con probabilidad [1.87443684e-08 9.99999981e-01]\n",
      "El cliente  Escritura con probabilidad [0.01698541 0.98301459]\n",
      "El cliente  Escritura con probabilidad [0.03801081 0.96198919]\n",
      "El cliente  Escritura con probabilidad [0.02388187 0.97611813]\n",
      "El cliente  Escritura con probabilidad [0.05892648 0.94107352]\n",
      "El cliente  Escritura con probabilidad [0.12879283 0.87120717]\n",
      "El cliente  Escritura con probabilidad [0.05216453 0.94783547]\n",
      "El cliente  Escritura con probabilidad [0.17055934 0.82944066]\n",
      "El cliente  Escritura con probabilidad [0.0255397 0.9744603]\n",
      "El cliente  Escritura con probabilidad [0.27983224 0.72016776]\n",
      "El cliente  Escritura con probabilidad [0.2847004 0.7152996]\n",
      "El cliente  Escritura con probabilidad [0.15317906 0.84682094]\n",
      "El cliente  Escritura con probabilidad [0.23468938 0.76531062]\n",
      "El cliente  Escritura con probabilidad [0.13189145 0.86810855]\n",
      "El cliente  Escritura con probabilidad [0.04026942 0.95973058]\n",
      "El cliente  Escritura con probabilidad [0.19800357 0.80199643]\n",
      "El cliente  Escritura con probabilidad [0.02398815 0.97601185]\n",
      "El cliente  Escritura con probabilidad [0.01631307 0.98368693]\n",
      "El cliente  Escritura con probabilidad [0.12382969 0.87617031]\n",
      "El cliente  Escritura con probabilidad [0.30723474 0.69276526]\n",
      "El cliente  Escritura con probabilidad [0.10375437 0.89624563]\n",
      "El cliente  Escritura con probabilidad [0.23332149 0.76667851]\n",
      "El cliente  Escritura con probabilidad [0.3241143 0.6758857]\n",
      "El cliente  Escritura con probabilidad [0.29228292 0.70771708]\n",
      "El cliente  Escritura con probabilidad [0.08802934 0.91197066]\n",
      "El cliente  Escritura con probabilidad [0.03811904 0.96188096]\n",
      "El cliente  Escritura con probabilidad [0.09709099 0.90290901]\n",
      "El cliente  Escritura con probabilidad [0.2532073 0.7467927]\n",
      "El cliente  Escritura con probabilidad [0.25853932 0.74146068]\n",
      "El cliente  Escritura con probabilidad [0.06892319 0.93107681]\n",
      "El cliente  Escritura con probabilidad [0.04228146 0.95771854]\n",
      "El cliente  Escritura con probabilidad [0.28676515 0.71323485]\n",
      "El cliente  Escritura con probabilidad [0.01731725 0.98268275]\n",
      "El cliente  Escritura con probabilidad [0.03833222 0.96166778]\n",
      "El cliente  Escritura con probabilidad [3.36001008e-05 9.99966400e-01]\n",
      "El cliente  Escritura con probabilidad [0.26296223 0.73703777]\n",
      "El cliente  Escritura con probabilidad [0.25218052 0.74781948]\n",
      "El cliente  Escritura con probabilidad [0.07464933 0.92535067]\n",
      "El cliente  Escritura con probabilidad [8.11777897e-04 9.99188222e-01]\n",
      "El cliente  Escritura con probabilidad [0.33648154 0.66351846]\n",
      "El cliente  Escritura con probabilidad [0.12722544 0.87277456]\n",
      "El cliente  Escritura con probabilidad [0.26535524 0.73464476]\n",
      "El cliente  Escritura con probabilidad [0.12805568 0.87194432]\n",
      "El cliente  Escritura con probabilidad [0.21708897 0.78291103]\n",
      "El cliente  Escritura con probabilidad [0.06713145 0.93286855]\n",
      "El cliente  Escritura con probabilidad [0.2033439 0.7966561]\n",
      "El cliente  Escritura con probabilidad [0.13246852 0.86753148]\n",
      "El cliente  Escritura con probabilidad [0.20351704 0.79648296]\n",
      "El cliente  Escritura con probabilidad [0.09910562 0.90089438]\n",
      "El cliente  Escritura con probabilidad [0.0890976 0.9109024]\n",
      "El cliente  Escritura con probabilidad [0.20338736 0.79661264]\n",
      "El cliente  Escritura con probabilidad [0.10813389 0.89186611]\n",
      "El cliente  Escritura con probabilidad [7.10964741e-06 9.99992890e-01]\n",
      "El cliente  Escritura con probabilidad [0.07965362 0.92034638]\n",
      "El cliente  Escritura con probabilidad [0.12116246 0.87883754]\n",
      "El cliente  Escritura con probabilidad [0.05539739 0.94460261]\n",
      "El cliente  Escritura con probabilidad [0.1093198 0.8906802]\n",
      "El cliente  Escritura con probabilidad [8.65973959e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [2.15782742e-07 9.99999784e-01]\n",
      "El cliente  Escritura con probabilidad [0.13192183 0.86807817]\n",
      "El cliente  Escritura con probabilidad [0.06154838 0.93845162]\n",
      "El cliente  Escritura con probabilidad [0.12492877 0.87507123]\n",
      "El cliente  Escritura con probabilidad [0.14013633 0.85986367]\n",
      "El cliente  Escritura con probabilidad [0.25341798 0.74658202]\n",
      "El cliente  Escritura con probabilidad [0.09684684 0.90315316]\n",
      "El cliente  Escritura con probabilidad [0.00256846 0.99743154]\n",
      "El cliente  Escritura con probabilidad [0.17385921 0.82614079]\n",
      "El cliente  Escritura con probabilidad [0.14214943 0.85785057]\n",
      "El cliente  Escritura con probabilidad [0.02892743 0.97107257]\n",
      "El cliente  Escritura con probabilidad [0.19376175 0.80623825]\n",
      "El cliente  Escritura con probabilidad [0.16951268 0.83048732]\n",
      "El cliente  Escritura con probabilidad [0.07189761 0.92810239]\n",
      "El cliente  Escritura con probabilidad [5.41508119e-05 9.99945849e-01]\n",
      "El cliente  Escritura con probabilidad [0.28865802 0.71134198]\n",
      "El cliente  Escritura con probabilidad [1.58517600e-06 9.99998415e-01]\n",
      "El cliente  Escritura con probabilidad [0.091738 0.908262]\n",
      "El cliente  Escritura con probabilidad [0.16977507 0.83022493]\n",
      "El cliente  Escritura con probabilidad [0.32701726 0.67298274]\n",
      "El cliente  Escritura con probabilidad [0.141901 0.858099]\n",
      "El cliente  Escritura con probabilidad [0.11874529 0.88125471]\n",
      "El cliente  Escritura con probabilidad [0.34472112 0.65527888]\n",
      "El cliente  Escritura con probabilidad [0.08018982 0.91981018]\n",
      "El cliente  Escritura con probabilidad [1.85162910e-06 9.99998148e-01]\n",
      "El cliente  Escritura con probabilidad [0.07548761 0.92451239]\n",
      "El cliente  Escritura con probabilidad [0.05513265 0.94486735]\n",
      "El cliente  Escritura con probabilidad [7.86382846e-06 9.99992136e-01]\n",
      "El cliente  Escritura con probabilidad [0.29424602 0.70575398]\n",
      "El cliente  Escritura con probabilidad [0.01638012 0.98361988]\n",
      "El cliente  Escritura con probabilidad [0.17588915 0.82411085]\n",
      "El cliente  Escritura con probabilidad [1.91322794e-06 9.99998087e-01]\n",
      "El cliente  Escritura con probabilidad [1.92650321e-06 9.99998073e-01]\n",
      "El cliente  Escritura con probabilidad [0.03442526 0.96557474]\n",
      "El cliente  Escritura con probabilidad [0.10266784 0.89733216]\n",
      "El cliente  Escritura con probabilidad [4.3863313e-10 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.00311828 0.99688172]\n",
      "El cliente  Escritura con probabilidad [0.10411769 0.89588231]\n",
      "El cliente  Escritura con probabilidad [0.19636008 0.80363992]\n",
      "El cliente  Escritura con probabilidad [0.13258918 0.86741082]\n",
      "El cliente  Escritura con probabilidad [0.10069393 0.89930607]\n",
      "El cliente  Escritura con probabilidad [0.04002768 0.95997232]\n",
      "El cliente  Escritura con probabilidad [0.07663604 0.92336396]\n",
      "El cliente  Escritura con probabilidad [0.043796 0.956204]\n",
      "El cliente  Escritura con probabilidad [0.23132908 0.76867092]\n",
      "El cliente  Escritura con probabilidad [0.00373281 0.99626719]\n",
      "El cliente  Escritura con probabilidad [0.12752819 0.87247181]\n",
      "El cliente  Escritura con probabilidad [0.02134207 0.97865793]\n",
      "El cliente  Escritura con probabilidad [0.00145276 0.99854724]\n",
      "El cliente  Escritura con probabilidad [0.26052509 0.73947491]\n",
      "El cliente  Escritura con probabilidad [0.00320529 0.99679471]\n",
      "El cliente  Escritura con probabilidad [0.04349979 0.95650021]\n",
      "El cliente  Escritura con probabilidad [7.31582990e-06 9.99992684e-01]\n",
      "El cliente  Escritura con probabilidad [0.35163686 0.64836314]\n",
      "El cliente  Escritura con probabilidad [0.17675052 0.82324948]\n",
      "El cliente  Escritura con probabilidad [0.21173594 0.78826406]\n",
      "El cliente  Escritura con probabilidad [0.28400246 0.71599754]\n",
      "El cliente  Escritura con probabilidad [0.01539347 0.98460653]\n",
      "El cliente  Escritura con probabilidad [2.99203751e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [9.98399455e-06 9.99990016e-01]\n",
      "El cliente  Escritura con probabilidad [0.2164033 0.7835967]\n",
      "El cliente  Escritura con probabilidad [0.11793772 0.88206228]\n",
      "El cliente  Escritura con probabilidad [0.19905537 0.80094463]\n",
      "El cliente  Escritura con probabilidad [0.10580246 0.89419754]\n",
      "El cliente  Escritura con probabilidad [0.05050573 0.94949427]\n",
      "El cliente  Escritura con probabilidad [1.25245318e-07 9.99999875e-01]\n",
      "El cliente  Escritura con probabilidad [0.15891641 0.84108359]\n",
      "El cliente  Escritura con probabilidad [4.32181278e-04 9.99567819e-01]\n",
      "El cliente  Escritura con probabilidad [6.52788934e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.07771217 0.92228783]\n",
      "El cliente  Escritura con probabilidad [0.15437182 0.84562818]\n",
      "El cliente  Escritura con probabilidad [0.11878998 0.88121002]\n",
      "El cliente  Escritura con probabilidad [0.17597285 0.82402715]\n",
      "El cliente  Escritura con probabilidad [0.00121781 0.99878219]\n",
      "El cliente  Escritura con probabilidad [0.44747089 0.55252911]\n",
      "El cliente  Escritura con probabilidad [0.09898262 0.90101738]\n",
      "El cliente  Escritura con probabilidad [0.05650124 0.94349876]\n",
      "El cliente  Escritura con probabilidad [0.10577141 0.89422859]\n",
      "El cliente  Escritura con probabilidad [0.09195783 0.90804217]\n",
      "El cliente  Escritura con probabilidad [0.08673407 0.91326593]\n",
      "El cliente  Escritura con probabilidad [0.11075023 0.88924977]\n",
      "El cliente  Escritura con probabilidad [4.59875049e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [2.47024320e-04 9.99752976e-01]\n",
      "El cliente  Escritura con probabilidad [4.82362424e-08 9.99999952e-01]\n",
      "El cliente  Escritura con probabilidad [0.05137291 0.94862709]\n",
      "El cliente  Escritura con probabilidad [0.02197003 0.97802997]\n",
      "El cliente  Escritura con probabilidad [0.12174789 0.87825211]\n",
      "El cliente  Escritura con probabilidad [0.12835223 0.87164777]\n",
      "El cliente  Escritura con probabilidad [0.19750406 0.80249594]\n",
      "El cliente  Escritura con probabilidad [0.07995188 0.92004812]\n",
      "El cliente  Escritura con probabilidad [2.49491539e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.00222576 0.99777424]\n",
      "El cliente  Escritura con probabilidad [7.68875963e-06 9.99992311e-01]\n",
      "El cliente  Escritura con probabilidad [0.1989705 0.8010295]\n",
      "El cliente  Escritura con probabilidad [0.05968205 0.94031795]\n",
      "El cliente  Escritura con probabilidad [0.11732414 0.88267586]\n",
      "El cliente  Escritura con probabilidad [0.18567454 0.81432546]\n",
      "El cliente  Escritura con probabilidad [0.29330601 0.70669399]\n",
      "El cliente  Escritura con probabilidad [0.24397484 0.75602516]\n",
      "El cliente  Escritura con probabilidad [0.03364416 0.96635584]\n",
      "El cliente  Escritura con probabilidad [0.22866492 0.77133508]\n",
      "El cliente  Escritura con probabilidad [0.23837498 0.76162502]\n",
      "El cliente  Escritura con probabilidad [0.26714711 0.73285289]\n",
      "El cliente  Escritura con probabilidad [0.16423972 0.83576028]\n",
      "El cliente  Escritura con probabilidad [0.0861148 0.9138852]\n",
      "El cliente  Escritura con probabilidad [0.00327008 0.99672992]\n",
      "El cliente  Escritura con probabilidad [3.64359246e-07 9.99999636e-01]\n",
      "El cliente  Escritura con probabilidad [0.05198414 0.94801586]\n",
      "El cliente  Escritura con probabilidad [0.08242448 0.91757552]\n",
      "El cliente  Escritura con probabilidad [0.00285415 0.99714585]\n",
      "El cliente  Escritura con probabilidad [0.02088186 0.97911814]\n",
      "El cliente  Escritura con probabilidad [0.17523419 0.82476581]\n",
      "El cliente  Escritura con probabilidad [4.64974940e-05 9.99953503e-01]\n",
      "El cliente  Escritura con probabilidad [0.05288913 0.94711087]\n",
      "El cliente  Escritura con probabilidad [0.01589575 0.98410425]\n",
      "El cliente  Escritura con probabilidad [0.10635465 0.89364535]\n",
      "El cliente  Escritura con probabilidad [0.26092351 0.73907649]\n",
      "El cliente  Escritura con probabilidad [5.51561596e-09 9.99999994e-01]\n",
      "El cliente  Escritura con probabilidad [0.22158149 0.77841851]\n",
      "El cliente  Escritura con probabilidad [0.25607885 0.74392115]\n",
      "El cliente  Escritura con probabilidad [0.01958081 0.98041919]\n",
      "El cliente  Escritura con probabilidad [0.10045432 0.89954568]\n",
      "El cliente  Escritura con probabilidad [0.08254846 0.91745154]\n",
      "El cliente  Escritura con probabilidad [0.00792574 0.99207426]\n",
      "El cliente  Escritura con probabilidad [2.88043392e-05 9.99971196e-01]\n",
      "El cliente  Escritura con probabilidad [0.17771977 0.82228023]\n",
      "El cliente  Escritura con probabilidad [0.05068976 0.94931024]\n",
      "El cliente  Escritura con probabilidad [0.05648127 0.94351873]\n",
      "El cliente  Escritura con probabilidad [0.24798385 0.75201615]\n",
      "El cliente  Escritura con probabilidad [0.06008946 0.93991054]\n",
      "El cliente  Escritura con probabilidad [0.10317247 0.89682753]\n",
      "El cliente  Escritura con probabilidad [0.21799936 0.78200064]\n",
      "El cliente  Escritura con probabilidad [0.20721586 0.79278414]\n",
      "El cliente  Escritura con probabilidad [0.13382949 0.86617051]\n",
      "El cliente  Escritura con probabilidad [4.26507537e-05 9.99957349e-01]\n",
      "El cliente  Escritura con probabilidad [0.01696937 0.98303063]\n",
      "El cliente  Escritura con probabilidad [0.2807149 0.7192851]\n",
      "El cliente  Escritura con probabilidad [0.17728362 0.82271638]\n",
      "El cliente  Escritura con probabilidad [0.116126 0.883874]\n",
      "El cliente  Escritura con probabilidad [0.02211545 0.97788455]\n",
      "El cliente  Escritura con probabilidad [2.87509811e-06 9.99997125e-01]\n",
      "El cliente  Escritura con probabilidad [0.23531018 0.76468982]\n",
      "El cliente  Escritura con probabilidad [0.14608086 0.85391914]\n",
      "El cliente  Escritura con probabilidad [0.21002297 0.78997703]\n",
      "El cliente  Escritura con probabilidad [0.03978007 0.96021993]\n",
      "El cliente  Escritura con probabilidad [0.12847213 0.87152787]\n",
      "El cliente  Escritura con probabilidad [0.07820081 0.92179919]\n",
      "El cliente  Escritura con probabilidad [0.01795949 0.98204051]\n",
      "El cliente  Escritura con probabilidad [0.1860204 0.8139796]\n",
      "El cliente  Escritura con probabilidad [0.06721553 0.93278447]\n",
      "El cliente  Escritura con probabilidad [0.08491671 0.91508329]\n",
      "El cliente  Escritura con probabilidad [0.24777512 0.75222488]\n",
      "El cliente  Escritura con probabilidad [0.19851364 0.80148636]\n",
      "El cliente  Escritura con probabilidad [0.03126637 0.96873363]\n",
      "El cliente  Escritura con probabilidad [0.1489353 0.8510647]\n",
      "El cliente  Escritura con probabilidad [0.06534419 0.93465581]\n",
      "El cliente  Escritura con probabilidad [0.10023367 0.89976633]\n",
      "El cliente  Escritura con probabilidad [0.02030779 0.97969221]\n",
      "El cliente  Escritura con probabilidad [0.10669481 0.89330519]\n",
      "El cliente  Escritura con probabilidad [0.15216583 0.84783417]\n",
      "El cliente  Escritura con probabilidad [0.15071147 0.84928853]\n",
      "El cliente  Escritura con probabilidad [0.19034708 0.80965292]\n",
      "El cliente  Escritura con probabilidad [5.64931000e-05 9.99943507e-01]\n",
      "El cliente  Escritura con probabilidad [1.67823089e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.16674867 0.83325133]\n",
      "El cliente  Escritura con probabilidad [0.09118584 0.90881416]\n",
      "El cliente  Escritura con probabilidad [0.41696957 0.58303043]\n",
      "El cliente  Escritura con probabilidad [0.13293623 0.86706377]\n",
      "El cliente  Escritura con probabilidad [0.07230553 0.92769447]\n",
      "El cliente  Escritura con probabilidad [0.39644861 0.60355139]\n",
      "El cliente  Escritura con probabilidad [0.02477752 0.97522248]\n",
      "El cliente  Escritura con probabilidad [0.01931554 0.98068446]\n",
      "El cliente  Escritura con probabilidad [0.14248231 0.85751769]\n",
      "El cliente  Escritura con probabilidad [0.03148205 0.96851795]\n",
      "El cliente  Escritura con probabilidad [0.28318301 0.71681699]\n",
      "El cliente  Escritura con probabilidad [0.12251893 0.87748107]\n",
      "El cliente  Escritura con probabilidad [0.2186513 0.7813487]\n",
      "El cliente  Escritura con probabilidad [1.01733733e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [2.61278574e-08 9.99999974e-01]\n",
      "El cliente  Escritura con probabilidad [4.16857953e-08 9.99999958e-01]\n",
      "El cliente  Escritura con probabilidad [0.02072263 0.97927737]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.00286647 0.99713353]\n",
      "El cliente  Escritura con probabilidad [0.01704517 0.98295483]\n",
      "El cliente  Escritura con probabilidad [5.20140808e-05 9.99947986e-01]\n",
      "El cliente  Escritura con probabilidad [0.22116125 0.77883875]\n",
      "El cliente  Escritura con probabilidad [1.44405407e-04 9.99855595e-01]\n",
      "El cliente  Escritura con probabilidad [0.08811455 0.91188545]\n",
      "El cliente  Escritura con probabilidad [0.05095777 0.94904223]\n",
      "El cliente  Escritura con probabilidad [0.15649276 0.84350724]\n",
      "El cliente  Escritura con probabilidad [0.18851939 0.81148061]\n",
      "El cliente  Escritura con probabilidad [0.17198416 0.82801584]\n",
      "El cliente  Escritura con probabilidad [0.00556977 0.99443023]\n",
      "El cliente  Escritura con probabilidad [0.2206779 0.7793221]\n",
      "El cliente  Escritura con probabilidad [0.19325233 0.80674767]\n",
      "El cliente  Escritura con probabilidad [0.07770175 0.92229825]\n",
      "El cliente  Escritura con probabilidad [0.24661911 0.75338089]\n",
      "El cliente  Escritura con probabilidad [0.39220559 0.60779441]\n",
      "El cliente  Escritura con probabilidad [0.11863639 0.88136361]\n",
      "El cliente  Escritura con probabilidad [0.25728391 0.74271609]\n",
      "El cliente  Escritura con probabilidad [0.00360444 0.99639556]\n",
      "El cliente  Escritura con probabilidad [0.21182223 0.78817777]\n",
      "El cliente  Escritura con probabilidad [0.10517144 0.89482856]\n",
      "El cliente  Escritura con probabilidad [0.21398916 0.78601084]\n",
      "El cliente  Escritura con probabilidad [0.19635058 0.80364942]\n",
      "El cliente  Escritura con probabilidad [0.2336311 0.7663689]\n",
      "El cliente  Escritura con probabilidad [0.43541273 0.56458727]\n",
      "El cliente  Escritura con probabilidad [0.11467284 0.88532716]\n",
      "El cliente  Escritura con probabilidad [0.16021231 0.83978769]\n",
      "El cliente  Escritura con probabilidad [0.38390211 0.61609789]\n",
      "El cliente  Escritura con probabilidad [0.25096189 0.74903811]\n",
      "El cliente  Escritura con probabilidad [0.02631254 0.97368746]\n",
      "El cliente  Escritura con probabilidad [0.28274658 0.71725342]\n",
      "El cliente  Escritura con probabilidad [1.22410765e-04 9.99877589e-01]\n",
      "El cliente  Escritura con probabilidad [0.24019065 0.75980935]\n",
      "El cliente  Escritura con probabilidad [0.08149955 0.91850045]\n",
      "El cliente  Escritura con probabilidad [2.55573461e-04 9.99744427e-01]\n",
      "El cliente  Escritura con probabilidad [0.18438235 0.81561765]\n",
      "El cliente  Escritura con probabilidad [3.33066907e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.06213061e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.30236467 0.69763533]\n",
      "El cliente  Escritura con probabilidad [1.51121015e-04 9.99848879e-01]\n",
      "El cliente  Escritura con probabilidad [1.53634619e-04 9.99846365e-01]\n",
      "El cliente  Escritura con probabilidad [0.11501682 0.88498318]\n",
      "El cliente  Escritura con probabilidad [0.08670305 0.91329695]\n",
      "El cliente  Escritura con probabilidad [0.14279233 0.85720767]\n",
      "El cliente  Escritura con probabilidad [0.02577686 0.97422314]\n",
      "El cliente  Escritura con probabilidad [0.21867324 0.78132676]\n",
      "El cliente  Escritura con probabilidad [0.13907022 0.86092978]\n",
      "El cliente  Escritura con probabilidad [0.20666487 0.79333513]\n",
      "El cliente  Escritura con probabilidad [3.63877928e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.19167652 0.80832348]\n",
      "El cliente  Escritura con probabilidad [0.08982549 0.91017451]\n",
      "El cliente  Escritura con probabilidad [0.09057444 0.90942556]\n",
      "El cliente  Escritura con probabilidad [0.14946869 0.85053131]\n",
      "El cliente  Escritura con probabilidad [0.24426341 0.75573659]\n",
      "El cliente  Escritura con probabilidad [0.10225035 0.89774965]\n",
      "El cliente  Escritura con probabilidad [0.06218117 0.93781883]\n",
      "El cliente  Escritura con probabilidad [5.14005805e-04 9.99485994e-01]\n",
      "El cliente  Escritura con probabilidad [4.98220629e-08 9.99999950e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.16660692 0.83339308]\n",
      "El cliente  Escritura con probabilidad [0.09606533 0.90393467]\n",
      "El cliente  Escritura con probabilidad [0.24844595 0.75155405]\n",
      "El cliente  Escritura con probabilidad [0.05501412 0.94498588]\n",
      "El cliente  Escritura con probabilidad [0.19396822 0.80603178]\n",
      "El cliente  Escritura con probabilidad [0.07382334 0.92617666]\n",
      "El cliente  Escritura con probabilidad [0.10638818 0.89361182]\n",
      "El cliente  Escritura con probabilidad [0.02092886 0.97907114]\n",
      "El cliente  Escritura con probabilidad [0.05907737 0.94092263]\n",
      "El cliente  Escritura con probabilidad [0.02568285 0.97431715]\n",
      "El cliente  Escritura con probabilidad [0.05367998 0.94632002]\n",
      "El cliente  Escritura con probabilidad [0.25875865 0.74124135]\n",
      "El cliente  Escritura con probabilidad [0.23706213 0.76293787]\n",
      "El cliente  Escritura con probabilidad [0.05035929 0.94964071]\n",
      "El cliente  Escritura con probabilidad [0.19850291 0.80149709]\n",
      "El cliente  Escritura con probabilidad [0.32248452 0.67751548]\n",
      "El cliente  Escritura con probabilidad [0.31818732 0.68181268]\n",
      "El cliente  Escritura con probabilidad [0.08511004 0.91488996]\n",
      "El cliente  Escritura con probabilidad [0.22700701 0.77299299]\n",
      "El cliente  Escritura con probabilidad [2.06358215e-08 9.99999979e-01]\n",
      "El cliente  Escritura con probabilidad [0.11372904 0.88627096]\n",
      "El cliente  Escritura con probabilidad [0.10285011 0.89714989]\n",
      "El cliente  Escritura con probabilidad [0.2749655 0.7250345]\n",
      "El cliente  Escritura con probabilidad [0.02154328 0.97845672]\n",
      "El cliente  Escritura con probabilidad [0.22674658 0.77325342]\n",
      "El cliente  Escritura con probabilidad [0.17627638 0.82372362]\n",
      "El cliente  Escritura con probabilidad [9.99698802e-09 9.99999990e-01]\n",
      "El cliente  Escritura con probabilidad [0.14955856 0.85044144]\n",
      "El cliente  Escritura con probabilidad [0.12390766 0.87609234]\n",
      "El cliente  Escritura con probabilidad [4.79793251e-08 9.99999952e-01]\n",
      "El cliente  Escritura con probabilidad [1.07958047e-04 9.99892042e-01]\n",
      "El cliente  Escritura con probabilidad [0.23179508 0.76820492]\n",
      "El cliente  Escritura con probabilidad [0.13698454 0.86301546]\n",
      "El cliente  Escritura con probabilidad [0.08059238 0.91940762]\n",
      "El cliente  Escritura con probabilidad [0.3894273 0.6105727]\n",
      "El cliente  Escritura con probabilidad [0.00899692 0.99100308]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.26896159 0.73103841]\n",
      "El cliente  Escritura con probabilidad [0.03662275 0.96337725]\n",
      "El cliente  Escritura con probabilidad [0.25140093 0.74859907]\n",
      "El cliente  Escritura con probabilidad [0.20727065 0.79272935]\n",
      "El cliente  Escritura con probabilidad [0.03545187 0.96454813]\n",
      "El cliente  Escritura con probabilidad [0.25108928 0.74891072]\n",
      "El cliente  Escritura con probabilidad [0.00450927 0.99549073]\n",
      "El cliente  Escritura con probabilidad [0.01090342 0.98909658]\n",
      "El cliente  Escritura con probabilidad [8.6408658e-12 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.03528027 0.96471973]\n",
      "El cliente  Escritura con probabilidad [0.01217322 0.98782678]\n",
      "El cliente  Escritura con probabilidad [1.56553353e-04 9.99843447e-01]\n",
      "El cliente  Escritura con probabilidad [0.11215017 0.88784983]\n",
      "El cliente  Escritura con probabilidad [0.18105313 0.81894687]\n",
      "El cliente  Escritura con probabilidad [0.24503345 0.75496655]\n",
      "El cliente  Escritura con probabilidad [0.0725924 0.9274076]\n",
      "El cliente  Escritura con probabilidad [0.19325082 0.80674918]\n",
      "El cliente  Escritura con probabilidad [0.20366522 0.79633478]\n",
      "El cliente  Escritura con probabilidad [0.19615955 0.80384045]\n",
      "El cliente  Escritura con probabilidad [0.14542228 0.85457772]\n",
      "El cliente  Escritura con probabilidad [0.02170895 0.97829105]\n",
      "El cliente  Escritura con probabilidad [0.10368535 0.89631465]\n",
      "El cliente  Escritura con probabilidad [0.04143596 0.95856404]\n",
      "El cliente  Escritura con probabilidad [0.16853398 0.83146602]\n",
      "El cliente  Escritura con probabilidad [9.66038360e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [9.11255640e-05 9.99908874e-01]\n",
      "El cliente  Escritura con probabilidad [2.11397733e-06 9.99997886e-01]\n",
      "El cliente  Escritura con probabilidad [0.06678395 0.93321605]\n",
      "El cliente  Escritura con probabilidad [0.28208609 0.71791391]\n",
      "El cliente  Escritura con probabilidad [0.22736544 0.77263456]\n",
      "El cliente  Escritura con probabilidad [0.28352019 0.71647981]\n",
      "El cliente  Escritura con probabilidad [0.24530717 0.75469283]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.16945763 0.83054237]\n",
      "El cliente  Escritura con probabilidad [0.11387115 0.88612885]\n",
      "El cliente  Escritura con probabilidad [0.1245714 0.8754286]\n",
      "El cliente  Escritura con probabilidad [0.08697559 0.91302441]\n",
      "El cliente  Escritura con probabilidad [0.07186519 0.92813481]\n",
      "El cliente  Escritura con probabilidad [0.08175303 0.91824697]\n",
      "El cliente  Escritura con probabilidad [0.05223753 0.94776247]\n",
      "El cliente  Escritura con probabilidad [0.14400462 0.85599538]\n",
      "El cliente  Escritura con probabilidad [0.01855302 0.98144698]\n",
      "El cliente  Escritura con probabilidad [1.67457425e-07 9.99999833e-01]\n",
      "El cliente  Escritura con probabilidad [0.05811042 0.94188958]\n",
      "El cliente  Escritura con probabilidad [0.00530606 0.99469394]\n",
      "El cliente  Escritura con probabilidad [0.07788816 0.92211184]\n",
      "El cliente  Escritura con probabilidad [0.15742612 0.84257388]\n",
      "El cliente  Escritura con probabilidad [0.34432995 0.65567005]\n",
      "El cliente  Escritura con probabilidad [0.02811374 0.97188626]\n",
      "El cliente  Escritura con probabilidad [0.23854589 0.76145411]\n",
      "El cliente  Escritura con probabilidad [0.14330424 0.85669576]\n",
      "El cliente  Escritura con probabilidad [0.07756506 0.92243494]\n",
      "El cliente  Escritura con probabilidad [0.01760377 0.98239623]\n",
      "El cliente  Escritura con probabilidad [0.17516976 0.82483024]\n",
      "El cliente  Escritura con probabilidad [1.13803907e-05 9.99988620e-01]\n",
      "El cliente  Escritura con probabilidad [0.22814327 0.77185673]\n",
      "El cliente  Escritura con probabilidad [0.17887848 0.82112152]\n",
      "El cliente  Escritura con probabilidad [0.12994344 0.87005656]\n",
      "El cliente  Escritura con probabilidad [2.54690327e-07 9.99999745e-01]\n",
      "El cliente  Escritura con probabilidad [0.21643548 0.78356452]\n",
      "El cliente  Escritura con probabilidad [0.27000407 0.72999593]\n",
      "El cliente  Escritura con probabilidad [1.92641265e-06 9.99998074e-01]\n",
      "El cliente  Escritura con probabilidad [1.13279919e-07 9.99999887e-01]\n",
      "El cliente  Escritura con probabilidad [1.45039536e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.11718065 0.88281935]\n",
      "El cliente  Escritura con probabilidad [0.02146618 0.97853382]\n",
      "El cliente  Escritura con probabilidad [1.00326861e-08 9.99999990e-01]\n",
      "El cliente  Escritura con probabilidad [0.18672154 0.81327846]\n",
      "El cliente  Escritura con probabilidad [0.29691677 0.70308323]\n",
      "El cliente  Escritura con probabilidad [0.22659772 0.77340228]\n",
      "El cliente  Escritura con probabilidad [6.96666364e-07 9.99999303e-01]\n",
      "El cliente  Escritura con probabilidad [0.30633072 0.69366928]\n",
      "El cliente  Escritura con probabilidad [0.01575632 0.98424368]\n",
      "El cliente  Escritura con probabilidad [0.05520238 0.94479762]\n",
      "El cliente  Escritura con probabilidad [0.16295149 0.83704851]\n",
      "El cliente  Escritura con probabilidad [0.11607142 0.88392858]\n",
      "El cliente  Escritura con probabilidad [0.20324109 0.79675891]\n",
      "El cliente  Escritura con probabilidad [0.16636088 0.83363912]\n",
      "El cliente  Escritura con probabilidad [0.16823672 0.83176328]\n",
      "El cliente  Escritura con probabilidad [0.10447413 0.89552587]\n",
      "El cliente  Escritura con probabilidad [1.64157576e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [2.40033116e-04 9.99759967e-01]\n",
      "El cliente  Escritura con probabilidad [0.2008262 0.7991738]\n",
      "El cliente  Escritura con probabilidad [0.02986913 0.97013087]\n",
      "El cliente  Escritura con probabilidad [9.32365197e-06 9.99990676e-01]\n",
      "El cliente  Escritura con probabilidad [0.03502458 0.96497542]\n",
      "El cliente  Escritura con probabilidad [0.07599041 0.92400959]\n",
      "El cliente  Escritura con probabilidad [4.44654000e-08 9.99999956e-01]\n",
      "El cliente  Escritura con probabilidad [0.10884856 0.89115144]\n",
      "El cliente  Escritura con probabilidad [0.10021244 0.89978756]\n",
      "El cliente  Escritura con probabilidad [0.09521546 0.90478454]\n",
      "El cliente  Escritura con probabilidad [0.08061841 0.91938159]\n",
      "El cliente  Escritura con probabilidad [0.43149967 0.56850033]\n",
      "El cliente  Escritura con probabilidad [0.07451483 0.92548517]\n",
      "El cliente  Escritura con probabilidad [0.06096131 0.93903869]\n",
      "El cliente  Escritura con probabilidad [0.20383708 0.79616292]\n",
      "El cliente  Escritura con probabilidad [0.03293343 0.96706657]\n",
      "El cliente  Escritura con probabilidad [0.26859396 0.73140604]\n",
      "El cliente  Escritura con probabilidad [0.02346481 0.97653519]\n",
      "El cliente  Escritura con probabilidad [0.24855331 0.75144669]\n",
      "El cliente  Escritura con probabilidad [0.08333685 0.91666315]\n",
      "El cliente  Escritura con probabilidad [1.26566880e-07 9.99999873e-01]\n",
      "El cliente  Escritura con probabilidad [0.17532567 0.82467433]\n",
      "El cliente  Escritura con probabilidad [0.18779001 0.81220999]\n",
      "El cliente  Escritura con probabilidad [0.13358628 0.86641372]\n",
      "El cliente  Escritura con probabilidad [0.02508017 0.97491983]\n",
      "El cliente  Escritura con probabilidad [0.10909141 0.89090859]\n",
      "El cliente  Escritura con probabilidad [0.23905264 0.76094736]\n",
      "El cliente  Escritura con probabilidad [0.07490205 0.92509795]\n",
      "El cliente  Escritura con probabilidad [0.0065402 0.9934598]\n",
      "El cliente  Escritura con probabilidad [1.03435438e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.00586016 0.99413984]\n",
      "El cliente  Escritura con probabilidad [0.00159599 0.99840401]\n",
      "El cliente  Escritura con probabilidad [0.16400298 0.83599702]\n",
      "El cliente  Escritura con probabilidad [5.35434682e-06 9.99994646e-01]\n",
      "El cliente  Escritura con probabilidad [1.59281721e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.0917884 0.9082116]\n",
      "El cliente  Escritura con probabilidad [0.00109581 0.99890419]\n",
      "El cliente  Escritura con probabilidad [0.23246671 0.76753329]\n",
      "El cliente  Escritura con probabilidad [0.02572575 0.97427425]\n",
      "El cliente  Escritura con probabilidad [0.03694438 0.96305562]\n",
      "El cliente  Escritura con probabilidad [0.29220956 0.70779044]\n",
      "El cliente  Escritura con probabilidad [0.17410282 0.82589718]\n",
      "El cliente  Escritura con probabilidad [0.07582193 0.92417807]\n",
      "El cliente  Escritura con probabilidad [6.00209573e-08 9.99999940e-01]\n",
      "El cliente  Escritura con probabilidad [0.30372922 0.69627078]\n",
      "El cliente  Escritura con probabilidad [0.21570335 0.78429665]\n",
      "El cliente  Escritura con probabilidad [0.24294913 0.75705087]\n",
      "El cliente  Escritura con probabilidad [0.20432804 0.79567196]\n",
      "El cliente  Escritura con probabilidad [0.06866686 0.93133314]\n",
      "El cliente  Escritura con probabilidad [0.26430815 0.73569185]\n",
      "El cliente  Escritura con probabilidad [0.08022448 0.91977552]\n",
      "El cliente  Escritura con probabilidad [0.01555187 0.98444813]\n",
      "El cliente  Escritura con probabilidad [0.0580232 0.9419768]\n",
      "El cliente  Escritura con probabilidad [0.15580261 0.84419739]\n",
      "El cliente  Escritura con probabilidad [7.27281764e-08 9.99999927e-01]\n",
      "El cliente  Escritura con probabilidad [0.23295312 0.76704688]\n",
      "El cliente  Escritura con probabilidad [0.10221931 0.89778069]\n",
      "El cliente  Escritura con probabilidad [0.06189414 0.93810586]\n",
      "El cliente  Escritura con probabilidad [0.1195539 0.8804461]\n",
      "El cliente  Escritura con probabilidad [0.22232893 0.77767107]\n",
      "El cliente  Escritura con probabilidad [0.19939537 0.80060463]\n",
      "El cliente  Escritura con probabilidad [0.11607301 0.88392699]\n",
      "El cliente  Escritura con probabilidad [0.06178513 0.93821487]\n",
      "El cliente  Escritura con probabilidad [4.15552037e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.19199675 0.80800325]\n",
      "El cliente  Escritura con probabilidad [0.22682388 0.77317612]\n",
      "El cliente  Escritura con probabilidad [0.18162312 0.81837688]\n",
      "El cliente  Escritura con probabilidad [0.01512936 0.98487064]\n",
      "El cliente  Escritura con probabilidad [0.09391604 0.90608396]\n",
      "El cliente  Escritura con probabilidad [8.09685652e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17212573 0.82787427]\n",
      "El cliente  Escritura con probabilidad [1.53529808e-07 9.99999846e-01]\n",
      "El cliente  Escritura con probabilidad [0.08161926 0.91838074]\n",
      "El cliente  Escritura con probabilidad [0.09178975 0.90821025]\n",
      "El cliente  Escritura con probabilidad [0.07074787 0.92925213]\n",
      "El cliente  Escritura con probabilidad [3.91402251e-08 9.99999961e-01]\n",
      "El cliente  Escritura con probabilidad [0.27783165 0.72216835]\n",
      "El cliente  Escritura con probabilidad [0.09064519 0.90935481]\n",
      "El cliente  Escritura con probabilidad [0.16396003 0.83603997]\n",
      "El cliente  Escritura con probabilidad [0.0524934 0.9475066]\n",
      "El cliente  Escritura con probabilidad [8.01355056e-04 9.99198645e-01]\n",
      "El cliente  Escritura con probabilidad [3.42170736e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.09609718 0.90390282]\n",
      "El cliente  Escritura con probabilidad [0.15490458 0.84509542]\n",
      "El cliente  Escritura con probabilidad [0.04706029 0.95293971]\n",
      "El cliente  Escritura con probabilidad [0.27515974 0.72484026]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.04978431 0.95021569]\n",
      "El cliente  Escritura con probabilidad [7.05514304e-08 9.99999929e-01]\n",
      "El cliente  Escritura con probabilidad [0.11710081 0.88289919]\n",
      "El cliente  Escritura con probabilidad [2.02023526e-06 9.99997980e-01]\n",
      "El cliente  Escritura con probabilidad [0.10978333 0.89021667]\n",
      "El cliente  Escritura con probabilidad [0.11421735 0.88578265]\n",
      "El cliente  Escritura con probabilidad [0.12331768 0.87668232]\n",
      "El cliente  Escritura con probabilidad [3.88926356e-07 9.99999611e-01]\n",
      "El cliente  Escritura con probabilidad [0.02219359 0.97780641]\n",
      "El cliente  Escritura con probabilidad [0.14054942 0.85945058]\n",
      "El cliente  Escritura con probabilidad [0.13145315 0.86854685]\n",
      "El cliente  Escritura con probabilidad [0.09883853 0.90116147]\n",
      "El cliente  Escritura con probabilidad [0.10834979 0.89165021]\n",
      "El cliente  Escritura con probabilidad [4.20330437e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.02493007 0.97506993]\n",
      "El cliente  Escritura con probabilidad [0.02397578 0.97602422]\n",
      "El cliente  Escritura con probabilidad [3.84141811e-08 9.99999962e-01]\n",
      "El cliente  Escritura con probabilidad [0.16739066 0.83260934]\n",
      "El cliente  Escritura con probabilidad [0.1389725 0.8610275]\n",
      "El cliente  Escritura con probabilidad [7.56735868e-04 9.99243264e-01]\n",
      "El cliente  Escritura con probabilidad [0.21398549 0.78601451]\n",
      "El cliente  Escritura con probabilidad [0.24110334 0.75889666]\n",
      "El cliente  Escritura con probabilidad [0.13469544 0.86530456]\n",
      "El cliente  Escritura con probabilidad [0.01731246 0.98268754]\n",
      "El cliente  Escritura con probabilidad [0.26635328 0.73364672]\n",
      "El cliente  Escritura con probabilidad [0.07431866 0.92568134]\n",
      "El cliente  Escritura con probabilidad [0.08678666 0.91321334]\n",
      "El cliente  Escritura con probabilidad [0.19513117 0.80486883]\n",
      "El cliente  Escritura con probabilidad [0.1224283 0.8775717]\n",
      "El cliente  Escritura con probabilidad [0.07117276 0.92882724]\n",
      "El cliente  Escritura con probabilidad [0.14278326 0.85721674]\n",
      "El cliente  Escritura con probabilidad [0.08024344 0.91975656]\n",
      "El cliente  Escritura con probabilidad [0.42285427 0.57714573]\n",
      "El cliente  Escritura con probabilidad [0.12114947 0.87885053]\n",
      "El cliente  Escritura con probabilidad [0.10361889 0.89638111]\n",
      "El cliente  Escritura con probabilidad [0.045424 0.954576]\n",
      "El cliente  Escritura con probabilidad [1.61279846e-05 9.99983872e-01]\n",
      "El cliente  Escritura con probabilidad [0.16163127 0.83836873]\n",
      "El cliente  Escritura con probabilidad [0.06209454 0.93790546]\n",
      "El cliente  Escritura con probabilidad [5.79680849e-07 9.99999420e-01]\n",
      "El cliente  Escritura con probabilidad [0.22112099 0.77887901]\n",
      "El cliente  Escritura con probabilidad [0.12444768 0.87555232]\n",
      "El cliente  Escritura con probabilidad [9.27988374e-05 9.99907201e-01]\n",
      "El cliente  Escritura con probabilidad [0.05032388 0.94967612]\n",
      "El cliente  Escritura con probabilidad [0.2079309 0.7920691]\n",
      "El cliente  Escritura con probabilidad [0.12910227 0.87089773]\n",
      "El cliente  Escritura con probabilidad [6.95480339e-05 9.99930452e-01]\n",
      "El cliente  Escritura con probabilidad [0.09767958 0.90232042]\n",
      "El cliente  Escritura con probabilidad [0.31638729 0.68361271]\n",
      "El cliente  Escritura con probabilidad [0.03167445 0.96832555]\n",
      "El cliente  Escritura con probabilidad [0.06232406 0.93767594]\n",
      "El cliente  Escritura con probabilidad [0.0661587 0.9338413]\n",
      "El cliente  Escritura con probabilidad [0.181737 0.818263]\n",
      "El cliente  Escritura con probabilidad [0.0897008 0.9102992]\n",
      "El cliente  Escritura con probabilidad [4.03761247e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.28492836e-07 9.99999872e-01]\n",
      "El cliente  Escritura con probabilidad [0.19086519 0.80913481]\n",
      "El cliente  Escritura con probabilidad [0.38137905 0.61862095]\n",
      "El cliente  Escritura con probabilidad [0.23192975 0.76807025]\n",
      "El cliente  Escritura con probabilidad [0.26380182 0.73619818]\n",
      "El cliente  Escritura con probabilidad [0.07588096 0.92411904]\n",
      "El cliente  Escritura con probabilidad [0.1045703 0.8954297]\n",
      "El cliente  Escritura con probabilidad [0.09762796 0.90237204]\n",
      "El cliente  Escritura con probabilidad [0.17644396 0.82355604]\n",
      "El cliente  Escritura con probabilidad [3.57334351e-05 9.99964267e-01]\n",
      "El cliente  Escritura con probabilidad [1.04083815e-04 9.99895916e-01]\n",
      "El cliente  Escritura con probabilidad [0.08329848 0.91670152]\n",
      "El cliente  Escritura con probabilidad [1.52577062e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17672646 0.82327354]\n",
      "El cliente  Escritura con probabilidad [0.22829934 0.77170066]\n",
      "El cliente  Escritura con probabilidad [0.23349964 0.76650036]\n",
      "El cliente  Escritura con probabilidad [0.14514444 0.85485556]\n",
      "El cliente  Escritura con probabilidad [0.15694411 0.84305589]\n",
      "El cliente  Escritura con probabilidad [0.20268906 0.79731094]\n",
      "El cliente  Escritura con probabilidad [0.06187136 0.93812864]\n",
      "El cliente  Escritura con probabilidad [0.20449432 0.79550568]\n",
      "El cliente  Escritura con probabilidad [5.08612041e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [2.37511251e-05 9.99976249e-01]\n",
      "El cliente  Escritura con probabilidad [0.01826602 0.98173398]\n",
      "El cliente  Escritura con probabilidad [0.10895795 0.89104205]\n",
      "El cliente  Escritura con probabilidad [3.76438892e-08 9.99999962e-01]\n",
      "El cliente  Escritura con probabilidad [5.20616883e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.08614414 0.91385586]\n",
      "El cliente  Escritura con probabilidad [0.1456964 0.8543036]\n",
      "El cliente  Escritura con probabilidad [0.25856927 0.74143073]\n",
      "El cliente  Escritura con probabilidad [0.00128825 0.99871175]\n",
      "El cliente  Escritura con probabilidad [0.04873037 0.95126963]\n",
      "El cliente  Escritura con probabilidad [3.81960935e-05 9.99961804e-01]\n",
      "El cliente  Escritura con probabilidad [0.31566053 0.68433947]\n",
      "El cliente  Escritura con probabilidad [0.0286985 0.9713015]\n",
      "El cliente  Escritura con probabilidad [8.28452625e-06 9.99991715e-01]\n",
      "El cliente  Escritura con probabilidad [0.44032361 0.55967639]\n",
      "El cliente  Escritura con probabilidad [0.19302015 0.80697985]\n",
      "El cliente  Escritura con probabilidad [2.74553883e-04 9.99725446e-01]\n",
      "El cliente  Escritura con probabilidad [0.22340571 0.77659429]\n",
      "El cliente  Escritura con probabilidad [0.05250613 0.94749387]\n",
      "El cliente  Escritura con probabilidad [4.46866766e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.22559799 0.77440201]\n",
      "El cliente  Escritura con probabilidad [9.61466664e-06 9.99990385e-01]\n",
      "El cliente  Escritura con probabilidad [0.02143837 0.97856163]\n",
      "El cliente  Escritura con probabilidad [0.01313299 0.98686701]\n",
      "El cliente  Escritura con probabilidad [0.02300506 0.97699494]\n",
      "El cliente  Escritura con probabilidad [0.26065631 0.73934369]\n",
      "El cliente  Escritura con probabilidad [0.1806387 0.8193613]\n",
      "El cliente  Escritura con probabilidad [0.01512083 0.98487917]\n",
      "El cliente  Escritura con probabilidad [4.17310758e-05 9.99958269e-01]\n",
      "El cliente  Escritura con probabilidad [0.04709735 0.95290265]\n",
      "El cliente  Escritura con probabilidad [0.18612403 0.81387597]\n",
      "El cliente  Escritura con probabilidad [0.02686526 0.97313474]\n",
      "El cliente  Escritura con probabilidad [0.04457566 0.95542434]\n",
      "El cliente  Escritura con probabilidad [2.33891420e-05 9.99976611e-01]\n",
      "El cliente  Escritura con probabilidad [0.04873185 0.95126815]\n",
      "El cliente  Escritura con probabilidad [5.74668909e-04 9.99425331e-01]\n",
      "El cliente  Escritura con probabilidad [0.04463095 0.95536905]\n",
      "El cliente  Escritura con probabilidad [0.25348637 0.74651363]\n",
      "El cliente  Escritura con probabilidad [2.55419539e-05 9.99974458e-01]\n",
      "El cliente  Escritura con probabilidad [0.17890045 0.82109955]\n",
      "El cliente  Escritura con probabilidad [0.11545817 0.88454183]\n",
      "El cliente  Escritura con probabilidad [0.15556619 0.84443381]\n",
      "El cliente  Escritura con probabilidad [0.25422427 0.74577573]\n",
      "El cliente  Escritura con probabilidad [2.73588385e-06 9.99997264e-01]\n",
      "El cliente  Escritura con probabilidad [0.2374258 0.7625742]\n",
      "El cliente  Escritura con probabilidad [3.26189921e-06 9.99996738e-01]\n",
      "El cliente  Escritura con probabilidad [0.26553247 0.73446753]\n",
      "El cliente  Escritura con probabilidad [0.25170139 0.74829861]\n",
      "El cliente  Escritura con probabilidad [0.20831575 0.79168425]\n",
      "El cliente  Escritura con probabilidad [0.17805842 0.82194158]\n",
      "El cliente  Escritura con probabilidad [0.10139846 0.89860154]\n",
      "El cliente  Escritura con probabilidad [0.008924 0.991076]\n",
      "El cliente  Escritura con probabilidad [0.25237806 0.74762194]\n",
      "El cliente  Escritura con probabilidad [0.19465739 0.80534261]\n",
      "El cliente  Escritura con probabilidad [0.00299548 0.99700452]\n",
      "El cliente  Escritura con probabilidad [0.23460169 0.76539831]\n",
      "El cliente  Escritura con probabilidad [0.00769312 0.99230688]\n",
      "El cliente  Escritura con probabilidad [0.007021 0.992979]\n",
      "El cliente  Escritura con probabilidad [3.98363431e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.19794195 0.80205805]\n",
      "El cliente  Escritura con probabilidad [8.29589897e-04 9.99170410e-01]\n",
      "El cliente  Escritura con probabilidad [0.04837264 0.95162736]\n",
      "El cliente  Escritura con probabilidad [0.01536242 0.98463758]\n",
      "El cliente  Escritura con probabilidad [0.08078953 0.91921047]\n",
      "El cliente  Escritura con probabilidad [6.04206440e-09 9.99999994e-01]\n",
      "El cliente  Escritura con probabilidad [0.17459265 0.82540735]\n",
      "El cliente  Escritura con probabilidad [0.22964253 0.77035747]\n",
      "El cliente  Escritura con probabilidad [0.29407206 0.70592794]\n",
      "El cliente  Escritura con probabilidad [3.91073218e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.03661135 0.96338865]\n",
      "El cliente  Escritura con probabilidad [0.21542212 0.78457788]\n",
      "El cliente  Escritura con probabilidad [0.23272931 0.76727069]\n",
      "El cliente  Escritura con probabilidad [0.07730703 0.92269297]\n",
      "El cliente  Escritura con probabilidad [0.17501341 0.82498659]\n",
      "El cliente  Escritura con probabilidad [0.18525178 0.81474822]\n",
      "El cliente  Escritura con probabilidad [0.00719436 0.99280564]\n",
      "El cliente  Escritura con probabilidad [0.20388975 0.79611025]\n",
      "El cliente  Escritura con probabilidad [1.14144546e-05 9.99988586e-01]\n",
      "El cliente  Escritura con probabilidad [0.18223499 0.81776501]\n",
      "El cliente  Escritura con probabilidad [0.31809523 0.68190477]\n",
      "El cliente  Escritura con probabilidad [0.01922613 0.98077387]\n",
      "El cliente  Escritura con probabilidad [0.11345591 0.88654409]\n",
      "El cliente  Escritura con probabilidad [0.25073175 0.74926825]\n",
      "El cliente  Escritura con probabilidad [0.19138028 0.80861972]\n",
      "El cliente  Escritura con probabilidad [0.3941866 0.6058134]\n",
      "El cliente  Escritura con probabilidad [0.05282944 0.94717056]\n",
      "El cliente  Escritura con probabilidad [0.08593668 0.91406332]\n",
      "El cliente  Escritura con probabilidad [6.56959082e-08 9.99999934e-01]\n",
      "El cliente  Escritura con probabilidad [5.13478298e-07 9.99999487e-01]\n",
      "El cliente  Escritura con probabilidad [0.21832828 0.78167172]\n",
      "El cliente  Escritura con probabilidad [0.01075228 0.98924772]\n",
      "El cliente  Escritura con probabilidad [0.31511473 0.68488527]\n",
      "El cliente  Escritura con probabilidad [3.14518113e-05 9.99968548e-01]\n",
      "El cliente  Escritura con probabilidad [0.02642835 0.97357165]\n",
      "El cliente  Escritura con probabilidad [0.15590179 0.84409821]\n",
      "El cliente  Escritura con probabilidad [0.06790417 0.93209583]\n",
      "El cliente  Escritura con probabilidad [0.20044421 0.79955579]\n",
      "El cliente  Escritura con probabilidad [1.49827590e-06 9.99998502e-01]\n",
      "El cliente  Escritura con probabilidad [0.24442694 0.75557306]\n",
      "El cliente  Escritura con probabilidad [0.17519922 0.82480078]\n",
      "El cliente  Escritura con probabilidad [0.07573737 0.92426263]\n",
      "El cliente  Escritura con probabilidad [0.09898812 0.90101188]\n",
      "El cliente  Escritura con probabilidad [0.02725627 0.97274373]\n",
      "El cliente  Escritura con probabilidad [0.09620389 0.90379611]\n",
      "El cliente  Escritura con probabilidad [0.13478812 0.86521188]\n",
      "El cliente  Escritura con probabilidad [0.11981128 0.88018872]\n",
      "El cliente  Escritura con probabilidad [0.06038616 0.93961384]\n",
      "El cliente  Escritura con probabilidad [0.05016985 0.94983015]\n",
      "El cliente  Escritura con probabilidad [1.22450885e-04 9.99877549e-01]\n",
      "El cliente  Escritura con probabilidad [0.01475407 0.98524593]\n",
      "El cliente  Escritura con probabilidad [1.31006317e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.05982939 0.94017061]\n",
      "El cliente  Escritura con probabilidad [5.76034775e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.2106519 0.7893481]\n",
      "El cliente  Escritura con probabilidad [0.139894 0.860106]\n",
      "El cliente  Escritura con probabilidad [0.07496604 0.92503396]\n",
      "El cliente  Escritura con probabilidad [1.94543278e-06 9.99998055e-01]\n",
      "El cliente  Escritura con probabilidad [0.31566053 0.68433947]\n",
      "El cliente  Escritura con probabilidad [0.07207968 0.92792032]\n",
      "El cliente  Escritura con probabilidad [0.14274471 0.85725529]\n",
      "El cliente  Escritura con probabilidad [0.0679086 0.9320914]\n",
      "El cliente  Escritura con probabilidad [0.05742878 0.94257122]\n",
      "El cliente  Escritura con probabilidad [0.2398495 0.7601505]\n",
      "El cliente  Escritura con probabilidad [0.01591846 0.98408154]\n",
      "El cliente  Escritura con probabilidad [0.18116755 0.81883245]\n",
      "El cliente  Escritura con probabilidad [4.11006689e-05 9.99958899e-01]\n",
      "El cliente  Escritura con probabilidad [0.02121073 0.97878927]\n",
      "El cliente  Escritura con probabilidad [1.25708333e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.06321161 0.93678839]\n",
      "El cliente  Escritura con probabilidad [5.41322542e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.15022882 0.84977118]\n",
      "El cliente  Escritura con probabilidad [0.18435173 0.81564827]\n",
      "El cliente  Escritura con probabilidad [0.01667304 0.98332696]\n",
      "El cliente  Escritura con probabilidad [0.01461714 0.98538286]\n",
      "El cliente  Escritura con probabilidad [0.0730613 0.9269387]\n",
      "El cliente  Escritura con probabilidad [0.22775202 0.77224798]\n",
      "El cliente  Escritura con probabilidad [0.23727889 0.76272111]\n",
      "El cliente  Escritura con probabilidad [3.85848854e-04 9.99614151e-01]\n",
      "El cliente  Escritura con probabilidad [2.26358132e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.21411455 0.78588545]\n",
      "El cliente  Escritura con probabilidad [6.44297486e-06 9.99993557e-01]\n",
      "El cliente  Escritura con probabilidad [0.18654151 0.81345849]\n",
      "El cliente  Escritura con probabilidad [0.22068948 0.77931052]\n",
      "El cliente  Escritura con probabilidad [0.05310496 0.94689504]\n",
      "El cliente  Escritura con probabilidad [0.19864326 0.80135674]\n",
      "El cliente  Escritura con probabilidad [0.11469593 0.88530407]\n",
      "El cliente  Escritura con probabilidad [0.01571308 0.98428692]\n",
      "El cliente  Escritura con probabilidad [0.00100245 0.99899755]\n",
      "El cliente  Escritura con probabilidad [0.1578861 0.8421139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.04436192 0.95563808]\n",
      "El cliente  Escritura con probabilidad [0.08109278 0.91890722]\n",
      "El cliente  Escritura con probabilidad [0.10044956 0.89955044]\n",
      "El cliente  Escritura con probabilidad [0.14786378 0.85213622]\n",
      "El cliente  Escritura con probabilidad [0.24245903 0.75754097]\n",
      "El cliente  Escritura con probabilidad [0.17805798 0.82194202]\n",
      "El cliente  Escritura con probabilidad [0.08158314 0.91841686]\n",
      "El cliente  Escritura con probabilidad [0.10486573 0.89513427]\n",
      "El cliente  Escritura con probabilidad [0.14640586 0.85359414]\n",
      "El cliente  Escritura con probabilidad [0.22284733 0.77715267]\n",
      "El cliente  Escritura con probabilidad [0.02366008 0.97633992]\n",
      "El cliente  Escritura con probabilidad [0.21200252 0.78799748]\n",
      "El cliente  Escritura con probabilidad [0.10173234 0.89826766]\n",
      "El cliente  Escritura con probabilidad [0.0614458 0.9385542]\n",
      "El cliente  Escritura con probabilidad [0.11363748 0.88636252]\n",
      "El cliente  Escritura con probabilidad [0.26219848 0.73780152]\n",
      "El cliente  Escritura con probabilidad [0.15902593 0.84097407]\n",
      "El cliente  Escritura con probabilidad [0.14316266 0.85683734]\n",
      "El cliente  Escritura con probabilidad [0.02795282 0.97204718]\n",
      "El cliente  Escritura con probabilidad [0.0062562 0.9937438]\n",
      "El cliente  Escritura con probabilidad [0.25379145 0.74620855]\n",
      "El cliente  Escritura con probabilidad [0.24545824 0.75454176]\n",
      "El cliente  Escritura con probabilidad [0.16419455 0.83580545]\n",
      "El cliente  Escritura con probabilidad [0.00300856 0.99699144]\n",
      "El cliente  Escritura con probabilidad [0.17050828 0.82949172]\n",
      "El cliente  Escritura con probabilidad [2.12952198e-06 9.99997870e-01]\n",
      "El cliente  Escritura con probabilidad [3.46177547e-05 9.99965382e-01]\n",
      "El cliente  Escritura con probabilidad [0.20851509 0.79148491]\n",
      "El cliente  Escritura con probabilidad [0.17342359 0.82657641]\n",
      "El cliente  Escritura con probabilidad [0.02052549 0.97947451]\n",
      "El cliente  Escritura con probabilidad [0.10127827 0.89872173]\n",
      "El cliente  Escritura con probabilidad [0.13038101 0.86961899]\n",
      "El cliente  Escritura con probabilidad [0.05675355 0.94324645]\n",
      "El cliente  Escritura con probabilidad [0.03252498 0.96747502]\n",
      "El cliente  Escritura con probabilidad [0.2415291 0.7584709]\n",
      "El cliente  Escritura con probabilidad [0.15579864 0.84420136]\n",
      "El cliente  Escritura con probabilidad [0.11833919 0.88166081]\n",
      "El cliente  Escritura con probabilidad [0.14201543 0.85798457]\n",
      "El cliente  Escritura con probabilidad [0.12593466 0.87406534]\n",
      "El cliente  Escritura con probabilidad [0.0481511 0.9518489]\n",
      "El cliente  Escritura con probabilidad [0.09034649 0.90965351]\n",
      "El cliente  Escritura con probabilidad [0.16467191 0.83532809]\n",
      "El cliente  Escritura con probabilidad [0.17566985 0.82433015]\n",
      "El cliente  Escritura con probabilidad [0.10075762 0.89924238]\n",
      "El cliente  Escritura con probabilidad [0.22104484 0.77895516]\n",
      "El cliente  Escritura con probabilidad [0.25106887 0.74893113]\n",
      "El cliente  Escritura con probabilidad [0.03740041 0.96259959]\n",
      "El cliente  Escritura con probabilidad [0.35159026 0.64840974]\n",
      "El cliente  Escritura con probabilidad [0.19476236 0.80523764]\n",
      "El cliente  Escritura con probabilidad [0.13163844 0.86836156]\n",
      "El cliente  Escritura con probabilidad [1.06531672e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [2.25594898e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.17313862 0.82686138]\n",
      "El cliente  Escritura con probabilidad [6.81284140e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [9.98555547e-06 9.99990014e-01]\n",
      "El cliente  Escritura con probabilidad [0.00612677 0.99387323]\n",
      "El cliente  Escritura con probabilidad [0.06900282 0.93099718]\n",
      "El cliente  Escritura con probabilidad [8.97233399e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.11976742 0.88023258]\n",
      "El cliente  Escritura con probabilidad [0.02403303 0.97596697]\n",
      "El cliente  Escritura con probabilidad [0.21011494 0.78988506]\n",
      "El cliente  Escritura con probabilidad [3.67894493e-07 9.99999632e-01]\n",
      "El cliente  Escritura con probabilidad [0.2253374 0.7746626]\n",
      "El cliente  Escritura con probabilidad [0.26619973 0.73380027]\n",
      "El cliente  Escritura con probabilidad [7.57741885e-04 9.99242258e-01]\n",
      "El cliente  Escritura con probabilidad [0.09470934 0.90529066]\n",
      "El cliente  Escritura con probabilidad [0.06273292 0.93726708]\n",
      "El cliente  Escritura con probabilidad [0.06526842 0.93473158]\n",
      "El cliente  Escritura con probabilidad [0.18721096 0.81278904]\n",
      "El cliente  Escritura con probabilidad [0.05316187 0.94683813]\n",
      "El cliente  Escritura con probabilidad [0.15587923 0.84412077]\n",
      "El cliente  Escritura con probabilidad [0.24175026 0.75824974]\n",
      "El cliente  Escritura con probabilidad [0.23768573 0.76231427]\n",
      "El cliente  Escritura con probabilidad [0.01799388 0.98200612]\n",
      "El cliente  Escritura con probabilidad [0.35426581 0.64573419]\n",
      "El cliente  Escritura con probabilidad [0.19098905 0.80901095]\n",
      "El cliente  Escritura con probabilidad [0.22897818 0.77102182]\n",
      "El cliente  Escritura con probabilidad [0.19695606 0.80304394]\n",
      "El cliente  Escritura con probabilidad [1.75519406e-06 9.99998245e-01]\n",
      "El cliente  Escritura con probabilidad [0.13515819 0.86484181]\n",
      "El cliente  Escritura con probabilidad [0.19899887 0.80100113]\n",
      "El cliente  Escritura con probabilidad [0.08349832 0.91650168]\n",
      "El cliente  Escritura con probabilidad [0.09405131 0.90594869]\n",
      "El cliente  Escritura con probabilidad [0.00710084 0.99289916]\n",
      "El cliente  Escritura con probabilidad [0.15884153 0.84115847]\n",
      "El cliente  Escritura con probabilidad [0.05816834 0.94183166]\n",
      "El cliente  Escritura con probabilidad [0.02507184 0.97492816]\n",
      "El cliente  Escritura con probabilidad [0.00340538 0.99659462]\n",
      "El cliente  Escritura con probabilidad [0.03514255 0.96485745]\n",
      "El cliente  Escritura con probabilidad [2.87707058e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.04523443 0.95476557]\n",
      "El cliente  Escritura con probabilidad [0.08431978 0.91568022]\n",
      "El cliente  Escritura con probabilidad [0.43866631 0.56133369]\n",
      "El cliente  Escritura con probabilidad [0.12269248 0.87730752]\n",
      "El cliente  Escritura con probabilidad [0.2357433 0.7642567]\n",
      "El cliente  Escritura con probabilidad [0.07396998 0.92603002]\n",
      "El cliente  Escritura con probabilidad [0.07121223 0.92878777]\n",
      "El cliente  Escritura con probabilidad [0.03795827 0.96204173]\n",
      "El cliente  Escritura con probabilidad [0.06729377 0.93270623]\n",
      "El cliente  Escritura con probabilidad [0.11120155 0.88879845]\n",
      "El cliente  Escritura con probabilidad [0.02739441 0.97260559]\n",
      "El cliente  Escritura con probabilidad [5.73334020e-06 9.99994267e-01]\n",
      "El cliente  Escritura con probabilidad [0.10602931 0.89397069]\n",
      "El cliente  Escritura con probabilidad [0.07927012 0.92072988]\n",
      "El cliente  Escritura con probabilidad [0.22608924 0.77391076]\n",
      "El cliente  Escritura con probabilidad [0.28489084 0.71510916]\n",
      "El cliente  Escritura con probabilidad [0.22042216 0.77957784]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.13624295 0.86375705]\n",
      "El cliente  Escritura con probabilidad [8.33829815e-05 9.99916617e-01]\n",
      "El cliente  Escritura con probabilidad [0.00136696 0.99863304]\n",
      "El cliente  Escritura con probabilidad [0.05834203 0.94165797]\n",
      "El cliente  Escritura con probabilidad [0.29126958 0.70873042]\n",
      "El cliente  Escritura con probabilidad [0.25784545 0.74215455]\n",
      "El cliente  Escritura con probabilidad [0.13448505 0.86551495]\n",
      "El cliente  Escritura con probabilidad [0.08020906 0.91979094]\n",
      "El cliente  Escritura con probabilidad [0.00403238 0.99596762]\n",
      "El cliente  Escritura con probabilidad [0.22808308 0.77191692]\n",
      "El cliente  Escritura con probabilidad [0.31495154 0.68504846]\n",
      "El cliente  Escritura con probabilidad [0.10517346 0.89482654]\n",
      "El cliente  Escritura con probabilidad [0.19805357 0.80194643]\n",
      "El cliente  Escritura con probabilidad [0.00962387 0.99037613]\n",
      "El cliente  Escritura con probabilidad [0.16871739 0.83128261]\n",
      "El cliente  Escritura con probabilidad [0.09063722 0.90936278]\n",
      "El cliente  Escritura con probabilidad [0.18001931 0.81998069]\n",
      "El cliente  Escritura con probabilidad [0.10017842 0.89982158]\n",
      "El cliente  Escritura con probabilidad [1.52241064e-05 9.99984776e-01]\n",
      "El cliente  Escritura con probabilidad [0.43034992 0.56965008]\n",
      "El cliente  Escritura con probabilidad [2.84184454e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.07545242 0.92454758]\n",
      "El cliente  Escritura con probabilidad [0.160005 0.839995]\n",
      "El cliente  Escritura con probabilidad [0.04409866 0.95590134]\n",
      "El cliente  Escritura con probabilidad [0.32385562 0.67614438]\n",
      "El cliente  Escritura con probabilidad [0.1656223 0.8343777]\n",
      "El cliente  Escritura con probabilidad [0.01971539 0.98028461]\n",
      "El cliente  Escritura con probabilidad [0.42853148 0.57146852]\n",
      "El cliente  Escritura con probabilidad [0.01427903 0.98572097]\n",
      "El cliente  Escritura con probabilidad [0.05057551 0.94942449]\n",
      "El cliente  Escritura con probabilidad [0.0843004 0.9156996]\n",
      "El cliente  Escritura con probabilidad [0.14587296 0.85412704]\n",
      "El cliente  Escritura con probabilidad [0.14431575 0.85568425]\n",
      "El cliente  Escritura con probabilidad [0.01796692 0.98203308]\n",
      "El cliente  Escritura con probabilidad [0.35994537 0.64005463]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.26830595 0.73169405]\n",
      "El cliente  Escritura con probabilidad [0.07189802 0.92810198]\n",
      "El cliente  Escritura con probabilidad [0.01442862 0.98557138]\n",
      "El cliente  Escritura con probabilidad [2.15759921e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.02456791 0.97543209]\n",
      "El cliente  Escritura con probabilidad [0.26530703 0.73469297]\n",
      "El cliente  Escritura con probabilidad [0.14842113 0.85157887]\n",
      "El cliente  Escritura con probabilidad [0.14639429 0.85360571]\n",
      "El cliente  Escritura con probabilidad [4.80238333e-05 9.99951976e-01]\n",
      "El cliente  Escritura con probabilidad [0.22458185 0.77541815]\n",
      "El cliente  Escritura con probabilidad [0.2632873 0.7367127]\n",
      "El cliente  Escritura con probabilidad [0.22525221 0.77474779]\n",
      "El cliente  Escritura con probabilidad [0.0849565 0.9150435]\n",
      "El cliente  Escritura con probabilidad [0.30527224 0.69472776]\n",
      "El cliente  Escritura con probabilidad [0.20715975 0.79284025]\n",
      "El cliente  Escritura con probabilidad [0.13433294 0.86566706]\n",
      "El cliente  Escritura con probabilidad [0.30354052 0.69645948]\n",
      "El cliente  Escritura con probabilidad [0.27323978 0.72676022]\n",
      "El cliente  Escritura con probabilidad [0.0878468 0.9121532]\n",
      "El cliente  Escritura con probabilidad [0.07090674 0.92909326]\n",
      "El cliente  Escritura con probabilidad [0.2100934 0.7899066]\n",
      "El cliente  Escritura con probabilidad [0.21073347 0.78926653]\n",
      "El cliente  Escritura con probabilidad [0.42131119 0.57868881]\n",
      "El cliente  Escritura con probabilidad [7.27174778e-06 9.99992728e-01]\n",
      "El cliente  Escritura con probabilidad [0.22112589 0.77887411]\n",
      "El cliente  Escritura con probabilidad [0.09268733 0.90731267]\n",
      "El cliente  Escritura con probabilidad [4.61437154e-06 9.99995386e-01]\n",
      "El cliente  Escritura con probabilidad [0.17358022 0.82641978]\n",
      "El cliente  Escritura con probabilidad [0.22419832 0.77580168]\n",
      "El cliente  Escritura con probabilidad [0.1920446 0.8079554]\n",
      "El cliente  Escritura con probabilidad [0.01933511 0.98066489]\n",
      "El cliente  Escritura con probabilidad [0.00462561 0.99537439]\n",
      "El cliente  Escritura con probabilidad [0.14491089 0.85508911]\n",
      "El cliente  Escritura con probabilidad [0.24395918 0.75604082]\n",
      "El cliente  Escritura con probabilidad [0.08393406 0.91606594]\n",
      "El cliente  Escritura con probabilidad [0.28060293 0.71939707]\n",
      "El cliente  Escritura con probabilidad [0.22293472 0.77706528]\n",
      "El cliente  Escritura con probabilidad [1.13139122e-06 9.99998869e-01]\n",
      "El cliente  Escritura con probabilidad [0.10636335 0.89363665]\n",
      "El cliente  Escritura con probabilidad [0.02098079 0.97901921]\n",
      "El cliente  Escritura con probabilidad [0.29102479 0.70897521]\n",
      "El cliente  Escritura con probabilidad [0.20106056 0.79893944]\n",
      "El cliente  Escritura con probabilidad [0.19350719 0.80649281]\n",
      "El cliente  Escritura con probabilidad [0.24910779 0.75089221]\n",
      "El cliente  Escritura con probabilidad [0.24278204 0.75721796]\n",
      "El cliente  Escritura con probabilidad [4.03508304e-05 9.99959649e-01]\n",
      "El cliente  Escritura con probabilidad [0.26265246 0.73734754]\n",
      "El cliente  Escritura con probabilidad [0.00967017 0.99032983]\n",
      "El cliente  Escritura con probabilidad [0.20512082 0.79487918]\n",
      "El cliente  Escritura con probabilidad [6.35923235e-07 9.99999364e-01]\n",
      "El cliente  Escritura con probabilidad [0.21139461 0.78860539]\n",
      "El cliente  Escritura con probabilidad [0.3332093 0.6667907]\n",
      "El cliente  Escritura con probabilidad [0.26381803 0.73618197]\n",
      "El cliente  Escritura con probabilidad [2.29836372e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.66342629e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.06885406 0.93114594]\n",
      "El cliente  Escritura con probabilidad [1.83379077e-05 9.99981662e-01]\n",
      "El cliente  Escritura con probabilidad [0.02031335 0.97968665]\n",
      "El cliente  Escritura con probabilidad [0.44580602 0.55419398]\n",
      "El cliente  Escritura con probabilidad [0.01319155 0.98680845]\n",
      "El cliente  Escritura con probabilidad [2.28429053e-04 9.99771571e-01]\n",
      "El cliente  Escritura con probabilidad [1.19335164e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.22119076 0.77880924]\n",
      "El cliente  Escritura con probabilidad [0.14300952 0.85699048]\n",
      "El cliente  Escritura con probabilidad [0.29181106 0.70818894]\n",
      "El cliente  Escritura con probabilidad [0.13469177 0.86530823]\n",
      "El cliente  Escritura con probabilidad [0.14999806 0.85000194]\n",
      "El cliente  Escritura con probabilidad [0.17709624 0.82290376]\n",
      "El cliente  Escritura con probabilidad [0.01815273 0.98184727]\n",
      "El cliente  Escritura con probabilidad [8.15097211e-06 9.99991849e-01]\n",
      "El cliente  Escritura con probabilidad [0.16400015 0.83599985]\n",
      "El cliente  Escritura con probabilidad [0.25187669 0.74812331]\n",
      "El cliente  Escritura con probabilidad [0.18903014 0.81096986]\n",
      "El cliente  Escritura con probabilidad [0.10738906 0.89261094]\n",
      "El cliente  Escritura con probabilidad [0.02850046 0.97149954]\n",
      "El cliente  Escritura con probabilidad [0.07309855 0.92690145]\n",
      "El cliente  Escritura con probabilidad [0.02036814 0.97963186]\n",
      "El cliente  Escritura con probabilidad [0.00178182 0.99821818]\n",
      "El cliente  Escritura con probabilidad [0.00440568 0.99559432]\n",
      "El cliente  Escritura con probabilidad [0.24244441 0.75755559]\n",
      "El cliente  Escritura con probabilidad [0.15375409 0.84624591]\n",
      "El cliente  Escritura con probabilidad [0.23455359 0.76544641]\n",
      "El cliente  Escritura con probabilidad [0.24005065 0.75994935]\n",
      "El cliente  Escritura con probabilidad [0.16747857 0.83252143]\n",
      "El cliente  Escritura con probabilidad [0.24739495 0.75260505]\n",
      "El cliente  Escritura con probabilidad [0.20424205 0.79575795]\n",
      "El cliente  Escritura con probabilidad [0.21886009 0.78113991]\n",
      "El cliente  Escritura con probabilidad [0.18854958 0.81145042]\n",
      "El cliente  Escritura con probabilidad [0.17993218 0.82006782]\n",
      "El cliente  Escritura con probabilidad [0.10529284 0.89470716]\n",
      "El cliente  Escritura con probabilidad [0.01906261 0.98093739]\n",
      "El cliente  Escritura con probabilidad [0.09364487 0.90635513]\n",
      "El cliente  Escritura con probabilidad [0.04369484 0.95630516]\n",
      "El cliente  Escritura con probabilidad [0.04975597 0.95024403]\n",
      "El cliente  Escritura con probabilidad [0.30280053 0.69719947]\n",
      "El cliente  Escritura con probabilidad [0.16467097 0.83532903]\n",
      "El cliente  Escritura con probabilidad [0.17524181 0.82475819]\n",
      "El cliente  Escritura con probabilidad [0.18328064 0.81671936]\n",
      "El cliente  Escritura con probabilidad [0.12423558 0.87576442]\n",
      "El cliente  Escritura con probabilidad [0.03586449 0.96413551]\n",
      "El cliente  Escritura con probabilidad [0.02349406 0.97650594]\n",
      "El cliente  Escritura con probabilidad [0.06485984 0.93514016]\n",
      "El cliente  Escritura con probabilidad [0.21256389 0.78743611]\n",
      "El cliente  Escritura con probabilidad [0.14418886 0.85581114]\n",
      "El cliente  Escritura con probabilidad [1.08342048e-06 9.99998917e-01]\n",
      "El cliente  Escritura con probabilidad [0.14648939 0.85351061]\n",
      "El cliente  Escritura con probabilidad [0.09470393 0.90529607]\n",
      "El cliente  Escritura con probabilidad [0.14474955 0.85525045]\n",
      "El cliente  Escritura con probabilidad [0.21830505 0.78169495]\n",
      "El cliente  Escritura con probabilidad [0.08022456 0.91977544]\n",
      "El cliente  Escritura con probabilidad [0.38986646 0.61013354]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.06127309 0.93872691]\n",
      "El cliente  Escritura con probabilidad [0.25176681 0.74823319]\n",
      "El cliente  Escritura con probabilidad [0.06291697 0.93708303]\n",
      "El cliente  Escritura con probabilidad [0.21275302 0.78724698]\n",
      "El cliente  Escritura con probabilidad [0.25877109 0.74122891]\n",
      "El cliente  Escritura con probabilidad [5.81744471e-04 9.99418256e-01]\n",
      "El cliente  Escritura con probabilidad [4.08221390e-07 9.99999592e-01]\n",
      "El cliente  Escritura con probabilidad [0.13384025 0.86615975]\n",
      "El cliente  Escritura con probabilidad [0.1236301 0.8763699]\n",
      "El cliente  Escritura con probabilidad [0.08007903 0.91992097]\n",
      "El cliente  Escritura con probabilidad [0.13372948 0.86627052]\n",
      "El cliente  Escritura con probabilidad [0.07139933 0.92860067]\n",
      "El cliente  Escritura con probabilidad [0.21161127 0.78838873]\n",
      "El cliente  Escritura con probabilidad [1.00761001e-07 9.99999899e-01]\n",
      "El cliente  Escritura con probabilidad [1.93036698e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.018344 0.981656]\n",
      "El cliente  Escritura con probabilidad [0.01895104 0.98104896]\n",
      "El cliente  Escritura con probabilidad [0.15000304 0.84999696]\n",
      "El cliente  Escritura con probabilidad [0.16706447 0.83293553]\n",
      "El cliente  Escritura con probabilidad [0.23518536 0.76481464]\n",
      "El cliente  Escritura con probabilidad [0.0889116 0.9110884]\n",
      "El cliente  Escritura con probabilidad [0.142272 0.857728]\n",
      "El cliente  Escritura con probabilidad [0.20904443 0.79095557]\n",
      "El cliente  Escritura con probabilidad [0.17636308 0.82363692]\n",
      "El cliente  Escritura con probabilidad [0.0958799 0.9041201]\n",
      "El cliente  Escritura con probabilidad [0.14314746 0.85685254]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.01998197 0.98001803]\n",
      "El cliente  Escritura con probabilidad [0.17309886 0.82690114]\n",
      "El cliente  Escritura con probabilidad [2.68454195e-07 9.99999732e-01]\n",
      "El cliente  Escritura con probabilidad [0.20405023 0.79594977]\n",
      "El cliente  Escritura con probabilidad [4.53417091e-07 9.99999547e-01]\n",
      "El cliente  Escritura con probabilidad [0.3020682 0.6979318]\n",
      "El cliente  Escritura con probabilidad [0.23033207 0.76966793]\n",
      "El cliente  Escritura con probabilidad [0.08422601 0.91577399]\n",
      "El cliente  Escritura con probabilidad [0.02316392 0.97683608]\n",
      "El cliente  Escritura con probabilidad [0.16979313 0.83020687]\n",
      "El cliente  Escritura con probabilidad [0.01838008 0.98161992]\n",
      "El cliente  Escritura con probabilidad [0.14882377 0.85117623]\n",
      "El cliente  Escritura con probabilidad [0.11042624 0.88957376]\n",
      "El cliente  Escritura con probabilidad [7.88187202e-04 9.99211813e-01]\n",
      "El cliente  Escritura con probabilidad [0.12782421 0.87217579]\n",
      "El cliente  Escritura con probabilidad [0.13052158 0.86947842]\n",
      "El cliente  Escritura con probabilidad [0.23426168 0.76573832]\n",
      "El cliente  Escritura con probabilidad [3.00377413e-05 9.99969962e-01]\n",
      "El cliente  Escritura con probabilidad [0.20559165 0.79440835]\n",
      "El cliente  Escritura con probabilidad [0.23888272 0.76111728]\n",
      "El cliente  Escritura con probabilidad [0.04959919 0.95040081]\n",
      "El cliente  Escritura con probabilidad [0.10307077 0.89692923]\n",
      "El cliente  Escritura con probabilidad [0.20472886 0.79527114]\n",
      "El cliente  Escritura con probabilidad [0.06584053 0.93415947]\n",
      "El cliente  Escritura con probabilidad [0.2564197 0.7435803]\n",
      "El cliente  Escritura con probabilidad [0.01506554 0.98493446]\n",
      "El cliente  Escritura con probabilidad [0.20158379 0.79841621]\n",
      "El cliente  Escritura con probabilidad [0.23567461 0.76432539]\n",
      "El cliente  Escritura con probabilidad [0.15033909 0.84966091]\n",
      "El cliente  Escritura con probabilidad [0.00307423 0.99692577]\n",
      "El cliente  Escritura con probabilidad [0.05212189 0.94787811]\n",
      "El cliente  Escritura con probabilidad [0.23881766 0.76118234]\n",
      "El cliente  Escritura con probabilidad [0.05249469 0.94750531]\n",
      "El cliente  Escritura con probabilidad [0.09362006 0.90637994]\n",
      "El cliente  Escritura con probabilidad [1.46855460e-04 9.99853145e-01]\n",
      "El cliente  Escritura con probabilidad [0.1364821 0.8635179]\n",
      "El cliente  Escritura con probabilidad [1.94297911e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.20473003 0.79526997]\n",
      "El cliente  Escritura con probabilidad [0.14818741 0.85181259]\n",
      "El cliente  Escritura con probabilidad [0.12561625 0.87438375]\n",
      "El cliente  Escritura con probabilidad [0.08920613 0.91079387]\n",
      "El cliente  Escritura con probabilidad [0.13491564 0.86508436]\n",
      "El cliente  Escritura con probabilidad [1.17696243e-07 9.99999882e-01]\n",
      "El cliente  Escritura con probabilidad [0.21530215 0.78469785]\n",
      "El cliente  Escritura con probabilidad [0.15961083 0.84038917]\n",
      "El cliente  Escritura con probabilidad [0.18939367 0.81060633]\n",
      "El cliente  Escritura con probabilidad [0.01664422 0.98335578]\n",
      "El cliente  Escritura con probabilidad [0.15366259 0.84633741]\n",
      "El cliente  Escritura con probabilidad [0.2440346 0.7559654]\n",
      "El cliente  Escritura con probabilidad [0.03856138 0.96143862]\n",
      "El cliente  Escritura con probabilidad [0.05984597 0.94015403]\n",
      "El cliente  Escritura con probabilidad [6.35333464e-04 9.99364667e-01]\n",
      "El cliente  Escritura con probabilidad [0.2153529 0.7846471]\n",
      "El cliente  Escritura con probabilidad [0.18595223 0.81404777]\n",
      "El cliente  Escritura con probabilidad [4.06527870e-05 9.99959347e-01]\n",
      "El cliente  Escritura con probabilidad [0.19026478 0.80973522]\n",
      "El cliente  Escritura con probabilidad [0.21212158 0.78787842]\n",
      "El cliente  Escritura con probabilidad [7.57955868e-05 9.99924204e-01]\n",
      "El cliente  Escritura con probabilidad [0.14985855 0.85014145]\n",
      "El cliente  Escritura con probabilidad [1.54666814e-06 9.99998453e-01]\n",
      "El cliente  Escritura con probabilidad [8.02101502e-04 9.99197898e-01]\n",
      "El cliente  Escritura con probabilidad [0.10305364 0.89694636]\n",
      "El cliente  Escritura con probabilidad [0.08657203 0.91342797]\n",
      "El cliente  Escritura con probabilidad [0.08736342 0.91263658]\n",
      "El cliente  Escritura con probabilidad [4.61718658e-07 9.99999538e-01]\n",
      "El cliente  Escritura con probabilidad [0.1694205 0.8305795]\n",
      "El cliente  Escritura con probabilidad [6.77730013e-05 9.99932227e-01]\n",
      "El cliente  Escritura con probabilidad [0.03561315 0.96438685]\n",
      "El cliente  Escritura con probabilidad [0.09803994 0.90196006]\n",
      "El cliente  Escritura con probabilidad [0.01731444 0.98268556]\n",
      "El cliente  Escritura con probabilidad [0.15790929 0.84209071]\n",
      "El cliente  Escritura con probabilidad [4.19608404e-07 9.99999580e-01]\n",
      "El cliente  Escritura con probabilidad [0.03157573 0.96842427]\n",
      "El cliente  Escritura con probabilidad [3.05325287e-06 9.99996947e-01]\n",
      "El cliente  Escritura con probabilidad [0.1094317 0.8905683]\n",
      "El cliente  Escritura con probabilidad [0.16376765 0.83623235]\n",
      "El cliente  Escritura con probabilidad [0.29477459 0.70522541]\n",
      "El cliente  Escritura con probabilidad [4.82059718e-08 9.99999952e-01]\n",
      "El cliente  Escritura con probabilidad [0.06644015 0.93355985]\n",
      "El cliente  Escritura con probabilidad [0.09118049 0.90881951]\n",
      "El cliente  Escritura con probabilidad [0.22877104 0.77122896]\n",
      "El cliente  Escritura con probabilidad [0.06389234 0.93610766]\n",
      "El cliente  Escritura con probabilidad [0.03196387 0.96803613]\n",
      "El cliente  Escritura con probabilidad [8.03509703e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.21719508 0.78280492]\n",
      "El cliente  Escritura con probabilidad [0.14549446 0.85450554]\n",
      "El cliente  Escritura con probabilidad [0.08273952 0.91726048]\n",
      "El cliente  Escritura con probabilidad [0.18894647 0.81105353]\n",
      "El cliente  Escritura con probabilidad [0.02191824 0.97808176]\n",
      "El cliente  Escritura con probabilidad [0.13842838 0.86157162]\n",
      "El cliente  Escritura con probabilidad [0.21263938 0.78736062]\n",
      "El cliente  Escritura con probabilidad [0.16245702 0.83754298]\n",
      "El cliente  Escritura con probabilidad [0.14713812 0.85286188]\n",
      "El cliente  Escritura con probabilidad [0.24372436 0.75627564]\n",
      "El cliente  Escritura con probabilidad [0.17039156 0.82960844]\n",
      "El cliente  Escritura con probabilidad [1.86799957e-08 9.99999981e-01]\n",
      "El cliente  Escritura con probabilidad [0.32169522 0.67830478]\n",
      "El cliente  Escritura con probabilidad [0.07568542 0.92431458]\n",
      "El cliente  Escritura con probabilidad [0.00588707 0.99411293]\n",
      "El cliente  Escritura con probabilidad [0.14952466 0.85047534]\n",
      "El cliente  Escritura con probabilidad [0.13690776 0.86309224]\n",
      "El cliente  Escritura con probabilidad [0.1091729 0.8908271]\n",
      "El cliente  Escritura con probabilidad [0.14178958 0.85821042]\n",
      "El cliente  Escritura con probabilidad [0.26308015 0.73691985]\n",
      "El cliente  Escritura con probabilidad [9.33113460e-07 9.99999067e-01]\n",
      "El cliente  Escritura con probabilidad [0.31206289 0.68793711]\n",
      "El cliente  Escritura con probabilidad [0.22416701 0.77583299]\n",
      "El cliente  Escritura con probabilidad [0.01477094 0.98522906]\n",
      "El cliente  Escritura con probabilidad [0.15898226 0.84101774]\n",
      "El cliente  Escritura con probabilidad [0.27739377 0.72260623]\n",
      "El cliente  Escritura con probabilidad [0.13745979 0.86254021]\n",
      "El cliente  Escritura con probabilidad [0.11186637 0.88813363]\n",
      "El cliente  Escritura con probabilidad [0.24792213 0.75207787]\n",
      "El cliente  Escritura con probabilidad [0.42714832 0.57285168]\n",
      "El cliente  Escritura con probabilidad [0.05188853 0.94811147]\n",
      "El cliente  Escritura con probabilidad [0.12548106 0.87451894]\n",
      "El cliente  Escritura con probabilidad [0.12066486 0.87933514]\n",
      "El cliente  Escritura con probabilidad [3.50900686e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.0863132 0.9136868]\n",
      "El cliente  Escritura con probabilidad [0.20682418 0.79317582]\n",
      "El cliente  Escritura con probabilidad [0.06791104 0.93208896]\n",
      "El cliente  Escritura con probabilidad [0.24453827 0.75546173]\n",
      "El cliente  Escritura con probabilidad [0.17043511 0.82956489]\n",
      "El cliente  Escritura con probabilidad [0.0500377 0.9499623]\n",
      "El cliente  Escritura con probabilidad [0.00572609 0.99427391]\n",
      "El cliente  Escritura con probabilidad [0.06561442 0.93438558]\n",
      "El cliente  Escritura con probabilidad [0.18199175 0.81800825]\n",
      "El cliente  Escritura con probabilidad [3.33515526e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.32545745 0.67454255]\n",
      "El cliente  Escritura con probabilidad [4.78898476e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [0.01238055 0.98761945]\n",
      "El cliente  Escritura con probabilidad [0.08682238 0.91317762]\n",
      "El cliente  Escritura con probabilidad [0.15816993 0.84183007]\n",
      "El cliente  Escritura con probabilidad [1.33226763e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.07119495 0.92880505]\n",
      "El cliente  Escritura con probabilidad [0.27261008 0.72738992]\n",
      "El cliente  Escritura con probabilidad [0.20528106 0.79471894]\n",
      "El cliente  Escritura con probabilidad [0.2384996 0.7615004]\n",
      "El cliente  Escritura con probabilidad [0.09579811 0.90420189]\n",
      "El cliente  Escritura con probabilidad [4.44377868e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.23705599 0.76294401]\n",
      "El cliente  Escritura con probabilidad [3.93889384e-05 9.99960611e-01]\n",
      "El cliente  Escritura con probabilidad [0.18902953 0.81097047]\n",
      "El cliente  Escritura con probabilidad [0.22862631 0.77137369]\n",
      "El cliente  Escritura con probabilidad [0.17126872 0.82873128]\n",
      "El cliente  Escritura con probabilidad [4.64891407e-06 9.99995351e-01]\n",
      "El cliente  Escritura con probabilidad [0.14994177 0.85005823]\n",
      "El cliente  Escritura con probabilidad [0.04955699 0.95044301]\n",
      "El cliente  Escritura con probabilidad [0.02315275 0.97684725]\n",
      "El cliente  Escritura con probabilidad [3.75329210e-04 9.99624671e-01]\n",
      "El cliente  Escritura con probabilidad [0.0295083 0.9704917]\n",
      "El cliente  Escritura con probabilidad [3.13749027e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.10015825e-04 9.99889984e-01]\n",
      "El cliente  Escritura con probabilidad [0.09705129 0.90294871]\n",
      "El cliente  Escritura con probabilidad [0.05749102 0.94250898]\n",
      "El cliente  Escritura con probabilidad [0.25043229 0.74956771]\n",
      "El cliente  Escritura con probabilidad [0.13777534 0.86222466]\n",
      "El cliente  Escritura con probabilidad [0.23744227 0.76255773]\n",
      "El cliente  Escritura con probabilidad [0.06814961 0.93185039]\n",
      "El cliente  Escritura con probabilidad [3.52634655e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.09221462 0.90778538]\n",
      "El cliente  Escritura con probabilidad [1.90125392e-05 9.99980987e-01]\n",
      "El cliente  Escritura con probabilidad [0.2668869 0.7331131]\n",
      "El cliente  Escritura con probabilidad [0.15950172 0.84049828]\n",
      "El cliente  Escritura con probabilidad [0.05068011 0.94931989]\n",
      "El cliente  Escritura con probabilidad [0.04453234 0.95546766]\n",
      "El cliente  Escritura con probabilidad [9.7211128e-13 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [5.74435033e-09 9.99999994e-01]\n",
      "El cliente  Escritura con probabilidad [0.13749096 0.86250904]\n",
      "El cliente  Escritura con probabilidad [0.04946325 0.95053675]\n",
      "El cliente  Escritura con probabilidad [0.05624887 0.94375113]\n",
      "El cliente  Escritura con probabilidad [6.87850887e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.23679419 0.76320581]\n",
      "El cliente  Escritura con probabilidad [0.15966694 0.84033306]\n",
      "El cliente  Escritura con probabilidad [2.86161983e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.65796957e-04 9.99834203e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.05866065 0.94133935]\n",
      "El cliente  Escritura con probabilidad [0.00128072 0.99871928]\n",
      "El cliente  Escritura con probabilidad [3.119988e-05 9.999688e-01]\n",
      "El cliente  Escritura con probabilidad [0.06054529 0.93945471]\n",
      "El cliente  Escritura con probabilidad [0.24650207 0.75349793]\n",
      "El cliente  Escritura con probabilidad [3.27189742e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.02601095 0.97398905]\n",
      "El cliente  Escritura con probabilidad [0.26187481 0.73812519]\n",
      "El cliente  Escritura con probabilidad [0.23006891 0.76993109]\n",
      "El cliente  Escritura con probabilidad [0.15879413 0.84120587]\n",
      "El cliente  Escritura con probabilidad [0.02767885 0.97232115]\n",
      "El cliente  Escritura con probabilidad [0.00968318 0.99031682]\n",
      "El cliente  Escritura con probabilidad [0.25090412 0.74909588]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.23960687 0.76039313]\n",
      "El cliente  Escritura con probabilidad [0.02032609 0.97967391]\n",
      "El cliente  Escritura con probabilidad [0.01861315 0.98138685]\n",
      "El cliente  Escritura con probabilidad [0.16944242 0.83055758]\n",
      "El cliente  Escritura con probabilidad [1.11148755e-05 9.99988885e-01]\n",
      "El cliente  Escritura con probabilidad [0.15858148 0.84141852]\n",
      "El cliente  Escritura con probabilidad [0.30374687 0.69625313]\n",
      "El cliente  Escritura con probabilidad [0.02483555 0.97516445]\n",
      "El cliente  Escritura con probabilidad [0.16538413 0.83461587]\n",
      "El cliente  Escritura con probabilidad [0.1890269 0.8109731]\n",
      "El cliente  Escritura con probabilidad [0.22293184 0.77706816]\n",
      "El cliente  Escritura con probabilidad [0.25291817 0.74708183]\n",
      "El cliente  Escritura con probabilidad [0.21514394 0.78485606]\n",
      "El cliente  Escritura con probabilidad [0.19729661 0.80270339]\n",
      "El cliente  Escritura con probabilidad [0.19507267 0.80492733]\n",
      "El cliente  Escritura con probabilidad [0.11651116 0.88348884]\n",
      "El cliente  Escritura con probabilidad [0.10446307 0.89553693]\n",
      "El cliente  Escritura con probabilidad [2.37680696e-05 9.99976232e-01]\n",
      "El cliente  Escritura con probabilidad [0.30138201 0.69861799]\n",
      "El cliente  Escritura con probabilidad [3.49538176e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [1.2863266e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.01929085 0.98070915]\n",
      "El cliente  Escritura con probabilidad [0.17493384 0.82506616]\n",
      "El cliente  Escritura con probabilidad [0.18568626 0.81431374]\n",
      "El cliente  Escritura con probabilidad [0.03036761 0.96963239]\n",
      "El cliente  Escritura con probabilidad [0.23879275 0.76120725]\n",
      "El cliente  Escritura con probabilidad [0.26373942 0.73626058]\n",
      "El cliente  Escritura con probabilidad [0.01110202 0.98889798]\n",
      "El cliente  Escritura con probabilidad [0.11668749 0.88331251]\n",
      "El cliente  Escritura con probabilidad [0.26681567 0.73318433]\n",
      "El cliente  Escritura con probabilidad [0.01708322 0.98291678]\n",
      "El cliente  Escritura con probabilidad [0.18163924 0.81836076]\n",
      "El cliente  Escritura con probabilidad [0.0015352 0.9984648]\n",
      "El cliente  Escritura con probabilidad [0.00245586 0.99754414]\n",
      "El cliente  Escritura con probabilidad [0.16014886 0.83985114]\n",
      "El cliente  Escritura con probabilidad [0.18092684 0.81907316]\n",
      "El cliente  Escritura con probabilidad [0.05475123 0.94524877]\n",
      "El cliente  Escritura con probabilidad [0.09248332 0.90751668]\n",
      "El cliente  Escritura con probabilidad [0.21897413 0.78102587]\n",
      "El cliente  Escritura con probabilidad [0.11515455 0.88484545]\n",
      "El cliente  Escritura con probabilidad [0.07715337 0.92284663]\n",
      "El cliente  Escritura con probabilidad [0.21242368 0.78757632]\n",
      "El cliente  Escritura con probabilidad [0.08282553 0.91717447]\n",
      "El cliente  Escritura con probabilidad [0.2707218 0.7292782]\n",
      "El cliente  Escritura con probabilidad [0.25811276 0.74188724]\n",
      "El cliente  Escritura con probabilidad [0.17656867 0.82343133]\n",
      "El cliente  Escritura con probabilidad [0.05197803 0.94802197]\n",
      "El cliente  Escritura con probabilidad [0.00430394 0.99569606]\n",
      "El cliente  Escritura con probabilidad [0.11167834 0.88832166]\n",
      "El cliente  Escritura con probabilidad [3.61514766e-06 9.99996385e-01]\n",
      "El cliente  Escritura con probabilidad [0.11141397 0.88858603]\n",
      "El cliente  Escritura con probabilidad [4.79182720e-04 9.99520817e-01]\n",
      "El cliente  Escritura con probabilidad [0.00320786 0.99679214]\n",
      "El cliente  Escritura con probabilidad [0.22615175 0.77384825]\n",
      "El cliente  Escritura con probabilidad [0.15831321 0.84168679]\n",
      "El cliente  Escritura con probabilidad [9.57934720e-06 9.99990421e-01]\n",
      "El cliente  Escritura con probabilidad [0.20117124 0.79882876]\n",
      "El cliente  Escritura con probabilidad [0.15250147 0.84749853]\n",
      "El cliente  Escritura con probabilidad [4.93409216e-04 9.99506591e-01]\n",
      "El cliente  Escritura con probabilidad [0.07937851 0.92062149]\n",
      "El cliente  Escritura con probabilidad [0.17295654 0.82704346]\n",
      "El cliente  Escritura con probabilidad [0.01747371 0.98252629]\n",
      "El cliente  Escritura con probabilidad [0.17666495 0.82333505]\n",
      "El cliente  Escritura con probabilidad [0.00210829 0.99789171]\n",
      "El cliente  Escritura con probabilidad [1.42241645e-06 9.99998578e-01]\n",
      "El cliente  Escritura con probabilidad [0.07515965 0.92484035]\n",
      "El cliente  Escritura con probabilidad [0.16951388 0.83048612]\n",
      "El cliente  Escritura con probabilidad [0.31541622 0.68458378]\n",
      "El cliente  Escritura con probabilidad [1.50701881e-05 9.99984930e-01]\n",
      "El cliente  Escritura con probabilidad [0.23100432 0.76899568]\n",
      "El cliente  Escritura con probabilidad [0.20081692 0.79918308]\n",
      "El cliente  Escritura con probabilidad [0.07752421 0.92247579]\n",
      "El cliente  Escritura con probabilidad [0.06981873 0.93018127]\n",
      "El cliente  Escritura con probabilidad [0.44237577 0.55762423]\n",
      "El cliente  Escritura con probabilidad [2.94074542e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17471285 0.82528715]\n",
      "El cliente  Escritura con probabilidad [0.10934038 0.89065962]\n",
      "El cliente  Escritura con probabilidad [0.01942798 0.98057202]\n",
      "El cliente  Escritura con probabilidad [7.17237461e-08 9.99999928e-01]\n",
      "El cliente  Escritura con probabilidad [0.39228384 0.60771616]\n",
      "El cliente  Escritura con probabilidad [0.27991348 0.72008652]\n",
      "El cliente  Escritura con probabilidad [0.06483876 0.93516124]\n",
      "El cliente  Escritura con probabilidad [0.23255343 0.76744657]\n",
      "El cliente  Escritura con probabilidad [3.33746635e-04 9.99666253e-01]\n",
      "El cliente  Escritura con probabilidad [7.69201265e-04 9.99230799e-01]\n",
      "El cliente  Escritura con probabilidad [0.00922998 0.99077002]\n",
      "El cliente  Escritura con probabilidad [0.00163434 0.99836566]\n",
      "El cliente  Escritura con probabilidad [0.11144946 0.88855054]\n",
      "El cliente  Escritura con probabilidad [0.07371027 0.92628973]\n",
      "El cliente  Escritura con probabilidad [0.09073914 0.90926086]\n",
      "El cliente  Escritura con probabilidad [0.03610116 0.96389884]\n",
      "El cliente  Escritura con probabilidad [0.07745219 0.92254781]\n",
      "El cliente  Escritura con probabilidad [0.30179503 0.69820497]\n",
      "El cliente  Escritura con probabilidad [0.26339126 0.73660874]\n",
      "El cliente  Escritura con probabilidad [4.17676423e-07 9.99999582e-01]\n",
      "El cliente  Escritura con probabilidad [0.10082759 0.89917241]\n",
      "El cliente  Escritura con probabilidad [0.05057476 0.94942524]\n",
      "El cliente  Escritura con probabilidad [0.19049103 0.80950897]\n",
      "El cliente  Escritura con probabilidad [4.12547587e-05 9.99958745e-01]\n",
      "El cliente  Escritura con probabilidad [6.58210763e-05 9.99934179e-01]\n",
      "El cliente  Escritura con probabilidad [0.21664784 0.78335216]\n",
      "El cliente  Escritura con probabilidad [0.1182512 0.8817488]\n",
      "El cliente  Escritura con probabilidad [0.21179607 0.78820393]\n",
      "El cliente  Escritura con probabilidad [0.04569269 0.95430731]\n",
      "El cliente  Escritura con probabilidad [0.26008262 0.73991738]\n",
      "El cliente  Escritura con probabilidad [0.09161245 0.90838755]\n",
      "El cliente  Escritura con probabilidad [3.97076271e-05 9.99960292e-01]\n",
      "El cliente  Escritura con probabilidad [0.02760102 0.97239898]\n",
      "El cliente  Escritura con probabilidad [0.00845376 0.99154624]\n",
      "El cliente  Escritura con probabilidad [0.00164 0.99836]\n",
      "El cliente  Escritura con probabilidad [0.20739516 0.79260484]\n",
      "El cliente  Escritura con probabilidad [0.06159067 0.93840933]\n",
      "El cliente  Escritura con probabilidad [0.01059823 0.98940177]\n",
      "El cliente  Escritura con probabilidad [0.01505486 0.98494514]\n",
      "El cliente  Escritura con probabilidad [0.05803714 0.94196286]\n",
      "El cliente  Escritura con probabilidad [0.32984151 0.67015849]\n",
      "El cliente  Escritura con probabilidad [0.04246376 0.95753624]\n",
      "El cliente  Escritura con probabilidad [0.2890457 0.7109543]\n",
      "El cliente  Escritura con probabilidad [0.19229423 0.80770577]\n",
      "El cliente  Escritura con probabilidad [0.17817155 0.82182845]\n",
      "El cliente  Escritura con probabilidad [0.2288852 0.7711148]\n",
      "El cliente  Escritura con probabilidad [0.05494981 0.94505019]\n",
      "El cliente  Escritura con probabilidad [0.17012044 0.82987956]\n",
      "El cliente  Escritura con probabilidad [0.00837754 0.99162246]\n",
      "El cliente  Escritura con probabilidad [1.15546839e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [5.94026650e-09 9.99999994e-01]\n",
      "El cliente  Escritura con probabilidad [0.04605813 0.95394187]\n",
      "El cliente  Escritura con probabilidad [0.20404263 0.79595737]\n",
      "El cliente  Escritura con probabilidad [0.17569104 0.82430896]\n",
      "El cliente  Escritura con probabilidad [0.33895052 0.66104948]\n",
      "El cliente  Escritura con probabilidad [0.26891273 0.73108727]\n",
      "El cliente  Escritura con probabilidad [0.05844605 0.94155395]\n",
      "El cliente  Escritura con probabilidad [0.15210318 0.84789682]\n",
      "El cliente  Escritura con probabilidad [0.24643911 0.75356089]\n",
      "El cliente  Escritura con probabilidad [0.13389368 0.86610632]\n",
      "El cliente  Escritura con probabilidad [0.05692806 0.94307194]\n",
      "El cliente  Escritura con probabilidad [0.33947057 0.66052943]\n",
      "El cliente  Escritura con probabilidad [0.4170544 0.5829456]\n",
      "El cliente  Escritura con probabilidad [0.24600117 0.75399883]\n",
      "El cliente  Escritura con probabilidad [0.04122023 0.95877977]\n",
      "El cliente  Escritura con probabilidad [0.0786921 0.9213079]\n",
      "El cliente  Escritura con probabilidad [0.14583657 0.85416343]\n",
      "El cliente  Escritura con probabilidad [0.32363973 0.67636027]\n",
      "El cliente  Escritura con probabilidad [3.15350045e-04 9.99684650e-01]\n",
      "El cliente  Escritura con probabilidad [0.284887 0.715113]\n",
      "El cliente  Escritura con probabilidad [0.23404628 0.76595372]\n",
      "El cliente  Escritura con probabilidad [1.36493822e-07 9.99999864e-01]\n",
      "El cliente  Escritura con probabilidad [0.31226791 0.68773209]\n",
      "El cliente  Escritura con probabilidad [6.26582786e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [8.13424483e-05 9.99918658e-01]\n",
      "El cliente  Escritura con probabilidad [0.05678299 0.94321701]\n",
      "El cliente  Escritura con probabilidad [0.17001648 0.82998352]\n",
      "El cliente  Escritura con probabilidad [0.26717744 0.73282256]\n",
      "El cliente  Escritura con probabilidad [0.10510037 0.89489963]\n",
      "El cliente  Escritura con probabilidad [0.16658967 0.83341033]\n",
      "El cliente  Escritura con probabilidad [1.36362810e-06 9.99998636e-01]\n",
      "El cliente  Escritura con probabilidad [5.17236898e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [0.26668199 0.73331801]\n",
      "El cliente  Escritura con probabilidad [0.2563791 0.7436209]\n",
      "El cliente  Escritura con probabilidad [0.48404183 0.51595817]\n",
      "El cliente  Escritura con probabilidad [0.13870044 0.86129956]\n",
      "El cliente  Escritura con probabilidad [0.05664557 0.94335443]\n",
      "El cliente  Escritura con probabilidad [0.3054082 0.6945918]\n",
      "El cliente  Escritura con probabilidad [2.52835320e-07 9.99999747e-01]\n",
      "El cliente  Escritura con probabilidad [0.38960072 0.61039928]\n",
      "El cliente  Escritura con probabilidad [0.02276653 0.97723347]\n",
      "El cliente  Escritura con probabilidad [0.10332423 0.89667577]\n",
      "El cliente  Escritura con probabilidad [1.04341781e-07 9.99999896e-01]\n",
      "El cliente  Escritura con probabilidad [0.08196796 0.91803204]\n",
      "El cliente  Escritura con probabilidad [0.08240363 0.91759637]\n",
      "El cliente  Escritura con probabilidad [6.85999571e-04 9.99314000e-01]\n",
      "El cliente  Escritura con probabilidad [0.02097611 0.97902389]\n",
      "El cliente  Escritura con probabilidad [0.24667494 0.75332506]\n",
      "El cliente  Escritura con probabilidad [0.13062096 0.86937904]\n",
      "El cliente  Escritura con probabilidad [0.12431424 0.87568576]\n",
      "El cliente  Escritura con probabilidad [1.46230749e-06 9.99998538e-01]\n",
      "El cliente  Escritura con probabilidad [0.18275854 0.81724146]\n",
      "El cliente  Escritura con probabilidad [0.08188403 0.91811597]\n",
      "El cliente  Escritura con probabilidad [9.42204850e-06 9.99990578e-01]\n",
      "El cliente  Escritura con probabilidad [0.20986726 0.79013274]\n",
      "El cliente  Escritura con probabilidad [0.10232526 0.89767474]\n",
      "El cliente  Escritura con probabilidad [3.34466644e-04 9.99665533e-01]\n",
      "El cliente  Escritura con probabilidad [0.10684078 0.89315922]\n",
      "El cliente  Escritura con probabilidad [0.06364169 0.93635831]\n",
      "El cliente  Escritura con probabilidad [0.07550794 0.92449206]\n",
      "El cliente  Escritura con probabilidad [0.03907355 0.96092645]\n",
      "El cliente  Escritura con probabilidad [0.05597024 0.94402976]\n",
      "El cliente  Escritura con probabilidad [0.03469353 0.96530647]\n",
      "El cliente  Escritura con probabilidad [0.1740067 0.8259933]\n",
      "El cliente  Escritura con probabilidad [0.04194053 0.95805947]\n",
      "El cliente  Escritura con probabilidad [0.13579357 0.86420643]\n",
      "El cliente  Escritura con probabilidad [0.21278614 0.78721386]\n",
      "El cliente  Escritura con probabilidad [0.06555641 0.93444359]\n",
      "El cliente  Escritura con probabilidad [0.0954277 0.9045723]\n",
      "El cliente  Escritura con probabilidad [0.22119142 0.77880858]\n",
      "El cliente  Escritura con probabilidad [0.21744606 0.78255394]\n",
      "El cliente  Escritura con probabilidad [0.20240234 0.79759766]\n",
      "El cliente  Escritura con probabilidad [0.08192946 0.91807054]\n",
      "El cliente  Escritura con probabilidad [0.04719781 0.95280219]\n",
      "El cliente  Escritura con probabilidad [0.21541368 0.78458632]\n",
      "El cliente  Escritura con probabilidad [0.1026178 0.8973822]\n",
      "El cliente  Escritura con probabilidad [0.16152323 0.83847677]\n",
      "El cliente  Escritura con probabilidad [0.24142484 0.75857516]\n",
      "El cliente  Escritura con probabilidad [3.67311879e-08 9.99999963e-01]\n",
      "El cliente  Escritura con probabilidad [1.39933318e-08 9.99999986e-01]\n",
      "El cliente  Escritura con probabilidad [0.30195685 0.69804315]\n",
      "El cliente  Escritura con probabilidad [0.23146974 0.76853026]\n",
      "El cliente  Escritura con probabilidad [0.24426217 0.75573783]\n",
      "El cliente  Escritura con probabilidad [0.12432094 0.87567906]\n",
      "El cliente  Escritura con probabilidad [0.1769463 0.8230537]\n",
      "El cliente  Escritura con probabilidad [0.15524389 0.84475611]\n",
      "El cliente  Escritura con probabilidad [0.10840802 0.89159198]\n",
      "El cliente  Escritura con probabilidad [0.27001063 0.72998937]\n",
      "El cliente  Escritura con probabilidad [0.21551133 0.78448867]\n",
      "El cliente  Escritura con probabilidad [0.01668852 0.98331148]\n",
      "El cliente  Escritura con probabilidad [0.00599167 0.99400833]\n",
      "El cliente  Escritura con probabilidad [0.29766752 0.70233248]\n",
      "El cliente  Escritura con probabilidad [0.0033181 0.9966819]\n",
      "El cliente  Escritura con probabilidad [0.23397715 0.76602285]\n",
      "El cliente  Escritura con probabilidad [0.06070691 0.93929309]\n",
      "El cliente  Escritura con probabilidad [0.10381188 0.89618812]\n",
      "El cliente  Escritura con probabilidad [0.1434122 0.8565878]\n",
      "El cliente  Escritura con probabilidad [0.00644126 0.99355874]\n",
      "El cliente  Escritura con probabilidad [0.06072234 0.93927766]\n",
      "El cliente  Escritura con probabilidad [0.07113723 0.92886277]\n",
      "El cliente  Escritura con probabilidad [1.37001055e-04 9.99862999e-01]\n",
      "El cliente  Escritura con probabilidad [0.03002329 0.96997671]\n",
      "El cliente  Escritura con probabilidad [0.13110714 0.86889286]\n",
      "El cliente  Escritura con probabilidad [0.23682997 0.76317003]\n",
      "El cliente  Escritura con probabilidad [0.28317147 0.71682853]\n",
      "El cliente  Escritura con probabilidad [0.1565233 0.8434767]\n",
      "El cliente  Escritura con probabilidad [0.19082339 0.80917661]\n",
      "El cliente  Escritura con probabilidad [0.1419721 0.8580279]\n",
      "El cliente  Escritura con probabilidad [0.17444264 0.82555736]\n",
      "El cliente  Escritura con probabilidad [0.12684908 0.87315092]\n",
      "El cliente  Escritura con probabilidad [0.22091204 0.77908796]\n",
      "El cliente  Escritura con probabilidad [0.10011587 0.89988413]\n",
      "El cliente  Escritura con probabilidad [0.00976685 0.99023315]\n",
      "El cliente  Escritura con probabilidad [0.11797024 0.88202976]\n",
      "El cliente  Escritura con probabilidad [0.11528564 0.88471436]\n",
      "El cliente  Escritura con probabilidad [0.03742567 0.96257433]\n",
      "El cliente  Escritura con probabilidad [0.05950953 0.94049047]\n",
      "El cliente  Escritura con probabilidad [0.10607746 0.89392254]\n",
      "El cliente  Escritura con probabilidad [0.01113737 0.98886263]\n",
      "El cliente  Escritura con probabilidad [0.16198106 0.83801894]\n",
      "El cliente  Escritura con probabilidad [0.28398759 0.71601241]\n",
      "El cliente  Escritura con probabilidad [0.05720427 0.94279573]\n",
      "El cliente  Escritura con probabilidad [0.1567051 0.8432949]\n",
      "El cliente  Escritura con probabilidad [0.2472797 0.7527203]\n",
      "El cliente  Escritura con probabilidad [0.11225562 0.88774438]\n",
      "El cliente  Escritura con probabilidad [0.26979313 0.73020687]\n",
      "El cliente  Escritura con probabilidad [0.08463232 0.91536768]\n",
      "El cliente  Escritura con probabilidad [0.22148911 0.77851089]\n",
      "El cliente  Escritura con probabilidad [0.18161526 0.81838474]\n",
      "El cliente  Escritura con probabilidad [0.41765905 0.58234095]\n",
      "El cliente  Escritura con probabilidad [0.02533316 0.97466684]\n",
      "El cliente  Desiste con probabilidad [0.57292714 0.42707286]\n",
      "El cliente  Escritura con probabilidad [6.07223266e-05 9.99939278e-01]\n",
      "El cliente  Escritura con probabilidad [0.02209319 0.97790681]\n",
      "El cliente  Escritura con probabilidad [0.14737674 0.85262326]\n",
      "El cliente  Escritura con probabilidad [0.08660445 0.91339555]\n",
      "El cliente  Escritura con probabilidad [0.03302495 0.96697505]\n",
      "El cliente  Escritura con probabilidad [0.10151855 0.89848145]\n",
      "El cliente  Escritura con probabilidad [0.11048967 0.88951033]\n",
      "El cliente  Escritura con probabilidad [0.13750228 0.86249772]\n",
      "El cliente  Escritura con probabilidad [0.20793079 0.79206921]\n",
      "El cliente  Escritura con probabilidad [0.17371678 0.82628322]\n",
      "El cliente  Escritura con probabilidad [3.03453929e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.0029956 0.9970044]\n",
      "El cliente  Escritura con probabilidad [2.03858200e-04 9.99796142e-01]\n",
      "El cliente  Escritura con probabilidad [0.30333274 0.69666726]\n",
      "El cliente  Escritura con probabilidad [0.22660581 0.77339419]\n",
      "El cliente  Escritura con probabilidad [0.19085045 0.80914955]\n",
      "El cliente  Escritura con probabilidad [0.0526939 0.9473061]\n",
      "El cliente  Escritura con probabilidad [0.09090334 0.90909666]\n",
      "El cliente  Escritura con probabilidad [0.05888668 0.94111332]\n",
      "El cliente  Escritura con probabilidad [0.10042878 0.89957122]\n",
      "El cliente  Escritura con probabilidad [0.02316906 0.97683094]\n",
      "El cliente  Escritura con probabilidad [0.15564825 0.84435175]\n",
      "El cliente  Escritura con probabilidad [0.02300869 0.97699131]\n",
      "El cliente  Escritura con probabilidad [0.22708947 0.77291053]\n",
      "El cliente  Escritura con probabilidad [0.09879172 0.90120828]\n",
      "El cliente  Escritura con probabilidad [0.11519801 0.88480199]\n",
      "El cliente  Escritura con probabilidad [0.00583179 0.99416821]\n",
      "El cliente  Escritura con probabilidad [8.65973959e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.11523706 0.88476294]\n",
      "El cliente  Escritura con probabilidad [0.00543952 0.99456048]\n",
      "El cliente  Escritura con probabilidad [4.17092778e-05 9.99958291e-01]\n",
      "El cliente  Escritura con probabilidad [6.87205848e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17608656 0.82391344]\n",
      "El cliente  Escritura con probabilidad [6.58804551e-05 9.99934120e-01]\n",
      "El cliente  Escritura con probabilidad [0.2774179 0.7225821]\n",
      "El cliente  Escritura con probabilidad [0.06512074 0.93487926]\n",
      "El cliente  Escritura con probabilidad [0.04002127 0.95997873]\n",
      "El cliente  Escritura con probabilidad [0.11149013 0.88850987]\n",
      "El cliente  Escritura con probabilidad [0.0376815 0.9623185]\n",
      "El cliente  Escritura con probabilidad [0.21591695 0.78408305]\n",
      "El cliente  Escritura con probabilidad [0.11940187 0.88059813]\n",
      "El cliente  Escritura con probabilidad [0.2903184 0.7096816]\n",
      "El cliente  Escritura con probabilidad [0.0612781 0.9387219]\n",
      "El cliente  Escritura con probabilidad [0.24897031 0.75102969]\n",
      "El cliente  Escritura con probabilidad [0.15118575 0.84881425]\n",
      "El cliente  Escritura con probabilidad [0.04939787 0.95060213]\n",
      "El cliente  Escritura con probabilidad [0.20472998 0.79527002]\n",
      "El cliente  Escritura con probabilidad [0.21297348 0.78702652]\n",
      "El cliente  Escritura con probabilidad [0.45192978 0.54807022]\n",
      "El cliente  Escritura con probabilidad [0.20145816 0.79854184]\n",
      "El cliente  Escritura con probabilidad [1.94902176e-04 9.99805098e-01]\n",
      "El cliente  Escritura con probabilidad [0.41171773 0.58828227]\n",
      "El cliente  Escritura con probabilidad [0.07081376 0.92918624]\n",
      "El cliente  Escritura con probabilidad [0.15991998 0.84008002]\n",
      "El cliente  Escritura con probabilidad [7.25653142e-04 9.99274347e-01]\n",
      "El cliente  Escritura con probabilidad [0.2067985 0.7932015]\n",
      "El cliente  Escritura con probabilidad [0.08776901 0.91223099]\n",
      "El cliente  Escritura con probabilidad [0.06342422 0.93657578]\n",
      "El cliente  Escritura con probabilidad [0.19298908 0.80701092]\n",
      "El cliente  Escritura con probabilidad [3.43446638e-05 9.99965655e-01]\n",
      "El cliente  Escritura con probabilidad [0.03516301 0.96483699]\n",
      "El cliente  Escritura con probabilidad [0.23522219 0.76477781]\n",
      "El cliente  Escritura con probabilidad [0.18881088 0.81118912]\n",
      "El cliente  Escritura con probabilidad [0.33701339 0.66298661]\n",
      "El cliente  Escritura con probabilidad [0.00845441 0.99154559]\n",
      "El cliente  Escritura con probabilidad [0.13817795 0.86182205]\n",
      "El cliente  Escritura con probabilidad [0.0341769 0.9658231]\n",
      "El cliente  Escritura con probabilidad [0.20636185 0.79363815]\n",
      "El cliente  Escritura con probabilidad [0.25761881 0.74238119]\n",
      "El cliente  Escritura con probabilidad [0.00509516 0.99490484]\n",
      "El cliente  Escritura con probabilidad [0.20537496 0.79462504]\n",
      "El cliente  Escritura con probabilidad [0.12847941 0.87152059]\n",
      "El cliente  Escritura con probabilidad [5.78803672e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.09603063 0.90396937]\n",
      "El cliente  Escritura con probabilidad [0.2205996 0.7794004]\n",
      "El cliente  Escritura con probabilidad [0.12643798 0.87356202]\n",
      "El cliente  Escritura con probabilidad [0.02024568 0.97975432]\n",
      "El cliente  Escritura con probabilidad [0.23861522 0.76138478]\n",
      "El cliente  Escritura con probabilidad [0.01437546 0.98562454]\n",
      "El cliente  Escritura con probabilidad [0.02347222 0.97652778]\n",
      "El cliente  Escritura con probabilidad [0.00301326 0.99698674]\n",
      "El cliente  Escritura con probabilidad [1.00181151e-08 9.99999990e-01]\n",
      "El cliente  Escritura con probabilidad [0.18202641 0.81797359]\n",
      "El cliente  Escritura con probabilidad [0.11168889 0.88831111]\n",
      "El cliente  Escritura con probabilidad [0.16645235 0.83354765]\n",
      "El cliente  Escritura con probabilidad [0.06534205 0.93465795]\n",
      "El cliente  Escritura con probabilidad [0.08107861 0.91892139]\n",
      "El cliente  Escritura con probabilidad [9.02235475e-04 9.99097765e-01]\n",
      "El cliente  Escritura con probabilidad [0.16815351 0.83184649]\n",
      "El cliente  Escritura con probabilidad [0.05141504 0.94858496]\n",
      "El cliente  Escritura con probabilidad [0.28308939 0.71691061]\n",
      "El cliente  Escritura con probabilidad [0.28874173 0.71125827]\n",
      "El cliente  Escritura con probabilidad [0.14938429 0.85061571]\n",
      "El cliente  Escritura con probabilidad [0.11085093 0.88914907]\n",
      "El cliente  Escritura con probabilidad [0.00648059 0.99351941]\n",
      "El cliente  Escritura con probabilidad [0.22901199 0.77098801]\n",
      "El cliente  Escritura con probabilidad [0.08049093 0.91950907]\n",
      "El cliente  Escritura con probabilidad [2.74711633e-05 9.99972529e-01]\n",
      "El cliente  Escritura con probabilidad [0.07500399 0.92499601]\n",
      "El cliente  Escritura con probabilidad [0.04689171 0.95310829]\n",
      "El cliente  Escritura con probabilidad [0.15030575 0.84969425]\n",
      "El cliente  Escritura con probabilidad [0.0064426 0.9935574]\n",
      "El cliente  Escritura con probabilidad [0.28670202 0.71329798]\n",
      "El cliente  Escritura con probabilidad [0.20571214 0.79428786]\n",
      "El cliente  Escritura con probabilidad [0.22717403 0.77282597]\n",
      "El cliente  Escritura con probabilidad [0.16744929 0.83255071]\n",
      "El cliente  Escritura con probabilidad [0.05934781 0.94065219]\n",
      "El cliente  Escritura con probabilidad [0.21914085 0.78085915]\n",
      "El cliente  Escritura con probabilidad [0.00882028 0.99117972]\n",
      "El cliente  Escritura con probabilidad [0.07453049 0.92546951]\n",
      "El cliente  Escritura con probabilidad [0.22269108 0.77730892]\n",
      "El cliente  Escritura con probabilidad [0.14981552 0.85018448]\n",
      "El cliente  Escritura con probabilidad [0.22432194 0.77567806]\n",
      "El cliente  Escritura con probabilidad [4.07826550e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.18106628 0.81893372]\n",
      "El cliente  Escritura con probabilidad [0.0453058 0.9546942]\n",
      "El cliente  Escritura con probabilidad [0.10769863 0.89230137]\n",
      "El cliente  Escritura con probabilidad [0.16467058 0.83532942]\n",
      "El cliente  Escritura con probabilidad [0.17580356 0.82419644]\n",
      "El cliente  Escritura con probabilidad [0.09793005 0.90206995]\n",
      "El cliente  Escritura con probabilidad [0.18078325 0.81921675]\n",
      "El cliente  Escritura con probabilidad [0.17945125 0.82054875]\n",
      "El cliente  Escritura con probabilidad [0.11846361 0.88153639]\n",
      "El cliente  Escritura con probabilidad [0.05631098 0.94368902]\n",
      "El cliente  Escritura con probabilidad [0.01088997 0.98911003]\n",
      "El cliente  Escritura con probabilidad [0.15020848 0.84979152]\n",
      "El cliente  Escritura con probabilidad [0.12847638 0.87152362]\n",
      "El cliente  Escritura con probabilidad [0.19563295 0.80436705]\n",
      "El cliente  Escritura con probabilidad [0.06783391 0.93216609]\n",
      "El cliente  Escritura con probabilidad [0.06831996 0.93168004]\n",
      "El cliente  Escritura con probabilidad [0.06784242 0.93215758]\n",
      "El cliente  Escritura con probabilidad [0.076483 0.923517]\n",
      "El cliente  Escritura con probabilidad [2.22044605e-16 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.31577454 0.68422546]\n",
      "El cliente  Escritura con probabilidad [0.20779544 0.79220456]\n",
      "El cliente  Escritura con probabilidad [0.00397888 0.99602112]\n",
      "El cliente  Escritura con probabilidad [0.0014165 0.9985835]\n",
      "El cliente  Escritura con probabilidad [0.23524149 0.76475851]\n",
      "El cliente  Escritura con probabilidad [0.28126936 0.71873064]\n",
      "El cliente  Escritura con probabilidad [0.17333712 0.82666288]\n",
      "El cliente  Escritura con probabilidad [0.08277326 0.91722674]\n",
      "El cliente  Escritura con probabilidad [0.03039646 0.96960354]\n",
      "El cliente  Escritura con probabilidad [0.02045999 0.97954001]\n",
      "El cliente  Escritura con probabilidad [0.05623294 0.94376706]\n",
      "El cliente  Escritura con probabilidad [0.07626294 0.92373706]\n",
      "El cliente  Escritura con probabilidad [0.0068834 0.9931166]\n",
      "El cliente  Escritura con probabilidad [2.94792020e-07 9.99999705e-01]\n",
      "El cliente  Escritura con probabilidad [0.12549262 0.87450738]\n",
      "El cliente  Escritura con probabilidad [1.07109208e-07 9.99999893e-01]\n",
      "El cliente  Escritura con probabilidad [0.14024984 0.85975016]\n",
      "El cliente  Escritura con probabilidad [2.17494566e-05 9.99978251e-01]\n",
      "El cliente  Escritura con probabilidad [6.43862692e-05 9.99935614e-01]\n",
      "El cliente  Escritura con probabilidad [0.13278791 0.86721209]\n",
      "El cliente  Escritura con probabilidad [0.13376021 0.86623979]\n",
      "El cliente  Escritura con probabilidad [0.24094141 0.75905859]\n",
      "El cliente  Escritura con probabilidad [0.32542666 0.67457334]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.00589341 0.99410659]\n",
      "El cliente  Escritura con probabilidad [0.14970032 0.85029968]\n",
      "El cliente  Escritura con probabilidad [0.06062867 0.93937133]\n",
      "El cliente  Escritura con probabilidad [1.16684264e-04 9.99883316e-01]\n",
      "El cliente  Escritura con probabilidad [0.01500625 0.98499375]\n",
      "El cliente  Escritura con probabilidad [0.12319044 0.87680956]\n",
      "El cliente  Escritura con probabilidad [0.06593353 0.93406647]\n",
      "El cliente  Escritura con probabilidad [0.23146937 0.76853063]\n",
      "El cliente  Escritura con probabilidad [6.93185509e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.10173072 0.89826928]\n",
      "El cliente  Escritura con probabilidad [0.0930644 0.9069356]\n",
      "El cliente  Escritura con probabilidad [0.11580947 0.88419053]\n",
      "El cliente  Escritura con probabilidad [0.10240716 0.89759284]\n",
      "El cliente  Escritura con probabilidad [2.32537051e-07 9.99999767e-01]\n",
      "El cliente  Escritura con probabilidad [0.13162761 0.86837239]\n",
      "El cliente  Escritura con probabilidad [0.28703607 0.71296393]\n",
      "El cliente  Escritura con probabilidad [0.08719642 0.91280358]\n",
      "El cliente  Escritura con probabilidad [1.34070288e-04 9.99865930e-01]\n",
      "El cliente  Escritura con probabilidad [0.12532707 0.87467293]\n",
      "El cliente  Escritura con probabilidad [2.87885450e-07 9.99999712e-01]\n",
      "El cliente  Escritura con probabilidad [0.33184692 0.66815308]\n",
      "El cliente  Escritura con probabilidad [0.01474442 0.98525558]\n",
      "El cliente  Escritura con probabilidad [0.13230389 0.86769611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.09025389 0.90974611]\n",
      "El cliente  Escritura con probabilidad [0.03809651 0.96190349]\n",
      "El cliente  Escritura con probabilidad [0.22826293 0.77173707]\n",
      "El cliente  Escritura con probabilidad [5.43668599e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [0.18102347 0.81897653]\n",
      "El cliente  Escritura con probabilidad [1.44034679e-04 9.99855965e-01]\n",
      "El cliente  Escritura con probabilidad [1.56699869e-06 9.99998433e-01]\n",
      "El cliente  Escritura con probabilidad [0.3208188 0.6791812]\n",
      "El cliente  Escritura con probabilidad [0.07272128 0.92727872]\n",
      "El cliente  Escritura con probabilidad [1.1558754e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.13033468 0.86966532]\n",
      "El cliente  Escritura con probabilidad [0.01521513 0.98478487]\n",
      "El cliente  Escritura con probabilidad [0.18517195 0.81482805]\n",
      "El cliente  Escritura con probabilidad [6.25165808e-06 9.99993748e-01]\n",
      "El cliente  Escritura con probabilidad [0.25452398 0.74547602]\n",
      "El cliente  Escritura con probabilidad [0.02523321 0.97476679]\n",
      "El cliente  Escritura con probabilidad [2.01407779e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.05735412 0.94264588]\n",
      "El cliente  Escritura con probabilidad [8.99640626e-06 9.99991004e-01]\n",
      "El cliente  Escritura con probabilidad [2.37980191e-07 9.99999762e-01]\n",
      "El cliente  Escritura con probabilidad [0.23985118 0.76014882]\n",
      "El cliente  Escritura con probabilidad [0.21645469 0.78354531]\n",
      "El cliente  Escritura con probabilidad [0.19008725 0.80991275]\n",
      "El cliente  Escritura con probabilidad [1.36803704e-05 9.99986320e-01]\n",
      "El cliente  Escritura con probabilidad [1.90668965e-06 9.99998093e-01]\n",
      "El cliente  Escritura con probabilidad [0.10125907 0.89874093]\n",
      "El cliente  Escritura con probabilidad [0.25783953 0.74216047]\n",
      "El cliente  Escritura con probabilidad [3.42135477e-06 9.99996579e-01]\n",
      "El cliente  Escritura con probabilidad [6.85762558e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.12718121 0.87281879]\n",
      "El cliente  Escritura con probabilidad [0.20450098 0.79549902]\n",
      "El cliente  Escritura con probabilidad [0.43499553 0.56500447]\n",
      "El cliente  Escritura con probabilidad [0.20176603 0.79823397]\n",
      "El cliente  Escritura con probabilidad [2.03562789e-08 9.99999980e-01]\n",
      "El cliente  Escritura con probabilidad [0.00226475 0.99773525]\n",
      "El cliente  Escritura con probabilidad [0.1997081 0.8002919]\n",
      "El cliente  Escritura con probabilidad [0.15438248 0.84561752]\n",
      "El cliente  Escritura con probabilidad [0.14773857 0.85226143]\n",
      "El cliente  Escritura con probabilidad [2.22044605e-16 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.19185816 0.80814184]\n",
      "El cliente  Escritura con probabilidad [0.07791061 0.92208939]\n",
      "El cliente  Escritura con probabilidad [0.07396293 0.92603707]\n",
      "El cliente  Escritura con probabilidad [0.05621277 0.94378723]\n",
      "El cliente  Escritura con probabilidad [0.19861267 0.80138733]\n",
      "El cliente  Escritura con probabilidad [0.15427168 0.84572832]\n",
      "El cliente  Escritura con probabilidad [0.02823029 0.97176971]\n",
      "El cliente  Escritura con probabilidad [0.070992 0.929008]\n",
      "El cliente  Escritura con probabilidad [0.17091522 0.82908478]\n",
      "El cliente  Escritura con probabilidad [1.09043492e-04 9.99890957e-01]\n",
      "El cliente  Escritura con probabilidad [0.09148027 0.90851973]\n",
      "El cliente  Escritura con probabilidad [0.10310764 0.89689236]\n",
      "El cliente  Escritura con probabilidad [0.16825212 0.83174788]\n",
      "El cliente  Escritura con probabilidad [9.78146048e-07 9.99999022e-01]\n",
      "El cliente  Escritura con probabilidad [0.25017407 0.74982593]\n",
      "El cliente  Escritura con probabilidad [0.30155469 0.69844531]\n",
      "El cliente  Escritura con probabilidad [0.11228466 0.88771534]\n",
      "El cliente  Escritura con probabilidad [0.01547689 0.98452311]\n",
      "El cliente  Escritura con probabilidad [5.09086829e-08 9.99999949e-01]\n",
      "El cliente  Escritura con probabilidad [0.16074554 0.83925446]\n",
      "El cliente  Escritura con probabilidad [0.06023756 0.93976244]\n",
      "El cliente  Escritura con probabilidad [0.19950611 0.80049389]\n",
      "El cliente  Escritura con probabilidad [0.06876307 0.93123693]\n",
      "El cliente  Escritura con probabilidad [7.45221399e-07 9.99999255e-01]\n",
      "El cliente  Escritura con probabilidad [0.02560339 0.97439661]\n",
      "El cliente  Escritura con probabilidad [0.19653216 0.80346784]\n",
      "El cliente  Escritura con probabilidad [3.09974268e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17414873 0.82585127]\n",
      "El cliente  Escritura con probabilidad [0.01380713 0.98619287]\n",
      "El cliente  Escritura con probabilidad [7.45670192e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [6.16930507e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [8.75061356e-05 9.99912494e-01]\n",
      "El cliente  Escritura con probabilidad [0.02994401 0.97005599]\n",
      "El cliente  Escritura con probabilidad [0.03334908 0.96665092]\n",
      "El cliente  Escritura con probabilidad [0.23115134 0.76884866]\n",
      "El cliente  Escritura con probabilidad [0.05589077 0.94410923]\n",
      "El cliente  Escritura con probabilidad [0.28166025 0.71833975]\n",
      "El cliente  Escritura con probabilidad [0.11650763 0.88349237]\n",
      "El cliente  Escritura con probabilidad [0.07995853 0.92004147]\n",
      "El cliente  Escritura con probabilidad [0.24476478 0.75523522]\n",
      "El cliente  Escritura con probabilidad [0.17247812 0.82752188]\n",
      "El cliente  Escritura con probabilidad [0.12366338 0.87633662]\n",
      "El cliente  Escritura con probabilidad [0.00256389 0.99743611]\n",
      "El cliente  Escritura con probabilidad [0.2383636 0.7616364]\n",
      "El cliente  Escritura con probabilidad [0.0759101 0.9240899]\n",
      "El cliente  Escritura con probabilidad [1.30566460e-08 9.99999987e-01]\n",
      "El cliente  Escritura con probabilidad [0.28478867 0.71521133]\n",
      "El cliente  Escritura con probabilidad [0.04488539 0.95511461]\n",
      "El cliente  Escritura con probabilidad [0.14747882 0.85252118]\n",
      "El cliente  Escritura con probabilidad [0.01568898 0.98431102]\n",
      "El cliente  Escritura con probabilidad [8.17281328e-05 9.99918272e-01]\n",
      "El cliente  Escritura con probabilidad [1.20292665e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.00171416 0.99828584]\n",
      "El cliente  Escritura con probabilidad [2.37368112e-05 9.99976263e-01]\n",
      "El cliente  Escritura con probabilidad [0.02661517 0.97338483]\n",
      "El cliente  Escritura con probabilidad [0.19510555 0.80489445]\n",
      "El cliente  Escritura con probabilidad [0.15948359 0.84051641]\n",
      "El cliente  Escritura con probabilidad [0.14788943 0.85211057]\n",
      "El cliente  Escritura con probabilidad [0.08147421 0.91852579]\n",
      "El cliente  Escritura con probabilidad [0.25854365 0.74145635]\n",
      "El cliente  Escritura con probabilidad [0.04671456 0.95328544]\n",
      "El cliente  Escritura con probabilidad [0.17727896 0.82272104]\n",
      "El cliente  Escritura con probabilidad [0.23207194 0.76792806]\n",
      "El cliente  Escritura con probabilidad [0.03303802 0.96696198]\n",
      "El cliente  Escritura con probabilidad [0.0922516 0.9077484]\n",
      "El cliente  Escritura con probabilidad [0.12722446 0.87277554]\n",
      "El cliente  Escritura con probabilidad [0.09955406 0.90044594]\n",
      "El cliente  Escritura con probabilidad [0.13242167 0.86757833]\n",
      "El cliente  Escritura con probabilidad [0.06525216 0.93474784]\n",
      "El cliente  Escritura con probabilidad [0.10876487 0.89123513]\n",
      "El cliente  Escritura con probabilidad [0.00725805 0.99274195]\n",
      "El cliente  Escritura con probabilidad [0.18100197 0.81899803]\n",
      "El cliente  Escritura con probabilidad [0.1342773 0.8657227]\n",
      "El cliente  Escritura con probabilidad [0.04830073 0.95169927]\n",
      "El cliente  Escritura con probabilidad [0.18636688 0.81363312]\n",
      "El cliente  Escritura con probabilidad [0.15419206 0.84580794]\n",
      "El cliente  Escritura con probabilidad [0.07090921 0.92909079]\n",
      "El cliente  Escritura con probabilidad [0.17703854 0.82296146]\n",
      "El cliente  Escritura con probabilidad [0.30388254 0.69611746]\n",
      "El cliente  Escritura con probabilidad [0.0084724 0.9915276]\n",
      "El cliente  Escritura con probabilidad [0.19928479 0.80071521]\n",
      "El cliente  Escritura con probabilidad [0.26152047 0.73847953]\n",
      "El cliente  Escritura con probabilidad [0.07604088 0.92395912]\n",
      "El cliente  Escritura con probabilidad [0.28171456 0.71828544]\n",
      "El cliente  Escritura con probabilidad [0.20488642 0.79511358]\n",
      "El cliente  Escritura con probabilidad [0.09044555 0.90955445]\n",
      "El cliente  Escritura con probabilidad [0.09432156 0.90567844]\n",
      "El cliente  Escritura con probabilidad [0.25061678 0.74938322]\n",
      "El cliente  Escritura con probabilidad [0.17279367 0.82720633]\n",
      "El cliente  Escritura con probabilidad [0.04752545 0.95247455]\n",
      "El cliente  Escritura con probabilidad [0.0354638 0.9645362]\n",
      "El cliente  Escritura con probabilidad [0.20293889 0.79706111]\n",
      "El cliente  Escritura con probabilidad [0.14203516 0.85796484]\n",
      "El cliente  Escritura con probabilidad [0.00165081 0.99834919]\n",
      "El cliente  Escritura con probabilidad [0.19994055 0.80005945]\n",
      "El cliente  Escritura con probabilidad [0.10883978 0.89116022]\n",
      "El cliente  Escritura con probabilidad [0.19092115 0.80907885]\n",
      "El cliente  Escritura con probabilidad [0.01525293 0.98474707]\n",
      "El cliente  Escritura con probabilidad [0.06685472 0.93314528]\n",
      "El cliente  Escritura con probabilidad [0.26634292 0.73365708]\n",
      "El cliente  Escritura con probabilidad [0.21099229 0.78900771]\n",
      "El cliente  Escritura con probabilidad [0.04046468 0.95953532]\n",
      "El cliente  Escritura con probabilidad [5.24025268e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.14469947 0.85530053]\n",
      "El cliente  Escritura con probabilidad [0.02492168 0.97507832]\n",
      "El cliente  Escritura con probabilidad [0.05931085 0.94068915]\n",
      "El cliente  Escritura con probabilidad [0.21286827 0.78713173]\n",
      "El cliente  Escritura con probabilidad [3.19041191e-05 9.99968096e-01]\n",
      "El cliente  Escritura con probabilidad [0.12622168 0.87377832]\n",
      "El cliente  Escritura con probabilidad [0.18641948 0.81358052]\n",
      "El cliente  Escritura con probabilidad [4.16305597e-07 9.99999584e-01]\n",
      "El cliente  Escritura con probabilidad [0.01771056 0.98228944]\n",
      "El cliente  Escritura con probabilidad [0.28316468 0.71683532]\n",
      "El cliente  Escritura con probabilidad [0.06598788 0.93401212]\n",
      "El cliente  Escritura con probabilidad [5.24480697e-07 9.99999476e-01]\n",
      "El cliente  Escritura con probabilidad [0.07217431 0.92782569]\n",
      "El cliente  Escritura con probabilidad [0.00578246 0.99421754]\n",
      "El cliente  Escritura con probabilidad [0.1787325 0.8212675]\n",
      "El cliente  Escritura con probabilidad [0.18501145 0.81498855]\n",
      "El cliente  Escritura con probabilidad [0.02063918 0.97936082]\n",
      "El cliente  Escritura con probabilidad [2.11407226e-04 9.99788593e-01]\n",
      "El cliente  Escritura con probabilidad [0.10232559 0.89767441]\n",
      "El cliente  Escritura con probabilidad [0.24752677 0.75247323]\n",
      "El cliente  Escritura con probabilidad [0.06026077 0.93973923]\n",
      "El cliente  Escritura con probabilidad [0.04141777 0.95858223]\n",
      "El cliente  Escritura con probabilidad [0.03377591 0.96622409]\n",
      "El cliente  Escritura con probabilidad [0.01881428 0.98118572]\n",
      "El cliente  Escritura con probabilidad [0.18196013 0.81803987]\n",
      "El cliente  Escritura con probabilidad [8.92743516e-05 9.99910726e-01]\n",
      "El cliente  Escritura con probabilidad [8.06517065e-05 9.99919348e-01]\n",
      "El cliente  Escritura con probabilidad [2.53788017e-07 9.99999746e-01]\n",
      "El cliente  Escritura con probabilidad [0.11153446 0.88846554]\n",
      "El cliente  Escritura con probabilidad [0.1141986 0.8858014]\n",
      "El cliente  Escritura con probabilidad [0.01685639 0.98314361]\n",
      "El cliente  Escritura con probabilidad [0.00852919 0.99147081]\n",
      "El cliente  Escritura con probabilidad [0.08206285 0.91793715]\n",
      "El cliente  Escritura con probabilidad [0.10731626 0.89268374]\n",
      "El cliente  Escritura con probabilidad [4.25172243e-06 9.99995748e-01]\n",
      "El cliente  Escritura con probabilidad [0.05465205 0.94534795]\n",
      "El cliente  Escritura con probabilidad [0.28208609 0.71791391]\n",
      "El cliente  Escritura con probabilidad [0.24453797 0.75546203]\n",
      "El cliente  Escritura con probabilidad [0.02250485 0.97749515]\n",
      "El cliente  Escritura con probabilidad [0.19825309 0.80174691]\n",
      "El cliente  Escritura con probabilidad [3.33066907e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.43247081 0.56752919]\n",
      "El cliente  Escritura con probabilidad [0.06828688 0.93171312]\n",
      "El cliente  Escritura con probabilidad [0.06550438 0.93449562]\n",
      "El cliente  Escritura con probabilidad [0.02622211 0.97377789]\n",
      "El cliente  Escritura con probabilidad [0.43762416 0.56237584]\n",
      "El cliente  Escritura con probabilidad [0.0010624 0.9989376]\n",
      "El cliente  Escritura con probabilidad [0.12669099 0.87330901]\n",
      "El cliente  Escritura con probabilidad [3.97237132e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.13876692 0.86123308]\n",
      "El cliente  Escritura con probabilidad [0.08489135 0.91510865]\n",
      "El cliente  Escritura con probabilidad [0.16616964 0.83383036]\n",
      "El cliente  Escritura con probabilidad [0.07259622 0.92740378]\n",
      "El cliente  Escritura con probabilidad [0.07740711 0.92259289]\n",
      "El cliente  Escritura con probabilidad [0.00133197 0.99866803]\n",
      "El cliente  Escritura con probabilidad [0.17485965 0.82514035]\n",
      "El cliente  Escritura con probabilidad [0.223572 0.776428]\n",
      "El cliente  Escritura con probabilidad [1.29115245e-08 9.99999987e-01]\n",
      "El cliente  Escritura con probabilidad [3.33414894e-05 9.99966659e-01]\n",
      "El cliente  Escritura con probabilidad [0.05275472 0.94724528]\n",
      "El cliente  Escritura con probabilidad [0.08161808 0.91838192]\n",
      "El cliente  Escritura con probabilidad [0.3041117 0.6958883]\n",
      "El cliente  Escritura con probabilidad [0.17778083 0.82221917]\n",
      "El cliente  Escritura con probabilidad [0.2070146 0.7929854]\n",
      "El cliente  Escritura con probabilidad [0.01954933 0.98045067]\n",
      "El cliente  Escritura con probabilidad [0.09775715 0.90224285]\n",
      "El cliente  Escritura con probabilidad [0.02072824 0.97927176]\n",
      "El cliente  Escritura con probabilidad [0.25811423 0.74188577]\n",
      "El cliente  Escritura con probabilidad [0.20174603 0.79825397]\n",
      "El cliente  Escritura con probabilidad [0.13521828 0.86478172]\n",
      "El cliente  Escritura con probabilidad [0.24361457 0.75638543]\n",
      "El cliente  Escritura con probabilidad [0.26607704 0.73392296]\n",
      "El cliente  Escritura con probabilidad [7.0324635e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [2.25563651e-06 9.99997744e-01]\n",
      "El cliente  Escritura con probabilidad [0.3919349 0.6080651]\n",
      "El cliente  Escritura con probabilidad [0.14298675 0.85701325]\n",
      "El cliente  Escritura con probabilidad [0.13669696 0.86330304]\n",
      "El cliente  Escritura con probabilidad [0.03001823 0.96998177]\n",
      "El cliente  Escritura con probabilidad [3.2132963e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.10415607 0.89584393]\n",
      "El cliente  Escritura con probabilidad [0.12343075 0.87656925]\n",
      "El cliente  Escritura con probabilidad [6.25210080e-05 9.99937479e-01]\n",
      "El cliente  Escritura con probabilidad [0.03379984 0.96620016]\n",
      "El cliente  Escritura con probabilidad [0.17655296 0.82344704]\n",
      "El cliente  Escritura con probabilidad [0.21390635 0.78609365]\n",
      "El cliente  Escritura con probabilidad [0.22543196 0.77456804]\n",
      "El cliente  Escritura con probabilidad [1.21656640e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.22200431 0.77799569]\n",
      "El cliente  Escritura con probabilidad [0.14476416 0.85523584]\n",
      "El cliente  Escritura con probabilidad [0.16032788 0.83967212]\n",
      "El cliente  Escritura con probabilidad [0.02061755 0.97938245]\n",
      "El cliente  Escritura con probabilidad [1.74273930e-06 9.99998257e-01]\n",
      "El cliente  Escritura con probabilidad [0.14888226 0.85111774]\n",
      "El cliente  Escritura con probabilidad [5.84997856e-08 9.99999942e-01]\n",
      "El cliente  Escritura con probabilidad [0.21017405 0.78982595]\n",
      "El cliente  Escritura con probabilidad [5.26805488e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.24117043 0.75882957]\n",
      "El cliente  Escritura con probabilidad [0.22272022 0.77727978]\n",
      "El cliente  Escritura con probabilidad [0.41668608 0.58331392]\n",
      "El cliente  Escritura con probabilidad [0.17351051 0.82648949]\n",
      "El cliente  Escritura con probabilidad [2.52474950e-06 9.99997475e-01]\n",
      "El cliente  Escritura con probabilidad [0.21271982 0.78728018]\n",
      "El cliente  Escritura con probabilidad [0.11597028 0.88402972]\n",
      "El cliente  Escritura con probabilidad [4.03640515e-07 9.99999596e-01]\n",
      "El cliente  Escritura con probabilidad [0.20276859 0.79723141]\n",
      "El cliente  Escritura con probabilidad [0.32234904 0.67765096]\n",
      "El cliente  Escritura con probabilidad [0.12295823 0.87704177]\n",
      "El cliente  Escritura con probabilidad [0.20547094 0.79452906]\n",
      "El cliente  Escritura con probabilidad [0.21953931 0.78046069]\n",
      "El cliente  Escritura con probabilidad [0.23827884 0.76172116]\n",
      "El cliente  Escritura con probabilidad [0.22228973 0.77771027]\n",
      "El cliente  Escritura con probabilidad [4.59576222e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [0.18675011 0.81324989]\n",
      "El cliente  Escritura con probabilidad [0.00347891 0.99652109]\n",
      "El cliente  Escritura con probabilidad [0.08210532 0.91789468]\n",
      "El cliente  Escritura con probabilidad [0.21094175 0.78905825]\n",
      "El cliente  Escritura con probabilidad [0.28395987 0.71604013]\n",
      "El cliente  Escritura con probabilidad [0.01822771 0.98177229]\n",
      "El cliente  Escritura con probabilidad [0.06829116 0.93170884]\n",
      "El cliente  Escritura con probabilidad [0.14642236 0.85357764]\n",
      "El cliente  Escritura con probabilidad [0.21669165 0.78330835]\n",
      "El cliente  Escritura con probabilidad [0.14270198 0.85729802]\n",
      "El cliente  Escritura con probabilidad [0.02135551 0.97864449]\n",
      "El cliente  Escritura con probabilidad [0.10992485 0.89007515]\n",
      "El cliente  Escritura con probabilidad [6.17386240e-04 9.99382614e-01]\n",
      "El cliente  Escritura con probabilidad [0.06657723 0.93342277]\n",
      "El cliente  Escritura con probabilidad [0.13688268 0.86311732]\n",
      "El cliente  Escritura con probabilidad [0.2183992 0.7816008]\n",
      "El cliente  Escritura con probabilidad [0.16076008 0.83923992]\n",
      "El cliente  Escritura con probabilidad [1.98625043e-06 9.99998014e-01]\n",
      "El cliente  Escritura con probabilidad [0.19126671 0.80873329]\n",
      "El cliente  Escritura con probabilidad [0.13200819 0.86799181]\n",
      "El cliente  Escritura con probabilidad [0.10583074 0.89416926]\n",
      "El cliente  Escritura con probabilidad [0.12222927 0.87777073]\n",
      "El cliente  Escritura con probabilidad [0.10518163 0.89481837]\n",
      "El cliente  Escritura con probabilidad [0.05800339 0.94199661]\n",
      "El cliente  Escritura con probabilidad [0.01037139 0.98962861]\n",
      "El cliente  Escritura con probabilidad [0.15244474 0.84755526]\n",
      "El cliente  Escritura con probabilidad [0.11037652 0.88962348]\n",
      "El cliente  Escritura con probabilidad [0.03742103 0.96257897]\n",
      "El cliente  Escritura con probabilidad [0.05530859 0.94469141]\n",
      "El cliente  Escritura con probabilidad [0.32227276 0.67772724]\n",
      "El cliente  Escritura con probabilidad [0.34122068 0.65877932]\n",
      "El cliente  Escritura con probabilidad [0.02446318 0.97553682]\n",
      "El cliente  Escritura con probabilidad [7.82707232e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.08527535 0.91472465]\n",
      "El cliente  Escritura con probabilidad [2.34916360e-06 9.99997651e-01]\n",
      "El cliente  Escritura con probabilidad [0.18690613 0.81309387]\n",
      "El cliente  Escritura con probabilidad [0.26565101 0.73434899]\n",
      "El cliente  Escritura con probabilidad [0.08942951 0.91057049]\n",
      "El cliente  Escritura con probabilidad [0.02753615 0.97246385]\n",
      "El cliente  Escritura con probabilidad [0.06379223 0.93620777]\n",
      "El cliente  Escritura con probabilidad [0.02760383 0.97239617]\n",
      "El cliente  Escritura con probabilidad [0.00886039 0.99113961]\n",
      "El cliente  Escritura con probabilidad [1.26165406e-08 9.99999987e-01]\n",
      "El cliente  Escritura con probabilidad [0.0046312 0.9953688]\n",
      "El cliente  Escritura con probabilidad [0.07690852 0.92309148]\n",
      "El cliente  Escritura con probabilidad [0.21761033 0.78238967]\n",
      "El cliente  Escritura con probabilidad [0.19962162 0.80037838]\n",
      "El cliente  Escritura con probabilidad [0.14885766 0.85114234]\n",
      "El cliente  Escritura con probabilidad [0.02760657 0.97239343]\n",
      "El cliente  Escritura con probabilidad [0.11432472 0.88567528]\n",
      "El cliente  Escritura con probabilidad [0.16478568 0.83521432]\n",
      "El cliente  Escritura con probabilidad [0.05419372 0.94580628]\n",
      "El cliente  Escritura con probabilidad [0.19732907 0.80267093]\n",
      "El cliente  Escritura con probabilidad [0.19367459 0.80632541]\n",
      "El cliente  Escritura con probabilidad [0.11090087 0.88909913]\n",
      "El cliente  Escritura con probabilidad [0.28408784 0.71591216]\n",
      "El cliente  Escritura con probabilidad [1.68958533e-04 9.99831041e-01]\n",
      "El cliente  Escritura con probabilidad [0.13344098 0.86655902]\n",
      "El cliente  Escritura con probabilidad [0.20287279 0.79712721]\n",
      "El cliente  Escritura con probabilidad [0.20066663 0.79933337]\n",
      "El cliente  Escritura con probabilidad [0.06351777 0.93648223]\n",
      "El cliente  Escritura con probabilidad [0.16875443 0.83124557]\n",
      "El cliente  Escritura con probabilidad [0.21587835 0.78412165]\n",
      "El cliente  Escritura con probabilidad [0.06042389 0.93957611]\n",
      "El cliente  Escritura con probabilidad [0.05690253 0.94309747]\n",
      "El cliente  Escritura con probabilidad [0.08287614 0.91712386]\n",
      "El cliente  Escritura con probabilidad [0.26331077 0.73668923]\n",
      "El cliente  Escritura con probabilidad [0.26205635 0.73794365]\n",
      "El cliente  Escritura con probabilidad [6.19218454e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [8.1236573e-11 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.18612685 0.81387315]\n",
      "El cliente  Escritura con probabilidad [0.00163021 0.99836979]\n",
      "El cliente  Escritura con probabilidad [2.50673685e-06 9.99997493e-01]\n",
      "El cliente  Escritura con probabilidad [0.10299928 0.89700072]\n",
      "El cliente  Escritura con probabilidad [0.1253766 0.8746234]\n",
      "El cliente  Escritura con probabilidad [0.03635737 0.96364263]\n",
      "El cliente  Escritura con probabilidad [0.23111279 0.76888721]\n",
      "El cliente  Escritura con probabilidad [6.43484579e-07 9.99999357e-01]\n",
      "El cliente  Escritura con probabilidad [0.12794998 0.87205002]\n",
      "El cliente  Escritura con probabilidad [0.24481065 0.75518935]\n",
      "El cliente  Escritura con probabilidad [0.02953054 0.97046946]\n",
      "El cliente  Escritura con probabilidad [0.06451538 0.93548462]\n",
      "El cliente  Escritura con probabilidad [0.08401586 0.91598414]\n",
      "El cliente  Escritura con probabilidad [1.70525736e-06 9.99998295e-01]\n",
      "El cliente  Escritura con probabilidad [0.12178792 0.87821208]\n",
      "El cliente  Escritura con probabilidad [0.17187599 0.82812401]\n",
      "El cliente  Escritura con probabilidad [0.20792545 0.79207455]\n",
      "El cliente  Escritura con probabilidad [0.06342181 0.93657819]\n",
      "El cliente  Escritura con probabilidad [0.01768673 0.98231327]\n",
      "El cliente  Escritura con probabilidad [1.23455682e-08 9.99999988e-01]\n",
      "El cliente  Escritura con probabilidad [4.65305452e-08 9.99999953e-01]\n",
      "El cliente  Escritura con probabilidad [0.17365249 0.82634751]\n",
      "El cliente  Escritura con probabilidad [0.24575341 0.75424659]\n",
      "El cliente  Escritura con probabilidad [0.22781441 0.77218559]\n",
      "El cliente  Escritura con probabilidad [0.04185892 0.95814108]\n",
      "El cliente  Escritura con probabilidad [0.18706744 0.81293256]\n",
      "El cliente  Escritura con probabilidad [0.23283194 0.76716806]\n",
      "El cliente  Escritura con probabilidad [0.25901234 0.74098766]\n",
      "El cliente  Escritura con probabilidad [0.26302957 0.73697043]\n",
      "El cliente  Escritura con probabilidad [0.1399872 0.8600128]\n",
      "El cliente  Escritura con probabilidad [0.25751783 0.74248217]\n",
      "El cliente  Escritura con probabilidad [1.66764402e-09 9.99999998e-01]\n",
      "El cliente  Escritura con probabilidad [0.24528114 0.75471886]\n",
      "El cliente  Escritura con probabilidad [0.0794457 0.9205543]\n",
      "El cliente  Escritura con probabilidad [3.99680289e-15 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.22052908 0.77947092]\n",
      "El cliente  Escritura con probabilidad [0.17107228 0.82892772]\n",
      "El cliente  Escritura con probabilidad [0.27305343 0.72694657]\n",
      "El cliente  Escritura con probabilidad [0.05121215 0.94878785]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.0089679 0.9910321]\n",
      "El cliente  Escritura con probabilidad [0.09977383 0.90022617]\n",
      "El cliente  Escritura con probabilidad [0.00602517 0.99397483]\n",
      "El cliente  Escritura con probabilidad [0.26630197 0.73369803]\n",
      "El cliente  Escritura con probabilidad [0.26555782 0.73444218]\n",
      "El cliente  Escritura con probabilidad [0.03863512 0.96136488]\n",
      "El cliente  Escritura con probabilidad [0.12920389 0.87079611]\n",
      "El cliente  Escritura con probabilidad [0.00187407 0.99812593]\n",
      "El cliente  Escritura con probabilidad [5.44586598e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.43027983 0.56972017]\n",
      "El cliente  Escritura con probabilidad [0.16370814 0.83629186]\n",
      "El cliente  Escritura con probabilidad [0.19392789 0.80607211]\n",
      "El cliente  Escritura con probabilidad [0.01787275 0.98212725]\n",
      "El cliente  Escritura con probabilidad [0.13389033 0.86610967]\n",
      "El cliente  Escritura con probabilidad [0.06613092 0.93386908]\n",
      "El cliente  Escritura con probabilidad [0.41815887 0.58184113]\n",
      "El cliente  Escritura con probabilidad [0.16518266 0.83481734]\n",
      "El cliente  Escritura con probabilidad [1.19904087e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.00541272 0.99458728]\n",
      "El cliente  Escritura con probabilidad [0.07105242 0.92894758]\n",
      "El cliente  Escritura con probabilidad [0.38872847 0.61127153]\n",
      "El cliente  Escritura con probabilidad [0.21523145 0.78476855]\n",
      "El cliente  Escritura con probabilidad [7.46675168e-08 9.99999925e-01]\n",
      "El cliente  Escritura con probabilidad [0.07223659 0.92776341]\n",
      "El cliente  Escritura con probabilidad [1.55356857e-05 9.99984464e-01]\n",
      "El cliente  Escritura con probabilidad [0.31334891 0.68665109]\n",
      "El cliente  Escritura con probabilidad [2.10324120e-06 9.99997897e-01]\n",
      "El cliente  Escritura con probabilidad [0.09559688 0.90440312]\n",
      "El cliente  Escritura con probabilidad [0.00103733 0.99896267]\n",
      "El cliente  Escritura con probabilidad [0.07799723 0.92200277]\n",
      "El cliente  Escritura con probabilidad [0.136741 0.863259]\n",
      "El cliente  Escritura con probabilidad [2.37879499e-07 9.99999762e-01]\n",
      "El cliente  Escritura con probabilidad [0.12003585 0.87996415]\n",
      "El cliente  Escritura con probabilidad [1.35411851e-04 9.99864588e-01]\n",
      "El cliente  Escritura con probabilidad [0.2770275 0.7229725]\n",
      "El cliente  Escritura con probabilidad [0.2033201 0.7966799]\n",
      "El cliente  Escritura con probabilidad [3.17965803e-05 9.99968203e-01]\n",
      "El cliente  Escritura con probabilidad [0.08770613 0.91229387]\n",
      "El cliente  Escritura con probabilidad [5.13757659e-09 9.99999995e-01]\n",
      "El cliente  Escritura con probabilidad [7.05564073e-08 9.99999929e-01]\n",
      "El cliente  Escritura con probabilidad [0.03946108 0.96053892]\n",
      "El cliente  Escritura con probabilidad [0.03666377 0.96333623]\n",
      "El cliente  Escritura con probabilidad [0.45566576 0.54433424]\n",
      "El cliente  Escritura con probabilidad [0.28903717 0.71096283]\n",
      "El cliente  Escritura con probabilidad [0.15646748 0.84353252]\n",
      "El cliente  Escritura con probabilidad [0.02256556 0.97743444]\n",
      "El cliente  Escritura con probabilidad [0.09271644 0.90728356]\n",
      "El cliente  Escritura con probabilidad [0.20947147 0.79052853]\n",
      "El cliente  Escritura con probabilidad [0.00582777 0.99417223]\n",
      "El cliente  Escritura con probabilidad [0.28533938 0.71466062]\n",
      "El cliente  Escritura con probabilidad [0.00136738 0.99863262]\n",
      "El cliente  Escritura con probabilidad [4.92704956e-07 9.99999507e-01]\n",
      "El cliente  Escritura con probabilidad [0.05284705 0.94715295]\n",
      "El cliente  Escritura con probabilidad [0.18498147 0.81501853]\n",
      "El cliente  Escritura con probabilidad [4.87027488e-04 9.99512973e-01]\n",
      "El cliente  Escritura con probabilidad [0.05231326 0.94768674]\n",
      "El cliente  Escritura con probabilidad [9.36923598e-08 9.99999906e-01]\n",
      "El cliente  Escritura con probabilidad [0.2674493 0.7325507]\n",
      "El cliente  Escritura con probabilidad [0.2668869 0.7331131]\n",
      "El cliente  Escritura con probabilidad [0.11438442 0.88561558]\n",
      "El cliente  Escritura con probabilidad [0.17442883 0.82557117]\n",
      "El cliente  Escritura con probabilidad [0.19114375 0.80885625]\n",
      "El cliente  Escritura con probabilidad [0.26807811 0.73192189]\n",
      "El cliente  Escritura con probabilidad [0.14722866 0.85277134]\n",
      "El cliente  Escritura con probabilidad [0.10246257 0.89753743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.17665717 0.82334283]\n",
      "El cliente  Escritura con probabilidad [0.28071232 0.71928768]\n",
      "El cliente  Escritura con probabilidad [0.02319189 0.97680811]\n",
      "El cliente  Escritura con probabilidad [9.48362501e-07 9.99999052e-01]\n",
      "El cliente  Escritura con probabilidad [1.90103784e-05 9.99980990e-01]\n",
      "El cliente  Escritura con probabilidad [0.02338293 0.97661707]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.27259948 0.72740052]\n",
      "El cliente  Escritura con probabilidad [0.22577368 0.77422632]\n",
      "El cliente  Escritura con probabilidad [0.01480803 0.98519197]\n",
      "El cliente  Escritura con probabilidad [0.26528204 0.73471796]\n",
      "El cliente  Escritura con probabilidad [0.22513887 0.77486113]\n",
      "El cliente  Escritura con probabilidad [0.08461442 0.91538558]\n",
      "El cliente  Escritura con probabilidad [0.15047241 0.84952759]\n",
      "El cliente  Escritura con probabilidad [0.43304833 0.56695167]\n",
      "El cliente  Escritura con probabilidad [0.01598574 0.98401426]\n",
      "El cliente  Escritura con probabilidad [0.22174177 0.77825823]\n",
      "El cliente  Escritura con probabilidad [0.13233766 0.86766234]\n",
      "El cliente  Escritura con probabilidad [0.19663354 0.80336646]\n",
      "El cliente  Escritura con probabilidad [0.02018865 0.97981135]\n",
      "El cliente  Escritura con probabilidad [0.1211506 0.8788494]\n",
      "El cliente  Escritura con probabilidad [0.0132967 0.9867033]\n",
      "El cliente  Escritura con probabilidad [0.03454613 0.96545387]\n",
      "El cliente  Escritura con probabilidad [0.28738427 0.71261573]\n",
      "El cliente  Escritura con probabilidad [0.01895213 0.98104787]\n",
      "El cliente  Escritura con probabilidad [2.62678421e-08 9.99999974e-01]\n",
      "El cliente  Escritura con probabilidad [0.00316079 0.99683921]\n",
      "El cliente  Escritura con probabilidad [0.1614914 0.8385086]\n",
      "El cliente  Escritura con probabilidad [0.12864168 0.87135832]\n",
      "El cliente  Escritura con probabilidad [1.30049305e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.17904009 0.82095991]\n",
      "El cliente  Escritura con probabilidad [0.22220599 0.77779401]\n",
      "El cliente  Escritura con probabilidad [0.22933718 0.77066282]\n",
      "El cliente  Escritura con probabilidad [0.03194578 0.96805422]\n",
      "El cliente  Escritura con probabilidad [1.96751171e-04 9.99803249e-01]\n",
      "El cliente  Escritura con probabilidad [0.16509508 0.83490492]\n",
      "El cliente  Escritura con probabilidad [0.11545744 0.88454256]\n",
      "El cliente  Escritura con probabilidad [0.04728149 0.95271851]\n",
      "El cliente  Escritura con probabilidad [3.67861874e-07 9.99999632e-01]\n",
      "El cliente  Escritura con probabilidad [0.09378456 0.90621544]\n",
      "El cliente  Escritura con probabilidad [0.35288665 0.64711335]\n",
      "El cliente  Escritura con probabilidad [0.28133215 0.71866785]\n",
      "El cliente  Escritura con probabilidad [0.2631038 0.7368962]\n",
      "El cliente  Escritura con probabilidad [0.27430639 0.72569361]\n",
      "El cliente  Escritura con probabilidad [0.1908317 0.8091683]\n",
      "El cliente  Escritura con probabilidad [0.24408697 0.75591303]\n",
      "El cliente  Escritura con probabilidad [0.07648898 0.92351102]\n",
      "El cliente  Escritura con probabilidad [0.00630616 0.99369384]\n",
      "El cliente  Escritura con probabilidad [2.08726947e-04 9.99791273e-01]\n",
      "El cliente  Escritura con probabilidad [0.11791035 0.88208965]\n",
      "El cliente  Escritura con probabilidad [0.25201943 0.74798057]\n",
      "El cliente  Escritura con probabilidad [0.18530427 0.81469573]\n",
      "El cliente  Escritura con probabilidad [0.14170239 0.85829761]\n",
      "El cliente  Escritura con probabilidad [0.19520534 0.80479466]\n",
      "El cliente  Escritura con probabilidad [3.84619791e-05 9.99961538e-01]\n",
      "El cliente  Escritura con probabilidad [0.06181254 0.93818746]\n",
      "El cliente  Escritura con probabilidad [0.22716621 0.77283379]\n",
      "El cliente  Escritura con probabilidad [0.19998623 0.80001377]\n",
      "El cliente  Escritura con probabilidad [0.0696899 0.9303101]\n",
      "El cliente  Escritura con probabilidad [0.05902107 0.94097893]\n",
      "El cliente  Escritura con probabilidad [0.19768031 0.80231969]\n",
      "El cliente  Escritura con probabilidad [0.00880462 0.99119538]\n",
      "El cliente  Escritura con probabilidad [0.09586867 0.90413133]\n",
      "El cliente  Escritura con probabilidad [0.19265439 0.80734561]\n",
      "El cliente  Escritura con probabilidad [0.26528204 0.73471796]\n",
      "El cliente  Escritura con probabilidad [0.07226113 0.92773887]\n",
      "El cliente  Escritura con probabilidad [0.14956955 0.85043045]\n",
      "El cliente  Escritura con probabilidad [0.07679931 0.92320069]\n",
      "El cliente  Escritura con probabilidad [0.00133915 0.99866085]\n",
      "El cliente  Escritura con probabilidad [2.31947179e-04 9.99768053e-01]\n",
      "El cliente  Escritura con probabilidad [0.22412174 0.77587826]\n",
      "El cliente  Escritura con probabilidad [0.01355135 0.98644865]\n",
      "El cliente  Escritura con probabilidad [6.12813128e-06 9.99993872e-01]\n",
      "El cliente  Escritura con probabilidad [5.57650912e-06 9.99994423e-01]\n",
      "El cliente  Escritura con probabilidad [0.19331036 0.80668964]\n",
      "El cliente  Escritura con probabilidad [0.21829026 0.78170974]\n",
      "El cliente  Escritura con probabilidad [0.1527084 0.8472916]\n",
      "El cliente  Escritura con probabilidad [0.01683724 0.98316276]\n",
      "El cliente  Escritura con probabilidad [0.11069002 0.88930998]\n",
      "El cliente  Escritura con probabilidad [0.17658367 0.82341633]\n",
      "El cliente  Escritura con probabilidad [0.00562354 0.99437646]\n",
      "El cliente  Escritura con probabilidad [0.05972889 0.94027111]\n",
      "El cliente  Escritura con probabilidad [0.00479269 0.99520731]\n",
      "El cliente  Escritura con probabilidad [1.29420353e-05 9.99987058e-01]\n",
      "El cliente  Escritura con probabilidad [0.06877899 0.93122101]\n",
      "El cliente  Escritura con probabilidad [0.04678312 0.95321688]\n",
      "El cliente  Escritura con probabilidad [0.04333059 0.95666941]\n",
      "El cliente  Escritura con probabilidad [0.02599939 0.97400061]\n",
      "El cliente  Escritura con probabilidad [0.10153752 0.89846248]\n",
      "El cliente  Escritura con probabilidad [0.16681264 0.83318736]\n",
      "El cliente  Escritura con probabilidad [0.23140604 0.76859396]\n",
      "El cliente  Escritura con probabilidad [0.19642752 0.80357248]\n",
      "El cliente  Escritura con probabilidad [0.12997217 0.87002783]\n",
      "El cliente  Escritura con probabilidad [0.07732059 0.92267941]\n",
      "El cliente  Escritura con probabilidad [0.02249318 0.97750682]\n",
      "El cliente  Escritura con probabilidad [1.75327280e-08 9.99999982e-01]\n",
      "El cliente  Escritura con probabilidad [0.28780341 0.71219659]\n",
      "El cliente  Escritura con probabilidad [7.68436204e-10 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [0.17359809 0.82640191]\n",
      "El cliente  Escritura con probabilidad [0.05940528 0.94059472]\n",
      "El cliente  Escritura con probabilidad [0.20139348 0.79860652]\n",
      "El cliente  Escritura con probabilidad [4.86015485e-04 9.99513985e-01]\n",
      "El cliente  Escritura con probabilidad [7.72128370e-08 9.99999923e-01]\n",
      "El cliente  Escritura con probabilidad [0.19897831 0.80102169]\n",
      "El cliente  Escritura con probabilidad [0.19648305 0.80351695]\n",
      "El cliente  Escritura con probabilidad [0.04494798 0.95505202]\n",
      "El cliente  Escritura con probabilidad [0.1192941 0.8807059]\n",
      "El cliente  Escritura con probabilidad [0.08221253 0.91778747]\n",
      "El cliente  Escritura con probabilidad [0.23058794 0.76941206]\n",
      "El cliente  Escritura con probabilidad [0.10627835 0.89372165]\n",
      "El cliente  Escritura con probabilidad [0.22737858 0.77262142]\n",
      "El cliente  Escritura con probabilidad [0.249753 0.750247]\n",
      "El cliente  Escritura con probabilidad [5.04563687e-08 9.99999950e-01]\n",
      "El cliente  Escritura con probabilidad [0.02636827 0.97363173]\n",
      "El cliente  Escritura con probabilidad [6.63136675e-06 9.99993369e-01]\n",
      "El cliente  Escritura con probabilidad [7.45177384e-06 9.99992548e-01]\n",
      "El cliente  Escritura con probabilidad [0.14229307 0.85770693]\n",
      "El cliente  Escritura con probabilidad [0.20049037 0.79950963]\n",
      "El cliente  Escritura con probabilidad [0.05164923 0.94835077]\n",
      "El cliente  Escritura con probabilidad [4.30467439e-10 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.09560401 0.90439599]\n",
      "El cliente  Escritura con probabilidad [0.065651 0.934349]\n",
      "El cliente  Escritura con probabilidad [0.14290108 0.85709892]\n",
      "El cliente  Escritura con probabilidad [0.44472645 0.55527355]\n",
      "El cliente  Escritura con probabilidad [0.25810769 0.74189231]\n",
      "El cliente  Escritura con probabilidad [0.13450154 0.86549846]\n",
      "El cliente  Escritura con probabilidad [0.07930353 0.92069647]\n",
      "El cliente  Escritura con probabilidad [0.28698911 0.71301089]\n",
      "El cliente  Escritura con probabilidad [1.2915069e-10 1.0000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.09700907 0.90299093]\n",
      "El cliente  Escritura con probabilidad [0.23238865 0.76761135]\n",
      "El cliente  Escritura con probabilidad [0.28746892 0.71253108]\n",
      "El cliente  Escritura con probabilidad [0.06603904 0.93396096]\n",
      "El cliente  Escritura con probabilidad [0.28184704 0.71815296]\n",
      "El cliente  Escritura con probabilidad [0.0062539 0.9937461]\n",
      "El cliente  Escritura con probabilidad [8.68281073e-06 9.99991317e-01]\n",
      "El cliente  Escritura con probabilidad [0.0353793 0.9646207]\n",
      "El cliente  Escritura con probabilidad [1.89024144e-05 9.99981098e-01]\n",
      "El cliente  Escritura con probabilidad [0.23366735 0.76633265]\n",
      "El cliente  Escritura con probabilidad [0.07467057 0.92532943]\n",
      "El cliente  Escritura con probabilidad [0.09996372 0.90003628]\n",
      "El cliente  Escritura con probabilidad [1.92773503e-06 9.99998072e-01]\n",
      "El cliente  Escritura con probabilidad [0.08276016 0.91723984]\n",
      "El cliente  Escritura con probabilidad [0.03434462 0.96565538]\n",
      "El cliente  Escritura con probabilidad [0.08322657 0.91677343]\n",
      "El cliente  Escritura con probabilidad [0.02199259 0.97800741]\n",
      "El cliente  Escritura con probabilidad [0.11507234 0.88492766]\n",
      "El cliente  Escritura con probabilidad [0.08599055 0.91400945]\n",
      "El cliente  Escritura con probabilidad [0.23974408 0.76025592]\n",
      "El cliente  Escritura con probabilidad [2.28324785e-06 9.99997717e-01]\n",
      "El cliente  Escritura con probabilidad [3.23402465e-06 9.99996766e-01]\n",
      "El cliente  Escritura con probabilidad [0.04024817 0.95975183]\n",
      "El cliente  Escritura con probabilidad [0.00466576 0.99533424]\n",
      "El cliente  Escritura con probabilidad [0.11604997 0.88395003]\n",
      "El cliente  Escritura con probabilidad [6.77778329e-05 9.99932222e-01]\n",
      "El cliente  Escritura con probabilidad [5.82615430e-07 9.99999417e-01]\n",
      "El cliente  Escritura con probabilidad [0.11795999 0.88204001]\n",
      "El cliente  Escritura con probabilidad [0.116025 0.883975]\n",
      "El cliente  Escritura con probabilidad [0.27854138 0.72145862]\n",
      "El cliente  Escritura con probabilidad [0.01893162 0.98106838]\n",
      "El cliente  Escritura con probabilidad [0.07757115 0.92242885]\n",
      "El cliente  Escritura con probabilidad [0.01780173 0.98219827]\n",
      "El cliente  Escritura con probabilidad [0.19229791 0.80770209]\n",
      "El cliente  Escritura con probabilidad [2.75811664e-04 9.99724188e-01]\n",
      "El cliente  Escritura con probabilidad [4.06651936e-07 9.99999593e-01]\n",
      "El cliente  Escritura con probabilidad [0.41708894 0.58291106]\n",
      "El cliente  Escritura con probabilidad [0.18678265 0.81321735]\n",
      "El cliente  Escritura con probabilidad [0.14877942 0.85122058]\n",
      "El cliente  Escritura con probabilidad [0.31182609 0.68817391]\n",
      "El cliente  Escritura con probabilidad [0.23999134 0.76000866]\n",
      "El cliente  Escritura con probabilidad [0.26405086 0.73594914]\n",
      "El cliente  Escritura con probabilidad [0.16202265 0.83797735]\n",
      "El cliente  Escritura con probabilidad [0.15669808 0.84330192]\n",
      "El cliente  Escritura con probabilidad [0.18737306 0.81262694]\n",
      "El cliente  Escritura con probabilidad [0.048022 0.951978]\n",
      "El cliente  Escritura con probabilidad [0.18666929 0.81333071]\n",
      "El cliente  Escritura con probabilidad [0.30260449 0.69739551]\n",
      "El cliente  Escritura con probabilidad [0.06901167 0.93098833]\n",
      "El cliente  Escritura con probabilidad [0.02152971 0.97847029]\n",
      "El cliente  Escritura con probabilidad [0.1321898 0.8678102]\n",
      "El cliente  Escritura con probabilidad [0.21318143 0.78681857]\n",
      "El cliente  Escritura con probabilidad [0.01855547 0.98144453]\n",
      "El cliente  Escritura con probabilidad [0.15822641 0.84177359]\n",
      "El cliente  Escritura con probabilidad [0.03004868 0.96995132]\n",
      "El cliente  Escritura con probabilidad [0.07902516 0.92097484]\n",
      "El cliente  Escritura con probabilidad [0.02727169 0.97272831]\n",
      "El cliente  Escritura con probabilidad [0.42058677 0.57941323]\n",
      "El cliente  Escritura con probabilidad [0.24163574 0.75836426]\n",
      "El cliente  Escritura con probabilidad [0.07727134 0.92272866]\n",
      "El cliente  Escritura con probabilidad [0.05026614 0.94973386]\n",
      "El cliente  Escritura con probabilidad [0.17731061 0.82268939]\n",
      "El cliente  Escritura con probabilidad [0.07835391 0.92164609]\n",
      "El cliente  Escritura con probabilidad [0.15490458 0.84509542]\n",
      "El cliente  Escritura con probabilidad [0.07096036 0.92903964]\n",
      "El cliente  Escritura con probabilidad [0.21806134 0.78193866]\n",
      "El cliente  Escritura con probabilidad [0.04576519 0.95423481]\n",
      "El cliente  Escritura con probabilidad [0.20873295 0.79126705]\n",
      "El cliente  Escritura con probabilidad [0.21747914 0.78252086]\n",
      "El cliente  Escritura con probabilidad [0.00232877 0.99767123]\n",
      "El cliente  Escritura con probabilidad [0.15249108 0.84750892]\n",
      "El cliente  Escritura con probabilidad [2.36613915e-06 9.99997634e-01]\n",
      "El cliente  Escritura con probabilidad [0.01411783 0.98588217]\n",
      "El cliente  Escritura con probabilidad [0.20922574 0.79077426]\n",
      "El cliente  Escritura con probabilidad [0.26833011 0.73166989]\n",
      "El cliente  Escritura con probabilidad [0.00244642 0.99755358]\n",
      "El cliente  Escritura con probabilidad [0.30551947 0.69448053]\n",
      "El cliente  Escritura con probabilidad [0.15288667 0.84711333]\n",
      "El cliente  Escritura con probabilidad [9.99389378e-06 9.99990006e-01]\n",
      "El cliente  Escritura con probabilidad [0.03055056 0.96944944]\n",
      "El cliente  Escritura con probabilidad [0.02709284 0.97290716]\n",
      "El cliente  Escritura con probabilidad [0.06076607 0.93923393]\n",
      "El cliente  Escritura con probabilidad [0.2056785 0.7943215]\n",
      "El cliente  Escritura con probabilidad [0.02329746 0.97670254]\n",
      "El cliente  Escritura con probabilidad [0.04424308 0.95575692]\n",
      "El cliente  Escritura con probabilidad [0.22015218 0.77984782]\n",
      "El cliente  Escritura con probabilidad [0.18113752 0.81886248]\n",
      "El cliente  Escritura con probabilidad [0.24763224 0.75236776]\n",
      "El cliente  Escritura con probabilidad [0.11900541 0.88099459]\n",
      "El cliente  Escritura con probabilidad [0.19648442 0.80351558]\n",
      "El cliente  Escritura con probabilidad [0.18589279 0.81410721]\n",
      "El cliente  Escritura con probabilidad [0.0024021 0.9975979]\n",
      "El cliente  Escritura con probabilidad [0.24437705 0.75562295]\n",
      "El cliente  Escritura con probabilidad [0.06781674 0.93218326]\n",
      "El cliente  Escritura con probabilidad [0.21377447 0.78622553]\n",
      "El cliente  Escritura con probabilidad [0.19867383 0.80132617]\n",
      "El cliente  Escritura con probabilidad [0.23194404 0.76805596]\n",
      "El cliente  Escritura con probabilidad [0.22987239 0.77012761]\n",
      "El cliente  Escritura con probabilidad [0.05790922 0.94209078]\n",
      "El cliente  Escritura con probabilidad [0.04968862 0.95031138]\n",
      "El cliente  Escritura con probabilidad [0.14856598 0.85143402]\n",
      "El cliente  Escritura con probabilidad [1.00505800e-06 9.99998995e-01]\n",
      "El cliente  Escritura con probabilidad [0.22832243 0.77167757]\n",
      "El cliente  Escritura con probabilidad [0.25340752 0.74659248]\n",
      "El cliente  Escritura con probabilidad [0.13682619 0.86317381]\n",
      "El cliente  Escritura con probabilidad [0.10113059 0.89886941]\n",
      "El cliente  Escritura con probabilidad [0.27498236 0.72501764]\n",
      "El cliente  Escritura con probabilidad [0.23296844 0.76703156]\n",
      "El cliente  Escritura con probabilidad [0.22881639 0.77118361]\n",
      "El cliente  Escritura con probabilidad [0.40790561 0.59209439]\n",
      "El cliente  Escritura con probabilidad [0.09302122 0.90697878]\n",
      "El cliente  Escritura con probabilidad [0.13702173 0.86297827]\n",
      "El cliente  Escritura con probabilidad [0.32182764 0.67817236]\n",
      "El cliente  Escritura con probabilidad [3.25739967e-06 9.99996743e-01]\n",
      "El cliente  Escritura con probabilidad [8.82988824e-06 9.99991170e-01]\n",
      "El cliente  Escritura con probabilidad [0.01639064 0.98360936]\n",
      "El cliente  Escritura con probabilidad [0.11110467 0.88889533]\n",
      "El cliente  Escritura con probabilidad [0.13770807 0.86229193]\n",
      "El cliente  Escritura con probabilidad [0.07789235 0.92210765]\n",
      "El cliente  Escritura con probabilidad [0.02978227 0.97021773]\n",
      "El cliente  Escritura con probabilidad [0.21958289 0.78041711]\n",
      "El cliente  Escritura con probabilidad [3.25668159e-09 9.99999997e-01]\n",
      "El cliente  Escritura con probabilidad [0.28224177 0.71775823]\n",
      "El cliente  Escritura con probabilidad [0.26483401 0.73516599]\n",
      "El cliente  Escritura con probabilidad [0.09781187 0.90218813]\n",
      "El cliente  Escritura con probabilidad [0.21408192 0.78591808]\n",
      "El cliente  Escritura con probabilidad [0.17030149 0.82969851]\n",
      "El cliente  Escritura con probabilidad [0.22990845 0.77009155]\n",
      "El cliente  Escritura con probabilidad [0.22553199 0.77446801]\n",
      "El cliente  Escritura con probabilidad [0.03969081 0.96030919]\n",
      "El cliente  Escritura con probabilidad [0.17339526 0.82660474]\n",
      "El cliente  Escritura con probabilidad [0.13229413 0.86770587]\n",
      "El cliente  Escritura con probabilidad [0.07755872 0.92244128]\n",
      "El cliente  Escritura con probabilidad [0.11043788 0.88956212]\n",
      "El cliente  Escritura con probabilidad [0.18470089 0.81529911]\n",
      "El cliente  Escritura con probabilidad [0.13782353 0.86217647]\n",
      "El cliente  Escritura con probabilidad [0.00574444 0.99425556]\n",
      "El cliente  Escritura con probabilidad [0.19733533 0.80266467]\n",
      "El cliente  Escritura con probabilidad [0.06824605 0.93175395]\n",
      "El cliente  Escritura con probabilidad [0.18997829 0.81002171]\n",
      "El cliente  Escritura con probabilidad [0. 1.]\n",
      "El cliente  Escritura con probabilidad [0.21947581 0.78052419]\n",
      "El cliente  Escritura con probabilidad [0.34065139 0.65934861]\n",
      "El cliente  Escritura con probabilidad [0.10363944 0.89636056]\n",
      "El cliente  Escritura con probabilidad [0.2242775 0.7757225]\n",
      "El cliente  Escritura con probabilidad [2.22022035e-04 9.99777978e-01]\n",
      "El cliente  Escritura con probabilidad [0.19623263 0.80376737]\n",
      "El cliente  Escritura con probabilidad [0.06759395 0.93240605]\n",
      "El cliente  Escritura con probabilidad [0.19589782 0.80410218]\n",
      "El cliente  Escritura con probabilidad [0.1880717 0.8119283]\n",
      "El cliente  Escritura con probabilidad [5.61992331e-04 9.99438008e-01]\n",
      "El cliente  Escritura con probabilidad [0.01526964 0.98473036]\n",
      "El cliente  Escritura con probabilidad [0.1556501 0.8443499]\n",
      "El cliente  Escritura con probabilidad [0.31186922 0.68813078]\n",
      "El cliente  Escritura con probabilidad [0.01681391 0.98318609]\n",
      "El cliente  Escritura con probabilidad [0.15589845 0.84410155]\n",
      "El cliente  Escritura con probabilidad [0.1168983 0.8831017]\n",
      "El cliente  Escritura con probabilidad [0.17183769 0.82816231]\n",
      "El cliente  Escritura con probabilidad [0.21014194 0.78985806]\n",
      "El cliente  Escritura con probabilidad [0.06595019 0.93404981]\n",
      "El cliente  Escritura con probabilidad [0.06255697 0.93744303]\n",
      "El cliente  Escritura con probabilidad [0.27297731 0.72702269]\n",
      "El cliente  Escritura con probabilidad [1.33625718e-06 9.99998664e-01]\n",
      "El cliente  Escritura con probabilidad [0.00226239 0.99773761]\n",
      "El cliente  Escritura con probabilidad [0.18331306 0.81668694]\n",
      "El cliente  Escritura con probabilidad [0.01272383 0.98727617]\n",
      "El cliente  Escritura con probabilidad [1.19405731e-05 9.99988059e-01]\n",
      "El cliente  Escritura con probabilidad [0.00479974 0.99520026]\n",
      "El cliente  Escritura con probabilidad [0.0200855 0.9799145]\n",
      "El cliente  Escritura con probabilidad [7.09622556e-05 9.99929038e-01]\n",
      "El cliente  Escritura con probabilidad [0.00513024 0.99486976]\n",
      "El cliente  Escritura con probabilidad [0.19577806 0.80422194]\n",
      "El cliente  Escritura con probabilidad [0.17388413 0.82611587]\n",
      "El cliente  Escritura con probabilidad [0.13827025 0.86172975]\n",
      "El cliente  Escritura con probabilidad [0.1078214 0.8921786]\n",
      "El cliente  Escritura con probabilidad [0.30516291 0.69483709]\n",
      "El cliente  Escritura con probabilidad [0.14616428 0.85383572]\n",
      "El cliente  Escritura con probabilidad [2.69784125e-04 9.99730216e-01]\n",
      "El cliente  Escritura con probabilidad [1.86869582e-04 9.99813130e-01]\n",
      "El cliente  Escritura con probabilidad [0.07256583 0.92743417]\n",
      "El cliente  Escritura con probabilidad [0.00143002 0.99856998]\n",
      "El cliente  Escritura con probabilidad [0.08530956 0.91469044]\n",
      "El cliente  Escritura con probabilidad [0.13642028 0.86357972]\n",
      "El cliente  Escritura con probabilidad [0.00188108 0.99811892]\n",
      "El cliente  Escritura con probabilidad [0.29890594 0.70109406]\n",
      "El cliente  Escritura con probabilidad [0.21860793 0.78139207]\n",
      "El cliente  Escritura con probabilidad [0.02611586 0.97388414]\n",
      "El cliente  Escritura con probabilidad [0.14588056 0.85411944]\n",
      "El cliente  Escritura con probabilidad [7.89436822e-04 9.99210563e-01]\n",
      "El cliente  Escritura con probabilidad [1.02556205e-04 9.99897444e-01]\n",
      "El cliente  Escritura con probabilidad [0.19909558 0.80090442]\n",
      "El cliente  Escritura con probabilidad [0.01666673 0.98333327]\n",
      "El cliente  Escritura con probabilidad [3.99580917e-05 9.99960042e-01]\n",
      "El cliente  Escritura con probabilidad [0.02343623 0.97656377]\n",
      "El cliente  Escritura con probabilidad [0.01835545 0.98164455]\n",
      "El cliente  Escritura con probabilidad [0.11058677 0.88941323]\n",
      "El cliente  Escritura con probabilidad [0.04258352 0.95741648]\n",
      "El cliente  Escritura con probabilidad [0.07288324 0.92711676]\n",
      "El cliente  Escritura con probabilidad [0.11669886 0.88330114]\n",
      "El cliente  Escritura con probabilidad [0.12638132 0.87361868]\n",
      "El cliente  Escritura con probabilidad [0.2144888 0.7855112]\n",
      "El cliente  Escritura con probabilidad [0.26039348 0.73960652]\n",
      "El cliente  Escritura con probabilidad [0.13982239 0.86017761]\n",
      "El cliente  Escritura con probabilidad [0.16819723 0.83180277]\n",
      "El cliente  Escritura con probabilidad [0.14982473 0.85017527]\n",
      "El cliente  Escritura con probabilidad [0.14498486 0.85501514]\n",
      "El cliente  Escritura con probabilidad [1.19382637e-09 9.99999999e-01]\n",
      "El cliente  Escritura con probabilidad [1.83276566e-06 9.99998167e-01]\n",
      "El cliente  Escritura con probabilidad [0.04421307 0.95578693]\n",
      "El cliente  Escritura con probabilidad [0.17862715 0.82137285]\n",
      "El cliente  Escritura con probabilidad [0.13833381 0.86166619]\n",
      "El cliente  Escritura con probabilidad [5.41722223e-12 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.19333027 0.80666973]\n",
      "El cliente  Escritura con probabilidad [3.91876243e-09 9.99999996e-01]\n",
      "El cliente  Escritura con probabilidad [0.03953818 0.96046182]\n",
      "El cliente  Escritura con probabilidad [0.1903217 0.8096783]\n",
      "El cliente  Escritura con probabilidad [0.05771107 0.94228893]\n",
      "El cliente  Escritura con probabilidad [0.28972906 0.71027094]\n",
      "El cliente  Escritura con probabilidad [0.07859044 0.92140956]\n",
      "El cliente  Escritura con probabilidad [8.56841409e-09 9.99999991e-01]\n",
      "El cliente  Escritura con probabilidad [0.12749111 0.87250889]\n",
      "El cliente  Escritura con probabilidad [1.19005419e-07 9.99999881e-01]\n",
      "El cliente  Escritura con probabilidad [0.1763343 0.8236657]\n",
      "El cliente  Escritura con probabilidad [0.28294578 0.71705422]\n",
      "El cliente  Escritura con probabilidad [0.02024173 0.97975827]\n",
      "El cliente  Escritura con probabilidad [0.06262468 0.93737532]\n",
      "El cliente  Escritura con probabilidad [0.29643786 0.70356214]\n",
      "El cliente  Escritura con probabilidad [0.0918481 0.9081519]\n",
      "El cliente  Escritura con probabilidad [0.00306687 0.99693313]\n",
      "El cliente  Escritura con probabilidad [0.24142549 0.75857451]\n",
      "El cliente  Escritura con probabilidad [0.04308655 0.95691345]\n",
      "El cliente  Escritura con probabilidad [0.01396769 0.98603231]\n",
      "El cliente  Escritura con probabilidad [0.16864076 0.83135924]\n",
      "El cliente  Escritura con probabilidad [0.25719386 0.74280614]\n",
      "El cliente  Escritura con probabilidad [0.12375559 0.87624441]\n",
      "El cliente  Escritura con probabilidad [0.11696092 0.88303908]\n",
      "El cliente  Escritura con probabilidad [8.33000214e-06 9.99991670e-01]\n",
      "El cliente  Escritura con probabilidad [0.20454874 0.79545126]\n",
      "El cliente  Escritura con probabilidad [0.28868848 0.71131152]\n",
      "El cliente  Escritura con probabilidad [1.45460322e-08 9.99999985e-01]\n",
      "El cliente  Escritura con probabilidad [0.09904902 0.90095098]\n",
      "El cliente  Escritura con probabilidad [0.09058577 0.90941423]\n",
      "El cliente  Escritura con probabilidad [0.02947859 0.97052141]\n",
      "El cliente  Escritura con probabilidad [0.16765345 0.83234655]\n",
      "El cliente  Escritura con probabilidad [0.15416284 0.84583716]\n",
      "El cliente  Escritura con probabilidad [0.03399847 0.96600153]\n",
      "El cliente  Escritura con probabilidad [0.11717639 0.88282361]\n",
      "El cliente  Escritura con probabilidad [0.01068665 0.98931335]\n",
      "El cliente  Escritura con probabilidad [0.19327394 0.80672606]\n",
      "El cliente  Escritura con probabilidad [0.31993671 0.68006329]\n",
      "El cliente  Escritura con probabilidad [0.06064377 0.93935623]\n",
      "El cliente  Escritura con probabilidad [1.97402727e-07 9.99999803e-01]\n",
      "El cliente  Escritura con probabilidad [0.20467977 0.79532023]\n",
      "El cliente  Escritura con probabilidad [0.00404909 0.99595091]\n",
      "El cliente  Escritura con probabilidad [0.27936733 0.72063267]\n",
      "El cliente  Escritura con probabilidad [0.4395954 0.5604046]\n",
      "El cliente  Escritura con probabilidad [0.20033223 0.79966777]\n",
      "El cliente  Escritura con probabilidad [3.88091961e-08 9.99999961e-01]\n",
      "El cliente  Escritura con probabilidad [0.196716 0.803284]\n",
      "El cliente  Escritura con probabilidad [3.96297434e-04 9.99603703e-01]\n",
      "El cliente  Escritura con probabilidad [0.0676801 0.9323199]\n",
      "El cliente  Escritura con probabilidad [0.20903124 0.79096876]\n",
      "El cliente  Escritura con probabilidad [0.44279611 0.55720389]\n",
      "El cliente  Escritura con probabilidad [0.15332806 0.84667194]\n",
      "El cliente  Escritura con probabilidad [0.42052596 0.57947404]\n",
      "El cliente  Escritura con probabilidad [0.04007251 0.95992749]\n",
      "El cliente  Escritura con probabilidad [0.17865648 0.82134352]\n",
      "El cliente  Escritura con probabilidad [0.02796607 0.97203393]\n",
      "El cliente  Escritura con probabilidad [0.19610309 0.80389691]\n",
      "El cliente  Escritura con probabilidad [0.00100348 0.99899652]\n",
      "El cliente  Escritura con probabilidad [2.84030577e-11 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.01621495 0.98378505]\n",
      "El cliente  Escritura con probabilidad [0.12707652 0.87292348]\n",
      "El cliente  Escritura con probabilidad [0.13519981 0.86480019]\n",
      "El cliente  Escritura con probabilidad [3.53815416e-04 9.99646185e-01]\n",
      "El cliente  Escritura con probabilidad [4.69679169e-06 9.99995303e-01]\n",
      "El cliente  Escritura con probabilidad [0.01317327 0.98682673]\n",
      "El cliente  Escritura con probabilidad [0.1693507 0.8306493]\n",
      "El cliente  Escritura con probabilidad [0.13247965 0.86752035]\n",
      "El cliente  Escritura con probabilidad [9.8151286e-09 9.9999999e-01]\n",
      "El cliente  Escritura con probabilidad [1.75746817e-06 9.99998243e-01]\n",
      "El cliente  Escritura con probabilidad [0.02980854 0.97019146]\n",
      "El cliente  Escritura con probabilidad [0.24174795 0.75825205]\n",
      "El cliente  Escritura con probabilidad [0.29841772 0.70158228]\n",
      "El cliente  Escritura con probabilidad [0.01367416 0.98632584]\n",
      "El cliente  Escritura con probabilidad [0.22522695 0.77477305]\n",
      "El cliente  Escritura con probabilidad [0.04242951 0.95757049]\n",
      "El cliente  Escritura con probabilidad [0.13379465 0.86620535]\n",
      "El cliente  Escritura con probabilidad [0.03001565 0.96998435]\n",
      "El cliente  Escritura con probabilidad [0.33195764 0.66804236]\n",
      "El cliente  Escritura con probabilidad [0.01593935 0.98406065]\n",
      "El cliente  Escritura con probabilidad [0.08620455 0.91379545]\n",
      "El cliente  Escritura con probabilidad [0.01623924 0.98376076]\n",
      "El cliente  Escritura con probabilidad [0.20159012 0.79840988]\n",
      "El cliente  Escritura con probabilidad [0.07148188 0.92851812]\n",
      "El cliente  Escritura con probabilidad [0.24180913 0.75819087]\n",
      "El cliente  Escritura con probabilidad [2.93963866e-07 9.99999706e-01]\n",
      "El cliente  Escritura con probabilidad [0.20567703 0.79432297]\n",
      "El cliente  Escritura con probabilidad [0.06677993 0.93322007]\n",
      "El cliente  Escritura con probabilidad [0.0212746 0.9787254]\n",
      "El cliente  Escritura con probabilidad [0.43588619 0.56411381]\n",
      "El cliente  Escritura con probabilidad [0.00807254 0.99192746]\n",
      "El cliente  Escritura con probabilidad [0.0249684 0.9750316]\n",
      "El cliente  Escritura con probabilidad [4.57383899e-07 9.99999543e-01]\n",
      "El cliente  Escritura con probabilidad [0.08174407 0.91825593]\n",
      "El cliente  Escritura con probabilidad [2.55588175e-06 9.99997444e-01]\n",
      "El cliente  Escritura con probabilidad [1.37667655e-14 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.02478793 0.97521207]\n",
      "El cliente  Escritura con probabilidad [0.12036239 0.87963761]\n",
      "El cliente  Escritura con probabilidad [0.12219554 0.87780446]\n",
      "El cliente  Escritura con probabilidad [0.18554582 0.81445418]\n",
      "El cliente  Escritura con probabilidad [0.20918966 0.79081034]\n",
      "El cliente  Escritura con probabilidad [0.23417392 0.76582608]\n",
      "El cliente  Escritura con probabilidad [0.39426972 0.60573028]\n",
      "El cliente  Escritura con probabilidad [0.29181241 0.70818759]\n",
      "El cliente  Escritura con probabilidad [2.99050956e-07 9.99999701e-01]\n",
      "El cliente  Escritura con probabilidad [0.25049389 0.74950611]\n",
      "El cliente  Escritura con probabilidad [0.1212928 0.8787072]\n",
      "El cliente  Escritura con probabilidad [0.0200263 0.9799737]\n",
      "El cliente  Escritura con probabilidad [0.33935372 0.66064628]\n",
      "El cliente  Escritura con probabilidad [0.26766011 0.73233989]\n",
      "El cliente  Escritura con probabilidad [0.11827064 0.88172936]\n",
      "El cliente  Escritura con probabilidad [0.11724586 0.88275414]\n",
      "El cliente  Escritura con probabilidad [0.26323646 0.73676354]\n",
      "El cliente  Escritura con probabilidad [0.26147579 0.73852421]\n",
      "El cliente  Escritura con probabilidad [0.33948201 0.66051799]\n",
      "El cliente  Escritura con probabilidad [0.19259436 0.80740564]\n",
      "El cliente  Escritura con probabilidad [0.00498089 0.99501911]\n",
      "El cliente  Escritura con probabilidad [0.09657324 0.90342676]\n",
      "El cliente  Escritura con probabilidad [2.78816456e-04 9.99721184e-01]\n",
      "El cliente  Escritura con probabilidad [0.08413764 0.91586236]\n",
      "El cliente  Escritura con probabilidad [0.41871041 0.58128959]\n",
      "El cliente  Escritura con probabilidad [0.10651918 0.89348082]\n",
      "El cliente  Escritura con probabilidad [0.05875073 0.94124927]\n",
      "El cliente  Escritura con probabilidad [5.09986067e-06 9.99994900e-01]\n",
      "El cliente  Escritura con probabilidad [0.15809332 0.84190668]\n",
      "El cliente  Escritura con probabilidad [0.04085118 0.95914882]\n",
      "El cliente  Escritura con probabilidad [2.70037302e-07 9.99999730e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El cliente  Escritura con probabilidad [0.20218404 0.79781596]\n",
      "El cliente  Escritura con probabilidad [0.19003533 0.80996467]\n",
      "El cliente  Escritura con probabilidad [0.03073968 0.96926032]\n",
      "El cliente  Escritura con probabilidad [0.19885037 0.80114963]\n",
      "El cliente  Escritura con probabilidad [0.15995942 0.84004058]\n",
      "El cliente  Escritura con probabilidad [0.10983857 0.89016143]\n",
      "El cliente  Escritura con probabilidad [1.25487042e-08 9.99999987e-01]\n",
      "El cliente  Escritura con probabilidad [0.08074428 0.91925572]\n",
      "El cliente  Escritura con probabilidad [0.06098808 0.93901192]\n",
      "El cliente  Escritura con probabilidad [0.28168766 0.71831234]\n",
      "El cliente  Escritura con probabilidad [0.0081006 0.9918994]\n",
      "El cliente  Escritura con probabilidad [4.39426273e-13 1.00000000e+00]\n",
      "El cliente  Escritura con probabilidad [0.05141655 0.94858345]\n",
      "El cliente  Escritura con probabilidad [0.22150548 0.77849452]\n",
      "El cliente  Escritura con probabilidad [0.14909655 0.85090345]\n",
      "El cliente  Escritura con probabilidad [0.24588039 0.75411961]\n",
      "El cliente  Escritura con probabilidad [0.24297902 0.75702098]\n",
      "El cliente  Escritura con probabilidad [0.07863498 0.92136502]\n",
      "El cliente  Escritura con probabilidad [0.20684265 0.79315735]\n",
      "El cliente  Escritura con probabilidad [0.18154974 0.81845026]\n",
      "El cliente  Escritura con probabilidad [0.08855206 0.91144794]\n",
      "El cliente  Escritura con probabilidad [0.13321883 0.86678117]\n",
      "El cliente  Escritura con probabilidad [0.22525185 0.77474815]\n",
      "El cliente  Escritura con probabilidad [0.08221122 0.91778878]\n",
      "El cliente  Escritura con probabilidad [0.085368 0.914632]\n",
      "El cliente  Escritura con probabilidad [0.00762311 0.99237689]\n",
      "El cliente  Escritura con probabilidad [0.1008234 0.8991766]\n",
      "El cliente  Escritura con probabilidad [0.19542734 0.80457266]\n",
      "El cliente  Escritura con probabilidad [0.02235319 0.97764681]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred)):\n",
    "    print('El cliente ', pred[i], 'con probabilidad', prob[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Clusters</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>6.229345e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2.810436e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>1.294974e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>8.195074e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>5.014940e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>75</td>\n",
       "      <td>3.392737e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>2.584891e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200</td>\n",
       "      <td>1.386856e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>9.551588e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Clusters         Score\n",
       "0                   5  6.229345e+10\n",
       "1                  10  2.810436e+10\n",
       "2                  20  1.294974e+10\n",
       "3                  30  8.195074e+09\n",
       "4                  50  5.014940e+09\n",
       "5                  75  3.392737e+09\n",
       "6                 100  2.584891e+09\n",
       "7                 200  1.386856e+09\n",
       "8                 300  9.551588e+08"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementación de la regla del codo\n",
    "Nc = [5,10,20,30,50,75,100,200,300]\n",
    "kmeans = [KMeans(n_clusters=i) for i in Nc]\n",
    "score = [kmeans[i].fit(df_prepro).inertia_ for i in range(len(kmeans))]\n",
    "\n",
    "\n",
    "df_Elbow = pd.DataFrame({'Number of Clusters':Nc,\n",
    "                        'Score':score})\n",
    "\n",
    "df_Elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Elbow Curve'}, xlabel='Number of Clusters', ylabel='Score'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x612 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graficar los datos etiquetados con k-means\n",
    "fig, ax = plt.subplots(figsize=(11, 8.5))\n",
    "plt.title('Elbow Curve')\n",
    "sns.lineplot(x=\"Number of Clusters\",\n",
    "             y=\"Score\",\n",
    "            data=df_Elbow)\n",
    "sns.scatterplot(x=\"Number of Clusters\",\n",
    "             y=\"Score\",\n",
    "             data=df_Elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(df_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_ # centros \n",
    "clusters = kmeans.labels_ # clusters\n",
    "\n",
    "centroids_df = pd.DataFrame(centroids, columns=list(df_prepro.columns))\n",
    "\n",
    "centroids_df = pd.DataFrame(centroids, columns=list(df_prepro.columns))\n",
    "df_prepro[\"cluster\"] = clusters\n",
    "df_prepro[\"cluster\"] = df_prepro[\"cluster\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro = pd.merge(df_prepro.reset_index(), pd.DataFrame(df['Etiqueta']).reset_index(), on = 'index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro=df_prepro.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones para perfilar clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(grupo, columna):\n",
    "    serie=df_prepro[df_prepro['cluster']==grupo]\n",
    "    col=[]\n",
    "    for i in range(serie.shape[1]):\n",
    "        if columna in serie.columns[i]:\n",
    "            col.append(serie.columns[i])\n",
    "    return col        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3(grupo, etiqueta, columna):\n",
    "    serie = df_prepro[(df_prepro['Etiqueta']==etiqueta) & (df_prepro['cluster'] == grupo)]\n",
    "    df = pd.DataFrame(columns = [columna, 'frecuencia'])\n",
    "    frec = serie[get_columns(grupo, columna)].sum().tolist()\n",
    "    df.loc[:, columna] = get_columns(grupo, columna)\n",
    "    df.loc[:, 'frecuencia'] = frec\n",
    "    return df.sort_values(by='frecuencia', ascending = False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prom_montos(grupo, etiqueta, columna):\n",
    "    serie = df_prepro[(df_prepro['Etiqueta']==etiqueta) & (df_prepro['cluster'] == grupo)]\n",
    "    df = pd.DataFrame(columns = [columna, 'Promedio (UF)'])\n",
    "    frec = serie[get_columns(grupo, columna)].mean().tolist()\n",
    "    df.loc[:, columna] = get_columns(grupo,columna)\n",
    "    df.loc[:, 'Promedio (UF)'] = frec\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def montar_df(grupo, columna):\n",
    "    aux = df_prepro[df_prepro['cluster'] == grupo]\n",
    "    \n",
    "    esc = top_3(grupo, 'Escritura', columna)\n",
    "    des = top_3(grupo, 'Desiste', columna)\n",
    "    \n",
    "    dataframes = [des, esc]\n",
    "    result = pd.concat(dataframes, sort='False', keys=['Desiste', 'Escritura'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grupo_fam(grupo):\n",
    "    esc = df_prepro[(df_prepro['Etiqueta']=='Escritura') & (df_prepro['cluster']==grupo)]\n",
    "    des = df_prepro[(df_prepro['Etiqueta']=='Desiste') & (df_prepro['cluster']==grupo)]\n",
    "    \n",
    "    df_esc = pd.DataFrame(columns = ['Moda N° Grupo Familiar'])\n",
    "    df_des = pd.DataFrame(columns = ['Moda N° Grupo Familiar'])\n",
    "    \n",
    "    df_esc.loc[0, ['Moda N° Grupo Familiar']] = esc['N° Grupo Familiar'].mode()[0]\n",
    "    df_des.loc[0, ['Moda N° Grupo Familiar']] = des['N° Grupo Familiar'].mode()[0]\n",
    "    \n",
    "    dataframes = [df_des, df_esc]\n",
    "    result = pd.concat(dataframes, sort='False', keys=['Desiste', 'Escritura'])\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Columnas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ) index\n",
      "1 ) Monto Reserva\n",
      "2 ) Monto Pie\n",
      "3 ) Monto Carta de instruccion\n",
      "4 ) Monto Beneficio Minero\n",
      "5 ) Monto CH FFAA y Otros\n",
      "6 ) Monto Crédito Complementario\n",
      "7 ) Monto LEASING\n",
      "8 ) Monto Vivienda principal\n",
      "9 ) Monto CDP\n",
      "10 ) Monto CDP Cheque\n",
      "11 ) Monto CH\n",
      "12 ) Monto Subsidio\n",
      "13 ) Monto Ahorro\n",
      "14 ) Monto BAP\n",
      "15 ) N° Grupo Familiar\n",
      "16 ) Operacion Con Credito_No\n",
      "17 ) Operacion Con Credito_Si\n",
      "18 ) Zona_1-ZONA NORTE\n",
      "19 ) Zona_2-ZONA CENTRO\n",
      "20 ) Zona_3-ZONA SUR\n",
      "21 ) Zona_4-ZONA VERTICAL\n",
      "22 ) Zona_5-ZONA ANTOFAGASTA\n",
      "23 ) Tipo_Jurídico\n",
      "24 ) Tipo_Natural\n",
      "25 ) Nacionalidad Cliente_Alemana\n",
      "26 ) Nacionalidad Cliente_Argentina\n",
      "27 ) Nacionalidad Cliente_Boliviana\n",
      "28 ) Nacionalidad Cliente_Brasileña\n",
      "29 ) Nacionalidad Cliente_Chilena\n",
      "30 ) Nacionalidad Cliente_China\n",
      "31 ) Nacionalidad Cliente_Colombiana\n",
      "32 ) Nacionalidad Cliente_Cubana\n",
      "33 ) Nacionalidad Cliente_Ecuatoriana\n",
      "34 ) Nacionalidad Cliente_Española\n",
      "35 ) Nacionalidad Cliente_Extranjera\n",
      "36 ) Nacionalidad Cliente_Italiana\n",
      "37 ) Nacionalidad Cliente_Paraguaya\n",
      "38 ) Nacionalidad Cliente_Peruana\n",
      "39 ) Nacionalidad Cliente_Salvadoreña\n",
      "40 ) Nacionalidad Cliente_Uruguaya\n",
      "41 ) Nacionalidad Cliente_Venezolana\n",
      "42 ) Estado Civil_Casado(a)\n",
      "43 ) Estado Civil_Divorciado(a)\n",
      "44 ) Estado Civil_Separado(a)\n",
      "45 ) Estado Civil_Soltero(a)\n",
      "46 ) Estado Civil_Unión Civil\n",
      "47 ) Estado Civil_Viudo(a)\n",
      "48 ) Tipo Compra_Otros\n",
      "49 ) Tipo Compra_Primera Vivienda\n",
      "50 ) Tipo Compra_Segunda Vivienda\n",
      "51 ) Tipo Compra_Vivienda Inversion\n",
      "52 ) Rango de Ingresos_1.000.001 y 1.500.000\n",
      "53 ) Rango de Ingresos_1.500.001 y 2.000.000\n",
      "54 ) Rango de Ingresos_10.000.001 y 20.000.000\n",
      "55 ) Rango de Ingresos_2.000.001 y 2.500.000\n",
      "56 ) Rango de Ingresos_2.500.001 y 3.000.000\n",
      "57 ) Rango de Ingresos_3.000.001 y 4.000.000\n",
      "58 ) Rango de Ingresos_4.000.001 y 6.000.000\n",
      "59 ) Rango de Ingresos_6.000.001 y 8.000.000\n",
      "60 ) Rango de Ingresos_8.000.001 y 10.000.000\n",
      "61 ) Rango de Ingresos_Entre 500.001 y 1.000.000\n",
      "62 ) Rango de Ingresos_Hasta 500.000\n",
      "63 ) Rango de Ingresos_Sobre 20.000.000\n",
      "64 ) Profesion (Estandar)_Fuerzas Armadas\n",
      "65 ) Profesion (Estandar)_Independientes\n",
      "66 ) Profesion (Estandar)_Otros\n",
      "67 ) Profesion (Estandar)_Tecnicos\n",
      "68 ) Profesion (Estandar)_Universitarios\n",
      "69 ) Cargo Estandarizado_Administrativo\n",
      "70 ) Cargo Estandarizado_Analista\n",
      "71 ) Cargo Estandarizado_Apoyo\n",
      "72 ) Cargo Estandarizado_Docente\n",
      "73 ) Cargo Estandarizado_Ejecutivo\n",
      "74 ) Cargo Estandarizado_Encargado\n",
      "75 ) Cargo Estandarizado_Gerente\n",
      "76 ) Cargo Estandarizado_Independiente\n",
      "77 ) Cargo Estandarizado_Ingeniero\n",
      "78 ) Cargo Estandarizado_Jefe\n",
      "79 ) Cargo Estandarizado_Operario\n",
      "80 ) Cargo Estandarizado_Otro\n",
      "81 ) Cargo Estandarizado_Salud\n",
      "82 ) Cargo Estandarizado_Supervisor\n",
      "83 ) Cargo Estandarizado_Tecnico\n",
      "84 ) Rango de Edad_19-25\n",
      "85 ) Rango de Edad_26-30\n",
      "86 ) Rango de Edad_31-35\n",
      "87 ) Rango de Edad_36-40\n",
      "88 ) Rango de Edad_41-45\n",
      "89 ) Rango de Edad_46-50\n",
      "90 ) Rango de Edad_51-55\n",
      "91 ) Rango de Edad_56-60\n",
      "92 ) Rango de Edad_61-65\n",
      "93 ) Rango de Edad_66-70\n",
      "94 ) Rango de Edad_71-75\n",
      "95 ) Rango de Edad_76-80\n",
      "96 ) Rango de Edad_sobre 80\n",
      "97 ) Comuna Estandarizada_Alhué\n",
      "98 ) Comuna Estandarizada_Buin\n",
      "99 ) Comuna Estandarizada_Calera de Tango\n",
      "100 ) Comuna Estandarizada_Cerrillos\n",
      "101 ) Comuna Estandarizada_Cerro Navia\n",
      "102 ) Comuna Estandarizada_Colina\n",
      "103 ) Comuna Estandarizada_Conchalí\n",
      "104 ) Comuna Estandarizada_Curacaví\n",
      "105 ) Comuna Estandarizada_El Bosque\n",
      "106 ) Comuna Estandarizada_El Monte\n",
      "107 ) Comuna Estandarizada_Estación Central\n",
      "108 ) Comuna Estandarizada_Huechuraba\n",
      "109 ) Comuna Estandarizada_I Región de Tarapacá\n",
      "110 ) Comuna Estandarizada_II Región de Antofagasta\n",
      "111 ) Comuna Estandarizada_III Región de Atacama\n",
      "112 ) Comuna Estandarizada_IV Región de Coquimbo\n",
      "113 ) Comuna Estandarizada_IX Región de la Araucanía\n",
      "114 ) Comuna Estandarizada_Independencia\n",
      "115 ) Comuna Estandarizada_Isla de Maipo\n",
      "116 ) Comuna Estandarizada_La Cisterna\n",
      "117 ) Comuna Estandarizada_La Florida\n",
      "118 ) Comuna Estandarizada_La Granja\n",
      "119 ) Comuna Estandarizada_La Pintana\n",
      "120 ) Comuna Estandarizada_La Reina\n",
      "121 ) Comuna Estandarizada_Lampa\n",
      "122 ) Comuna Estandarizada_Las Condes\n",
      "123 ) Comuna Estandarizada_Lo Barnechea\n",
      "124 ) Comuna Estandarizada_Lo Espejo\n",
      "125 ) Comuna Estandarizada_Lo Prado\n",
      "126 ) Comuna Estandarizada_Macul\n",
      "127 ) Comuna Estandarizada_Maipu\n",
      "128 ) Comuna Estandarizada_Melipilla\n",
      "129 ) Comuna Estandarizada_Padre Hurtado\n",
      "130 ) Comuna Estandarizada_Paine\n",
      "131 ) Comuna Estandarizada_Pedro Aguirre Cerda\n",
      "132 ) Comuna Estandarizada_Peñaflor\n",
      "133 ) Comuna Estandarizada_Peñalolén\n",
      "134 ) Comuna Estandarizada_Pirque\n",
      "135 ) Comuna Estandarizada_Providencia\n",
      "136 ) Comuna Estandarizada_Pudahuel\n",
      "137 ) Comuna Estandarizada_Puente Alto\n",
      "138 ) Comuna Estandarizada_Quilicura\n",
      "139 ) Comuna Estandarizada_Quinta Normal\n",
      "140 ) Comuna Estandarizada_Recoleta\n",
      "141 ) Comuna Estandarizada_Renca\n",
      "142 ) Comuna Estandarizada_San Bernardo\n",
      "143 ) Comuna Estandarizada_San Joaquín\n",
      "144 ) Comuna Estandarizada_San José de Maipo\n",
      "145 ) Comuna Estandarizada_San Miguel\n",
      "146 ) Comuna Estandarizada_San Pedro\n",
      "147 ) Comuna Estandarizada_San Ramón\n",
      "148 ) Comuna Estandarizada_Santiago\n",
      "149 ) Comuna Estandarizada_Talagante\n",
      "150 ) Comuna Estandarizada_Til Til\n",
      "151 ) Comuna Estandarizada_V Región de Valparaíso\n",
      "152 ) Comuna Estandarizada_VI Región del Libertador General Bernardo O’Higgins\n",
      "153 ) Comuna Estandarizada_VII Región del Maule\n",
      "154 ) Comuna Estandarizada_VIII Región del Biobío\n",
      "155 ) Comuna Estandarizada_Vitacura\n",
      "156 ) Comuna Estandarizada_X Región de Los Lagos\n",
      "157 ) Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo\n",
      "158 ) Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena\n",
      "159 ) Comuna Estandarizada_XIV Región de Los Ríos\n",
      "160 ) Comuna Estandarizada_XV Región de Arica y Parinacota\n",
      "161 ) Comuna Estandarizada_XVI Región de Ñuble\n",
      "162 ) Comuna Estandarizada_Ñuñoa\n",
      "163 ) cluster\n",
      "164 ) Etiqueta\n"
     ]
    }
   ],
   "source": [
    "for i in range(df_prepro.shape[1]):\n",
    "    print(i, ')', df_prepro.columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupos\n",
    "\n",
    "Para efectos de la segmentación de clientes, se han considerado a priori 5 grupos de clientes, de los cuales se mostrarán atributos numéricos como los montos, atributos categoricos, como el top comunas, top cargos y profesiones, entre otras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupo 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de Montos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>Monto CH</th>\n",
       "      <th>Monto Subsidio</th>\n",
       "      <th>Monto Ahorro</th>\n",
       "      <th>Monto BAP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Etiqueta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desiste</th>\n",
       "      <td>83.601000</td>\n",
       "      <td>1235.805267</td>\n",
       "      <td>1007.068200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4552.691200</td>\n",
       "      <td>27.680000</td>\n",
       "      <td>7.823733</td>\n",
       "      <td>29.372000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.986667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Escritura</th>\n",
       "      <td>52.038377</td>\n",
       "      <td>352.038491</td>\n",
       "      <td>40.175222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.859993</td>\n",
       "      <td>9.63438</td>\n",
       "      <td>2.8779</td>\n",
       "      <td>4487.133159</td>\n",
       "      <td>23.239787</td>\n",
       "      <td>1.694744</td>\n",
       "      <td>11.210399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.458309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Monto Reserva    Monto Pie  Monto Carta de instruccion  \\\n",
       "Etiqueta                                                            \n",
       "Desiste        83.601000  1235.805267                 1007.068200   \n",
       "Escritura      52.038377   352.038491                   40.175222   \n",
       "\n",
       "           Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "Etiqueta                                                   \n",
       "Desiste                       0.0               0.000000   \n",
       "Escritura                     0.0               5.859993   \n",
       "\n",
       "           Monto Crédito Complementario  Monto LEASING  \\\n",
       "Etiqueta                                                 \n",
       "Desiste                         0.00000         0.0000   \n",
       "Escritura                       9.63438         2.8779   \n",
       "\n",
       "           Monto Vivienda principal  Monto CDP  Monto CDP Cheque   Monto CH  \\\n",
       "Etiqueta                                                                      \n",
       "Desiste                 4552.691200  27.680000          7.823733  29.372000   \n",
       "Escritura               4487.133159  23.239787          1.694744  11.210399   \n",
       "\n",
       "           Monto Subsidio  Monto Ahorro  Monto BAP  \n",
       "Etiqueta                                            \n",
       "Desiste               0.0           0.0  11.986667  \n",
       "Escritura             0.0           0.0  19.458309  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro[df_prepro['cluster']==0].groupby(['Etiqueta']).mean()[get_columns(0,'Monto')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rango de Edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rango de Edad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Edad_41-45</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Edad_36-40</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Edad_46-50</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rango de Edad_56-60</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rango de Edad_61-65</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Edad_41-45</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Edad_36-40</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Edad_46-50</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rango de Edad_51-55</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rango de Edad_56-60</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Rango de Edad  frecuencia\n",
       "Desiste   4  Rango de Edad_41-45          30\n",
       "          3  Rango de Edad_36-40          23\n",
       "          5  Rango de Edad_46-50          20\n",
       "          7  Rango de Edad_56-60          19\n",
       "          8  Rango de Edad_61-65          18\n",
       "Escritura 4  Rango de Edad_41-45         641\n",
       "          3  Rango de Edad_36-40         480\n",
       "          5  Rango de Edad_46-50         426\n",
       "          6  Rango de Edad_51-55         372\n",
       "          7  Rango de Edad_56-60         271"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(0, 'Rango de Edad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Nacionalidad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>4</th>\n",
       "      <td>Nacionalidad Cliente_Chilena</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nacionalidad Cliente_Extranjera</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Nacionalidad Cliente_Venezolana</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nacionalidad Cliente_Española</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Nacionalidad Cliente_Italiana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>4</th>\n",
       "      <td>Nacionalidad Cliente_Chilena</td>\n",
       "      <td>2854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nacionalidad Cliente_Extranjera</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nacionalidad Cliente_Argentina</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nacionalidad Cliente_Peruana</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nacionalidad Cliente_Española</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Nacionalidad  frecuencia\n",
       "Desiste   4      Nacionalidad Cliente_Chilena         141\n",
       "          10  Nacionalidad Cliente_Extranjera           5\n",
       "          16  Nacionalidad Cliente_Venezolana           2\n",
       "          9     Nacionalidad Cliente_Española           1\n",
       "          11    Nacionalidad Cliente_Italiana           1\n",
       "Escritura 4      Nacionalidad Cliente_Chilena        2854\n",
       "          10  Nacionalidad Cliente_Extranjera          44\n",
       "          1    Nacionalidad Cliente_Argentina           4\n",
       "          13     Nacionalidad Cliente_Peruana           3\n",
       "          9     Nacionalidad Cliente_Española           2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(0, 'Nacionalidad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado Civil y N° Grupo Familiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Estado Civil</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>0</th>\n",
       "      <td>Estado Civil_Casado(a)</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estado Civil_Soltero(a)</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estado Civil_Divorciado(a)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Estado Civil_Separado(a)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estado Civil_Viudo(a)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>0</th>\n",
       "      <td>Estado Civil_Casado(a)</td>\n",
       "      <td>1699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estado Civil_Soltero(a)</td>\n",
       "      <td>1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Estado Civil_Separado(a)</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estado Civil_Viudo(a)</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estado Civil_Divorciado(a)</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Estado Civil  frecuencia\n",
       "Desiste   0      Estado Civil_Casado(a)          77\n",
       "          3     Estado Civil_Soltero(a)          59\n",
       "          1  Estado Civil_Divorciado(a)           5\n",
       "          2    Estado Civil_Separado(a)           5\n",
       "          5       Estado Civil_Viudo(a)           4\n",
       "Escritura 0      Estado Civil_Casado(a)        1699\n",
       "          3     Estado Civil_Soltero(a)        1040\n",
       "          2    Estado Civil_Separado(a)         101\n",
       "          5       Estado Civil_Viudo(a)          44\n",
       "          1  Estado Civil_Divorciado(a)          25"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(0, 'Estado Civil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Moda N° Grupo Familiar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desiste</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Escritura</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Moda N° Grupo Familiar\n",
       "Desiste   0                      2\n",
       "Escritura 0                      2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grupo_fam(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipo Compra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tipo Compra</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Desiste</th>\n",
       "      <th>1</th>\n",
       "      <td>Tipo Compra_Primera Vivienda</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tipo Compra_Segunda Vivienda</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tipo Compra_Vivienda Inversion</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tipo Compra_Otros</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Escritura</th>\n",
       "      <th>1</th>\n",
       "      <td>Tipo Compra_Primera Vivienda</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tipo Compra_Segunda Vivienda</td>\n",
       "      <td>1077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tipo Compra_Vivienda Inversion</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tipo Compra_Otros</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Tipo Compra  frecuencia\n",
       "Desiste   1    Tipo Compra_Primera Vivienda          63\n",
       "          2    Tipo Compra_Segunda Vivienda          58\n",
       "          3  Tipo Compra_Vivienda Inversion          23\n",
       "          0               Tipo Compra_Otros           6\n",
       "Escritura 1    Tipo Compra_Primera Vivienda        1450\n",
       "          2    Tipo Compra_Segunda Vivienda        1077\n",
       "          3  Tipo Compra_Vivienda Inversion         301\n",
       "          0               Tipo Compra_Otros          81"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(0, 'Tipo Compra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profesión y Cargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Profesion</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>2</th>\n",
       "      <td>Profesion (Estandar)_Otros</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Profesion (Estandar)_Universitarios</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Profesion (Estandar)_Tecnicos</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profesion (Estandar)_Independientes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profesion (Estandar)_Fuerzas Armadas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>2</th>\n",
       "      <td>Profesion (Estandar)_Otros</td>\n",
       "      <td>2257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Profesion (Estandar)_Universitarios</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Profesion (Estandar)_Tecnicos</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profesion (Estandar)_Independientes</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profesion (Estandar)_Fuerzas Armadas</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Profesion  frecuencia\n",
       "Desiste   2            Profesion (Estandar)_Otros         109\n",
       "          4   Profesion (Estandar)_Universitarios          25\n",
       "          3         Profesion (Estandar)_Tecnicos          14\n",
       "          1   Profesion (Estandar)_Independientes           2\n",
       "          0  Profesion (Estandar)_Fuerzas Armadas           0\n",
       "Escritura 2            Profesion (Estandar)_Otros        2257\n",
       "          4   Profesion (Estandar)_Universitarios         452\n",
       "          3         Profesion (Estandar)_Tecnicos         175\n",
       "          1   Profesion (Estandar)_Independientes          14\n",
       "          0  Profesion (Estandar)_Fuerzas Armadas          11"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(0, 'Profesion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Cargo</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>6</th>\n",
       "      <td>Cargo Estandarizado_Gerente</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cargo Estandarizado_Jefe</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cargo Estandarizado_Independiente</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cargo Estandarizado_Supervisor</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cargo Estandarizado_Administrativo</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>9</th>\n",
       "      <td>Cargo Estandarizado_Jefe</td>\n",
       "      <td>1068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cargo Estandarizado_Gerente</td>\n",
       "      <td>817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cargo Estandarizado_Supervisor</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cargo Estandarizado_Administrativo</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cargo Estandarizado_Tecnico</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Cargo  frecuencia\n",
       "Desiste   6          Cargo Estandarizado_Gerente          46\n",
       "          9             Cargo Estandarizado_Jefe          42\n",
       "          7    Cargo Estandarizado_Independiente          19\n",
       "          13      Cargo Estandarizado_Supervisor          15\n",
       "          0   Cargo Estandarizado_Administrativo          13\n",
       "Escritura 9             Cargo Estandarizado_Jefe        1068\n",
       "          6          Cargo Estandarizado_Gerente         817\n",
       "          13      Cargo Estandarizado_Supervisor         329\n",
       "          0   Cargo Estandarizado_Administrativo         297\n",
       "          14         Cargo Estandarizado_Tecnico         147"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(0, 'Cargo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rango de Ingresos y Comuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rango de Ingresos</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Ingresos_1.500.001 y 2.000.000</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Ingresos_2.500.001 y 3.000.000</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rango de Ingresos_1.000.001 y 1.500.000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Ingresos_2.000.001 y 2.500.000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Ingresos_3.000.001 y 4.000.000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>0</th>\n",
       "      <td>Rango de Ingresos_1.000.001 y 1.500.000</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Ingresos_1.500.001 y 2.000.000</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Ingresos_2.000.001 y 2.500.000</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Ingresos_2.500.001 y 3.000.000</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Ingresos_3.000.001 y 4.000.000</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Rango de Ingresos  frecuencia\n",
       "Desiste   1  Rango de Ingresos_1.500.001 y 2.000.000          26\n",
       "          4  Rango de Ingresos_2.500.001 y 3.000.000          26\n",
       "          0  Rango de Ingresos_1.000.001 y 1.500.000          25\n",
       "          3  Rango de Ingresos_2.000.001 y 2.500.000          21\n",
       "          5  Rango de Ingresos_3.000.001 y 4.000.000          21\n",
       "Escritura 0  Rango de Ingresos_1.000.001 y 1.500.000         620\n",
       "          1  Rango de Ingresos_1.500.001 y 2.000.000         599\n",
       "          3  Rango de Ingresos_2.000.001 y 2.500.000         393\n",
       "          4  Rango de Ingresos_2.500.001 y 3.000.000         389\n",
       "          5  Rango de Ingresos_3.000.001 y 4.000.000         369"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(0, 'Rango de Ingresos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Comuna</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>25</th>\n",
       "      <td>Comuna Estandarizada_Las Condes</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Comuna Estandarizada_II Región de Antofagasta</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Comuna Estandarizada_Santiago</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Comuna Estandarizada_V Región de Valparaíso</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Comuna Estandarizada_Puente Alto</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>13</th>\n",
       "      <td>Comuna Estandarizada_II Región de Antofagasta</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Comuna Estandarizada_Las Condes</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Comuna Estandarizada_V Región de Valparaíso</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Comuna Estandarizada_Santiago</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Comuna Estandarizada_Puente Alto</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Comuna  frecuencia\n",
       "Desiste   25                Comuna Estandarizada_Las Condes          16\n",
       "          13  Comuna Estandarizada_II Región de Antofagasta          16\n",
       "          51                  Comuna Estandarizada_Santiago          12\n",
       "          54    Comuna Estandarizada_V Región de Valparaíso          10\n",
       "          40               Comuna Estandarizada_Puente Alto           9\n",
       "Escritura 13  Comuna Estandarizada_II Región de Antofagasta         353\n",
       "          25                Comuna Estandarizada_Las Condes         332\n",
       "          54    Comuna Estandarizada_V Región de Valparaíso         247\n",
       "          51                  Comuna Estandarizada_Santiago         203\n",
       "          40               Comuna Estandarizada_Puente Alto         198"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(0, 'Comuna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de Montos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>Monto CH</th>\n",
       "      <th>Monto Subsidio</th>\n",
       "      <th>Monto Ahorro</th>\n",
       "      <th>Monto BAP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Etiqueta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desiste</th>\n",
       "      <td>59.019867</td>\n",
       "      <td>789.788987</td>\n",
       "      <td>0.907067</td>\n",
       "      <td>5.253333</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.063467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5069.580107</td>\n",
       "      <td>3.232267</td>\n",
       "      <td>17.590507</td>\n",
       "      <td>3954.788240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Escritura</th>\n",
       "      <td>63.226737</td>\n",
       "      <td>777.525351</td>\n",
       "      <td>3.475690</td>\n",
       "      <td>10.440380</td>\n",
       "      <td>6.291945</td>\n",
       "      <td>3.937783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5128.571626</td>\n",
       "      <td>1.644710</td>\n",
       "      <td>23.167845</td>\n",
       "      <td>3940.270862</td>\n",
       "      <td>0.818368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.476562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Monto Reserva   Monto Pie  Monto Carta de instruccion  \\\n",
       "Etiqueta                                                           \n",
       "Desiste        59.019867  789.788987                    0.907067   \n",
       "Escritura      63.226737  777.525351                    3.475690   \n",
       "\n",
       "           Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "Etiqueta                                                   \n",
       "Desiste                  5.253333              12.000000   \n",
       "Escritura               10.440380               6.291945   \n",
       "\n",
       "           Monto Crédito Complementario  Monto LEASING  \\\n",
       "Etiqueta                                                 \n",
       "Desiste                        1.063467            0.0   \n",
       "Escritura                      3.937783            0.0   \n",
       "\n",
       "           Monto Vivienda principal  Monto CDP  Monto CDP Cheque     Monto CH  \\\n",
       "Etiqueta                                                                        \n",
       "Desiste                 5069.580107   3.232267         17.590507  3954.788240   \n",
       "Escritura               5128.571626   1.644710         23.167845  3940.270862   \n",
       "\n",
       "           Monto Subsidio  Monto Ahorro  Monto BAP  \n",
       "Etiqueta                                            \n",
       "Desiste          0.000000           0.0   1.013600  \n",
       "Escritura        0.818368           0.0   2.476562  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro[df_prepro['cluster']==1].groupby(['Etiqueta']).mean()[get_columns(1,'Monto')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rango de Edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rango de Edad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Edad_36-40</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Edad_41-45</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rango de Edad_31-35</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Edad_46-50</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Edad_26-30</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Edad_36-40</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Edad_41-45</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rango de Edad_31-35</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Edad_46-50</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rango de Edad_51-55</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Rango de Edad  frecuencia\n",
       "Desiste   3  Rango de Edad_36-40          85\n",
       "          4  Rango de Edad_41-45          80\n",
       "          2  Rango de Edad_31-35          71\n",
       "          5  Rango de Edad_46-50          35\n",
       "          1  Rango de Edad_26-30          31\n",
       "Escritura 3  Rango de Edad_36-40         462\n",
       "          4  Rango de Edad_41-45         432\n",
       "          2  Rango de Edad_31-35         401\n",
       "          5  Rango de Edad_46-50         202\n",
       "          6  Rango de Edad_51-55         162"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(1, 'Rango de Edad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Nacionalidad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>4</th>\n",
       "      <td>Nacionalidad Cliente_Chilena</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Nacionalidad Cliente_Venezolana</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nacionalidad Cliente_Extranjera</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nacionalidad Cliente_Argentina</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nacionalidad Cliente_Ecuatoriana</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>4</th>\n",
       "      <td>Nacionalidad Cliente_Chilena</td>\n",
       "      <td>1842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nacionalidad Cliente_Extranjera</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Nacionalidad Cliente_Venezolana</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nacionalidad Cliente_Argentina</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nacionalidad Cliente_Colombiana</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Nacionalidad  frecuencia\n",
       "Desiste   4       Nacionalidad Cliente_Chilena         351\n",
       "          16   Nacionalidad Cliente_Venezolana           8\n",
       "          10   Nacionalidad Cliente_Extranjera           3\n",
       "          1     Nacionalidad Cliente_Argentina           3\n",
       "          8   Nacionalidad Cliente_Ecuatoriana           3\n",
       "Escritura 4       Nacionalidad Cliente_Chilena        1842\n",
       "          10   Nacionalidad Cliente_Extranjera          26\n",
       "          16   Nacionalidad Cliente_Venezolana          24\n",
       "          1     Nacionalidad Cliente_Argentina          17\n",
       "          6    Nacionalidad Cliente_Colombiana          12"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(1, 'Nacionalidad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado Civil y N° Grupo Familiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Estado Civil</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>0</th>\n",
       "      <td>Estado Civil_Casado(a)</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estado Civil_Soltero(a)</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estado Civil_Divorciado(a)</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Estado Civil_Separado(a)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estado Civil_Viudo(a)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>3</th>\n",
       "      <td>Estado Civil_Soltero(a)</td>\n",
       "      <td>939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estado Civil_Casado(a)</td>\n",
       "      <td>907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estado Civil_Divorciado(a)</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Estado Civil_Separado(a)</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estado Civil_Viudo(a)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Estado Civil  frecuencia\n",
       "Desiste   0      Estado Civil_Casado(a)         178\n",
       "          3     Estado Civil_Soltero(a)         173\n",
       "          1  Estado Civil_Divorciado(a)          16\n",
       "          2    Estado Civil_Separado(a)           5\n",
       "          5       Estado Civil_Viudo(a)           2\n",
       "Escritura 3     Estado Civil_Soltero(a)         939\n",
       "          0      Estado Civil_Casado(a)         907\n",
       "          1  Estado Civil_Divorciado(a)          64\n",
       "          2    Estado Civil_Separado(a)          31\n",
       "          5       Estado Civil_Viudo(a)           7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(1, 'Estado Civil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Moda N° Grupo Familiar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desiste</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Escritura</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Moda N° Grupo Familiar\n",
       "Desiste   0                      2\n",
       "Escritura 0                      2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grupo_fam(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipo Compra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tipo Compra</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Desiste</th>\n",
       "      <th>1</th>\n",
       "      <td>Tipo Compra_Primera Vivienda</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tipo Compra_Segunda Vivienda</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tipo Compra_Vivienda Inversion</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tipo Compra_Otros</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Escritura</th>\n",
       "      <th>1</th>\n",
       "      <td>Tipo Compra_Primera Vivienda</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tipo Compra_Segunda Vivienda</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tipo Compra_Vivienda Inversion</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tipo Compra_Otros</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Tipo Compra  frecuencia\n",
       "Desiste   1    Tipo Compra_Primera Vivienda         196\n",
       "          2    Tipo Compra_Segunda Vivienda         130\n",
       "          3  Tipo Compra_Vivienda Inversion          30\n",
       "          0               Tipo Compra_Otros          19\n",
       "Escritura 1    Tipo Compra_Primera Vivienda         981\n",
       "          2    Tipo Compra_Segunda Vivienda         707\n",
       "          3  Tipo Compra_Vivienda Inversion         189\n",
       "          0               Tipo Compra_Otros          72"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(1, 'Tipo Compra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profesión y Cargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Profesion</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>2</th>\n",
       "      <td>Profesion (Estandar)_Otros</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Profesion (Estandar)_Universitarios</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Profesion (Estandar)_Tecnicos</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profesion (Estandar)_Fuerzas Armadas</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profesion (Estandar)_Independientes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>2</th>\n",
       "      <td>Profesion (Estandar)_Otros</td>\n",
       "      <td>1342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Profesion (Estandar)_Universitarios</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Profesion (Estandar)_Tecnicos</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profesion (Estandar)_Independientes</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profesion (Estandar)_Fuerzas Armadas</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Profesion  frecuencia\n",
       "Desiste   2            Profesion (Estandar)_Otros         242\n",
       "          4   Profesion (Estandar)_Universitarios          92\n",
       "          3         Profesion (Estandar)_Tecnicos          32\n",
       "          0  Profesion (Estandar)_Fuerzas Armadas           5\n",
       "          1   Profesion (Estandar)_Independientes           4\n",
       "Escritura 2            Profesion (Estandar)_Otros        1342\n",
       "          4   Profesion (Estandar)_Universitarios         455\n",
       "          3         Profesion (Estandar)_Tecnicos         129\n",
       "          1   Profesion (Estandar)_Independientes          14\n",
       "          0  Profesion (Estandar)_Fuerzas Armadas           9"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(1, 'Profesion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Cargo</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>6</th>\n",
       "      <td>Cargo Estandarizado_Gerente</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cargo Estandarizado_Jefe</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cargo Estandarizado_Independiente</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cargo Estandarizado_Administrativo</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cargo Estandarizado_Ingeniero</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>6</th>\n",
       "      <td>Cargo Estandarizado_Gerente</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cargo Estandarizado_Independiente</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cargo Estandarizado_Jefe</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cargo Estandarizado_Ingeniero</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cargo Estandarizado_Administrativo</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Cargo  frecuencia\n",
       "Desiste   6         Cargo Estandarizado_Gerente         115\n",
       "          9            Cargo Estandarizado_Jefe          69\n",
       "          7   Cargo Estandarizado_Independiente          62\n",
       "          0  Cargo Estandarizado_Administrativo          26\n",
       "          8       Cargo Estandarizado_Ingeniero          19\n",
       "Escritura 6         Cargo Estandarizado_Gerente         510\n",
       "          7   Cargo Estandarizado_Independiente         381\n",
       "          9            Cargo Estandarizado_Jefe         331\n",
       "          8       Cargo Estandarizado_Ingeniero         122\n",
       "          0  Cargo Estandarizado_Administrativo         115"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(1, 'Cargo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rango de Ingresos y Comuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rango de Ingresos</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Ingresos_1.500.001 y 2.000.000</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rango de Ingresos_1.000.001 y 1.500.000</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Ingresos_2.000.001 y 2.500.000</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Ingresos_2.500.001 y 3.000.000</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Ingresos_3.000.001 y 4.000.000</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Ingresos_1.500.001 y 2.000.000</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rango de Ingresos_1.000.001 y 1.500.000</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Ingresos_2.000.001 y 2.500.000</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Ingresos_2.500.001 y 3.000.000</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Ingresos_3.000.001 y 4.000.000</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Rango de Ingresos  frecuencia\n",
       "Desiste   1  Rango de Ingresos_1.500.001 y 2.000.000          92\n",
       "          0  Rango de Ingresos_1.000.001 y 1.500.000          65\n",
       "          3  Rango de Ingresos_2.000.001 y 2.500.000          53\n",
       "          4  Rango de Ingresos_2.500.001 y 3.000.000          53\n",
       "          5  Rango de Ingresos_3.000.001 y 4.000.000          49\n",
       "Escritura 1  Rango de Ingresos_1.500.001 y 2.000.000         506\n",
       "          0  Rango de Ingresos_1.000.001 y 1.500.000         339\n",
       "          3  Rango de Ingresos_2.000.001 y 2.500.000         295\n",
       "          4  Rango de Ingresos_2.500.001 y 3.000.000         265\n",
       "          5  Rango de Ingresos_3.000.001 y 4.000.000         221"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(1, 'Rango de Ingresos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Comuna</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>13</th>\n",
       "      <td>Comuna Estandarizada_II Región de Antofagasta</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Comuna Estandarizada_Puente Alto</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Comuna Estandarizada_V Región de Valparaíso</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Comuna Estandarizada_VIII Región del Biobío</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Comuna Estandarizada_Maipu</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>13</th>\n",
       "      <td>Comuna Estandarizada_II Región de Antofagasta</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Comuna Estandarizada_Puente Alto</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Comuna Estandarizada_Santiago</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Comuna Estandarizada_Las Condes</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Comuna Estandarizada_VIII Región del Biobío</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Comuna  frecuencia\n",
       "Desiste   13  Comuna Estandarizada_II Región de Antofagasta          66\n",
       "          40               Comuna Estandarizada_Puente Alto          36\n",
       "          54    Comuna Estandarizada_V Región de Valparaíso          31\n",
       "          57    Comuna Estandarizada_VIII Región del Biobío          31\n",
       "          30                     Comuna Estandarizada_Maipu          24\n",
       "Escritura 13  Comuna Estandarizada_II Región de Antofagasta         374\n",
       "          40               Comuna Estandarizada_Puente Alto         187\n",
       "          51                  Comuna Estandarizada_Santiago         136\n",
       "          25                Comuna Estandarizada_Las Condes         132\n",
       "          57    Comuna Estandarizada_VIII Región del Biobío         121"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(1, 'Comuna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de Montos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>Monto CH</th>\n",
       "      <th>Monto Subsidio</th>\n",
       "      <th>Monto Ahorro</th>\n",
       "      <th>Monto BAP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Etiqueta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desiste</th>\n",
       "      <td>138.106061</td>\n",
       "      <td>2494.629697</td>\n",
       "      <td>302.930303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.745455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13109.739394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>7489.894242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Escritura</th>\n",
       "      <td>127.566337</td>\n",
       "      <td>1977.310535</td>\n",
       "      <td>370.883868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.464609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14110.176379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.789342</td>\n",
       "      <td>4286.981564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.87358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Monto Reserva    Monto Pie  Monto Carta de instruccion  \\\n",
       "Etiqueta                                                            \n",
       "Desiste       138.106061  2494.629697                  302.930303   \n",
       "Escritura     127.566337  1977.310535                  370.883868   \n",
       "\n",
       "           Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "Etiqueta                                                   \n",
       "Desiste                       0.0                    0.0   \n",
       "Escritura                     0.0                    0.0   \n",
       "\n",
       "           Monto Crédito Complementario  Monto LEASING  \\\n",
       "Etiqueta                                                 \n",
       "Desiste                       13.745455            0.0   \n",
       "Escritura                     76.464609            0.0   \n",
       "\n",
       "           Monto Vivienda principal  Monto CDP  Monto CDP Cheque     Monto CH  \\\n",
       "Etiqueta                                                                        \n",
       "Desiste                13109.739394        0.0         29.000000  7489.894242   \n",
       "Escritura              14110.176379        0.0         28.789342  4286.981564   \n",
       "\n",
       "           Monto Subsidio  Monto Ahorro  Monto BAP  \n",
       "Etiqueta                                            \n",
       "Desiste               0.0           0.0    0.00000  \n",
       "Escritura             0.0           0.0    1.87358  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro[df_prepro['cluster']==2].groupby(['Etiqueta']).mean()[get_columns(2,'Monto')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rango de Edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rango de Edad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>6</th>\n",
       "      <td>Rango de Edad_51-55</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Edad_36-40</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Edad_41-45</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rango de Edad_56-60</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rango de Edad_61-65</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Edad_36-40</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rango de Edad_51-55</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Edad_41-45</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rango de Edad_56-60</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rango de Edad_61-65</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Rango de Edad  frecuencia\n",
       "Desiste   6  Rango de Edad_51-55           7\n",
       "          3  Rango de Edad_36-40           6\n",
       "          4  Rango de Edad_41-45           6\n",
       "          7  Rango de Edad_56-60           4\n",
       "          8  Rango de Edad_61-65           4\n",
       "Escritura 3  Rango de Edad_36-40          55\n",
       "          6  Rango de Edad_51-55          40\n",
       "          4  Rango de Edad_41-45          34\n",
       "          7  Rango de Edad_56-60          28\n",
       "          8  Rango de Edad_61-65          27"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(2, 'Rango de Edad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Nacionalidad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>4</th>\n",
       "      <td>Nacionalidad Cliente_Chilena</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nacionalidad Cliente_Extranjera</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nacionalidad Cliente_Brasileña</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nacionalidad Cliente_Alemana</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Nacionalidad Cliente_Uruguaya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>4</th>\n",
       "      <td>Nacionalidad Cliente_Chilena</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nacionalidad Cliente_Extranjera</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nacionalidad Cliente_Argentina</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Nacionalidad Cliente_Venezolana</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Nacionalidad Cliente_Uruguaya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Nacionalidad  frecuencia\n",
       "Desiste   4      Nacionalidad Cliente_Chilena          31\n",
       "          10  Nacionalidad Cliente_Extranjera           1\n",
       "          3    Nacionalidad Cliente_Brasileña           1\n",
       "          0      Nacionalidad Cliente_Alemana           0\n",
       "          15    Nacionalidad Cliente_Uruguaya           0\n",
       "Escritura 4      Nacionalidad Cliente_Chilena         220\n",
       "          10  Nacionalidad Cliente_Extranjera          13\n",
       "          1    Nacionalidad Cliente_Argentina           3\n",
       "          16  Nacionalidad Cliente_Venezolana           2\n",
       "          15    Nacionalidad Cliente_Uruguaya           2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(2, 'Nacionalidad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado Civil y N° Grupo Familiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Estado Civil</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>0</th>\n",
       "      <td>Estado Civil_Casado(a)</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estado Civil_Soltero(a)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estado Civil_Divorciado(a)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Estado Civil_Separado(a)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Estado Civil_Unión Civil</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>0</th>\n",
       "      <td>Estado Civil_Casado(a)</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estado Civil_Soltero(a)</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Estado Civil_Separado(a)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estado Civil_Divorciado(a)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estado Civil_Viudo(a)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Estado Civil  frecuencia\n",
       "Desiste   0      Estado Civil_Casado(a)          24\n",
       "          3     Estado Civil_Soltero(a)           5\n",
       "          1  Estado Civil_Divorciado(a)           2\n",
       "          2    Estado Civil_Separado(a)           2\n",
       "          4    Estado Civil_Unión Civil           0\n",
       "Escritura 0      Estado Civil_Casado(a)         187\n",
       "          3     Estado Civil_Soltero(a)          44\n",
       "          2    Estado Civil_Separado(a)           5\n",
       "          1  Estado Civil_Divorciado(a)           4\n",
       "          5       Estado Civil_Viudo(a)           3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(2, 'Estado Civil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Moda N° Grupo Familiar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desiste</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Escritura</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Moda N° Grupo Familiar\n",
       "Desiste   0                      2\n",
       "Escritura 0                      2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grupo_fam(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipo Compra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tipo Compra</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Desiste</th>\n",
       "      <th>1</th>\n",
       "      <td>Tipo Compra_Primera Vivienda</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tipo Compra_Segunda Vivienda</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tipo Compra_Vivienda Inversion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tipo Compra_Otros</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Escritura</th>\n",
       "      <th>1</th>\n",
       "      <td>Tipo Compra_Primera Vivienda</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tipo Compra_Segunda Vivienda</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tipo Compra_Otros</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tipo Compra_Vivienda Inversion</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Tipo Compra  frecuencia\n",
       "Desiste   1    Tipo Compra_Primera Vivienda          20\n",
       "          2    Tipo Compra_Segunda Vivienda          10\n",
       "          3  Tipo Compra_Vivienda Inversion           3\n",
       "          0               Tipo Compra_Otros           0\n",
       "Escritura 1    Tipo Compra_Primera Vivienda         111\n",
       "          2    Tipo Compra_Segunda Vivienda         102\n",
       "          0               Tipo Compra_Otros          22\n",
       "          3  Tipo Compra_Vivienda Inversion           8"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(2, 'Tipo Compra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profesión y Cargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Profesion</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>2</th>\n",
       "      <td>Profesion (Estandar)_Otros</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Profesion (Estandar)_Universitarios</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profesion (Estandar)_Fuerzas Armadas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profesion (Estandar)_Independientes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Profesion (Estandar)_Tecnicos</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>2</th>\n",
       "      <td>Profesion (Estandar)_Otros</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Profesion (Estandar)_Universitarios</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profesion (Estandar)_Independientes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Profesion (Estandar)_Tecnicos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profesion (Estandar)_Fuerzas Armadas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Profesion  frecuencia\n",
       "Desiste   2            Profesion (Estandar)_Otros          24\n",
       "          4   Profesion (Estandar)_Universitarios           9\n",
       "          0  Profesion (Estandar)_Fuerzas Armadas           0\n",
       "          1   Profesion (Estandar)_Independientes           0\n",
       "          3         Profesion (Estandar)_Tecnicos           0\n",
       "Escritura 2            Profesion (Estandar)_Otros         175\n",
       "          4   Profesion (Estandar)_Universitarios          66\n",
       "          1   Profesion (Estandar)_Independientes           1\n",
       "          3         Profesion (Estandar)_Tecnicos           1\n",
       "          0  Profesion (Estandar)_Fuerzas Armadas           0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(2, 'Profesion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Cargo</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>6</th>\n",
       "      <td>Cargo Estandarizado_Gerente</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cargo Estandarizado_Jefe</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cargo Estandarizado_Supervisor</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cargo Estandarizado_Apoyo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cargo Estandarizado_Independiente</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>6</th>\n",
       "      <td>Cargo Estandarizado_Gerente</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cargo Estandarizado_Jefe</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cargo Estandarizado_Independiente</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cargo Estandarizado_Administrativo</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cargo Estandarizado_Apoyo</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Cargo  frecuencia\n",
       "Desiste   6          Cargo Estandarizado_Gerente          23\n",
       "          9             Cargo Estandarizado_Jefe           5\n",
       "          13      Cargo Estandarizado_Supervisor           2\n",
       "          2            Cargo Estandarizado_Apoyo           1\n",
       "          7    Cargo Estandarizado_Independiente           1\n",
       "Escritura 6          Cargo Estandarizado_Gerente         187\n",
       "          9             Cargo Estandarizado_Jefe          21\n",
       "          7    Cargo Estandarizado_Independiente          14\n",
       "          0   Cargo Estandarizado_Administrativo           5\n",
       "          2            Cargo Estandarizado_Apoyo           3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(2, 'Cargo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rango de Ingresos y Comuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rango de Ingresos</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>6</th>\n",
       "      <td>Rango de Ingresos_4.000.001 y 6.000.000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Ingresos_3.000.001 y 4.000.000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rango de Ingresos_6.000.001 y 8.000.000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Ingresos_2.500.001 y 3.000.000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Ingresos_1.500.001 y 2.000.000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>6</th>\n",
       "      <td>Rango de Ingresos_4.000.001 y 6.000.000</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Ingresos_3.000.001 y 4.000.000</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Ingresos_2.500.001 y 3.000.000</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rango de Ingresos_6.000.001 y 8.000.000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Ingresos_2.000.001 y 2.500.000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Rango de Ingresos  frecuencia\n",
       "Desiste   6  Rango de Ingresos_4.000.001 y 6.000.000          10\n",
       "          5  Rango de Ingresos_3.000.001 y 4.000.000           7\n",
       "          7  Rango de Ingresos_6.000.001 y 8.000.000           7\n",
       "          4  Rango de Ingresos_2.500.001 y 3.000.000           3\n",
       "          1  Rango de Ingresos_1.500.001 y 2.000.000           2\n",
       "Escritura 6  Rango de Ingresos_4.000.001 y 6.000.000         108\n",
       "          5  Rango de Ingresos_3.000.001 y 4.000.000          49\n",
       "          4  Rango de Ingresos_2.500.001 y 3.000.000          23\n",
       "          7  Rango de Ingresos_6.000.001 y 8.000.000          20\n",
       "          3  Rango de Ingresos_2.000.001 y 2.500.000          12"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(2, 'Rango de Ingresos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Comuna</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>25</th>\n",
       "      <td>Comuna Estandarizada_Las Condes</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Comuna Estandarizada_Vitacura</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Comuna Estandarizada_Ñuñoa</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Comuna Estandarizada_II Región de Antofagasta</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Comuna Estandarizada_Lo Barnechea</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>25</th>\n",
       "      <td>Comuna Estandarizada_Las Condes</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Comuna Estandarizada_Providencia</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Comuna Estandarizada_Lo Barnechea</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Comuna Estandarizada_Vitacura</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comuna Estandarizada_Colina</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Comuna  frecuencia\n",
       "Desiste   25                Comuna Estandarizada_Las Condes          11\n",
       "          58                  Comuna Estandarizada_Vitacura           5\n",
       "          65                     Comuna Estandarizada_Ñuñoa           4\n",
       "          13  Comuna Estandarizada_II Región de Antofagasta           3\n",
       "          26              Comuna Estandarizada_Lo Barnechea           2\n",
       "Escritura 25                Comuna Estandarizada_Las Condes          99\n",
       "          38               Comuna Estandarizada_Providencia          31\n",
       "          26              Comuna Estandarizada_Lo Barnechea          26\n",
       "          58                  Comuna Estandarizada_Vitacura          14\n",
       "          5                     Comuna Estandarizada_Colina          10"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(2, 'Comuna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de Montos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>Monto CH</th>\n",
       "      <th>Monto Subsidio</th>\n",
       "      <th>Monto Ahorro</th>\n",
       "      <th>Monto BAP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Etiqueta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desiste</th>\n",
       "      <td>25.031859</td>\n",
       "      <td>328.666390</td>\n",
       "      <td>0.045126</td>\n",
       "      <td>1.901399</td>\n",
       "      <td>1.842960</td>\n",
       "      <td>22.193457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2474.020912</td>\n",
       "      <td>2.632906</td>\n",
       "      <td>4.094756</td>\n",
       "      <td>1958.027094</td>\n",
       "      <td>50.660397</td>\n",
       "      <td>9.167464</td>\n",
       "      <td>2.505596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Escritura</th>\n",
       "      <td>29.553189</td>\n",
       "      <td>339.947784</td>\n",
       "      <td>0.215777</td>\n",
       "      <td>3.994751</td>\n",
       "      <td>1.708438</td>\n",
       "      <td>12.648856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2456.700387</td>\n",
       "      <td>2.212272</td>\n",
       "      <td>3.600448</td>\n",
       "      <td>1977.082973</td>\n",
       "      <td>27.254022</td>\n",
       "      <td>6.358129</td>\n",
       "      <td>4.063402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Monto Reserva   Monto Pie  Monto Carta de instruccion  \\\n",
       "Etiqueta                                                           \n",
       "Desiste        25.031859  328.666390                    0.045126   \n",
       "Escritura      29.553189  339.947784                    0.215777   \n",
       "\n",
       "           Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "Etiqueta                                                   \n",
       "Desiste                  1.901399               1.842960   \n",
       "Escritura                3.994751               1.708438   \n",
       "\n",
       "           Monto Crédito Complementario  Monto LEASING  \\\n",
       "Etiqueta                                                 \n",
       "Desiste                       22.193457            0.0   \n",
       "Escritura                     12.648856            0.0   \n",
       "\n",
       "           Monto Vivienda principal  Monto CDP  Monto CDP Cheque     Monto CH  \\\n",
       "Etiqueta                                                                        \n",
       "Desiste                 2474.020912   2.632906          4.094756  1958.027094   \n",
       "Escritura               2456.700387   2.212272          3.600448  1977.082973   \n",
       "\n",
       "           Monto Subsidio  Monto Ahorro  Monto BAP  \n",
       "Etiqueta                                            \n",
       "Desiste         50.660397      9.167464   2.505596  \n",
       "Escritura       27.254022      6.358129   4.063402  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro[df_prepro['cluster']==3].groupby(['Etiqueta']).mean()[get_columns(3,'Monto')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rango de Edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rango de Edad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>2</th>\n",
       "      <td>Rango de Edad_31-35</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Edad_26-30</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Edad_36-40</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Edad_41-45</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Edad_46-50</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>2</th>\n",
       "      <td>Rango de Edad_31-35</td>\n",
       "      <td>1165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Edad_41-45</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Edad_36-40</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Edad_26-30</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Edad_46-50</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Rango de Edad  frecuencia\n",
       "Desiste   2  Rango de Edad_31-35         279\n",
       "          1  Rango de Edad_26-30         194\n",
       "          3  Rango de Edad_36-40         187\n",
       "          4  Rango de Edad_41-45         171\n",
       "          5  Rango de Edad_46-50          84\n",
       "Escritura 2  Rango de Edad_31-35        1165\n",
       "          4  Rango de Edad_41-45         890\n",
       "          3  Rango de Edad_36-40         887\n",
       "          1  Rango de Edad_26-30         651\n",
       "          5  Rango de Edad_46-50         473"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(3, 'Rango de Edad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Nacionalidad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>4</th>\n",
       "      <td>Nacionalidad Cliente_Chilena</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Nacionalidad Cliente_Venezolana</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nacionalidad Cliente_Peruana</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nacionalidad Cliente_Extranjera</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nacionalidad Cliente_Argentina</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>4</th>\n",
       "      <td>Nacionalidad Cliente_Chilena</td>\n",
       "      <td>4884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Nacionalidad Cliente_Venezolana</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nacionalidad Cliente_Extranjera</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nacionalidad Cliente_Colombiana</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nacionalidad Cliente_Argentina</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Nacionalidad  frecuencia\n",
       "Desiste   4      Nacionalidad Cliente_Chilena        1084\n",
       "          16  Nacionalidad Cliente_Venezolana           9\n",
       "          13     Nacionalidad Cliente_Peruana           5\n",
       "          10  Nacionalidad Cliente_Extranjera           4\n",
       "          1    Nacionalidad Cliente_Argentina           3\n",
       "Escritura 4      Nacionalidad Cliente_Chilena        4884\n",
       "          16  Nacionalidad Cliente_Venezolana          29\n",
       "          10  Nacionalidad Cliente_Extranjera          23\n",
       "          6   Nacionalidad Cliente_Colombiana          13\n",
       "          1    Nacionalidad Cliente_Argentina          12"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(3, 'Nacionalidad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado Civil y N° Grupo Familiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Estado Civil</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>3</th>\n",
       "      <td>Estado Civil_Soltero(a)</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estado Civil_Casado(a)</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estado Civil_Divorciado(a)</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Estado Civil_Separado(a)</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estado Civil_Viudo(a)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>3</th>\n",
       "      <td>Estado Civil_Soltero(a)</td>\n",
       "      <td>3085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estado Civil_Casado(a)</td>\n",
       "      <td>1622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estado Civil_Divorciado(a)</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Estado Civil_Separado(a)</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estado Civil_Viudo(a)</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Estado Civil  frecuencia\n",
       "Desiste   3     Estado Civil_Soltero(a)         684\n",
       "          0      Estado Civil_Casado(a)         348\n",
       "          1  Estado Civil_Divorciado(a)          51\n",
       "          2    Estado Civil_Separado(a)          14\n",
       "          5       Estado Civil_Viudo(a)           7\n",
       "Escritura 3     Estado Civil_Soltero(a)        3085\n",
       "          0      Estado Civil_Casado(a)        1622\n",
       "          1  Estado Civil_Divorciado(a)         190\n",
       "          2    Estado Civil_Separado(a)          55\n",
       "          5       Estado Civil_Viudo(a)          24"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(3, 'Estado Civil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Moda N° Grupo Familiar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desiste</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Escritura</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Moda N° Grupo Familiar\n",
       "Desiste   0                      2\n",
       "Escritura 0                      2"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grupo_fam(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipo Compra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tipo Compra</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Desiste</th>\n",
       "      <th>1</th>\n",
       "      <td>Tipo Compra_Primera Vivienda</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tipo Compra_Segunda Vivienda</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tipo Compra_Vivienda Inversion</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tipo Compra_Otros</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Escritura</th>\n",
       "      <th>1</th>\n",
       "      <td>Tipo Compra_Primera Vivienda</td>\n",
       "      <td>3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tipo Compra_Segunda Vivienda</td>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tipo Compra_Vivienda Inversion</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tipo Compra_Otros</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Tipo Compra  frecuencia\n",
       "Desiste   1    Tipo Compra_Primera Vivienda         762\n",
       "          2    Tipo Compra_Segunda Vivienda         183\n",
       "          3  Tipo Compra_Vivienda Inversion         122\n",
       "          0               Tipo Compra_Otros          41\n",
       "Escritura 1    Tipo Compra_Primera Vivienda        3333\n",
       "          2    Tipo Compra_Segunda Vivienda         865\n",
       "          3  Tipo Compra_Vivienda Inversion         635\n",
       "          0               Tipo Compra_Otros         149"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(3, 'Tipo Compra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profesión y Cargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Profesion</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>2</th>\n",
       "      <td>Profesion (Estandar)_Otros</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Profesion (Estandar)_Universitarios</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Profesion (Estandar)_Tecnicos</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profesion (Estandar)_Independientes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profesion (Estandar)_Fuerzas Armadas</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>2</th>\n",
       "      <td>Profesion (Estandar)_Otros</td>\n",
       "      <td>3410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Profesion (Estandar)_Universitarios</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Profesion (Estandar)_Tecnicos</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profesion (Estandar)_Independientes</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profesion (Estandar)_Fuerzas Armadas</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Profesion  frecuencia\n",
       "Desiste   2            Profesion (Estandar)_Otros         767\n",
       "          4   Profesion (Estandar)_Universitarios         232\n",
       "          3         Profesion (Estandar)_Tecnicos         103\n",
       "          1   Profesion (Estandar)_Independientes           4\n",
       "          0  Profesion (Estandar)_Fuerzas Armadas           2\n",
       "Escritura 2            Profesion (Estandar)_Otros        3410\n",
       "          4   Profesion (Estandar)_Universitarios        1075\n",
       "          3         Profesion (Estandar)_Tecnicos         443\n",
       "          1   Profesion (Estandar)_Independientes          33\n",
       "          0  Profesion (Estandar)_Fuerzas Armadas          21"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(3, 'Profesion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Cargo</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>0</th>\n",
       "      <td>Cargo Estandarizado_Administrativo</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cargo Estandarizado_Jefe</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cargo Estandarizado_Docente</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cargo Estandarizado_Gerente</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cargo Estandarizado_Tecnico</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>0</th>\n",
       "      <td>Cargo Estandarizado_Administrativo</td>\n",
       "      <td>978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cargo Estandarizado_Jefe</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cargo Estandarizado_Gerente</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cargo Estandarizado_Tecnico</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cargo Estandarizado_Independiente</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Cargo  frecuencia\n",
       "Desiste   0   Cargo Estandarizado_Administrativo         231\n",
       "          9             Cargo Estandarizado_Jefe         132\n",
       "          3          Cargo Estandarizado_Docente         105\n",
       "          6          Cargo Estandarizado_Gerente         102\n",
       "          14         Cargo Estandarizado_Tecnico          91\n",
       "Escritura 0   Cargo Estandarizado_Administrativo         978\n",
       "          9             Cargo Estandarizado_Jefe         751\n",
       "          6          Cargo Estandarizado_Gerente         452\n",
       "          14         Cargo Estandarizado_Tecnico         398\n",
       "          7    Cargo Estandarizado_Independiente         391"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(3, 'Cargo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rango de Ingresos y Comuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rango de Ingresos</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>9</th>\n",
       "      <td>Rango de Ingresos_Entre 500.001 y 1.000.000</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rango de Ingresos_1.000.001 y 1.500.000</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Ingresos_1.500.001 y 2.000.000</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Ingresos_2.000.001 y 2.500.000</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Ingresos_2.500.001 y 3.000.000</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>9</th>\n",
       "      <td>Rango de Ingresos_Entre 500.001 y 1.000.000</td>\n",
       "      <td>1744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rango de Ingresos_1.000.001 y 1.500.000</td>\n",
       "      <td>1634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Ingresos_1.500.001 y 2.000.000</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Ingresos_2.000.001 y 2.500.000</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Ingresos_2.500.001 y 3.000.000</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Rango de Ingresos  frecuencia\n",
       "Desiste   9  Rango de Ingresos_Entre 500.001 y 1.000.000         470\n",
       "          0      Rango de Ingresos_1.000.001 y 1.500.000         303\n",
       "          1      Rango de Ingresos_1.500.001 y 2.000.000         135\n",
       "          3      Rango de Ingresos_2.000.001 y 2.500.000          47\n",
       "          4      Rango de Ingresos_2.500.001 y 3.000.000          44\n",
       "Escritura 9  Rango de Ingresos_Entre 500.001 y 1.000.000        1744\n",
       "          0      Rango de Ingresos_1.000.001 y 1.500.000        1634\n",
       "          1      Rango de Ingresos_1.500.001 y 2.000.000         696\n",
       "          3      Rango de Ingresos_2.000.001 y 2.500.000         230\n",
       "          4      Rango de Ingresos_2.500.001 y 3.000.000         213"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(3, 'Rango de Ingresos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Comuna</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>57</th>\n",
       "      <td>Comuna Estandarizada_VIII Región del Biobío</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Comuna Estandarizada_Puente Alto</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Comuna Estandarizada_II Región de Antofagasta</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Comuna Estandarizada_X Región de Los Lagos</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Comuna Estandarizada_IV Región de Coquimbo</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>57</th>\n",
       "      <td>Comuna Estandarizada_VIII Región del Biobío</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Comuna Estandarizada_II Región de Antofagasta</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Comuna Estandarizada_Puente Alto</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Comuna Estandarizada_X Región de Los Lagos</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Comuna Estandarizada_V Región de Valparaíso</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Comuna  frecuencia\n",
       "Desiste   57    Comuna Estandarizada_VIII Región del Biobío         228\n",
       "          40               Comuna Estandarizada_Puente Alto         109\n",
       "          13  Comuna Estandarizada_II Región de Antofagasta          97\n",
       "          59     Comuna Estandarizada_X Región de Los Lagos          78\n",
       "          15     Comuna Estandarizada_IV Región de Coquimbo          76\n",
       "Escritura 57    Comuna Estandarizada_VIII Región del Biobío        1003\n",
       "          13  Comuna Estandarizada_II Región de Antofagasta         514\n",
       "          40               Comuna Estandarizada_Puente Alto         438\n",
       "          59     Comuna Estandarizada_X Región de Los Lagos         356\n",
       "          54    Comuna Estandarizada_V Región de Valparaíso         303"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(3, 'Comuna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupo 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de Montos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>Monto CH</th>\n",
       "      <th>Monto Subsidio</th>\n",
       "      <th>Monto Ahorro</th>\n",
       "      <th>Monto BAP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Etiqueta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desiste</th>\n",
       "      <td>38.970816</td>\n",
       "      <td>203.732513</td>\n",
       "      <td>11.598703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.413168</td>\n",
       "      <td>22.487032</td>\n",
       "      <td>3.585561</td>\n",
       "      <td>1566.778623</td>\n",
       "      <td>8.696070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>223.359332</td>\n",
       "      <td>80.192139</td>\n",
       "      <td>9.876497</td>\n",
       "      <td>12.757353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Escritura</th>\n",
       "      <td>22.046071</td>\n",
       "      <td>58.180700</td>\n",
       "      <td>3.335364</td>\n",
       "      <td>0.165098</td>\n",
       "      <td>4.434527</td>\n",
       "      <td>10.243570</td>\n",
       "      <td>1.455924</td>\n",
       "      <td>1788.156279</td>\n",
       "      <td>11.920187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.698270</td>\n",
       "      <td>30.539049</td>\n",
       "      <td>3.140174</td>\n",
       "      <td>26.459056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Monto Reserva   Monto Pie  Monto Carta de instruccion  \\\n",
       "Etiqueta                                                           \n",
       "Desiste        38.970816  203.732513                   11.598703   \n",
       "Escritura      22.046071   58.180700                    3.335364   \n",
       "\n",
       "           Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "Etiqueta                                                   \n",
       "Desiste                  0.000000               3.413168   \n",
       "Escritura                0.165098               4.434527   \n",
       "\n",
       "           Monto Crédito Complementario  Monto LEASING  \\\n",
       "Etiqueta                                                 \n",
       "Desiste                       22.487032       3.585561   \n",
       "Escritura                     10.243570       1.455924   \n",
       "\n",
       "           Monto Vivienda principal  Monto CDP  Monto CDP Cheque    Monto CH  \\\n",
       "Etiqueta                                                                       \n",
       "Desiste                 1566.778623   8.696070               0.0  223.359332   \n",
       "Escritura               1788.156279  11.920187               0.0   94.698270   \n",
       "\n",
       "           Monto Subsidio  Monto Ahorro  Monto BAP  \n",
       "Etiqueta                                            \n",
       "Desiste         80.192139      9.876497  12.757353  \n",
       "Escritura       30.539049      3.140174  26.459056  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro[df_prepro['cluster']==4].groupby(['Etiqueta']).mean()[get_columns(4,'Monto')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rango de Edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rango de Edad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Edad_41-45</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Edad_36-40</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rango de Edad_31-35</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Edad_46-50</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rango de Edad_51-55</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Edad_41-45</td>\n",
       "      <td>2980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rango de Edad_36-40</td>\n",
       "      <td>2275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rango de Edad_31-35</td>\n",
       "      <td>1569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rango de Edad_46-50</td>\n",
       "      <td>1474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rango de Edad_51-55</td>\n",
       "      <td>1135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Rango de Edad  frecuencia\n",
       "Desiste   4  Rango de Edad_41-45         162\n",
       "          3  Rango de Edad_36-40         140\n",
       "          2  Rango de Edad_31-35         109\n",
       "          5  Rango de Edad_46-50          86\n",
       "          6  Rango de Edad_51-55          66\n",
       "Escritura 4  Rango de Edad_41-45        2980\n",
       "          3  Rango de Edad_36-40        2275\n",
       "          2  Rango de Edad_31-35        1569\n",
       "          5  Rango de Edad_46-50        1474\n",
       "          6  Rango de Edad_51-55        1135"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(4, 'Rango de Edad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Nacionalidad</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>4</th>\n",
       "      <td>Nacionalidad Cliente_Chilena</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nacionalidad Cliente_Extranjera</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nacionalidad Cliente_Argentina</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nacionalidad Cliente_Peruana</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nacionalidad Cliente_Alemana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>4</th>\n",
       "      <td>Nacionalidad Cliente_Chilena</td>\n",
       "      <td>12050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nacionalidad Cliente_Extranjera</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nacionalidad Cliente_Argentina</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nacionalidad Cliente_Colombiana</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nacionalidad Cliente_Peruana</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Nacionalidad  frecuencia\n",
       "Desiste   4      Nacionalidad Cliente_Chilena         737\n",
       "          10  Nacionalidad Cliente_Extranjera           4\n",
       "          1    Nacionalidad Cliente_Argentina           2\n",
       "          13     Nacionalidad Cliente_Peruana           2\n",
       "          0      Nacionalidad Cliente_Alemana           1\n",
       "Escritura 4      Nacionalidad Cliente_Chilena       12050\n",
       "          10  Nacionalidad Cliente_Extranjera          40\n",
       "          1    Nacionalidad Cliente_Argentina           8\n",
       "          6   Nacionalidad Cliente_Colombiana           4\n",
       "          13     Nacionalidad Cliente_Peruana           3"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(4, 'Nacionalidad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado Civil y N° Grupo Familiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Estado Civil</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>3</th>\n",
       "      <td>Estado Civil_Soltero(a)</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estado Civil_Casado(a)</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estado Civil_Divorciado(a)</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Estado Civil_Separado(a)</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estado Civil_Viudo(a)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>3</th>\n",
       "      <td>Estado Civil_Soltero(a)</td>\n",
       "      <td>6980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Estado Civil_Casado(a)</td>\n",
       "      <td>4556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Estado Civil_Separado(a)</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Estado Civil_Viudo(a)</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estado Civil_Divorciado(a)</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Estado Civil  frecuencia\n",
       "Desiste   3     Estado Civil_Soltero(a)         451\n",
       "          0      Estado Civil_Casado(a)         256\n",
       "          1  Estado Civil_Divorciado(a)          18\n",
       "          2    Estado Civil_Separado(a)          14\n",
       "          5       Estado Civil_Viudo(a)           9\n",
       "Escritura 3     Estado Civil_Soltero(a)        6980\n",
       "          0      Estado Civil_Casado(a)        4556\n",
       "          2    Estado Civil_Separado(a)         375\n",
       "          5       Estado Civil_Viudo(a)         119\n",
       "          1  Estado Civil_Divorciado(a)          80"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(4, 'Estado Civil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Moda N° Grupo Familiar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Desiste</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Escritura</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Moda N° Grupo Familiar\n",
       "Desiste   0                      2\n",
       "Escritura 0                      2"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grupo_fam(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipo Compra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tipo Compra</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Desiste</th>\n",
       "      <th>1</th>\n",
       "      <td>Tipo Compra_Primera Vivienda</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tipo Compra_Segunda Vivienda</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tipo Compra_Vivienda Inversion</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tipo Compra_Otros</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Escritura</th>\n",
       "      <th>1</th>\n",
       "      <td>Tipo Compra_Primera Vivienda</td>\n",
       "      <td>8508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tipo Compra_Segunda Vivienda</td>\n",
       "      <td>2107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tipo Compra_Vivienda Inversion</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tipo Compra_Otros</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Tipo Compra  frecuencia\n",
       "Desiste   1    Tipo Compra_Primera Vivienda         543\n",
       "          2    Tipo Compra_Segunda Vivienda         115\n",
       "          3  Tipo Compra_Vivienda Inversion          73\n",
       "          0               Tipo Compra_Otros          17\n",
       "Escritura 1    Tipo Compra_Primera Vivienda        8508\n",
       "          2    Tipo Compra_Segunda Vivienda        2107\n",
       "          3  Tipo Compra_Vivienda Inversion        1250\n",
       "          0               Tipo Compra_Otros         249"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(4, 'Tipo Compra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profesión y Cargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Profesion</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>2</th>\n",
       "      <td>Profesion (Estandar)_Otros</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Profesion (Estandar)_Universitarios</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Profesion (Estandar)_Tecnicos</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profesion (Estandar)_Independientes</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profesion (Estandar)_Fuerzas Armadas</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>2</th>\n",
       "      <td>Profesion (Estandar)_Otros</td>\n",
       "      <td>9096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Profesion (Estandar)_Universitarios</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Profesion (Estandar)_Tecnicos</td>\n",
       "      <td>1242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Profesion (Estandar)_Fuerzas Armadas</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Profesion (Estandar)_Independientes</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Profesion  frecuencia\n",
       "Desiste   2            Profesion (Estandar)_Otros         514\n",
       "          4   Profesion (Estandar)_Universitarios         129\n",
       "          3         Profesion (Estandar)_Tecnicos          95\n",
       "          1   Profesion (Estandar)_Independientes           7\n",
       "          0  Profesion (Estandar)_Fuerzas Armadas           3\n",
       "Escritura 2            Profesion (Estandar)_Otros        9096\n",
       "          4   Profesion (Estandar)_Universitarios        1700\n",
       "          3         Profesion (Estandar)_Tecnicos        1242\n",
       "          0  Profesion (Estandar)_Fuerzas Armadas          39\n",
       "          1   Profesion (Estandar)_Independientes          37"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(4, 'Profesion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Cargo</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>0</th>\n",
       "      <td>Cargo Estandarizado_Administrativo</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cargo Estandarizado_Jefe</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cargo Estandarizado_Gerente</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cargo Estandarizado_Supervisor</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cargo Estandarizado_Tecnico</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>0</th>\n",
       "      <td>Cargo Estandarizado_Administrativo</td>\n",
       "      <td>4242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cargo Estandarizado_Jefe</td>\n",
       "      <td>2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cargo Estandarizado_Supervisor</td>\n",
       "      <td>1468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cargo Estandarizado_Tecnico</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cargo Estandarizado_Gerente</td>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Cargo  frecuencia\n",
       "Desiste   0   Cargo Estandarizado_Administrativo         278\n",
       "          9             Cargo Estandarizado_Jefe         102\n",
       "          6          Cargo Estandarizado_Gerente          71\n",
       "          13      Cargo Estandarizado_Supervisor          70\n",
       "          14         Cargo Estandarizado_Tecnico          65\n",
       "Escritura 0   Cargo Estandarizado_Administrativo        4242\n",
       "          9             Cargo Estandarizado_Jefe        2586\n",
       "          13      Cargo Estandarizado_Supervisor        1468\n",
       "          14         Cargo Estandarizado_Tecnico        1455\n",
       "          6          Cargo Estandarizado_Gerente        1162"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(4, 'Cargo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rango de Ingresos y Comuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rango de Ingresos</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>9</th>\n",
       "      <td>Rango de Ingresos_Entre 500.001 y 1.000.000</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rango de Ingresos_Hasta 500.000</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rango de Ingresos_1.000.001 y 1.500.000</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Ingresos_2.500.001 y 3.000.000</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Ingresos_1.500.001 y 2.000.000</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>9</th>\n",
       "      <td>Rango de Ingresos_Entre 500.001 y 1.000.000</td>\n",
       "      <td>5197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rango de Ingresos_1.000.001 y 1.500.000</td>\n",
       "      <td>2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rango de Ingresos_Hasta 500.000</td>\n",
       "      <td>1611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rango de Ingresos_1.500.001 y 2.000.000</td>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rango de Ingresos_2.500.001 y 3.000.000</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Rango de Ingresos  frecuencia\n",
       "Desiste   9   Rango de Ingresos_Entre 500.001 y 1.000.000         339\n",
       "          10              Rango de Ingresos_Hasta 500.000         131\n",
       "          0       Rango de Ingresos_1.000.001 y 1.500.000         121\n",
       "          4       Rango de Ingresos_2.500.001 y 3.000.000          45\n",
       "          1       Rango de Ingresos_1.500.001 y 2.000.000          42\n",
       "Escritura 9   Rango de Ingresos_Entre 500.001 y 1.000.000        5197\n",
       "          0       Rango de Ingresos_1.000.001 y 1.500.000        2532\n",
       "          10              Rango de Ingresos_Hasta 500.000        1611\n",
       "          1       Rango de Ingresos_1.500.001 y 2.000.000         931\n",
       "          4       Rango de Ingresos_2.500.001 y 3.000.000         675"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(4, 'Rango de Ingresos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Comuna</th>\n",
       "      <th>frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Desiste</th>\n",
       "      <th>57</th>\n",
       "      <td>Comuna Estandarizada_VIII Región del Biobío</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Comuna Estandarizada_IX Región de la Araucanía</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Comuna Estandarizada_II Región de Antofagasta</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Comuna Estandarizada_V Región de Valparaíso</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Comuna Estandarizada_Puente Alto</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Escritura</th>\n",
       "      <th>13</th>\n",
       "      <td>Comuna Estandarizada_II Región de Antofagasta</td>\n",
       "      <td>1410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Comuna Estandarizada_V Región de Valparaíso</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Comuna Estandarizada_VIII Región del Biobío</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Comuna Estandarizada_Santiago</td>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Comuna Estandarizada_Puente Alto</td>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Comuna  frecuencia\n",
       "Desiste   57     Comuna Estandarizada_VIII Región del Biobío         126\n",
       "          16  Comuna Estandarizada_IX Región de la Araucanía          79\n",
       "          13   Comuna Estandarizada_II Región de Antofagasta          66\n",
       "          54     Comuna Estandarizada_V Región de Valparaíso          59\n",
       "          40                Comuna Estandarizada_Puente Alto          51\n",
       "Escritura 13   Comuna Estandarizada_II Región de Antofagasta        1410\n",
       "          54     Comuna Estandarizada_V Región de Valparaíso        1070\n",
       "          57     Comuna Estandarizada_VIII Región del Biobío        1004\n",
       "          51                   Comuna Estandarizada_Santiago         952\n",
       "          40                Comuna Estandarizada_Puente Alto         931"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montar_df(4, 'Comuna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_palette(\"deep\", desat=.6)\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro = df_prepro.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df_prepro.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monto Reserva\n",
      "Monto Pie\n",
      "Monto Carta de instruccion\n",
      "Monto Beneficio Minero\n",
      "Monto CH FFAA y Otros\n",
      "Monto Crédito Complementario\n",
      "Monto LEASING\n",
      "Monto Vivienda principal\n",
      "Monto CDP\n",
      "Monto CDP Cheque\n",
      "Monto CH\n",
      "Monto Subsidio\n",
      "Monto Ahorro\n",
      "Monto BAP\n",
      "N° Grupo Familiar\n",
      "Operacion Con Credito_No\n",
      "Operacion Con Credito_Si\n",
      "Zona_1-ZONA NORTE\n",
      "Zona_2-ZONA CENTRO\n",
      "Zona_3-ZONA SUR\n",
      "Zona_4-ZONA VERTICAL\n",
      "Zona_5-ZONA ANTOFAGASTA\n",
      "Tipo_Jurídico\n",
      "Tipo_Natural\n",
      "Nacionalidad Cliente_Alemana\n",
      "Nacionalidad Cliente_Argentina\n",
      "Nacionalidad Cliente_Boliviana\n",
      "Nacionalidad Cliente_Brasileña\n",
      "Nacionalidad Cliente_Chilena\n",
      "Nacionalidad Cliente_China\n",
      "Nacionalidad Cliente_Colombiana\n",
      "Nacionalidad Cliente_Cubana\n",
      "Nacionalidad Cliente_Ecuatoriana\n",
      "Nacionalidad Cliente_Española\n",
      "Nacionalidad Cliente_Extranjera\n",
      "Nacionalidad Cliente_Italiana\n",
      "Nacionalidad Cliente_Paraguaya\n",
      "Nacionalidad Cliente_Peruana\n",
      "Nacionalidad Cliente_Salvadoreña\n",
      "Nacionalidad Cliente_Uruguaya\n",
      "Nacionalidad Cliente_Venezolana\n",
      "Estado Civil_Casado(a)\n",
      "Estado Civil_Divorciado(a)\n",
      "Estado Civil_Separado(a)\n",
      "Estado Civil_Soltero(a)\n",
      "Estado Civil_Unión Civil\n",
      "Estado Civil_Viudo(a)\n",
      "Tipo Compra_Otros\n",
      "Tipo Compra_Primera Vivienda\n",
      "Tipo Compra_Segunda Vivienda\n",
      "Tipo Compra_Vivienda Inversion\n",
      "Rango de Ingresos_1.000.001 y 1.500.000\n",
      "Rango de Ingresos_1.500.001 y 2.000.000\n",
      "Rango de Ingresos_10.000.001 y 20.000.000\n",
      "Rango de Ingresos_2.000.001 y 2.500.000\n",
      "Rango de Ingresos_2.500.001 y 3.000.000\n",
      "Rango de Ingresos_3.000.001 y 4.000.000\n",
      "Rango de Ingresos_4.000.001 y 6.000.000\n",
      "Rango de Ingresos_6.000.001 y 8.000.000\n",
      "Rango de Ingresos_8.000.001 y 10.000.000\n",
      "Rango de Ingresos_Entre 500.001 y 1.000.000\n",
      "Rango de Ingresos_Hasta 500.000\n",
      "Rango de Ingresos_Sobre 20.000.000\n",
      "Profesion (Estandar)_Fuerzas Armadas\n",
      "Profesion (Estandar)_Independientes\n",
      "Profesion (Estandar)_Otros\n",
      "Profesion (Estandar)_Tecnicos\n",
      "Profesion (Estandar)_Universitarios\n",
      "Cargo Estandarizado_Administrativo\n",
      "Cargo Estandarizado_Analista\n",
      "Cargo Estandarizado_Apoyo\n",
      "Cargo Estandarizado_Docente\n",
      "Cargo Estandarizado_Ejecutivo\n",
      "Cargo Estandarizado_Encargado\n",
      "Cargo Estandarizado_Gerente\n",
      "Cargo Estandarizado_Independiente\n",
      "Cargo Estandarizado_Ingeniero\n",
      "Cargo Estandarizado_Jefe\n",
      "Cargo Estandarizado_Operario\n",
      "Cargo Estandarizado_Otro\n",
      "Cargo Estandarizado_Salud\n",
      "Cargo Estandarizado_Supervisor\n",
      "Cargo Estandarizado_Tecnico\n",
      "Rango de Edad_19-25\n",
      "Rango de Edad_26-30\n",
      "Rango de Edad_31-35\n",
      "Rango de Edad_36-40\n",
      "Rango de Edad_41-45\n",
      "Rango de Edad_46-50\n",
      "Rango de Edad_51-55\n",
      "Rango de Edad_56-60\n",
      "Rango de Edad_61-65\n",
      "Rango de Edad_66-70\n",
      "Rango de Edad_71-75\n",
      "Rango de Edad_76-80\n",
      "Rango de Edad_sobre 80\n",
      "Comuna Estandarizada_Alhué\n",
      "Comuna Estandarizada_Buin\n",
      "Comuna Estandarizada_Calera de Tango\n",
      "Comuna Estandarizada_Cerrillos\n",
      "Comuna Estandarizada_Cerro Navia\n",
      "Comuna Estandarizada_Colina\n",
      "Comuna Estandarizada_Conchalí\n",
      "Comuna Estandarizada_Curacaví\n",
      "Comuna Estandarizada_El Bosque\n",
      "Comuna Estandarizada_El Monte\n",
      "Comuna Estandarizada_Estación Central\n",
      "Comuna Estandarizada_Huechuraba\n",
      "Comuna Estandarizada_I Región de Tarapacá\n",
      "Comuna Estandarizada_II Región de Antofagasta\n",
      "Comuna Estandarizada_III Región de Atacama\n",
      "Comuna Estandarizada_IV Región de Coquimbo\n",
      "Comuna Estandarizada_IX Región de la Araucanía\n",
      "Comuna Estandarizada_Independencia\n",
      "Comuna Estandarizada_Isla de Maipo\n",
      "Comuna Estandarizada_La Cisterna\n",
      "Comuna Estandarizada_La Florida\n",
      "Comuna Estandarizada_La Granja\n",
      "Comuna Estandarizada_La Pintana\n",
      "Comuna Estandarizada_La Reina\n",
      "Comuna Estandarizada_Lampa\n",
      "Comuna Estandarizada_Las Condes\n",
      "Comuna Estandarizada_Lo Barnechea\n",
      "Comuna Estandarizada_Lo Espejo\n",
      "Comuna Estandarizada_Lo Prado\n",
      "Comuna Estandarizada_Macul\n",
      "Comuna Estandarizada_Maipu\n",
      "Comuna Estandarizada_Melipilla\n",
      "Comuna Estandarizada_Padre Hurtado\n",
      "Comuna Estandarizada_Paine\n",
      "Comuna Estandarizada_Pedro Aguirre Cerda\n",
      "Comuna Estandarizada_Peñaflor\n",
      "Comuna Estandarizada_Peñalolén\n",
      "Comuna Estandarizada_Pirque\n",
      "Comuna Estandarizada_Providencia\n",
      "Comuna Estandarizada_Pudahuel\n",
      "Comuna Estandarizada_Puente Alto\n",
      "Comuna Estandarizada_Quilicura\n",
      "Comuna Estandarizada_Quinta Normal\n",
      "Comuna Estandarizada_Recoleta\n",
      "Comuna Estandarizada_Renca\n",
      "Comuna Estandarizada_San Bernardo\n",
      "Comuna Estandarizada_San Joaquín\n",
      "Comuna Estandarizada_San José de Maipo\n",
      "Comuna Estandarizada_San Miguel\n",
      "Comuna Estandarizada_San Pedro\n",
      "Comuna Estandarizada_San Ramón\n",
      "Comuna Estandarizada_Santiago\n",
      "Comuna Estandarizada_Talagante\n",
      "Comuna Estandarizada_Til Til\n",
      "Comuna Estandarizada_V Región de Valparaíso\n",
      "Comuna Estandarizada_VI Región del Libertador General Bernardo O’Higgins\n",
      "Comuna Estandarizada_VII Región del Maule\n",
      "Comuna Estandarizada_VIII Región del Biobío\n",
      "Comuna Estandarizada_Vitacura\n",
      "Comuna Estandarizada_X Región de Los Lagos\n",
      "Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo\n",
      "Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena\n",
      "Comuna Estandarizada_XIV Región de Los Ríos\n",
      "Comuna Estandarizada_XV Región de Arica y Parinacota\n",
      "Comuna Estandarizada_XVI Región de Ñuble\n",
      "Comuna Estandarizada_Ñuñoa\n",
      "cluster\n",
      "Etiqueta\n"
     ]
    }
   ],
   "source": [
    "for i in df_prepro.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>Monto CH</th>\n",
       "      <th>Monto Subsidio</th>\n",
       "      <th>Monto Ahorro</th>\n",
       "      <th>Monto BAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.00</td>\n",
       "      <td>349.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3317.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3238.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.48</td>\n",
       "      <td>36.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1662.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>910.00</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>234.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2108.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2091.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>232.09</td>\n",
       "      <td>2088.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2718.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.50</td>\n",
       "      <td>76.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2065.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1350.62</td>\n",
       "      <td>325.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24606</th>\n",
       "      <td>10.00</td>\n",
       "      <td>300.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3156.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2783.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24607</th>\n",
       "      <td>10.00</td>\n",
       "      <td>1480.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3973.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2324.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24608</th>\n",
       "      <td>10.00</td>\n",
       "      <td>1481.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5260.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3664.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24609</th>\n",
       "      <td>10.00</td>\n",
       "      <td>504.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5148.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4634.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24610</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5472.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4979.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24611 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "0              10.00     349.88                        0.00   \n",
       "1               3.48      36.52                        0.00   \n",
       "2               0.00     234.58                        0.00   \n",
       "3               0.00     232.09                     2088.81   \n",
       "4               3.50      76.50                        0.00   \n",
       "...              ...        ...                         ...   \n",
       "24606          10.00     300.00                        0.00   \n",
       "24607          10.00    1480.00                        0.00   \n",
       "24608          10.00    1481.00                        0.00   \n",
       "24609          10.00     504.00                        0.00   \n",
       "24610           0.00       0.00                        0.00   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "0                         0.0                    0.0   \n",
       "1                         0.0                    0.0   \n",
       "2                         0.0                    0.0   \n",
       "3                         0.0                    0.0   \n",
       "4                         0.0                    0.0   \n",
       "...                       ...                    ...   \n",
       "24606                     0.0                    0.0   \n",
       "24607                     0.0                    0.0   \n",
       "24608                     0.0                    0.0   \n",
       "24609                     0.0                    0.0   \n",
       "24610                     0.0                    0.0   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "0                               0.0            0.0                   3317.23   \n",
       "1                               0.0            0.0                   1662.11   \n",
       "2                               0.0            0.0                   2108.01   \n",
       "3                               0.0            0.0                   2718.95   \n",
       "4                               0.0            0.0                   2065.91   \n",
       "...                             ...            ...                       ...   \n",
       "24606                           0.0            0.0                   3156.00   \n",
       "24607                           0.0            0.0                   3973.00   \n",
       "24608                           0.0            0.0                   5260.00   \n",
       "24609                           0.0            0.0                   5148.00   \n",
       "24610                           0.0            0.0                   5472.00   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  Monto CH  Monto Subsidio  Monto Ahorro  \\\n",
       "0            0.0               0.0   3238.85             0.0           0.0   \n",
       "1            0.0               0.0    910.00           450.0           0.0   \n",
       "2            0.0               0.0   2091.60             0.0           0.0   \n",
       "3            0.0               0.0      0.00             0.0           0.0   \n",
       "4            0.0               0.0   1350.62           325.0           0.0   \n",
       "...          ...               ...       ...             ...           ...   \n",
       "24606        0.0               0.0   2783.00             0.0           0.0   \n",
       "24607        0.0               0.0   2324.00             0.0           0.0   \n",
       "24608        0.0               0.0   3664.00             0.0           0.0   \n",
       "24609        0.0               0.0   4634.00             0.0           0.0   \n",
       "24610        0.0               0.0   4979.00             0.0           0.0   \n",
       "\n",
       "       Monto BAP  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "...          ...  \n",
       "24606        0.0  \n",
       "24607        0.0  \n",
       "24608        0.0  \n",
       "24609        0.0  \n",
       "24610        0.0  \n",
       "\n",
       "[24611 rows x 14 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num =  df_prepro[get_columns(0, 'Monto')]\n",
    "df_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dividimos la data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_esc = df_prepro[df_prepro['Etiqueta'] == 'Escritura']\n",
    "y_esc = X_esc['Etiqueta'] #df_prepro[df_prepro['Etiqueta'] == 'Escritura']['Etiqueta']\n",
    "X_des = df_prepro[df_prepro['Etiqueta'] == 'Desiste']\n",
    "y_des = X_des['Etiqueta'] #df_prepro[df_prepro['Etiqueta'] == 'Desiste']['Etiqueta']\n",
    "\n",
    "X_train_esc, X_test_esc, y_train_esc, y_test_esc = train_test_split(X_esc, y_esc, test_size=0.2, random_state=42)\n",
    "X_train_des, X_test_des, y_train_des, y_test_des = train_test_split(X_des, y_des, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = pd.concat([X_train_esc, X_train_des])\n",
    "X_test_1 = pd.concat([X_test_esc, X_test_des])\n",
    "\n",
    "X_train = pd.concat([X_train_esc, X_train_des]).drop(['Etiqueta'], axis=1)\n",
    "X_test = pd.concat([X_test_esc, X_test_des]).drop(['Etiqueta'], axis=1)\n",
    "\n",
    "y_train = pd.concat([y_train_esc, y_train_des])\n",
    "y_test = pd.concat([y_test_esc, y_test_des])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura 22197\n",
      "Desiste 2414\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dic_etiq= dict()\n",
    "\n",
    "for i in range(df_prepro.shape[0]):\n",
    "    if str(df_prepro['Etiqueta'][i]) not in list(dic_etiq.keys()):\n",
    "        dic_etiq[str(df_prepro['Etiqueta'][i])] =1\n",
    "    else:\n",
    "        dic_etiq[str(df_prepro['Etiqueta'][i])] += 1\n",
    "\n",
    "        \n",
    "df_etiqueta = pd.DataFrame(columns= ['Etiqueta', 'frecuencia'])\n",
    "\n",
    "for i in range(len(dic_etiq)):\n",
    "    df_etiqueta.loc[i] = [list(dic_etiq.keys())[i], list(dic_etiq.values())[i]]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.bar(df_etiqueta['Etiqueta'],df_etiqueta['frecuencia'])\n",
    "plt.xticks(rotation=75)\n",
    "plt.title('Distribucion de Etiquetas')\n",
    "\n",
    "print('Escritura', len(df_prepro[df_prepro['Etiqueta']== 'Escritura']))\n",
    "print('Desiste', len(df_prepro[df_prepro['Etiqueta']== 'Desiste']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = get_columns(0, 'Monto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAI9CAYAAAANaL02AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAC8VElEQVR4nOzdeVxU1fvA8c84ILiGimgm9dUyXBDNUkcSyzHTURYXUrJQq69phZqaZaJkCOWeYphom1kqhiiCmKaY4IIbJWnYZi6koYiKOzAzvz/8cb+OKILOHXF63r3m9eree+45594Z5OE5Z87VmM1mM0IIIYQQQhWV7nYHhBBCCCHsmQRbQgghhBAqkmBLCCGEEEJFEmwJIYQQQqhIgi0hhBBCCBVJsCWEEEIIoSKHu90BIYQQQohiVR4LUaXeSz9+rEq9ZSGZLSGEEEIIFUlmSwghhBAVh8b+8kASbAkhhBCi4tBo7nYPrM7+wkchhBBCiApEMltCCCGEqDjscBjR/q5ICCGEEKICkcyWEEIIISoOmbMlhBBCCCHKQzJbQgghhKg47HDOlgRbQgghhKg4ZBhRCCGEEEKUh2S2hBBCCFFx2OEwov1dkRBCCCFEBSKZLSGEEEJUHHY4Z0uCLSGEEEJUHDKMKIQQQgghykMyW0IIIYSoOOxwGFEyW0IIIYQQKpLMlhBCCCEqDjucsyXBlhBCCCEqDhlGFEIIIYQQ5SGZLSGEEEJUHHY4jGh/VySEEEIIUYFIZksIIYQQFYdktoQQtmA0Gvniiy/o06cPAQEB9OjRg+nTp1NQUKBamzt27MDX1/e2zw8ICCA/P1/ZNpvNdOvWjeXLl5e7rnHjxvHZZ5+V2J+Tk0NQUNBt97E0c+fOJTw8vMT++Ph4Hn/8cQICAujVqxcBAQEEBQXx448/3nGbjz32GNnZ2fz888+MGDGi1LKZmZmEhYXdcZu25uHhgZ+fn3L/fH19iYmJueN6w8PDmTt3LgBDhgzhjz/+KLX8yy+/TF5e3h23K8TtkMyWEBXQpEmTOHv2LIsWLaJGjRpcvHiRt956i9DQUKZPn363u3dDCQkJFtv79u2jdevW9OvXz2pt1KtXj2XLllmtvrJ64oknLAKElJQUhg8fzg8//ICDw53/M9qyZUuioqJKLfPHH3+Qk5Nzx23dDYsWLaJ27doAnD9/noCAAB599FE6d+5slfoXLlx4yzJbt261SlvCBirZ37cRJdgSooLJzs4mMTGRLVu2UL16dQCqVq3K+++/T0ZGBgDnzp3j/fff58CBA2g0Gnx8fBg9ejQODg54enrSpUsXDhw4wIwZM3j++ecttqtWrUpkZCRnzpzBaDQSHBxMYGCgRR/++usvwsPDuXDhAidPnqRp06bMnj0bJycn9u7dS0REBJcuXcLR0ZG3336bDh064OHhwfbt26lduzbR0dGsWbMGrVbLiBEjmDhxInXr1iU4OJjWrVuTkZHB8ePH6dChA5MnT6ZSpbIl2bOzs/Hz8+PHH39k7ty5/P3335w8eZK///6bevXqMX36dNzc3MjJySE8PJzjx49TWFhIz549GTZsGADz589n48aNXL58mUuXLvHOO+/QtWvXcr1HHTp04OTJk+Tn5zNt2jTOnDnD0aNHefrppxk5ciQzZsxg165dGI1GmjdvzoQJE6hevTq7d+9m8uTJaDQaWrZsiclkAq5mFSdPnkxSUhIXLlwgIiKCjIwMtFotzzzzDM8//zxRUVGcO3eOd999lw8//JDY2FgWL15MpUqVcHV1ZeLEiTRq1Ihx48aVuT9Llixh2bJlODo64uTkRHh4OI888ohynUajEb1eT3R0NJ6engC8+eabtGvXjvbt2xMaGkpBQQFms5nAwEBeeOGFW9676tWr4+npycGDB5XPYtWqVblw4QIrVqxgy5YtfPLJJxQWFuLs7Mw777zDY489xvnz5wkNDeXAgQO4ubmh1Wp5/PHHAdDr9cyZM4eWLVsSFxfHF198QaVKlahVqxZTp05VAtlBgwaxYMECzp8/T3h4OGfOnEGj0fDyyy/Tq1cvduzYUeb+/Pnnn7d1/aIMZBhRCKG2/fv388gjjyiBVrG6devSrVs3ACIiInBxcSExMZEVK1bw66+/8vnnnwNQWFhI586dWbduHS1btrTYbtasGSNGjGDMmDHEx8fz9ddf8/nnn/PTTz9ZtLV8+XJ69erF8uXLWb9+PdnZ2fzwww8UFhbyxhtv8MYbb5CUlMTkyZP54IMPlKABYMWKFaSlpREXF0diYiJNmjRh3LhxyvEjR46wePFiVq9eTWpqKjt37rzte7V7927mzJnDd999R5UqVZSs19ixY+nbty/x8fHExcWxbds2kpOT+fvvv9m2bRuLFy8mMTGRUaNG3TKjdD2z2UxsbCyPPvqokq25fPkya9asYezYsSxYsACtVkt8fDyrV6/Gzc2NGTNmUFBQwMiRIxk3bhyrVq2iffv2XL58uUT9UVFRXLlyheTkZFatWkVGRgZHjhxhxIgRPPHEE3z44Yds376dTz/9lK+++orVq1fj6+vLG2+8gdlsLnN/jEYjH3zwAZ9++ikrVqygX79+7Nmzx6IvWq1WuY8AZ8+eZfv27fj5+fHZZ5+h1+uJj49nwYIF7N692+JzcDMHDx5k165dtG3bFoDff/+dmTNnkpiYyLFjx/joo49YsGABq1atYvLkyQwfPpyLFy8SFRWFs7Mz3333HXPmzOGvv/4qUXfxHxSffvopiYmJ6PV6PvnkEz788EPgaoatbt26vPbaawQHB5OYmMjChQuZNWuWMixc1v7c7vWLfyfJbAlRwVSqVOmW/2inpqaydOlSNBoNlStXJigoiEWLFvHqq68CV4e9rlW8fejQIY4cOcL48eOVY5cvX+aXX37h4YcfVvaNHTuWrVu3snDhQg4dOsSJEye4ePEiv/32G5UqVeLpp58GwNPTk8TExBJ969OnD1WrVgVg4MCBzJ8/X5lv1rlzZypVqkT16tV56KGHOHv27G3cpavatWunBKXNmzfn7NmzXLx4kV27dnH27FnmzJkDwMWLFzlw4AA9evRg2rRpJCYmcvjwYfbu3cuFCxdu2c7u3bsJCAhAo9FQUFBA48aNLYK04gwLwA8//MC5c+fYtm0bcDX4rVOnDr/99hsODg506NABAF9f3xvOwdq2bRvvvvsuWq0WrVbL119/DaAEPABpaWn06NFDCfb69OlDZGQk2dnZZe6PVqule/fuBAUF8fTTT9OxY0eeeuqpEv3p27cvgYGBjBs3jqSkJPR6PTVq1KBr16688847ZGZm0qFDByZMmHDTDOWgQYOUz3WVKlV4++238fLyYseOHdx///088MADwNWhvhMnTjB48GDlXI1Gw5EjR9i+fTvjx49Ho9FQu3btG2Yjt2/fTseOHbn//vsBLOopdujQIa5cucKzzz4LXB2afvbZZ0lLS6N9+/Zl7k95rl+Ukx0uairBlhAVjJeXFwcPHuT8+fMW2a2cnBwmTpxIVFQUJpMJzTX/IJlMJoqKipTt4kDn+m2j0UiNGjUs5lfl5uZSo0YNi+zW6NGjMRqNGAwGnn76aY4fP47ZbEar1Vq0C/Dbb7/RuHFji76U1jdnZ2fl/zUajZKNuR03qstkMmE2m1m2bBlVqlQBIC8vDycnJ/bv38/rr7/O4MGDefLJJ2nbti3vv//+Ldu5fs7W9a693yaTifHjxyuBy4ULF7hy5QrHjh0rca03mu/l4OBgcf+OHz9ucZ3FbVzPbDYr97ks/QGYMWMGv/32G9u2bWPBggUkJCQoAWqxBx54gObNm/PDDz8QHx+vBOrF2dJt27axfft2oqOjiY+Pp379+iX6du2cretd39cOHTowe/Zsi+t3c3NTrrGYVqstUdf1n8/Lly/z999/W/whYTQaS3yGS7t3N+tP06ZNy3z9QkgYLkQFU69ePfz8/Bg/fjznz58Hrk4qnjRpEi4uLjg7O9OxY0e+/vprzGYzBQUFLF++HG9v71vW3ahRI5ydnZVg6/jx4/j6+rJv3z6Lclu2bOGNN96gR48eAOzduxej0Ujjxo3RaDTKZOP9+/czaNAgi1/+Pj4+rFixgosXLwKwePFi2rZtS+XKle/85pRB9erVad26NV988QUA+fn5PP/882zcuJFdu3bh6enJSy+9RLt27di4cSNGo9Gq7Xfs2JFvvvmGgoICTCYTEydOZNasWXh4eGA2m9m8eTMAGzduvGFWr0OHDqxcuRKTyURBQQEjRoxg165daLVaJSDw8fEhOTlZ+XbdihUrcHFx4aGHHipzf/Ly8njqqadwcXFh8ODBvPnmm/z88883vKZ+/fqxcOFCLl26pGTNxowZQ3JyMj179uS9996jevXqHDly5I7uXYcOHdi6dSt//vknAJs3b8bf35/Lly/j4+NDXFwcJpOJs2fPsnHjxhLnt2/fnu3bt3PixAkAli1bpnyhpPj+NW7cGAcHB9avXw9c/SNm3bp1N/z5Ka0/aly/+H+aSuq87iLJbAlRAb333nvMmzePoKAgtFotBQUFPPPMMwwfPhyACRMmEBERgZ+fH4WFhfj4+CgTwEtTuXJl5s2bR2RkJJ9++ilFRUWMHDmSxx9/nB07dijlRo0axRtvvEHVqlWpXr06bdu25ciRI1SuXJm5c+fywQcfMG3aNBwdHZk7d65FIBUYGMjx48d57rnnMJlMPPTQQ8yYMaPc9+Cjjz7i448/VrY7d+7M6NGjy3TujBkzmDx5Mn5+fhQUFODr64u/vz+5ubmsX78eg8GAyWSic+fOnD17VglqreH1119n6tSp9O7dG6PRSLNmzRg3bhyOjo5ER0czadIkZs2aRbNmzahTp06J80NCQoiMjCQgIACj0UiPHj149tlnOXz4MNHR0YSEhPDxxx8zePBgJdCtXbs2MTExNxzGull/qlevzmuvvcbgwYNxdnZGq9USERFxw2vS6/W8//77DBkyxKLe0NBQYmNjlYn8xfOwbtcjjzxCeHg4o0ePxmw24+DgwCeffEK1atUYPnw47733HgaDgdq1a/Poo4+WON/Dw4OxY8fy3//+F7g6z/GDDz4AoHv37gQHBzN37lzmzZtHREQEc+fOxWg08sYbb6DT6Sx+Bm7VHzWuX/w/OxxG1JjvJIcvhBBCCGFFVbpOVaXeS9+/o0q9ZSGZLSGEEEJUHLL0gxBCCCGEKA/JbAkhhBCi4rDDOVsSbAkhhBCi4rDDYUQJtsQ9ocpjIaq3sWXlB6q3AVBYpP4q0641nVRvA8Bkg+/XONhooUhbXAvAsTOXVG/jwTpVb13ICgoK1f8sn79SdOtCVlDvPudbF7pDhUbbrDBvq5+ZhrVss5zL3ZCYmMgnn3xCUVERgwYNKvEopv379xMWFkZhYSH3338/06dPp2bNmjetz/7CRyGEEELcuzQadV5llJOTw0cffcSSJUtYtWoVsbGx/PHHHxZlIiMjGTFiBKtXr6ZRo0Z89tlnpdYpmS0hhBBC2L38/Hzy8/NL7K9Zs6ZFVmrbtm3odDpcXFwA6NatG9999x0hIf8bYTGZTMqjvi5dusR9991XatsSbAkhhBCi4lBpztaiRYssFkouFhISoiwYDXDixAnq1q2rbLu5uZGZmWlxzrhx43j55Zf54IMPqFKlCsuXLy+1bQm2hBBCCGH3Bg0aRO/evUvsv36u1fXPdzWbzSWeuRkaGsqXX36Jl5cXX3zxBe+88w4LFiy4adsSbAkhhBCi4lBp6Yfrhwtvpn79+uzevVvZPnnypPIwdIDffvsNJycnvLy8AOjfv3+JB7hfTybICyGEEKLiuMsPovb29mb79u3k5eVx6dIl1q9fT6dOnZTjDz30EP/88w8HDx4Erj5UvmXLlqXWKZktIYQQQoj/V69ePUaNGsXAgQMpLCwkMDAQLy8vhgwZwogRI2jZsiUffvghb775JmazmTp16igPPL8ZeRC1uCfIOlvlI+tslZ+ss1V+ss5W+cg6W2VTxW+eKvVeSnxdlXrLQoYRhRBCCCFUJMGWSrKzs/Hw8CAsLMxif1ZWFh4eHsTHx99WvcuXLycpKanM5ePj42nXrh0BAQEEBATg6+vLs88+y4YNG26rfSGEEEJVd3lRUzXInC0Vubi4kJaWhtFoRKvVApCcnEzt2rVvu86MjAzatWtXrnP0ej1TpkxRtjds2EBYWBjPPPPMbfdDCCGEUIU8G1GUR7Vq1WjatCm7du1Cp9MBsHXrVry9vZUymzZtYvbs2ZhMJtzd3QkPD8fV1RW9Xo+/vz9btmzh0qVLTJ06lfz8fFJSUkhPT6du3bo0a9aM0NBQjh07hoODA6NGjbL4xsTN/P3338pqtxcuXCA8PJzff/8do9HIkCFD8PX15cCBA4SFhVFUVISTkxMffvgh//nPf0hNTSUqKoqioiIaNmzI5MmTqVWrFnq9Hi8vL7KysmjTpg1NmjTh5ZdfBmD48OH4+/vz0EMPMXnyZC5evEheXh6vvvoqzz//vAp3XgghhKg47C98rGAMBgPr1q0DIDMzEw8PDxwdHQE4deoUYWFhREdHk5iYSJs2bQgPD1fOdXFxIS4ujqCgIGJiYvD29kav1zNixAh8fHyYPHkyOp2OxMREoqKiGD9+PLm5uSX6kJKSQkBAAF26dOHJJ59k//79zJt3dQLiJ598QosWLYiPj+ebb75h/vz5HD16lEWLFvHSSy8RHx9Pv379+Omnn8jLy2PmzJl89tlnrFq1io4dOzJjxgylnU6dOrFu3TqCg4OVoc7z58/z448/8tRTT/Htt9/y+uuvs2LFCr766iumTZum2n0XQghxj5JhRFFeer1eyVytXbsWg8FAcnIycDX48vLyomHDhsDVhdGuXYHWx8cHgCZNmrB+/foSdaenpxMREQGAu7s7rVq1Yu/evXTp0qVEH6ZMmcL58+d59dVX+c9//kOjRo2Aq8+Aunz5MitWrADg4sWL/P777zz11FOEh4eTlpaGXq+nc+fOpKamcvz4cQYOHAhcXWX32udBtWrVCoDmzZtTUFDA4cOH+fHHH9Hr9VSuXJlx48aRlpZGTEwMv/32GxcvXrzzGyyEEEJUcBJsqax4KHHPnj2kp6czZswYJdgymSy/Bmw2mykq+t/XnJ2crn59X3OTiPz6VTvMZjNGo/GmfalevTpTp07Fz8+PDh068Nhjj2EymZg+fTotWrQAIDc3l/vuuw9HR0cee+wxNm3axJdffskPP/zA008/TZs2bZg/fz4AV65cUR7EeW1/Afz9/UlOTubHH3/k1VdfBeDNN9+kZs2adO7cmR49epRror8QQoh/CTucs2V/V1QBGQwGZs6ciaenJw4O/4tvizNR2dnZAMTGxtK+fftS69JqtUpApdPpiIuLA+Do0aNkZGTQunXrUs93d3fnxRdfJDIyErPZjE6nY+nSpcDVh2/6+/tz/Phx3nzzTX7++WeCgoIYOXIkv/zyC61ateKnn37ir7/+AmDevHk3HQr08/MjOTmZw4cP8/jjjwNX56uNGDGCZ555htTUVIBSg0MhhBD/QjKMKG5H586dCQ0NZeTIkRb7XV1dCQ8PJyQkhMLCQho0aEBkZGSpdXl7ezNr1ixq1KhBaGgoYWFhyjISERERFs9vupmhQ4cSFxdHYmIiISEhTJo0CV9fX4xGI2PHjuXBBx9k2LBhhIaGEh0djaOjI5MmTaJu3bp88MEHvPnmm5hMJurVq8f06dNv2Mb9999PrVq1eOyxx5TM3PDhwxkwYABOTk40bdqUBx54gOzsbB566KGy3EYhhBDiniQryIt7gqwgXz6ygnz5yQry5ScryJePrCBfNlX7fq5KvRdXvKxKvWUhw4hCCCGEECqSYUQhhBBCVBg3+1LYvUwyW0IIIYQQKpLMlhBCCCEqDvtLbEmwJYQQQoiKQ4YRhRBCCCFEuUhmS9wTbLUsQ8fe41Vv48dk9Z8JWWQ046hV/69DW33FvJIN/tA12WgRnMU/HVO9jYjuHqq3AXD6fKHqbSzbd1z1NgDG+DRWvQ2TyTYZm2pOWpu0oxbJbAlhx2wRaNmKLQItW7FFoCWEEGqSzJYQQgghKgx7zGxJsCWEEEKICsMegy0ZRhRCCCGEUJFktoQQQghRcdhfYksyW0IIIYQQapLMlhBCCCEqDHucsyXBlhBCCCEqDHsMtmQYUQghhBBCRRJs3cOys7Px8PAgLCzMYn9WVhYeHh7Ex8ffVr3Lly8nKSmpzOXj4+Np164dAQEBBAQE0K1bNyZOnEhRURE5OTkMGTLktvohhBDi30ej0ajyupsk2LrHubi4kJaWhtFoVPYlJydTu3bt264zIyODgoKCcp2j1+tJSEggISGB5ORkDhw4QFxcHPXq1WPhwoW33RchhBDiXidztu5x1apVo2nTpuzatQudTgfA1q1b8fb2Vsps2rSJ2bNnYzKZcHd3Jzw8HFdXV/R6Pf7+/mzZsoVLly4xdepU8vPzSUlJIT09nbp169KsWTNCQ0M5duwYDg4OjBo1ik6dOpXaJ61WyxNPPMHvv/9OdnY2AwcOJCUlhdzcXMLCwvjnn3/QaDSMGTPGop9CCCHE3c5CqUEyW3bAYDCwbt06ADIzM/Hw8MDR0RGAU6dOERYWRnR0NImJibRp04bw8HDlXBcXF+Li4ggKCiImJgZvb2/0ej0jRozAx8eHyZMno9PpSExMJCoqivHjx5Obm1tqf06fPs2WLVto3bq1xf7IyEj69u1LfHw8n3zyCWFhYZw/f966N0MIIcS9TaPS6y6SYMsO6PV6UlNTMZlMrF27FoPBoBzLzMzEy8uLhg0bAtC/f3/S09OV4z4+PgA0adKEM2fOlKg7PT2dwMBAANzd3WnVqhV79+4tUS4lJYWAgAD8/f0ZOHAgXbt2xdfX16LMtm3biIqKIiAggCFDhlBUVMTRo0fv+PqFEEKIikyGEe1A8VDinj17SE9PZ8yYMSQnJwNgMpksyprNZoqKipRtJycn4OZpW7PZXGL72vlhxfR6PVOmTCm1nyaTiUWLFuHi4gLAiRMnqFOnTukXJ4QQ4l9FhhFFhWUwGJg5cyaenp44OPwvhi7ORGVnZwMQGxtL+/btS61Lq9UqAZVOpyMuLg6Ao0ePkpGRUWJ4sKx0Oh1LliwB4I8//sDPz49Lly7dVl1CCCHEvUIyW3aic+fOhIaGMnLkSIv9rq6uhIeHExISQmFhIQ0aNCAyMrLUury9vZk1axY1atQgNDSUsLAwZRmJiIgI3NzcbquPEyZMICwsDD8/PwCmTZtG9erVb6suIYQQ9skeM1sa8/XjREJUQHsO5aveRsfe41VvA+DH5Gmqt+Gotc0/Vrb4R7GSjf7dLTLZ5p/CqT/8qXobEd09VG8D4Njpy6q3sXTfMdXbABjj01j1NgqKTLcuZAXVnW2TR6lVVatKvXVfilWl3pNf9Fel3rKQzJYQQgghKgx7zGxJsCWEEEKIisP+Yi2ZIC+EEEIIoSbJbAkhhBCiwrDHYUTJbAkhhBBCqEgyW0IIIYSoMOwxsyXBlhBCCCEqDHsMtmSdLXFPSP/jjOptuFSrrHobAI/1eFv1Nn5eN131NsTtuXCl6NaF7lB1J9v8HW2LXx7292vXfjzsVkWVeu9/dYUq9R5f0FeVestCMltCCCGEqDDsMbMlE+SFEEIIIVQkmS0hhBBCVBz2l9iSYEsIIYQQFYcMIwohhBBCiHKRzJYQQgghKgzJbAkhhBBCiHKRzJYQQgghKgzJbAkhhBBCiHK5K8FWdnY2Hh4ehIWFWezPysrCw8OD+Pj426p3+fLlJCUlleucgwcPMmzYMPz8/PDz82PMmDHk5eWVq45NmzbxxRdflOucawUHB7Njx44ylc3JyWHIkCG33Y5a7qRfQgghhEKj0usuumuZLRcXF9LS0jAajcq+5ORkateufdt1ZmRkUFBQUObyOTk5DBw4kH79+pGYmMjq1atp0qQJISEh5Wp33759nD9/vrzdvS316tVj4cKFt3Xuzp07rdyb/7mTfgkhhBDFNBqNKq+76a7N2apWrRpNmzZl165d6HQ6ALZu3Yq3t7dSZtOmTcyePRuTyYS7uzvh4eG4urqi1+vx9/dny5YtXLp0ialTp5Kfn09KSgrp6enUrVuXZs2aERoayrFjx3BwcGDUqFF06tTJog9Lly5Fp9Oh1+uBq2/wkCFDaNiwIUVFRZw6dYrx48dz7tw5Tpw4Qe/evRk5ciTx8fGsXLmSM2fO8NBDD/Hjjz8C0KBBAzp27HjDc65VUFBAaGgo+/bt44EHHuD06dPKsQULFrB27VqMRiMdO3Zk7NixFh+S7OxsBg4cSEpKCuPGjaN69ers37+fnJwc3njjDfr27cv27duZPv3qs/Huu+8+Zs6cybx58wB47rnn+Pbbb9HpdHh6enLy5Enefvtt5s+fz+LFiwEYN24c7dq1o0+fPnz55ZcsXboUrVZL586dGTt2LH///TfvvvsueXl5ODs7ExERQfXq1ZV+5ebm3vDez507l5ycHA4fPszff//Nc889x2uvvWaVz5MQQghRUd3VOVsGg4F169YBkJmZiYeHB46OjgCcOnWKsLAwoqOjSUxMpE2bNoSHhyvnuri4EBcXR1BQEDExMXh7e6PX6xkxYgQ+Pj5MnjwZnU5HYmIiUVFRjB8/ntzcXIv2s7KyaNGihcU+rVaLr68vDg4OJCUl4evry/Lly0lMTGTRokXKEGNOTg4rV67k448/JigoiKCgIPr27VvqOcWKg5q1a9cyYcIEjhw5AkBqair79u0jLi6OVatWkZOTw+rVq0u9h//88w9Llizhk08+Ydq0aQDMmzePSZMmER8fj7e3N7/88gsTJkwA4NtvvwXg9OnTDBkyhISEBBwcbhxzZ2ZmsmTJEuLi4li9ejX79+9n3759vP/++3Tr1o2kpCSGDx/OJ598YnFeaff+119/5bPPPuPbb79lwYIF5Ofnl3p9Qggh/l3sMbN1V4MtvV5PamoqJpOJtWvXYjAYlGOZmZl4eXnRsGFDAPr37096erpy3MfHB4AmTZpw5syZEnWnp6cTGBgIgLu7O61atWLv3r0WZTQaDZUrV75p/1555RXuv/9+PvvsMyIjIyksLOTSpUsANG/e/IZBSmnnFNu5c6dyrf/5z3947LHHANi+fTuZmZn06dOH3r17s2/fPv7444+b9g/gySefRKPR8Oijjyr3oUuXLoSEhBAeHk7z5s3p2LHjDc9t1apVqXXv2rWLzp07U6NGDRwcHPjyyy/x9PRk165dBAQEAPDUU08xZ84ci/NKu/ft27encuXK1KlTBxcXF86dO1dqH4QQQoh73V0NtoqHEvfs2UN6errFEKLJZLIoazabKSoqUradnJyAm39F1Gw2l9i+dn4YgKenJ/v27bPYZzKZCAkJITc3lylTprB48WIaNGjAa6+9Rq1atZR6nZ2db9huaecU02g0FvuKgzaj0cigQYNISEggISGBb7/9lmHDht2wndLuw+DBg1m8eDEPPvgg06dPL5F5KlZ8Ddf3p7CwUOnXtfXm5OSQn59vEWSazeYSAWFp9764vzdqVwghhJDMlgoMBgMzZ87E09PT4pd4cTYkOzsbgNjYWNq3b19qXVqtVvmlrtPpiIuLA+Do0aNkZGTQunVri/L9+/dn8+bNbN68GbgaFMybN49Tp07h6urK1q1beeWVVzAYDPz111/k5OSUCAKL2y0OBMtyTocOHUhMTMRkMvH333+TkZGh9DkhIYELFy5QVFTEG2+8oQyzlsdzzz3HhQsXGDx4MIMHD+aXX34p0c9r1apVi6NHj3LlyhXOnDnDnj17AHjiiSfYvHmz0p8xY8awb98+nnjiCdasWQPAtm3bmDhxokV9Zbn3QgghxI3YY7B11xc17dy5M6GhoSUmkbu6uhIeHk5ISAiFhYU0aNCAyMjIUuvy9vZm1qxZ1KhRg9DQUMLCwpRlJCIiInBzc7MoX7duXRYuXMi0adOYMWMGRqOR5s2bEx0dDcDQoUN5++23cXZ2pn79+nh6eirB37Xatm3LO++8g6ur603PefDBB5XyAwYM4Pfff8dgMPDAAw/w6KOPAleHVQ8cOEC/fv0wGo34+PjQu3fvct/T0aNHM27cOBwcHKhatSoRERHA1eHFgICAEktrNGnShKeeeoqePXvywAMP8PjjjwPQokULXnzxRYKCgjCZTHTt2hVvb28aNWrEhAkTWLJkCVWqVFHqL1aWey+EEEL8W2jMMo4j7gHpf5xRvQ2Xajefv2dNj/V4W/U2fl43XfU2xO25cKVkdtnaqjvZ5u9oW/zysL+1xO3Hw25VVKm30ag1qtT710c9Vam3LO76MKIQQgghhD2768OIQgghhBDF7vb8KjVIsCWEEEKICsMegy0ZRhRCCCGEUJFktoQQQghRYdhhYksyW0IIIYQQapLMlrgnuNZ0unUhK7DFH1S2WpahZbexqrdhq2uxxftiqzVwHLU2+BvXVpkBG9y08zZYKgOgurP9/DrU3OMLZsicLSHsmD39eNsi0LIVe3pfhBD/TvYTygshhBDinmeHiS0JtoQQQghRccgwohBCCCGEKBfJbAkhhBCiwrDDxJZktoQQQgghrpWYmEiPHj149tln+eabb0ocP3jwIMHBwfj7+/PKK69w9uzZUuuTYEsIIYQQFUalShpVXmWVk5PDRx99xJIlS1i1ahWxsbH88ccfynGz2cxrr73GkCFDWL16Nc2aNWPBggWlX9Nt3w0hhBBCCCvTaNR5ldW2bdvQ6XS4uLhQtWpVunXrxnfffacc379/P1WrVqVTp04ADBs2jBdeeKHUOmXOlhBCCCHsXn5+Pvn5+SX216xZk5o1ayrbJ06coG7dusq2m5sbmZmZyvaRI0dwdXVl/PjxZGVl0bhxYyZOnFhq2/dkZis7OxsPDw/CwsIs9mdlZeHh4UF8fPxt1bt8+XKSkpLKXH7u3Lk8+eSTBAQE4O/vj5+fH+np6bfVdmn279/P008/zQsvvMCcOXPYuHHjTcve6vj19Ho93bp1s9hXVFSETqdj3LhxAISGhvLzzz/fXueFEEKIctBoNKq8Fi1aRJcuXUq8Fi1aZNG+yWSyWH7CbDZbbBcVFbFz506ef/55Vq5cibu7O1OmTCn1mu7ZzJaLiwtpaWkYjUa0Wi0AycnJ1K5d+7brzMjIoF27duU6JygoiOHDhwNXg71XXnmFbdu23XYfbmTTpk34+/szevToW5YdOXJkueu/fPkyv/76Kx4eHgBs377d4oMVGRlZ7jqFEEKIimTQoEH07t27xP5rs1oA9evXZ/fu3cr2yZMncXNzU7br1q3LQw89RMuWLQHw9fVlxIgRpbZ9zwZb1apVo2nTpuzatQudTgfA1q1b8fb2Vsps2rSJ2bNnYzKZcHd3Jzw8HFdXV/R6Pf7+/mzZsoVLly4xdepU8vPzSUlJIT09nbp169KsWTNCQ0M5duwYDg4OjBo1ShmfvZlz585Rp04dZXvVqlUsWrQIk8lEixYteO+993BycqJjx45069aNPXv2oNVqmT17Nu7u7mRmZvLhhx9y+fJlatWqxfvvv8/BgwdZunQpAJUrVyY7O5t27drRp08fvvzyS5YuXYpWq6Vz586MHTuWcePGKcdXrFjBF198gUajoUWLFkycOJFq1aqV6Pezzz7LunXrlGArOTmZbt26cfnyZQCCg4MJCQkBICYmBmdnZ/788088PDyYMWMGlStXvum16nQ6PD09OXnyJHFxcXz22WesXr0arVbLk08+ydixY5VgWQghhFBr6Yfrhwtvxtvbm7lz55KXl0eVKlVYv349kydPVo4/9thj5OXlceDAAZo2bUpKSgotWrQotc57chixmMFgYN26dQBkZmbi4eGBo6MjAKdOnSIsLIzo6GgSExNp06YN4eHhyrkuLi7ExcURFBRETEwM3t7e6PV6RowYgY+PD5MnT0an05GYmEhUVBTjx48nNze3RB+WLVtGQEAABoOBwYMHM2jQIAB+//13li9fzrJly0hISKBOnTp89tlnwNUouUOHDqxatYq2bdvyzTffUFBQwIQJE5g5cyYrV67kpZdeYuLEiTz11FMEBQURFBSkBDzF17tkyRLi4uJYvXo1+/fvZ9++fcrxX3/9lfnz57N48WISExOpUqUKH3/88Q3vY/fu3fn+++8BKCgo4MCBA3h5ed2w7I8//khYWBhr167l2LFjbNmypdRrPX36NEOGDCEhIYFt27aRkpLCihUrWLlyJYcPH2bZsmVle7OFEEL8K6g1jFhW9erVY9SoUQwcOJBevXrh6+uLl5cXQ4YM4eeff8bZ2Zno6GgmTJhAz5492bFjhzLt5mbu2cwWXJ1vVJy5Wrt2LQaDgeTkZOBqMOLl5UXDhg0B6N+/v8VXM318fABo0qQJ69evL1F3eno6ERERALi7u9OqVSv27t1Lly5dLMpdO4x48OBBXnjhBRo1akRWVhaHDx+mX79+ABQWFtK8efMbtr97924OHTrE0aNHee2115Qy58+fv+m179q1i86dO1OjRg0Avvzyyxser1WrlnL977777g3rqlevHtWrV+fPP//kyJEjPPnkkzdtt0mTJtSvXx+Ahx9+mLNnz3Ls2LFSr7VVq1bA1Xvas2dPqlSpAkDfvn1ZtWrVLb/FIYQQQtiSn58ffn5+FvsWLlyo/H+rVq2Ii4src333dLBVPJS4Z88e0tPTGTNmjBJsmUwmi7Jms5mioiJl28nJCbj5M5jMZnOJbaPRWGp/GjduTJs2bfjpp59wcHDAYDAwYcIEAC5cuGBx/rXtm81mTCYTDRs2JCEhAQCj0XjDTFoxBwcHi77n5OQoQQzc+vqv1717d7777jsOHz7M4MGDOXDgwA3LFff72r4bjcZSr9XZ2fmGfQJK7ZMQQoh/H3k2YgVkMBiYOXMmnp6eODj8L3YszkRlZ2cDEBsbS/v27UutS6vVKkGCTqdTotajR4+SkZFB69atSz0/Pz+fX375hebNm9O+fXu+//57Tp06hdlsZtKkSSW+8XCtxo0bc/bsWWVS3ooVK3jrrbduWv6JJ55g8+bNXLhwgaKiIsaMGWMxjNiuXTtSUlI4c+YMcPWblqVdf3Gw9eeff1pkpcqirNeq0+lYs2YNly9fpqioiBUrVijz7YQQQgh7dU9ntgA6d+5MaGhoiW/hubq6Eh4eTkhICIWFhTRo0OCW36rz9vZm1qxZ1KhRg9DQUMLCwpRlJCIiIiy+jVBs2bJlbNiwgUqVKnHlyhWee+45OnToAEBISAiDBg3CZDLRrFkzXn311Zu2XblyZebMmUNkZCRXrlyhevXqTJ069ablW7RowYsvvkhQUBAmk4muXbvi7e3N6tWrAWjatClDhw4lODiYwsJCWrRowfvvv3/T+urVq0eNGjXK/W3M4rbKcq2dO3cmKyuLvn37UlRURMeOHXnxxRfL3Z4QQgj7ZYeJLTTm68fLhKiA/jhxSfU2bPXzbYsfuJbdxtqgFfh53XTV27Cn9wWgoKjkcLq1OTnaZtDCFr89LlyxzVSD6s73fO5BobHRT03jus6q1Nt6UtnXiiyPnyZ1uXUhldjPp0sIIYQQ9zx7nLMlwZYQQgghKgw7jLXu/QnyQgghhBAVmWS2hBBCCFFh2OMwomS2hBBCCCFUJJktIYQQQlQYdpjYkmBLCCGEEBWHPQ4jSrAl7gkmGyzo41DJVmsTqX8ttlj/CmyzntfetdNUbwOgUiXb/AOvtUE7tlpnyRark1V3ss2vKVvcM7ONVnOzw1jlnifBlhBCCCEqDHsMFmWCvBBCCCGEiiSzJYQQQogKQ+ZsCSGEEEKoyA5jLRlGFEIIIYRQk2S2hBBCCFFh2OMwomS2hBBCCCFUJJktIYQQQlQYdpjYksyWEEIIIYSa7qlgKzs7Gw8PD8LCwiz2Z2Vl4eHhQXx8/G3Vu3z5cpKSksp1zsGDBxk2bBh+fn74+fkxZswY8vLyAIiPj2fcuHEW5Xfs2EFwcHCJeubOncuTTz5JQECA8nr33XcB0Ov19OjRw+LY999/D0BRUREdO3Zk8uTJN+zflClT0Ol0FBQUlOu6rpWbm8tbb73FM888g5+fH6+88gq//PKLcjwqKordu3ffdv1CCCHE9TQajSqvu+meG0Z0cXEhLS0No9GIVqsFIDk5mdq1a992nRkZGbRr167M5XNychg4cCDh4eHo9XrMZjMxMTGEhISwZMmScrcfFBTE8OHDb3hswYIFNGzYsMT+zZs307JlS9auXctbb71FlSpVlGNFRUWsXbuWxx57jHXr1uHn51fuPl2+fJmBAwfSt29fpk+fjkajYevWrbz88sssWbKExo0bs2vXLtq3b1/uuoUQQoibkWHECqBatWo0a9aMXbt2Kfu2bt2Kt7e3sr1p0yYCAgLw8/Pj9ddfJzc3F7iaKZo9ezaBgYH07NmTffv2sW3bNlJSUoiKiiItLY3c3FyGDh2Kn58fvXv3JjU1tUQfli5dik6nQ6/XA1ej8CFDhjBgwACKiopUvgNXxcfH07VrV7y8vFizZo3FsR9++IEHH3yQXr16sWzZshLnbt++naCgIIu63nvvPYsyycnJ1KlTh1deeUX5i+DJJ5+kT58+fPrpp6xatYp9+/YxYcIEfv31V4KDgwkJCaFbt25kZWXd9D2YOnUq/v7+9OrVi48//tjat0UIIYSocO65YAvAYDCwbt06ADIzM/Hw8MDR0RGAU6dOERYWRnR0NImJibRp04bw8HDlXBcXF+Li4ggKCiImJgZvb2/0ej0jRozAx8eHyZMno9PpSExMJCoqivHjxyuBQrGsrCxatGhhsU+r1eLr64uDw9VkYUpKisXw34QJE256PcuWLbMoe/DgQeXYq6++qux/8803AcjLy2Pbtm106dIFg8FAbGysRX3x8fF0796dp556iqysLP744w+L4zqdjpMnT3LkyBEAVq1aRZ8+fSzK/Pzzz7Rs2bJEX9u2bcvPP/9Mr1698PT0JCIiAg8PDwA8PDxYt24dbm5uN3wP/v77b1JTU1m9ejVLly7ljz/+4MqVKze9L0IIIf597HEY8Z4MtvR6PampqZhMJtauXYvBYFCOZWZm4uXlpQy99e/fn/T0dOW4j48PAE2aNOHMmTMl6k5PTycwMBAAd3d3WrVqxd69ey3KaDQaKleufMs+JiQkKK+IiIiblg0KCrIo27hxY+XYggULlP2zZ88GYPXq1eh0Ou677z66dOnCr7/+qsylOnXqFFu3bsVgMODs7Eznzp1LZLc0Gg29e/dm9erVHDt2jFOnTtGqVasSZYxGY4m+FhYW3vRD6+XlBdz8PahXrx5OTk4EBQXx1Vdf8dZbb+Hk5FTqfRRCCCHudfdksFWtWjWaNm3Knj17SE9PtxhCNJlMFmXNZrPF0F7xL/ebBQxms7nE9vVBh6enJ/v27bPYZzKZCAkJKZEFU0N8fDw//vgjer0ef39/KlWqpARUq1evxmw2ExgYiF6vZ/v27SQkJHD58mWLOnr37s2aNWtISkoiICCgRBteXl789NNPJfb/+OOPeHp63rBfzs7OwM3fAwcHB7799ltGjhzJmTNnCAoK4q+//rqdWyCEEMJOSWarAjEYDMycORNPT09l6A5QMlHZ2dkAxMbG3nISt1arVQIqnU5HXFwcAEePHiUjI4PWrVtblO/fvz+bN29m8+bNwNVgYt68eZw6dQpXV1drXeIN7du3j3/++YcffviBlJQUUlJSiImJITExkfPnzxMfH8+UKVOUY1u2bOG+++4jOTnZop4HHniA+vXrK0OY1+vRoweXLl0iJiZGCUC3bNlCfHw8r7zyCmB53651s/fgl19+4cUXX6Rt27a88847PPzwwxJsCSGEsKDRqPO6m+65byMW69y5M6GhoYwcOdJiv6urK+Hh4YSEhFBYWEiDBg2IjIwstS5vb29mzZpFjRo1CA0NJSwsTFlGIiIiAjc3N4vydevWZeHChUybNo0ZM2ZgNBpp3rw50dHR1r3IG4iPj6dPnz5KFgmgffv2NGrUiK+++orTp0/TtWtX5VilSpUYNGgQy5YtKzEvq0ePHqxfv5569eqVaKdy5cosWrSIadOm0b17dzQaDQ0aNOCLL77g4YcfBq4Oyb733ntMnTrV4tybvQdubm60bt0aX19fqlSpQps2bejUqZM1b48QQghR4WjM14+biX+FoqIi3n77bbp3786zzz57t7tzS7/lXFS9DYdKtkn0muzoR65lt7Gqt7F37TTV2wCoVMk2f/oaTeq//45a23yW7enXhy2GmczY5n5VslEap5Gr860L3YanZ29Tpd4f3vS+dSGV3LPDiOL2mc1mfHx80Gg0PPPMM3e7O0IIIYRdu2eHEcXt02g0bN++/W53QwghhCjhbs+vUoMEW0IIIYSoMO72NwfVIMOIQgghhBAqksyWEEIIISoMO0xsSWZLCCGEEEJNktkS9wRbLctgi2//2+Cb/wDY4o9DWy3L0Mrwtupt/LxuuuptAJhs8AEwV7KjD5mNnLlQoHobVZ1s8yvXyfHefmNstXSFLUlmS4j/Z6NllmzCji7FJoGWEEKoSTJbQgghhKgw7DCxJcGWEEIIISoOWfpBCCGEEEKUi2S2hBBCCFFh2NP82WKS2RJCCCGEUJFktoQQQghRYdjjnC0JtoQQQghRYdhhrCXDiEIIIYQQaio12MrOzsbDw4OwsDCL/VlZWXh4eBAfH39bjS5fvpykpKRynXPw4EGGDRuGn58ffn5+jBkzhry8vFLPmTVrFuPHj7/pcQ8PDwCWLl3K0qVLAXj33Xf5+++/y9W3H374gaCgIPz9/fH19WX27NmYTKZy1VEeO3bsIDg4WLX6S1PednNychgyZIhKvRFCCGFvNCr9dzfdMrPl4uJCWloaRqNR2ZecnEzt2rVvu9GMjAwKCsr+aIScnBwGDhxIv379SExMZPXq1TRp0oSQkJBSz/v999955513bln/888/z/PPPw9cDWTM5rI/6iI1NZXw8HA+/PBDVq9eTVxcHAcOHCAqKqrMddxLdu7cWa7y9erVY+HChSr1RgghhKj4bjlnq1q1ajRt2pRdu3ah0+kA2Lp1K97e3kqZTZs2Kdkcd3d3wsPDcXV1Ra/X4+/vz5YtW7h06RJTp04lPz+flJQU0tPTqVu3Ls2aNSM0NJRjx47h4ODAqFGj6NSpk0Ufli5dik6nQ6/XA1cnzw0ZMoSGDRtSVFTEJ598wk8//cTx48d58cUXefLJJ5k0aRJnzpxh2LBhTJw4kebNm5Odnc3YsWO5ePEirVq1UuqfO3cuAE5OTpw4cYJXX32Vb775hsOHDxMZGcmVK1eoVasW4eHhPPTQQxZ9mz9/Pq+99hqNGjUCwNnZmUmTJnHw4EEA/vrrL8LCwjhz5gxVq1YlNDQULy8vxo0bR5UqVfjll1/Iz89n9OjRJCQkcODAAZ555hnGjRtHfHw8P/zwA6dOneLkyZN07tyZcePGWbR/+PBh5VqdnZ2Vay1L/UajkWnTprFz506MRiN9+vRh8ODB7Nixg5iYGJydnfnzzz/x8PBgxowZTJt29Tl4zz33HN9++y1ff/01CQkJXLp0CUdHR2bOnEnjxo3R6/V4eXmRlZXF9OnTefPNN0lJSSE3N/eW77UQQoh/t3/t0g8Gg4F169YBkJmZiYeHB46OjgCcOnWKsLAwoqOjSUxMpE2bNoSHhyvnuri4EBcXR1BQEDExMXh7e6PX6xkxYgQ+Pj5MnjwZnU5HYmIiUVFRjB8/ntzcXIv2s7KyaNGihcU+rVaLr68vDg5X48WCggKSk5MZMGAA77zzDmPHjmXlypVMnjyZUaNGATB58mT69OlDQkICbdq0KXGdr776Km5ubixYsIBq1aoxevRoJk6cyOrVqwkKCmL06NElzsnKyqJ58+YW++rXr68Eo2PHjiU4OJjExETeffddRo4cqWT1Tpw4QWxsLK+++irvvvsu77//PqtWrWL58uWcO3cOgD179jBnzhySkpLYu3cv33//vUVbN7vWstS/fPlyAFauXElcXBwbN25k9+7dAPz444+EhYWxdu1ajh07xpYtW5gwYQIA3377LefPn2fDhg0sXryYpKQknn76ab755hul7U6dOrFu3TqLDGhZ3mshhBD/bhqNRpXX3VSmYEuv15OamorJZGLt2rUYDAblWGZmJl5eXjRs2BCA/v37k56erhz38fEBoEmTJpw5c6ZE3enp6QQGBgLg7u5Oq1at2Lt3r0UZjUZD5cqVS+2jl5cXABcuXGDfvn28++67BAQEMGbMGC5evMjp06fZuXOn0nd/f38lYLyRQ4cOUbNmTaVeg8HAkSNHlCDo2r45OTndsI4LFy5w5MgRnn32WQBat27Nfffdp2S9irM6DRo0oEmTJtSpU4fq1avj4uLC2bNnAejSpQuurq5UrlyZHj16WNzb0q61LPVv376dlJQUAgICeO655/jnn3/49ddfgavvV/369alUqRIPP/yw0p9i1atXZ+bMmaxZs4aZM2eyadMmLl68qBy/NnNYrCzvtRBCCGFvyrT0Q/FQ4p49e0hPT2fMmDEkJycDlJgIbjabKSoqUraLA5GbRZXXz48ym80W88MAPD092bdvn8U+k8nEiBEjmDRpEnB1+K54f+XKlUlISFDK/vPPP7i4uFi0p9FoqFTp5rHmjSa4l9a3Rx55RNn3119/8cknn5T4YsH1dVwb7BVn6K6n1Wot+nT9dmnXeqv6jUYjY8eOVYLBvLw8qlWrxk8//WQRQGo0mhLv0/HjxwkODubFF1+kU6dOuLq6kpWVpRy/UQBalvdaCCHEv9u/eukHg8HAzJkz8fT0tPjFXZydyM7OBiA2Npb27duXWpdWq1V+yep0OuLi4gA4evQoGRkZtG7d2qJ8//792bx5M5s3bwau/pKeN28ep06dwtXV1aJsjRo1+M9//qMEIFu3buWFF14AwNvbm9WrVwOwfv16rly5ctO+NW7cmDNnzpCZmQlc/VJAgwYNlECm2H//+18+/vhjDh06BFzNNk2ZMoX777+f6tWr07BhQ9avXw/ATz/9RG5uLk2aNCn1/lwrLS2Nc+fOceXKFdasWWMxx6m0ay0LnU7H8uXLKSws5MKFCwwYMICffvqp1HO0Wi1FRUX8/PPPPPTQQwwePJiWLVuyYcOGWwZOZXmvhRBCCHtT5kVNO3fuTGhoKCNHjrTY7+rqSnh4OCEhIRQWFtKgQQMiIyNLrcvb25tZs2ZRo0YNQkNDCQsLU5aRiIiIwM3NzaJ83bp1WbhwIdOmTWPGjBkYjUaaN29OdHT0DeufPn06kyZN4tNPP8XR0ZGPPvoIjUZDWFgYY8eOJTY2Fk9PT6pVq1bi3KeffppXX32VTz/9lI8++ojJkydz6dIl7rvvPj766KMS5Tt16sSoUaMYNWoURqORoqIiunfvrnxTsrgvc+fOxdHRkblz595ySPRatWvXZsiQIZw+fRp/f398fHzYsWPHLa+1LIKCgjh8+DC9e/emqKiIPn360L59e4v6r9elSxcCAgJYvnw5S5cupUePHpjNZtq2bcvvv/9eantlea+FEEL8u1Wyw9SWxlyedQ6ETcXHx7Nz506mTJlyt7ty1x08eVn1Nmz1DZgik/o/crb6p8pog2tpZXhb9TYAfl433SbtFBaptwZfscoONlqv2o5+J569UKh6G1WdbPPQFidH27z/jVydVam3z2d7VKk3/pXHVam3LORxPUIIIYSoMOwwsSXBVkXWp08f+vTpc7e7IYQQQtjM3V6mQQ3ybEQhhBBCCBVJZksIIYQQFYYdJrYksyWEEEIIoSbJbAkhhBCiwrDHpR8k2BJCCCFEhWF/oZYEW+IeYbLBcnA2WDLKZmx1KZVssDiZrda/atltrE3ascX12OyjbIOGbJXluK/qzZ+Ve6+xxfp3onwk2BJCCCFEhSFLPwghhBBCiHKRzJYQQgghKgxbPTrNliTYEkIIIUSFIcOIQgghhBCiXCSzJYQQQogKww4TW5LZEkIIIYRQk2S2hBBCCFFhyJwtIYQQQghRLhJs3aHs7Gw8PDwICwuz2J+VlYWHhwfx8fG3Ve/y5ctJSkoqc/m5c+cyd+7cEvvj4+Np164dAQEBFq+9e/cqZX777Tc8PDxYt26dxbnHjh1j2LBh+Pn54evry8iRIzl16hQAO3bsIDg4WGmjffv25ObmKudmZ2ej1+uV7by8PEJDQ+natSsGg4HevXuzcePGMl+fEEKIf4dKGnVed5MMI1qBi4sLaWlpGI1GtFotAMnJydSuXfu268zIyKBdu3ZW6Z9er2fKlCk3Pb5ixQq6d+9ObGws3bp1U/aHhYXRq1cvfH19AYiJieG9997j448/LlHHhQsXeO+994iOji5xrKCggEGDBtGtWze+++47tFotBw8e5JVXXuGBBx6gadOmVrhKIYQQ9kCGEcUNVatWjWbNmrFr1y5l39atW/H29la2N23aREBAAH5+frz++utKFkiv1zN79mwCAwPp2bMn+/btY9u2baSkpBAVFUVaWhq5ubkMHToUPz8/evfuTWpqqtX6XlhYSGJiIm+++Sb79+/nyJEjyrHc3FwuXbqkbL/wwgu88MILN6ynW7duHD58mMTExBLH1q1bh5OTEyEhIUow2rhxYyZNmoTRaLTatQghhBDWkJiYSI8ePXj22Wf55ptvblruhx9+sBjFuRkJtqzEYDAow3CZmZl4eHjg6Hj1waanTp0iLCyM6OhoEhMTadOmDeHh4cq5Li4uxMXFERQURExMDN7e3uj1ekaMGIGPjw+TJ09Gp9ORmJhIVFQU48ePtxiyu5WUlBSLIcTnnntOObZ582YaNGhAo0aNeOaZZ4iNjVWOjR49mhkzZtCpUyfeeecdNm/efNNsm6OjIx9++CFTpkwp0be9e/fStm3bEuc89dRTtGjRoszXIYQQwv5pVHqVVU5ODh999BFLlixh1apVxMbG8scff5Qol5uby9SpU8tUpwRbVqLX60lNTcVkMrF27VoMBoNyLDMzEy8vLxo2bAhA//79SU9PV477+PgA0KRJE86cOVOi7vT0dAIDAwFwd3enVatWFnOuytK3hIQE5fXtt98qx1asWKEME/bo0YP4+HgKCgoA6NSpE6mpqURERFC7dm2mT5/O8OHDb9pOy5Yt6du3L++9916p/ZkxYwYBAQF069aNiIiIMl+HEEIIcbvy8/PJzs4u8crPz7cot23bNnQ6HS4uLlStWlWZAnO9CRMmEBISUqa2JdiykmrVqtG0aVP27NlDenq6xRCiyWSyKGs2mykqKlK2nZycgJuPU5vN5hLb1hh+O3XqFGlpaXz++efo9XomTJhAfn4+33//PWfOnOGDDz7AyclJyWwlJiaydetW8vLyblpnSEgIhw8ftpjc7+npyY8//qhsv/XWWyQkJDB06FDOnz9/x9chhBDCflTSaFR5LVq0iC5dupR4LVq0yKL9EydOULduXWXbzc2NnJwcizJfffUVzZs3p1WrVmW7pju/LaKYwWBg5syZeHp64uDwv+8eFGeisrOzAYiNjaV9+/al1qXVapWASqfTERcXB8DRo0fJyMigdevWd9zfhIQEdDodqamppKSksGnTJoYNG8ayZcuoUaMGKSkprFq1Sin/xx9/UKdOHe67776b1lm5cmU+/PBD5s+fr+zr0aMHly5d4pNPPqGwsBCAc+fOsWPHDipVko+gEEKI/9Fo1HkNGjSIjRs3lngNGjTIon2TyWSR/DCbzRbbv/32G+vXr+f1118v8zXJtxGtqHPnzoSGhjJy5EiL/a6uroSHhxMSEkJhYSENGjQgMjKy1Lq8vb2ZNWsWNWrUIDQ0lLCwMGUZiYiICNzc3EqcExMTw+eff65sv//++8D/5mxd66WXXmLlypWMGjXKYv8LL7zAp59+yqFDh1iwYAFTpkxhzpw5ODs74+bmxvz585VJ7jfTsmVLBg0apEyWr1y5Ml999RWzZ8+mV69eABiNRrp168Z///vfUusSQgghrKFmzZrUrFnzluXq16/P7t27le2TJ09a/M797rvvOHnyJH379qWwsJATJ04wYMAAlixZctM6Nebrx6iEqID+OHHp1oWEuAMtu421STs/r5tuk3bsRSUbLQNgT78KbXUlj7hVUaXeV7/dr0q9C54r2xeycnJyeP7554mLi6NKlSoEBQUxefJkvLy8SpTNzs5m4MCBpKSklFqnjOEIIYQQQvy/evXqMWrUKAYOHKisNenl5cWQIUP4+eefb6tOGUYUQgghRIVREdY09fPzw8/Pz2LfwoULS5Rr2LDhLbNaIMGWEEIIISoQWw0d25IMIwohhBBCqEgyW0IIIYSoMOwwsSWZLSGEEEIINUlmS9wTjp2xzdIPi386pnobIR0eUr0NAEet+n9LaSvZ5k9Qk0n9L7PbakkGWywx8dPaaaq3AZB77orqbXyQ8rvqbQB8+3LJ57da24XLd/7kj7Iouu6pJfeamz1N5V4mwZYQ/88WgZat2CLQshVbBFpCiIrDfv71+h97vCYhhBBCiApDMltCCCGEqDDscRhRMltCCCGEECqSzJYQQgghKgwbfe/GpiSzJYQQQgihIslsCSGEEKLCsMfMlgRbQgghhKgwZIK8EEIIIYQol3si2MrOzsbDw4OwsDCL/VlZWXh4eBAfH39b9S5fvpykpKQylT1//jyPPfYYOTk5Fvt37txJ79692bhxI3PmzLnp+bc6Xh4eHh5WqedaS5cuZenSpeU+706ua9y4cbf93gkhhLBPlTTqvO6me2YY0cXFhbS0NIxGI1qtFoDk5GRq165923VmZGTQrl27MpWtXr06Xbt2Zc2aNbz88svK/lWrVhEYGEiXLl3o0qXLTc+/1fG77fnnn7+t8yr6dQkhhBB32z0TbFWrVo2mTZuya9cudDodAFu3bsXb21sps2nTJmbPno3JZMLd3Z3w8HBcXV3R6/X4+/uzZcsWLl26xNSpU8nPzyclJYX09HTq1q1Ls2bNCA0N5dixYzg4ODBq1Cg6depk0Yc+ffowbdo0Jdi6cuUKP/zwA++88w7x8fHs3LmTrl278u233zJ//nwAFi9ezOHDh2nevDk7d+5kypQpZGZm8uGHH3L58mVq1arF+++/j7u7O8HBwbRs2ZI9e/aQl5fHhAkTeOqpp8jOzmbs2LFcvHiRVq1aKf3Jyclh/PjxnDt3jhMnTtC7d29Gjhxp0ef4+Hh++OEHTp06xcmTJ+ncuTPjxo1j586dTJ8+HZPJRJMmTWjYsCEAw4cPp2PHjnTr1o09e/ag1WqZPXs27u7ubNu2jSlTpmA2m2nQoAEzZ85k/fr1ynXp9Xq6d+/Otm3bAPjggw+U6/7oo4+4fPky+fn5vPvuuzzzzDNW/oQIIYSwB3Y4ZeveGEYsZjAYWLduHQCZmZl4eHjg6OgIwKlTpwgLCyM6OprExETatGlDeHi4cq6LiwtxcXEEBQURExODt7c3er2eESNG4OPjw+TJk9HpdCQmJhIVFcX48ePJzc21aL99+/bk5+dz8OBBADZs2ECHDh247777lDKdOnVi3759nD17FoA1a9bg7++vHC8oKGDChAnMnDmTlStX8tJLLzFx4kTleGFhIbGxsbz77rvK8NzkyZPp06cPCQkJtGnTRimblJSEr68vy5cvJzExkUWLFpGXl1fivu3Zs4c5c+aQlJTE3r17+f777wE4dOgQixYtYurUqRblT548SYcOHVi1ahVt27blm2++oaCggLfeeoupU6eSmJjIo48+ysqVK0u0VbVqVVatWsWIESN45513APj666+JiIhg5cqVREREWG04VQghhP2ppNGo8rqr13RXWy8nvV5PamoqJpOJtWvXYjAYlGOZmZl4eXkpGZr+/fuTnp6uHPfx8QGgSZMmnDlzpkTd6enpBAYGAuDu7k6rVq3Yu3evRRmNRkOvXr2UeV4JCQn07dvXooyjoyNdu3Zl/fr1HDt2jDNnzuDl5aUcP3ToEEePHuW1114jICCAGTNmcPTo0VL7uXPnTuVa/f39lQDzlVde4f777+ezzz4jMjKSwsJCLl26VOLaunTpgqurK5UrV6ZHjx7KfWnUqBE1atS44b2+th9nz57l119/pV69ejRr1gyAMWPGEBwcXOK8fv36AVffq5ycHPLy8pg+fTq///470dHRfPHFF1y4cOGGbQohhBD26J4KtoqHEvfs2UN6errFEKLJZLIoazabKSoqUradnJyAm3+l1Gw2l9g2Go0lyvXp04fk5GRyc3M5dOgQHTp0KFEmICCA5ORkkpOT8fPzszhmMplo2LAhCQkJJCQkEB8fz5IlS27Zz+L+aTQaKlW6+rZNmTKFxYsX06BBA1577TVq1apV4joAZY5bcfvF287Ozje8F9f3w2w24+joaNGnc+fO8c8//5Q4z8HhfyPTxW0NGDCAzMxMPD09GTZs2E3bFEIIISqp9Lqb7nb75WYwGJg5cyaenp4Wv9iLM1HZ2dkAxMbG0r59+1Lr0mq1SkCl0+mIi4sD4OjRo2RkZNC6desS5zRo0ID777+fqKgo/P39bxi8tW7dmhMnTpCQkGAxhAjQuHFjzp49y+7duwFYsWIFb731Vqn99Pb2ZvXq1QCsX7+eK1euAFfnrL3yyisYDAb++usvcnJySgSdAGlpaZw7d44rV66wZs2aEnPRyqJRo0acOnWKP/74A4BPP/30ht9eXLNmDQDff/89Dz/8MGazmUOHDjFy5Eg6derExo0bbxjECiGEEPbqnpkgX6xz586EhoaWmAju6upKeHg4ISEhFBYW0qBBAyIjI0uty9vbm1mzZlGjRg1CQ0MJCwtTliKIiIjAzc3thuf17duXt99+W5n7dCMGg4EtW7bg7u5usb9y5crMmTOHyMhIrly5QvXq1UvMmbpeWFgYY8eOJTY2Fk9PT6pVqwbA0KFDefvtt3F2dqZ+/fp4enqSnZ3Ngw8+aHF+7dq1GTJkCKdPn8bf3x8fHx927NhRapvXc3JyYvr06bz99tsUFhby4IMPMm3aNGUOXbGMjAzi4uKoUqUKU6ZMwcXFhcDAQHr27ImDgwM6nY7Lly9z8eLFcrUvhBDi38EeJ8hrzDcadxJ2o/hbklOmTFG9Lb1ez1dffaXMm7Om1N9KTvy3tsU/HVO9DYCQDg+p3oaj1jZJa60NFq8xmWzzT5Sjg23uWctuY1Vv46e101RvAyD33BXV2/gg5XfV2wD49uW2qrdx4bJtsvpFNxjhUMP991VWpd6J36nznk/u3kSVesvinhtGFEIIIYS4l9xzw4iifPr06UOfPn1s0lZKSopN2hFCCGG/7HEYUTJbQgghhBAqksyWEEIIISqMu/0cQzVIZksIIYQQQkWS2RJCCCFEhXG3H62jBgm2hBBCCFFh2GGsJetsiXvDoVOXVW+jiqP21oWs4PzlolsXulM2+sdKY4OGbPVPlK3+ITTaYN2w1oa3VW8D4JfvZ6jehktVR9XbALhwRf01sGzx3oPtgpUHazupUu/kDX+oUu/EZx5Rpd6ykMyWEEIIISoMmSAvhBBCCCHKRTJbQgghhKgwbDE9wdYk2BJCCCFEhSHDiEIIIYQQolwksyWEEEKICkMyW0IIIYQQolwksyWEEEKICkNjh6uaSrAlhBBCiApDhhGFzWVnZ+Ph4UFYWJjF/qysLDw8PIiPj7+tepcvX05SUlK5zjl48CDDhg3Dz88PPz8/xowZQ15eHgDx8fG0a9eOgIAA/P396d69Ox9//DFGo7HE8YCAALp168bEiRMpKrLBaupCCCHEXSTB1j3AxcWFtLQ0JXABSE5Opnbt2rddZ0ZGBgUFBWUun5OTw8CBA+nXrx+JiYmsXr2aJk2aEBISopTR6/UkJCSwevVq4uPj2b17N3Pnzi1xPCEhgeTkZA4cOEBcXNxtX4MQQgj7o9Go87qbZBjxHlCtWjWaNm3Krl270Ol0AGzduhVvb2+lzKZNm5g9ezYmkwl3d3fCw8NxdXVFr9fj7+/Pli1buHTpElOnTiU/P5+UlBTS09OpW7cuzZo1IzQ0lGPHjuHg4MCoUaPo1KmTRR+WLl2KTqdDr9cDV8fUhwwZQsOGDW+YnapatSqjR49myJAhjBw5ssRxrVbLE088we+//27NWyWEEEJUOJLZukcYDAbWrVsHQGZmJh4eHjg6Xn1A66lTpwgLCyM6OprExETatGlDeHi4cq6LiwtxcXEEBQURExODt7c3er2eESNG4OPjw+TJk9HpdCQmJhIVFcX48ePJzc21aD8rK4sWLVpY7NNqtfj6+uLgcOOYvUmTJpw5c0YZarzW6dOn2bJlC61bt76T2yKEEMLOVNJoVHnd1Wu6q62LMtPr9aSmpmIymVi7di0Gg0E5lpmZiZeXFw0bNgSgf//+pKenK8d9fHyA/wU/10tPTycwMBAAd3d3WrVqxd69ey3KaDQaKleuXK4+F3+jxMnp6pPhU1JSlDldAwcOpGvXrvj6+parTiGEEOJeI8OI94jiocQ9e/aQnp7OmDFjSE5OBsBkMlmUNZvNFkN7xcHOzb5OazabS2xfOz8MwNPTk3379lnsM5lMjBgxgkmTJt2w3l9//ZX69etTvXp14GrAOGXKlFtcqRBCiH8z+TaiuKsMBgMzZ87E09PTYuiuOBOVnZ0NQGxsLO3bty+1Lq1WqwRUOp1Omah+9OhRMjIySgzv9e/fn82bN7N582bgakA2b948Tp06haura4n6z507x5w5c3jhhRdu+3qFEEL8+8gEeXFXde7cmdDQ0BITzl1dXQkPDyckJITCwkIaNGhAZGRkqXV5e3sza9YsatSoQWhoKGFhYcoyEhEREbi5uVmUr1u3LgsXLmTatGnMmDEDo9FI8+bNiY6OVsoUDxNqNBqMRiPPPvssQ4YMsdLVCyGEEPcmjfn6MSQhKqBDpy6r3kYVR63qbQCcv2yDtcVs9FecxgYN2eqfKFv9Q2g0qd9Sa8PbqrcB8Mv3M1Rvw6Wqo+ptAFy4Yrx1oTtki/cebJfFebC2kyr1Rm89pEq9bzz5H1XqLQsZRhRCCCGEUJEMIwohhBCiwrjb86vUIMGWEEIIISoM+TaiEEIIIYQoF8lsCSGEEKLCuNurvatBMltCCCGEECqSzJYQQgghKgw7TGxJsCXuDQWFplsXukOnzxeq3gZAjSo2+LGz2ep5NmjIVv/w2uie5Z67onobtlj/CqB517dUb2P1kkmqtwHwSN3qqrdhstGacdp7PFqRYUQhhBBCCFEuktkSQgghRIVhh4ktyWwJIYQQQqhJMltCCCGEqDDsMQtkj9ckhBBCCFFhSGZLCCGEEBWGxg4nbUmwJYQQQogKw/5CLRlGFEIIIYRQ1b822MrOzsbDw4OwsDCL/VlZWXh4eBAfH39b9S5fvpykpKRynXPw4EGGDRuGn58ffn5+jBkzhry8PADi4+Np164dAQEB+Pv70717dz7++GOMRmOJ4wEBAXTr1o2JEydSVFRUop0TJ07w1ltv0bNnT/z9/Rk6dChHjx5V6hk3btxtXbMQQghhLZU0GlVed/Wa7mrrd5mLiwtpaWlK4AKQnJxM7dq1b7vOjIwMCgoKylw+JyeHgQMH0q9fPxITE1m9ejVNmjQhJCREKaPX60lISGD16tXEx8eze/du5s6dW+J4QkICycnJHDhwgLi4OIt2Ll68SHBwMG3btiUpKYnVq1fTs2dPXnrpJQoLbbNyuhBCCPFv9K+es1WtWjWaNm3Krl270Ol0AGzduhVvb2+lzKZNm5g9ezYmkwl3d3fCw8NxdXVFr9fj7+/Pli1buHTpElOnTiU/P5+UlBTS09OpW7cuzZo1IzQ0lGPHjuHg4MCoUaPo1KmTRR+WLl2KTqdDr9cDVycGDhkyhIYNG94wO1W1alVGjx7NkCFDGDlyZInjWq2WJ554gt9//91i/5o1a6hduzb9+/dX9vn7+1O5cmUlODx8+DDBwcEcO3aMDh06EBERAcCCBQtYu3YtRqORjh07MnbsWDQaDQsXLuTbb7+lVq1aPPzww9x///0MHz4cDw8Pfv31V+Bqxmznzp1MmTKFzMxMPvzwQy5fvkytWrV4//33cXd3L/f7JoQQwn7Z45ytf3WwBWAwGFi3bh06nY7MzEw8PDww///zq06dOkVYWBhLly6lYcOGfPrpp4SHhxMVFQVczYzFxcWxePFiYmJimDt3Lnq9nnbt2uHj48PIkSPR6XS89NJLHD16lOeff55Vq1bh6uqqtJ+VlaUEesW0Wi2+vr437XOTJk04c+aMMtR4rdOnT7NlyxZeffVVi/1ZWVm0aNGiRPnu3bsr/3/8+HFWrVpF1apVeeaZZ/j99985fvw4+/btIy4uDo1Gw9ixY1m9ejWNGjUiLi6O+Ph4NBoNQUFB3H///Tftc0FBARMmTGD+/Pk0aNCAtLQ0Jk6cyJdffnnTc4QQQvz72OGXESXY0uv1SuZq7dq1GAwGkpOTAcjMzMTLy4uGDRsC0L9/fxYsWKCc6+PjA1wNftavX1+i7vT0dCU75O7uTqtWrdi7dy9dunRRymg0GipXrlyuPhd/LdbJyQmAlJQUAgICMJvNmM1munbtWiJYq1Sp0i3beeKJJ3BxcQHgwQcf5PTp02zfvp3MzEz69OkDwOXLl2nQoAEnT57k6aefpnr1qw9v7dmzZ6nDkYcOHeLo0aO89tpryr7z58+X67qFEEKIe9G/PtgqHkrcs2cP6enpjBkzRgm2TCaTRVmz2WwxtFcc7NxsTRDzdU94N5vNFvPDADw9Pdm3b5/FPpPJxIgRI5g0adIN6/3111+pX7++Eujo9XqmTJlS6nV6enrecNJ/aGgogwcPBsDB4X8fB41Go/R30KBBvPTSSwDk5+ej1WpZuXKlRT0ODg4WwZbZbEaj0Sj3y2Qy0bBhQxISEgAwGo3k5uaW2mchhBD/Pva4zta/eoJ8MYPBwMyZM/H09LQIOIozUdnZ2QDExsbSvn37UuvSarVKQKXT6ZSJ6kePHiUjI4PWrVtblO/fvz+bN29m8+bNwNUgZd68eZw6dcpiuLHYuXPnmDNnDi+88EK5rrF79+78/ffffPvtt8q+FStWsHPnTh566KGbnqfT6UhISODChQsUFRXxxhtvsG7dOjp06MAPP/xAfn4+BQUFFpm9WrVq8fvvv2M2m0lJSQGgcePGnD17lt27dyttv/XWW+W6BiGEEOJe9K/PbAF07tyZ0NDQEhPOXV1dCQ8PJyQkhMLCQho0aEBkZGSpdXl7ezNr1ixq1KhBaGgoYWFhSkYpIiICNzc3i/J169Zl4cKFTJs2jRkzZmA0GmnevDnR0dFKmeJhQo1Gg9Fo5Nlnn2XIkCHlukZnZ2e+/PJLPvjgA7788ks0Gg0NGzbk888/L3V4Ua/Xc+DAAfr164fRaMTHx4fevXuj0WgYNmwYAwYMoEqVKkqWDWDMmDEMGzYMV1dXHn/8cU6fPk3lypWZM2cOkZGRXLlyherVqzN16tRyXYMQQgj7Z49ZII35+rEuIW5D8VIUw4cPV6X+3/65qEq917pwxXjrQlZQo4r9/I1jk2S/jUYUbPUvYc7Zy6q30aBWFdXbAGjeVf3s9Oolk1RvA+CRutVvXegOmWz0IdPaaBjuP67OqtS7/KdjqtTbr3UDVeotC3sMIIUQQgghbltiYiI9evTg2Wef5ZtvvilxfMOGDcpi46+//jpnz54ttT77+RNb3FVqZbSEEEL8u9zt6fE5OTl89NFHxMfHU7lyZYKCgmjfvj2PPPIIcPWb9JMmTWLFihXUq1ePOXPmMHfuXCZMmHDTOiWzJYQQQgi7l5+fT3Z2dolXfn6+Rblt27ah0+lwcXGhatWqdOvWje+++045XlhYyHvvvUe9evUA8PDw4Pjx46W2LZktIYQQQlQYai39sGjRIj7++OMS+0NCQixGZ06cOEHdunWVbTc3NzIzM5XtWrVq0bVrV+Dq2pMLFiwgODi41LYl2BJCCCGE3Rs0aBC9e/cusb9mzZoW2yaTySLgK1438nrnzp3jjTfeoGnTpjes91oSbAkhhBCiwlBrflPNmjVLBFY3Ur9+fWVNSICTJ0+WWLbpxIkTvPLKK+h0OsaPH3/LOmXOlhBCCCEqDI1Go8qrrLy9vdm+fTt5eXlcunSJ9evX06lTJ+W40Whk2LBhGAwGQkNDy1S3ZLbEPeH8laJbF7pDy/aVPsHRWoa2fVD1NmxxvwCqO9ngnxAzNvl6UiUbrU30QcrvqrfxdfDjqrcBtlkDy3+A+m0AZH0/Q/U2jCbbrLP1QB3brLNmr+rVq8eoUaMYOHAghYWFBAYG4uXlxZAhQxgxYgT//PMPv/zyC0ajkXXr1gFXH4lX2qLnEmwJISq2u/09cCGETVWEH3k/Pz/8/Pws9i1cuBCAli1bcuDAgXLVJ8OIQgghhBAqksyWEEIIISoMG43o25QEW0IIIYSoMCpViIFE65JhRCGEEEIIFUlmSwghhBAVhj0OI0pmSwghhBBCRZLZEkIIIUSFobHDOVsSbAkhhBCiwpBhRFGhZGdn4+HhQVhYmMX+rKwsPDw8iI+Pv616ly9fTlJSUrnOOXjwIMOGDVMWghszZgx5eXkAxMfHM27cOIvyO3bsuOVT0oUQQgh7IMHWPc7FxYW0tDSMRqOyLzk5mdq1a992nRkZGRQUFJS5fE5ODgMHDqRfv34kJiayevVqmjRpQkhIyG33QQghxL9TJTSqvO4mGUa8x1WrVo2mTZuya9cudDodAFu3bsXb21sps2nTJmbPno3JZMLd3Z3w8HBcXV3R6/X4+/uzZcsWLl26xNSpU8nPzyclJYX09HTq1q1Ls2bNCA0N5dixYzg4ODBq1CiLB3ICLF26FJ1Oh16vB64+RHTIkCE0bNiQoiLbPKNPCCGEqKgk2LIDBoOBdevWodPpyMzMxMPDA7P56gNPT506RVhYGEuXLqVhw4Z8+umnhIeHExUVBVzNjMXFxbF48WJiYmKYO3cuer2edu3a4ePjw8iRI9HpdLz00kscPXqU559/nlWrVuHq6qq0n5WVpQR6xbRaLb6+vsp2SkoKAQEByvbFixepX7++mrdFCCHEPUjmbIkKSa/Xk5qaislkYu3atRgMBuVYZmYmXl5eNGzYEID+/fuTnp6uHPfx8QGgSZMmnDlzpkTd6enpBAYGAuDu7k6rVq3Yu3evRRmNRkPlypVv2ceEhATlFRERcVvXKoQQQtxrJNiyA8VDiXv27CE9Pd1iCNFkMlmUNZvNFkN7Tk5OwNWA6UaKM2TXbl87PwzA09OTffv2WewzmUyEhISQm5tb/gsSQgjxr6XRqPO6myTYshMGg4GZM2fi6emJg8P/RoeLM1HZ2dkAxMbG0r59+1Lr0mq1SkCl0+mIi4sD4OjRo2RkZNC6dWuL8v3792fz5s1s3rwZuBqQzZs3j1OnTlkMNwohhBC3olHpv7tJ5mzZic6dOxMaGsrIkSMt9ru6uhIeHk5ISAiFhYU0aNCAyMjIUuvy9vZm1qxZ1KhRg9DQUMLCwpRlJCIiInBzc7MoX7duXRYuXMi0adOYMWMGRqOR5s2bEx0dbd2LFEIIIe5BGvP140RCVEAZh/NVb2PZvuOqtwEwtO2Dqrdx/optvgVa3ckGf6/Z6A9SW/3lOzw+U/U2vg5+XPU2AHYfPq16G/4DJqneBkDW9zNUb6PAaLp1ISt4sE5Vm7TjrNKP/8YD6kw/6dL07o20yDCiEEIIIYSKZBhRCCGEEBXG3Z5fpQYJtoQQQghRYdztbw6qQYYRhRBCCCFUJJktIYQQQlQY9jiMKJktIYQQQggVSWZLCCGEEBVGJftLbMk6W+Le8PeZAtXbcLDRT/j5yzZYA8uO1qY6c0H99x7gvqqONmnn/lrOqreRd75Q9TYACm2wbpStfu826/qW6m0MDH1N9TYA3n/2UZu0U7+mOj8zab+ps36bz6O1VKm3LGQYUQghhBBCRTKMKIQQQogKQ5Z+EEIIIYQQ5SKZLSGEEEJUGHaY2JLMlhBCCCGEmiSzJYQQQogKo5IdTtqSYEsIIYQQFYb9hVoyjCiEEEIIoSoJtlSQnZ2Nh4cHYWFhFvuzsrLw8PAgPj7+tupdvnw5SUlJZS5vNpuJiorCz88Pf39/AgMDSU1NveV5Hh4eZW5j48aNzJkzp8T+HTt2EBwcDEBoaCg///xzmesUQgjxL6ZR6XUXyTCiSlxcXEhLS8NoNKLVagFITk6mdu3at11nRkYG7dq1K3P5tWvXsn//flauXImDgwN//fUXzz//PGvWrKFOnTq33Y9rdenShS5dupRaJjIy0iptCSGEEPciyWyppFq1ajRr1oxdu3Yp+7Zu3Yq3t7eyvWnTJgICAvDz8+P1118nNzcXAL1ez+zZswkMDKRnz57s27ePbdu2kZKSQlRUFGlpaeTm5jJ06FD8/Pzo3bv3DTNWJ0+exGg0UlBw9XEnjRo1IioqCgcHB7Kzs9Hr9UrZuXPnMnfuXGV74sSJBAQEMHjwYI4dOwbAF198gb+/P7169VKydvHx8YwbNw6ALVu20LNnT/r06cPy5cuVuoKDg9mxYwcA8+fPp0ePHvj5+TFlyhSMRuOd3WghhBB2RaPSf3eTBFsqMhgMrFu3DoDMzEw8PDxwdLz6LKlTp04RFhZGdHQ0iYmJtGnThvDwcOVcFxcX4uLiCAoKIiYmBm9vb/R6PSNGjMDHx4fJkyej0+lITEwkKiqK8ePHK8FasV69enH+/Hk6dOjAK6+8woIFC2jUqBH33XffLfvetm1bEhIS6Nq1K5GRkRiNRmJiYlixYgXx8fEUFhaSk5OjlC8oKGDcuHFERUURHx+Ps3PJ579t3ryZlJQUVqxYwcqVKzl8+DDLli27rXsrhBDCPmk06rzuJgm2VKTX60lNTcVkMrF27VoMBoNyLDMzEy8vLxo2bAhA//79SU9PV477+PgA0KRJE86cOVOi7vT0dAIDAwFwd3enVatW7N2716LMfffdx7Jly1i+fDkdO3YkLS0NX19fjh49Wmq/nZ2d8ff3ByAgIICdO3ei1Wp57LHHCAwM5OOPP+all16iXr16yjm//vorbm5uPPzwwwD07t37hn3u2bMnVapUwcHBgb59+7J9+/ZS+yKEEELc6yTYUlG1atVo2rQpe/bsIT093WII0WQyWZQ1m80UFRUp205OTgBobhKOm83mEtvXD8l98cUXHDhwAA8PD1566SUWL15Mx44dWbduHRqNxqKOa9uuVKmSRb0ODlen9s2bN49JkyZhNpv573//y86dO5Vy19dXPE/tWtdf8/XtCiGEEHY4P16CLbUZDAZmzpyJp6enErQASiYqOzsbgNjYWNq3b19qXVqtVgmodDodcXFxABw9epSMjAxat25tUf7cuXPMnj2bCxcuAHD+/HmOHj1Ks2bNqFmzJmfOnCEvL4+CggLS0tKU8y5evMjGjRsBWLFiBd7e3uTl5dGjRw8effRRRo4cyZNPPsmvv/6qnOPh4UFubi4HDhwAYM2aNSX6r9PpWLNmDZcvX6aoqIgVK1ag0+nKdB+FEEKIe5V8G1FlnTt3JjQ0lJEjR1rsd3V1JTw8nJCQEAoLC2nQoMEtv7Xn7e3NrFmzqFGjBqGhoYSFhSnLSERERODm5mZR/vXXX+ejjz7C398fJycnKlWqxAsvvMCTTz4JwH//+18CAwOpX78+LVu2VM6rWbMmGzZsYM6cOdSrV48PP/yQ2rVr079/fwIDA6lSpQqNGjWib9++fPfddwA4Ojoya9Ysxo4di4ODA82bN7/hvcjKyqJv374UFRXRsWNHXnzxxfLfVCGEEPbrbqehVKAxXz8eJUQF9PeZAtXbcKhkm5/w85dtMHRqo3+sbPENnzMX1H/vAe6r6miTdu6vVfLLI9aWd75Q9TYACo0lpwZYm61+7zbr+pbqbQwMfU31NgDef/ZRm7RTv6Y6PzO7/8pXpd4nGtVUpd6ykGFEIYQQQggVyTCiEEIIISqMu71MgxoksyWEEEIIoSLJbAkhhBCiwrDDxJYEW0IIIYSoQOww2pJhRCGEEEIIFUlmSwghhBAVxt1+aLQaJNgS9wRbrOdjMtnfD7jazKi/TN991RwpLLKf5QAvXDbeutAdMppsc79MNlim0VbXYos1sL6K/ET1NgAmdplpk3ZE2UmwJYSo0Owp0BJC3Jos/SCEEEIIIcpFMltCCCGEqDDsMLElwZYQQgghKhA7jLZkGFEIIYQQQkWS2RJCCCFEhWGPSz9IZksIIYQQQkWS2RJCCCFEhWGPSz9IsCWEEEKICsMOYy0ZRrybsrOz8fDwICwszGJ/VlYWHh4exMfH31a9y5cvJykpqVznnD59mpYtW/L5559b7A8ODmbHjh231Q8hhBBCSLB117m4uJCWlobR+L9HeCQnJ1O7du3brjMjI4OCgoJynZOYmIheryc2NhazDR7BIYQQQtyQRqXXXSTB1l1WrVo1mjVrxq5du5R9W7duxdvbW9netGkTAQEB+Pn58frrr5ObmwuAXq9n9uzZBAYG0rNnT/bt28e2bdtISUkhKiqKtLQ0cnNzGTp0KH5+fvTu3ZvU1NQb9iM+Pp4BAwZQuXJl0tPTLY7FxcXRu3dvunTpQkpKCsBN6507dy6vvPIKPXr0YMmSJQQHBxMSEkK3bt3Iysq66bUIIYQQ9kqCrQrAYDCwbt06ADIzM/Hw8MDR0RGAU6dOERYWRnR0NImJibRp04bw8HDlXBcXF+Li4ggKCiImJgZvb2/0ej0jRozAx8eHyZMno9PpSExMJCoqivHjx5cIcA4cOEBubi5PPPEEBoOB2NhYi+M1atRg5cqVTJgwgejoaIBS6y0oKCA5OZkBAwYA4OHhwbp163Bzcyv1WoQQQgiNSv/dTRJsVQB6vZ7U1FRMJhNr167FYDAoxzIzM/Hy8qJhw4YA9O/f3yLz5OPjA0CTJk04c+ZMibrT09MJDAwEwN3dnVatWrF3716LMnFxcXTv3h2tVkuPHj3YsGGDRUD2zDPPAPDII49w+vTpW9br5eVlUX/x9q2uRQghhNBo1HndTRJsVQDVqlWjadOm7Nmzh/T0dIshRJPJZFHWbDZTVFSkbDs5OQGguckn6fr5V2az2WJ+WEFBAUlJSXz33Xfo9XpefvllAIvJ+VqttkQbpdXr7Oxscax4+1bXIoQQQtgjCbYqCIPBwMyZM/H09MTB4X8rchRnjLKzswGIjY2lffv2pdal1WqVwEen0xEXFwfA0aNHycjIoHXr1krZTZs2UatWLbZs2UJKSgopKSmEh4ffcqL8req9kdu5FiGEEP8udjg/XtbZqig6d+5MaGgoI0eOtNjv6upKeHg4ISEhFBYW0qBBAyIjI0uty9vbm1mzZlGjRg1CQ0MJCwtTMlURERG4ubkpZYsnxl/L19eXWbNmkZaWdtM2blXvjdzOtQghhBD3Oo1Zvucv7gGHTl1WvQ2tjQb1C4pMty50p+72n3FWVFhkm3+iHLW2uWk1qziq3sbFAuOtC1lBkUn9z7LRZJv3f2baX6q38VXkJ6q3AfB7ykybtNOwlpMq9WYdv6BKvc3ur6ZKvWUhw4hCCCGEECqSYUQhhBBCVBh3e5kGNUiwJYQQQogK424v06AGGUYUQgghhFCRZLaEEEIIUWHYYWJLMltCCCGEEGqSYEsIIYQQFUcFWNU0MTGRHj168Oyzz/LNN9+UOJ6VlUWfPn3o1q0boaGht3waiqyzJe4J2acLVG+jmpNW9TYA8s6rfy03e3yT9duxSTM2Yav1nGzxOSs02uZaTDa4Z/VdnG9dyArOXCxUvY0iow3W2AOa6MfYpJ1LP36sSr2/51xSpd4m9aqUqVxOTg7PP/888fHxVK5cmaCgIGbNmsUjjzyilPH19SUiIoLWrVszfvx4PD09SywQfi3JbAkhhBDC7uXn55OdnV3ilZ+fb1Fu27Zt6HQ6XFxcqFq1Kt26deO7775Tjv/9999cvnxZeURdnz59LI7fiEyQF0IIIUSFoVbGfNGiRXz8cclsXEhICMOHD1e2T5w4Qd26dZVtNzc3MjMzb3q8bt265OTklNq2BFtCCCGEsHuDBg2id+/eJfbXrFnTYttkMllMxTCbzRbbtzp+IxJsCSGEEKLCUGsqaM2aNUsEVjdSv359du/erWyfPHkSNzc3i+MnT55UtnNzcy2O34jM2RJCCCFExXGXv43o7e3N9u3bycvL49KlS6xfv55OnTopxx944AGcnJzYs2cPAAkJCRbHb0SCLSGEEEKI/1evXj1GjRrFwIED6dWrF76+vnh5eTFkyBB+/vlnAGbMmMGHH35I9+7duXjxIgMHDiy1Tln6QdwTZOmH8pGlH8pPln4oP1n6oXxk6YeyOXjysir1Nq5rm8/SjUhmSwghhBBCRRJs3QOys7Px8PAgLCzMYn9WVhYeHh7Ex8ffVr3Lly8nKSmpzOXnzp3Lk08+SUBAAP7+/vj5+ZGenm5R5vTp07Rs2ZLPP//cYv+4ceN4+umnCQgIICAgAIPBwNdff31b/RZCCGG/NBp1XneTfBvxHuHi4kJaWhpGoxGt9uowRHJyMrVr177tOjMyMmjXrl25zgkKClLWI8nKyuKVV15h27ZtyvHExET0ej2xsbG89NJLFsNZI0aMoE+fPsDVb2907dqVDh068PDDD9/2NQghhBAVnWS27hHVqlWjWbNm7Nq1S9m3detWvL29le1NmzYREBCAn58fr7/+Orm5uQDo9Xpmz55NYGAgPXv2ZN++fWzbto2UlBSioqJIS0sjNzeXoUOH4ufnR+/evUlNTb1ln86dO0edOnUs9sXHxzNgwAAqV65cIut1LVdXVxo1asQff/xR3lshhBDCjlWARyNanQRb9xCDwcC6desAyMzMxMPDA0dHRwBOnTpFWFgY0dHRJCYm0qZNG8LDw5VzXVxciIuLIygoiJiYGLy9vdHr9YwYMQIfHx8mT56MTqcjMTGRqKgoxo8frwRr11q2bJkyDDh48GAGDRqkHDtw4AC5ubk88cQTGAwGYmNjb3otBw4c4MiRI7Ro0cJat0cIIYQ9sMNoS4Kte4heryc1NRWTycTatWsxGAzKsczMTLy8vGjYsCEA/fv3t8gs+fj4ANCkSRPOnDlTou709HQCAwMBcHd3p1WrVuzdu7dEuaCgIBISEli7di1JSUnMnDlTWWskLi6O7t27o9Vq6dGjBxs2bLAI2KKiopTM28SJEwkPD1f6K4QQQtgrmbN1D6lWrRpNmzZlz549pKenM2bMGJKTk4Grjw+4ltlspqioSNl2cnICbr4kwPUrgJjNZoxGY6n9ady4MW3atOGnn36iZcuWJCUl4eDgQEpKilImPj6eV199FbCcsyWEEELciOZup6FUIJmte4zBYGDmzJl4enri4PC/WLk4E5WdnQ1AbGws7du3L7UurVarBFQ6nY64uDgAjh49SkZGhvJE85vJz8/nl19+oXnz5mzatIlatWqxZcsWUlJSSElJITw8nNjY2BKBnBBCCPFvIpmte0znzp0JDQ1l5MiRFvtdXV0JDw8nJCSEwsJCGjRoQGRkZKl1eXt7M2vWLGrUqEFoaChhYWHKMhIRERE3fNbTsmXL2LBhA5UqVeLKlSs899xzdOjQgaFDhzJgwACLsr6+vsyaNYu0tLQ7vGohhBD/Fnd7mQY1yAry4p4gK8iXj6wgX36ygnz5yQry5SMryJfN0bwrqtTrXttJlXrLQoYRhRBCCCFUJMOIQgghhKgw7CljXkwyW0IIIYQQKpLMlhBCCCEqEPtLbUmwJYQQQogKQ4YRhRBCCCFEuUhmSwghhBAVhh0mtiSzJYQQQgihJlnUVAghhBAVxvGz6iz8fP99lVWptywksyWEEEIIoSKZsyWEEEKICkNjh7O2JNgSQgghRMVhf7GWDCMKIYQQQqhJMltCCCGEqDDsMLElmS0hhBBCCDVJZksIIYQQFYY9Pq5Hgi0hhBBCVBj2+G1EGUYUQgghhFCRBFvCbpnNZo4ePapK3RcvXuTAgQOYzWYuXryoShv2pKDg6orQhw8f5ocffsBkMqnSTl5eHps2bWLDhg3k5uaq0oYQQmUalV53kTyuR9iNZcuWMW3aNC5duqTse+CBB9iwYYNV29m+fTthYWEYjUZiY2Px9fVl5syZdOzY0artnD9/nnPnznHtj2iDBg2s2obZbGbp0qWkp6dTVFRE+/btCQ4OplIl6/0d9vHHH3Pw4EHeeust+vXrxyOPPMIjjzzChAkTrNYGQFpaGuPHj6d169aYTCZ+/PFHIiMj6dy5s1Xq/+233zAajTRr1owPPviAc+fOodVqGTduHNWrV7dKG6tWrSr1eK9eve64DZPJRFxcHL/99huPPfYYPXv2vOM6S/P3338zYcIE/v77b77++mveeustPvjgAxo2bHjHdQcHB6MpZYLPV199dcdtXMsWPy95eXm8//77pKenYzQaad++Pe+//z6urq5Wa+PgwYP8+eeftGjRwur/pljDyfNFqtRbt/rdmzklc7aE3ViwYAEJCQnMnj2bUaNGsXnzZjIyMqzezqxZs1iyZAlDhgyhbt26fPPNN4wePdqqwdb8+fNZsGABLi4uyj6NRsPGjRut1gbAtGnTOHz4MH379sVsNhMfH092djahoaFWayMlJYUlS5bw1Vdf4e/vz9tvv02fPn2sVn+xjz76iCVLluDu7g7A0aNHCQkJsUqwlZKSQkREBJMmTaJZs2akpqYydOhQduzYwaeffsqbb755x20A7Nixo9Tj1gi2Jk2axIEDB3j88ceJiYnhr7/+IiQk5I7rvZmwsDBeeeUVZs6cSd26dfH19eWdd97hm2++ueO6hw8fDlwNgiZOnEhERMQd11kaW/y8hIWF8dhjjxEZGYnJZCI2NpbQ0FBiYmKsUv8333zDjBkzaNy4MUePHmXy5Ml069bNKnVbi/3N2ALMQtiJwMBAs9lsNsfExJg3btxoNpvN5p49e1q9nT59+pjNZrM5ICBA2efn52fVNrp06WI+deqUVeu8ET8/P7PRaFS2CwsLzd27d7dqG8X3KSgoyJyenm42Go1Wb8NsvvF74Ovra5W6e/fubT548KCyXXxN+fn5ZoPBYJU2buXSpUtWqad79+5mk8lkNpvN5ry8PKt/dq/Xu3dvs9ls+fPi7+9v9XaurV8ttvh5udG9sdbn2Gw2m3v06GHOzc01m81mc1ZWlrlfv35Wq9tacs8XqvK6mySzJexGlSpVSE9Px8PDgw0bNtCyZUsuX75s9Xbq16/Ppk2b0Gg05Ofn880331g9FX///fdz3333WbXOGzEajRQVFVG5cmVlW6vVWrWNDh064Ovri7OzM23btuXFF19Er9dbtQ24OsT65ZdfEhgYCEBcXBwPPPCAVeq+cuUKjRo1UrZ9fHwAqFGjhtXvF1zNpM2ePZuLFy9iNpsxmUxcvnyZ7du333HdTk5OytBbrVq1Sh2GswZnZ2f++ecfpZ3du3crnzdrUvs6wDY/LxqNhuPHj3P//fcDcOzYMRwcrPer2tHRkTp16gDQtGlTmXNqIxJsCbsxceJEvv32W8aNG0dcXBwGg0GV4ZHw8HAiIyM5fvw4zzzzDDqdjvDwcKu28Z///IcBAwbQvn17i19M1r4ePz8/Bg4cqMzbWbNmDb6+vlZt45133iE4OJj69etTqVIlJk6cSLNmzazaBkBkZCSTJ09m/vz5mM1mq74vhYWFmM1m5Rf6mDFjACgqKrKYU2ctH374IZMnT+aLL75g2LBhbNiwwWIu4p24Piix5nyjGxk3bhxDhw7lyJEjBAQEcPbsWWbPnq1qm2q50c+Ltee8jRw5kv79+9OqVSvMZjN79+5l8uTJVqv/+vffmoGctdjj0g8yQV7YjQ0bNvD0009XyH88yuvjjz++4X5rB1tGo5GtW7eyfft2JUB5+umnrdqGLSb8qm3ChAk88MADvPbaaxb7Y2JiyMnJISwszKrt9enTh/j4eObNm4enpyedOnWiR48eJCcn33Hd7du3t8gspqSkWGx/+OGHd9zG9QoLCzl06BBGo5HGjRtbLbP17rvvKv9//XWAOteSmpqq6s/LgQMHcHNzIzMzE5PJRKtWrZRMlDV07tyZkSNHKttz5syx2LbGvMA7lXfBqEq9tatZPwtdVhJsCbsxYsQIfvrpJzp37oy/vz+PP/64VesfOnQoMTEx6PX6Gw5ZWHvyel5eHnv37sVoNNK6dWtVgpPevXuzcuVKq9d7rZCQEB577DH69++vTPjdvXu31Sb82uJ9OX36NAMHDqRKlSo88cQTaDQa9uzZw5UrV/jqq6+oUaPGHbdxrQEDBhAZGclvv/3Gzz//zIgRI+jZsyfff//9Hdd9q/e7d+/ed9zGta4NiK5ljUDIVteyf/9+WrRowa5du254vG3btlZpB8BgMLB27Vqr1Xe9m70fxdQIUMvr9EV1gq1aVSXYEsIqzp8/z4YNG1i7di1Hjhyhe/fuFn+13YkTJ07g5ubG33//fcPj1pofBOovY1BsyJAhDB06FC8vL1Xm0QAEBASQkJBgsc/Pz4/ExESr1F/8vmRmZt4wA2Ct96WgoIB169axd+9eAFq2bInBYFDlvu3cuZNvvvmG6dOn8/zzz3PkyBECAwN55513rNpOXl4ezs7OVK1a1ar1XuvagKioqIiNGzfSuHFj3n77bau1YTKZlOHQvLw8ateubbW64WpmMyIiguDg4BLHNBqNVZeYGD58OB4eHrRq1QpnZ2dlvzUDupvZsmWL1ZewuR0SbAlxDzh69Chr1qwhOTmZ2rVr8+WXX1ql3mvXJ2rTpg09evSwSr030qdPH+bMmVNiGYPrg5Y7pdPpOHPmDHD1l0bxvKSsrCyrtdGrVy8++eQTiwm/b7zxhtUzampnBO6GM2fOoNVqMZlMVvvChNlsZu7cuSxdulR57+vXr88LL7zAf//7X6u0cav2n3/+eZYtW3bHdZ0+fZrhw4czYMAA5edx+PDh5OXlER0dbbF0yr3CFgHdtfLy8lixYgXLly/nypUrpKamqtJOedhjsHXvT24R4v998cUXJCUlUVBQgL+/PwsWLKB+/fpWq//a9Ynmz5/PwYMHVVufqKioSAm0ANzd3VVZdT09Pd3qdV5P7Qm/xZo2bcqqVavw8vKyyAhY45uiNxuiLGbtIeQDBw7w9ttvk5OTg9lspnHjxkybNo0HH3zwjuuOjo7mxx9/JCYmhkcffRSNRsOBAweIioriypUrvPHGG1a4gpv7888/OXHihFXqioyMxMfHh+7duyv7oqKiiI6O5oMPPmDatGlWaceWi6f27NmToKAgq9V3Mzt27GDZsmVs2LABjUbD+++/b/Uvx9wue3wQtWS2hN0IDQ3lxRdfVOWbbnA1c5KcnIxGo+H06dMMGjSI1atXq9LWsGHD0Ol0FssYpKenM3/+fKu2U1BQwOeff85ff/3FxIkT+fLLL3n11VetOjSm9oTfYjdaTsJaC8HeaOg4KSmJ+fPnM3DgQEaNGnXHbVyrT58+DB8+XBk2/v777/niiy9YsmTJHdfdo0cP4uPjLQJSgHPnzvHCCy9Y/TPdtGlTJWsKULt2bUaPHq18tu+Ev7//Tfvr6+tLUlLSHbcBV4d1AZYvX46zszO9evXCwcGBpKQkrly5YtU/HqzZ7xv58ssviY2NxdHREYPBgMFg4OWXXyYlJUW1NsvrzCV1MlsuVSSzJcQdy8jIIDIyUrX6bbk+kZrLGFwrPDyc2rVrs3//frRaLYcPH2b8+PHMmDHDam2MGjWKtWvXWv1bW9dT85fFtfO+8vLyCAsL4/DhwyxevBhPT0+rt2c2my3m53Xt2pXo6Gir1O3o6Fgi0AL11gw7cOCA1essVtrPoDWXtGjXrh0AU6dOZcWKFcr+1q1bW/1pCPXr12fgwIG0atUKJycnZb+1suizZs2iS5cuDBgwQPmyhy3WKCsPe1z6QYItYTfUHEYC265PVKdOHZusRbR//35WrlxJamoqVapUYdq0afj5+Vm1jUceeYSPP/5Y9Qm/hw4d4uuvv7ZYCDQ7O9sqj4UplpSUxJQpU+jbty8fffQRjo6OVqv7Wt7e3sybN49+/fqh1WpJTk7m4Ycf5tixY8CdfabVXlfrejdbxqTYnQQRDRo0YPPmzTz11FMW+1NTU60+SR6uLm77119/KQvc/vrrrxQVWfc5fq1bt7ZqfddLTU0lMTGRDz74gNzcXAwGg/KgeKEeCbaE3di7d6/yTbFi1nye4LFjxyy+Nn39tjW+Mn2zZQyKJ65be26QRqOhoKBAaev06dNW/yv3zJkz7Nixw+K5f2pM+B09ejRPP/00e/bsoXfv3nz//fc0adLEKnXn5eXx3nvvcejQIWJiYmjRooVV6r2Z4on+cXFxFvtffPHFO/4cXP+5vf6YtR0/fpzMzEx8fX1xcHBg/fr1VK9enccee+yO6x47diyDBg2iQ4cONG/eHCcnJ37++WdSU1NZuHChFXpvady4cQQHB1OvXj3MZjOnTp1i5syZVm0jJCRE1WVfXFxcCA4OJjg4mAMHDrBixQqKioro2bMnAwYM4IUXXrBaW7ergiXarELmbAlRRrZY08eWy0sArFq1im+//ZbDhw9jMBjYsGEDb7zxhlXm0xT7/fffSwQ9P/30k9X/gi9eTmLWrFl06tQJT09P+vbty5o1a+64bp1Ox8WLF+natesN57NVhLWJysrW62wFBQXx+eefK8tLFBQUEBwcTGxsrFXqP3HiBEuXLiUrKwuNRoOnpyf9+/dXbdHcgoICfvvtNzQaDR4eHlZfRNlWy75cq7CwkJSUFFauXGn1eaG349xl638ZCKCGs22zuteSzJawG2fPnmX69OkcOXKEqKgopk6dyrvvvkvNmjWtUr+1fwndiJubGwAXLlzgk08+4aOPPuLPP/8kLCxMlW/w9erVC09PT3bs2IHRaOSTTz6hadOmVql7z549mEwmJkyYQGRkpDJBuqioiEmTJrFu3TqrtFOsSpUqFBQU8J///If9+/fzxBNPWK3ut99+2ybzWubOncvw4cNVXQi0+HOs9tpUxa7PlhYUFFj1eXxubm5WW0vvZmzxvhT76KOPWLJkSYllX9QKtn788Ud++uknmjdvXiECLXslwZawGxMnTuTJJ58kMzOTqlWr4ubmxltvvcWCBQvudtfKbcKECcpX8B9++GFef/11QkNDWbp0qVXbKSoqIjs7m2rVqgFXJzMfOHDAKo/s2LZtGzt37uTEiRPMmTNH2e/g4ED//v3vuP7r+fv7M2zYMGbMmEH//v1JS0ujXr16Vqnb2pOgb6Z4eLJ4QrYabrQ21Xvvvafa2lSBgYH06dNH+YLEpk2bGDZsmFXbUJst3pdiai/7smPHDkaPHk2dOnUYPHgwM2bMoE2bNixevJj+/fszdOhQq7V122QYUfxfe/ceFVW1xwH8O4KBhYp4tS6looYBoqhRApIJFvFQHipB+CoMxSugloZPUAEfiRkaCrlSSxNQERCZi4qgEAoXkhR5ecsUFQx1EF8gj5n7B2vOnRG0jH3OMOPvs1ZryZm19j4EM+fHfnw36bzk58m5u7sjOTkZwNO3hndm7SWsK35frMyfPx9VVVUYPHiw0ugDy7/Uk5OTBTtv7f79+9DT08ONGzdQXFwMW1tbdOvWrcPtCpmzBLR+HykpKZg6dSr++OMPxMfHY/bs2Uy+l0WLFsHY2Bh+fn7cyJZMJkN0dDQqKyuZZVMpOn/+PAoKCqCrqwsrKysMHjyYeR9CkU/1FxYWoqKiApMnT253d+ffxXfsi5ubGzZs2IC7d+/C19cXqampGDhwIO7evQsfHx9eYyf+qnuPeJpG1KFpREI6TEtLC/fu3eMeipcvX+Zt5xXf5xYaGBggLi4Orq6uAIC0tDResqkqKirw73//m7cpssuXL6OgoAC7du3i1tN8+umnMDIyYt6Xl5cXtw7olVdeQZ8+feDu7s7kWKDAwMAOt/EsFi1ahDfeeAMA8NJLL0EqleKLL77A1q1bO9z2xYsX20R7iEQiBAQE8BZqeeXKFdy9excfffQRjh07xkuxJcRZoqGhoWhqaoKvry8+//xzjBkzBkVFRUyjUoSIfZEvFejfvz+3s7JHjx68Hdn1rCj6gZBOLCgoCNOnT0d1dTX+9a9/4ZdffsHatWuZ9/P4AtaQkBDmC1jXrVuH1atX48svv8QLL7wAS0tLXjLEBg8ejJs3b3JrxVgqKyuDr68vJk+ejIULF6KpqQlFRUX46KOPsGvXLmZrw2bMmMGFTiq2qa2t3W7Q6d/R3vRRbW0t9PX1eSlUq6qquJEMPT09LFy4EG5ubkzaFiqbSi4yMhI3btxASUkJ/Pz8kJiYiPLycixZsoRZH0K8JwGguLgYiYmJ+OabbzBlyhQEBgZi8uTJTPvgO/ZF8WesmOMFAJ1loksTdyNSsUU0xjvvvIOhQ4fi/PnzaGlpQVhYGC+jQUIsYDU0NMT8+fNhZmaGe/fu4cKFC0yPHpJraGiAo6MjhgwZovRXLYtpsU2bNmHTpk2wsbHhrr3//vuwsbHBxo0b8d1333W4D+D/9xoeHo4VK1YwafNxEokEq1atwtSpU/HWW28hKCgIP/30E/7xj38gJiYGr7/+OtP+RCIRKioquNGt3377jdmuN6GzqX766SckJSXBw8MDenp62LVrF1xdXZkWW0ItKm9paYFUKsWJEyewevVq1NfXo76+nmkfJ0+eRHR0NGpra5WKH1axLzdv3uSyzxT/Lf+a8IOKLaIxKisr8csvv2DChAkIDQ3Ftm3bsHr1auYJ30KcWxgZGYnS0lLs3LkT9fX12LZtGwoLC5lPZ/G5GPbmzZtKhZacra0tLyOOwcHBOHnyJHe4shyL9WJhYWEwNzeHubk50tPTUVpaip9++gn//e9/ERERgV27dnW4D0XBwcHw9fXlFvjX1tYyW0sldDaVfCRFPqLW2NjIfARNqLNE3d3dYWtri1GjRsHCwgLOzs7MN3tERERg+fLleP3113kZNVU8d/HxMxiFOJPxr9DAgS0qtojmWLp0KTw9PZGZmYnLly9j6dKlCA8PR3x8PNN+DA0NsXv3bqUFrKzzr06ePImUlBQArVvbd+3aBQ8PD2bFVklJCYYOHcprnMHTtve3tLA/+2zRokXtLvZnUWz9+uuv2Lx5M4DWESBHR0cumJPVocqKbGxskJWVhYsXL0JbWxuDBg1itp5m0KBBSExMRFxcHPLy8ri1dMnJybysc3J0dMSCBQtQV1eH3bt34/Dhw8zXhgnxngSATz75BDNnzuSKxb179zIfDezevTuvR1uxOvaHPBsqtojGePToEdzd3bF8+XJMnDgRlpaWvBxD0d4CVtYZWM3NzWhoaOAiGZqampi2HxcXh/DwcGzZsqXNa6zS3UeMGIHdu3fj448/Vrr+7bff4s033+xw+4+rqKhAeno683YB5XVOeXl5CA8P575mPY0EtB58vXfvXtTV1SlNJbHaJSpENpXc7NmzkZOTA0NDQ1RXVysdsM2KEO9JALhx4wYiIiLwn//8B9ra2rC2tsayZcuYFFwFBQUAWo+3Cg8Px/jx45Wmjlkfb9WpaeDQFhVbRGNoaWnh6NGjOHnyJObPn4+MjAxeFvyWl5e3WcB67NgxODg4MOvD29sbkyZN4hZ4Z2dnMz1GQ14s7Nmzh1mbj1uyZAlmzJiBzMxMDB8+HC0tLSgqKkJDQwO+//575v0NHjyY25bPmqGhIcRiMbdGR75gPiUlhdmRQIoWLFgAS0tL7qBgdWdoaIjx48dzhWNBQQHT4kGI9yQALFu2DOPHj+eK3oMHD2Lp0qWIjY3tcNuKf/hUV1ejoqKC+5qP462IsChni2iMiooK7N69G3Z2dnBwcMDChQsxZ84cZrvexGIxGhsbsWXLFgQFBXHXm5ubERsbi+PHjzPpR664uBgFBQXQ1taGpaUlzMzMmLX9pCRsOVYjKI8ePYJYLEZxcTFEIhGGDx8OJycnXraYz5o1C0VFRbws9q+urkZISAhu376Nzz77DLa2tli3bh0yMzOxY8cO5lEWHh4ef3qsjrpYuXIlsrOz0b9/f+4aq+JB6Pdke1l3bm5u3JQ/X3766SfY2toyb1eIuIy/o57tQD6nGz/nxv8lVGwRjXL16lX89ttveOedd1BVVaW0aLajDhw4gLNnzyIzM1MpUkBLSws2NjZcGjcLTwovZRUOamVlBS0tLXzwwQcYPnx4my3fQhxNxJo8/uFxfKV+19XVoXv37rhy5QqXVcRKeHg4bGxsYGtry2v2kRAP2/feew9isZiX70PI9yQAhISEYPTo0XBxcQHQmoafkZHBSyyLRCJBYmIi9u/fj0ePHiE7O5tp+6o4g/Gvamjmp11dFc7lUbFFNIZYLMb27dvR0NCA+Ph4uLq64osvvmCWTyR35swZWFtbM23zcYojT01NTfj5559haWmJjRs3Mmm/paUFZ86cgVgsRnl5OWxtbeHs7MxsFFBVfv75Z1y8eBGTJ0/GuXPneFvn0tzcjGPHjiE+Ph7FxcUoKipi2r6trS1u3bqldE0kEqGsrIxZH0I9bKdPn45t27ahe/fuTNtVxPd70sTEBCKRiPujpFu3bhCJRHj48CF69uyJ/Px8Zn3l5+cjPj4eGRkZEIlEWL16NSZMmICuXdkOy0yaNAlRUVFt4jL4HqV7XlGxRTSGh4cH9uzZg2nTpiE5ORk1NTX45JNPkJaWxrSf0tJSxMTEtFm8zOeaijt37mDhwoXMIwaA1mIuNzcX//73v3Hp0iWMHTtW8MR0Fr7//ntkZGSgpqYG8fHx8PHxwZQpUzBr1ixmfVy9ehX79+9HYmIi7t69C39/f/j4+PB2iDOf+H7Yyv9guHLlCm7cuAFLS0toaWlxr7M8EkoV70nWdu/ejYSEBHTt2hVOTk5wcnKCr68vMjMzeemvvaPM2jsmjLBBC+SJxujSpQv09PS4r/v27cvLAvng4GB4eXnB2NhYsMXLL774Iq5fv85L2127dkX//v0xYMAAlJaWIj8/n2mxtXr1anh4eGD48OHM2mxPUlIS9u/fjw8//BC9evXCwYMH4enpyaTYOn78OOLj41FSUoL3338fGzduxMqVK5lvo09ISICXl5dS0KQilv3xnU0ln74V4vBmod6TfP5cvvrqK4wfPx4+Pj7cxgg+vxeh4jJIKyq2iMYwNjbG3r170dzcjLKyMuzbt4+XaTFdXV1MmzaNebuKFA8+lslkuHbtWpvE747673//i/T0dBw7dgw9evSAo6MjvvvuO+a7+YYPH45NmzZBIpHAzc0Nbm5u6NOnD9M+gNZiW3FdkI6OjtJISkcEBgbCyckJCQkJGDBgAICnH3vzdwk50cD3w9bDwwMtLS1obGzkDtD+7bff0L9/f+ZTYkK8Jx/X1NSEnJwcWFhYMGkvOzsbqampWLt2LW7dugUnJydeomvkhIrLIK1oGpFojIcPH2L79u04ffo0pFIprKysEBAQwGVVsRIVFQUDAwPY2toqnS1maGjIrI/Tp09zGTsikQi9evVieiSMk5MTGhoa4ODgAEdHRy6pXI7l9yJXXV2NI0eOID4+Hq+//jo8PT3x3nvvMWt//fr1EIlEyMzMxOLFi5GQkAAjIyMsX768w21fvHgRhw4dQmpqKl599VW4uLhg165dOHnyZMdvvB1Lly5lOs3Wntu3byMsLAx5eXncw3bFihXMCuGrV69i1qxZWLRoERfBsGLFChQUFGDnzp1MCzsh3pPtaWxshK+vL/bu3cu03fLyciQmJuLIkSMwMDCAj48P0+gXAMjNzcWYMWOUrvERl0FaUbFFNNqRI0eYp1W3d7ixSCRidnYZwP/Wf8XvQXGERiaTMf9egNYH7+HDh5GWloZXXnkFzs7OOHPmDLS0tJgdQyOVSrF//36lYtvb25vZmYJA69TbyZMncejQIWRnZ8PGxgY+Pj7ME78nT56MH374gfkfCor4ftj6+/vDxcUFEydOVLqemJiIEydOYNu2bUz6AYR5T7antrYWkydP5m1dVVNTEzIzM5GUlMQdTN5RQsdlkFZUbBG1l5GRgdDQUOjr62Pbtm0YMGAAzp07h/DwcFy/fh2nT59W9S0+Mz8/P8yZMwfDhw/ndeu/ED766CPcunUL7u7u8PDw4EYbmpubMXbsWKY/n/v37+Pu3btK1/ga3ZBIJEhOTkZycnKbhcYd5enpyUVKKI7UqFM2VXuZVHLquhDb3t5eaXq/rq4On376KebOnaviO/vrhI7LIK2o2CJq74MPPsDixYtRVVWF0tJSGBkZITY2FtOmTcOcOXOUFs2zIJFIsGbNGpw5cwYtLS2wsrLCqlWrmGYUWVlZ4c6dO0of7Ky3/gtFiKgMANiwYQP2798PfX19AGxH6c6fP//EBf7JycnM8s/k+MwME+phO2HCBBw5cqTd11gXW0K8JwEobVIRiUTo0aMH888XoQj1viStqNgiak/xg9vW1hZGRkZYv349XnvtNV76CwgIwMiRI+Hl5QWpVIqEhAQUFhYyObLjaRobG9V+lItPDg4OSEpK4mXqTXFa18vLCwkJCe2+xlJGRgby8vKgpaWFsWPHtpny6yi+H7aLFy/G22+/DU9PT6XrfEwjCvGelEgk6Nq1K7p3745r167h2LFjMDU15eX/oRBhs5oQl6FOaDciUXuKO850dXURGxvL61qXq1evKm0B9/PzYz6N9PgDXSqVYvLkyWo59SKUN954A42Njbz87BUfRo8ePXria6xs2LABRUVFcHFxgVQqRVRUFIqLi+Hv78+sj549eyIoKIi3h+0XX3zBZd6ZmZlBR0cHxcXFqKqqYp4Xx/d7MicnB8HBwdiyZQuMjIwwZcoU2Nra4ujRo7h69So+/PBDpn0phs2GhITwEjarigib5xkVW0TtKX5QdO/enddCS95fdXU1/vnPfwIAqqqqmC3CnjFjBjeFpJharaWlhfHjxzPpQ1F9fT22bt2KvLw8tLS0YPTo0ViwYAFefPFF5n0BraNzYrEY8fHxiI+PZ9q2m5sbHBwcMGTIEKUCnEXxoPg79viDiY8HVWZmJtLS0rjfK29vb7i7uzMttvh+2Pbp0wfJyclIS0tDWVkZGhoa4OHhAScnJ6V1aCzw+Z4EgK1bt2Lfvn0wMjLCjh07MGTIEERGRuL+/fv46KOPmBZbmzdvxr59+9qEzbIutlQRl/E8o2KLqL2qqiourVrx33Kst9DPnz8fXl5esLCwgEwmw7lz55jl08gLg/DwcKxYsYJJm0+zZs0adOvWDWvXrgUA7N+/H6GhocyOBZL77bffkJCQgJSUFPTs2RMzZsxg2j7Q+pBavnw579v9hdCnTx/cvXuXS6ZvampCr169mPYhxMO2W7duXI4Xn/h8TwKto5nyw8bz8vK4tW56enrMRzb5DpuVs7W1xZ49ewSPy3heUbFF1N6SJUu4fwuRVm1nZwcLCwucP38eUqkUq1evRu/evZn2MW/ePJw+fRo2NjaIjY1FSUkJFi9ezPRgbQAoKSlRmm4JCQlhtkC6qakJR48eRXx8PMrLyzFu3Dh07doVR48e5WUkpXv37swXqss9raCvqqpi3p+BgQFcXV0xfvx4aGtrIycnBwYGBly/LP6A0KSHLd/vSZlMBplMhoaGBpw9e5b7zHn48CEaGhqY9QMIl+wuP5ZJcUpXiLiM5xUtkCfkL3rSNnY5lg/6WbNmwcbGBqampti4cSNmzpyJxMRE7Nmzh1kfQOvmgh9//BE9evQAANy9exdTp05lsjbM2toao0aNgru7O8aOHQsdHR2MHz+etw/zNWvW4ObNmxg7dqxSQjmLn8ufLYD38PDocB9C96eqbCqWhHpPbt26FSUlJZBKpbh37x7i4uJQXl6OLVu2YNCgQVi0aBGTfgD+w2aJatDIFiF/0ZIlS9C7d29YW1u3e9wIy2Krrq4Os2bNQlhYGDw8PODu7s7LLqGPP/4YU6ZMgb29PWQyGbKysjB79mwmbbu5uSE9PR337t3D7du38cEHHzBp90nq6+uhp6eHs2fPKl1n8XN5WnHzpHgDvvpjha8gTjmpVIqDBw/i4sWLGDlyJFxcXJj3IdR7MjAwEGKxmMuLA1qnE01NTTFv3jwmfciVl5fj66+/VrrGR7K7UHEZpBWNbBHyF5WVlUEsFiM3NxcmJiZwdnaGjY0NL4ddT5o0CWvWrMG8efOwd+9e3L9/H0uWLOGG/lm6ePEiCgoKIJVK8fbbb+ONN95g1nZLSwuXuJ6bmwug9Vid999/n9m5hY+rq6tDz549eWm7PaNGjWpT4KkDvh+2ISEhKC8vx5tvvonc3Fw4ODgwP7hbyPck34ROdldVhM3ziootolGEyKcBgOLiYojFYuTn58Pc3BwuLi4YPXo0s/bPnDmD7du3w97eHh9//DE+/PBDLFy4kFmmT1ZWFuzs7J44DcPH2qfbt2/j8OHDSEpKQm1tLXJycpi2X15ejgULFqChoQEJCQmYNm0avv76awwdOpRpP48bOXIkioqKeO2DD3w/bJ2cnCAWiyESiVBbW4uZM2cyj0hRxPd7km9CJ7u7ubm1+eNNXZP91QFNIxKNIVQ+DQAMGzYMw4YNQ2FhISIjI5Gamsr0gWttba1UWO3fv59Z20Drg8nOzg75+fntvs5HsdW7d2988skn+PDDD3mZEg0LC0N0dDQ+//xzvPzyy1i1ahVCQ0Nx8OBB5n0p4iuj6M6dO6ivr4dMJkNLSwuuXbvGNECT72wqHR0d7v9Nr169eM9y4vs9yTdPT094enoKluzOd1wGUUb/Z4nGECKfRiaToaCgAOnp6cjOzoapqSmmT5/OvKBLSkrC+vXr25zzx+q4Hvk0xbp161BaWgozMzPcu3cPFy5c4O2Dvry8HPHx8UhNTYWRkRHz8+Tq6+sxePBg7usxY8Zgw4YNTNpWLEoe19TUxKQPRVu2bMH333+P5uZm6Ovro6amBubm5jhw4ACzPvh+2D5eXPE1tSfUe1KO79FzvsNm5fiOyyDKqNgiGoPvfJrQ0FDk5OTAzMwMTk5OWLx4Mbp168asfUXR0dHYs2cPhgwZwkv7cps2bUJJSQl27tyJ+vp6bNu2DYWFhQgMDGTS/qNHj5CWlob4+HhUVFSgS5cuiI2N5SWiQ19fH+Xl5dxD/vDhw4Ks3ZozZw7zNpOTk3Hq1ClERERg7ty5uHTpEvbt28e0D74ftu1FZCh+zSK+Qsj3JCDM6LlQye5CRNiQ/6M1W0Rj+Pv7w8rKSimfJi8vDzExMUzaNzExgb6+Ppeu/vgHIcst8z4+Pswfru2ZMGECUlJSuMXqzc3N8PDwYLJuIzw8HOnp6Rg2bBicnJxgb28PV1dX3nbBVVZWIjg4GMXFxdDV1cWAAQOwceNGDBo0iJf++OTt7Y34+Hjs3LkTr732GhwcHHhZTyORSLiHrYWFBdOHrRDxFUK+J4HWjStRUVFtRs9Zblzx9PRkOoL5OCEjbMj/0cgW0RgREREICwtDTEwMl0/D8i91IfOHhg4diqCgIIwZM0YpcJL1B2FzczMaGhq4I45YTomlp6dj+PDhcHBwgJ2dHfT09Hj9S71///6Ii4vDw4cPIZVKoaenx1tffNPT00NycjKGDh2KvXv3om/fvszCM5/0sJVvWGD1O6ZYTEkkEujq6jI/BkroTDAh0t35DpsVMsKG/B8VW0Rj8J1Pw0eK85Pcv38fL730En755Rel66w/CL29vTFp0iRu91N2djamTp3KpO1Tp07h1KlTOHToENasWQNra2vU19ejsbERL7zwApM+AGD69OlPLeL4WIzPt4iICKSlpcHd3R1ZWVkICQnBggULmLQt1MNWJpNh69atiIuLw507dwAAr7zyCqZOnYpPP/2USR9CvicBYdLd+U52T0pK0pi4DHVC04hE7QmdT6NpiouLUVBQAG1tbVhaWsLMzIx5HxKJhIt9uHHjBiZPnowvvviCSdvyg7ufhPX6MKHiRfgiVDbVN998g59//hkLFy7EkCFDIBKJuNT1UaNGMQ8DFYKmpbure1yGOqFii6g9ofNp+DRnzhzExsbC3t6+3dEaVn/dCpGzlZGRgffee6/N9QsXLiApKQkrV67scB/t9ZmXlwctLS2MHTsWY8aMYdr+4wuki4qKmC6QNjExUfq5a2trQ0tLC48ePYKenh4KCgqY9CPH58PW2dkZhw4dgq6urtL1e/fuYerUqbxmbvElNze3ze8U63R3VSS7y+MyKioq1CouQ51QsUU0hlD5NHyqqalB3759cf369XZfZzVlsXXrVgQGBirtDlPEYqeYh4cHt0j6s88+w1dffdXhNp9mw4YNKCoqgouLC6RSKdLS0mBvbw9/f39mfQixQBpo3WU3atQouLq6QiQS4ejRo8jJyUF4eDjTfuT4eNi2F5opp/i7oQ6EHD0XItm9vbgMR0dH2NnZMV9XR1rRmi2iMYTKp+FT3759AfC/FkU+9davXz/861//4qUPxZ/B77//zksfijIzM5GWlsZlRXl7e8Pd3Z1psSXEAmkAOH/+PFavXs19/cEHH2D79u3M2hcim0qT1gA9ePAAZ8+exYMHD5SCgLW0tLBw4UKmffEdNit0XAZpRcUW0RhC5dNogmvXrmHz5s1ITExst1hgcYad0D+DPn364O7duzAwMADQurOyV69eTPsQYoE0AHTr1g2JiYlwcnKCVCpFSkoKs8wwoR62j+dqPf6aOhEy3Z3vsNmEhATo6+ujtLQUpaWlbUachd7h+bygYotoDF1dXUybNk3Vt6EWoqOjkZWVxWsfTU1NqK6uhlQq5f6tONrFaiu7nIGBAVxdXTF+/Hhoa2sjJycHBgYG3AOfxdQo3/Eichs3bkRYWBjCw8MhEokwZswYfPnll0zaFuphu2TJkie+xkeorRCEGD3nO2yWiinVoDVbRGNERUXBwMCAt3waoaWmpuLXX3+Fv78/jh49ykv+zalTp/Duu+8ybxcAt8i/vY8YllvZ5YQI0RRigTTfnrQeUI71SJ1UKuWmFCUSCTfyqI4mTpzY7ug5Hzte+QqbJapBxRbRGIo7EeX4eKgLITIyEjdu3EBJSQkOHDiAuXPnYujQoU8dLfg7rl27hpUrV+L69ev48ccf8fnnn2Pt2rV47bXXmPYjlPv377c5T5JFsS10vEhOTg6+/vrrNiMo6vS7XFtbi8DAQPj4+HA7ggMDAyGRSBAdHQ19fX3V3uDfwGe6OyW7azYqtgjphNzd3ZGUlAQPDw8kJyejubkZrq6uEIvFTPuZNWsWPvnkE0RGRiIpKQkHDhxASkoKfvzxxw63vWzZMqxduxYAcPnyZRgZGXW4zafZsGED9u/fzz3EZTIZs2Jb6HiRDz74AEuWLGkzgiJ0iGdHLFq0CMbGxvDz8+NGtmQyGaKjo1FZWclsWlRIfI6em5iYPDVslsU0OFEdWrNFNIYq8mn4In84yR+0jY2NvOzuqq2tha2tLSIjIyESifDhhx8yKbQAoLS0lPv3woULed/qf+LECWRnZ3NHD7Ek5AJpAOjVqxfTnYGqcPHiRURGRipdE4lECAgIwIQJE1R0Vx3DZ7o7JbtrNiq2iMYICQnByJEjER4ezuXTLF++nGk+jVAcHR2xYMEC1NXVYffu3Th8+DAvDyhdXV3cuHGDK+oKCwuZHqUjJ8QA+htvvIHGxkZeii05oeJF3nzzTaxbtw7vvPOO0gjKW2+9xbQfPj1tN6q6FhB8HaIOAKampjA1NcXnn3/Ohc1+9dVXlOyuIajYIhqD73waIc2ePRs5OTkwNDREdXU1AgMDeRnpWLp0KebMmYPKykq4ubmhrq6uzfmSf5fiw1aIGAg3Nzc4ODhgyJAh0NLS4q6zLISEihc5f/48AOXRQZFIpFaZcYaGhu1uwMjOzlbbRfJCjZ4PGzYMw4YN48JmU1NTKdldzdGaLaIx3N3dsX37dqV8mnnz5qlVUvWfHcfCx8hGU1MTLl++jJaWFgwaNIjZyJatrS28vb0BAPHx8dy/5VhkeSlycXGBn59fm/UzLHeK8blAWtNcunQJM2fOhLW1NczMzKCjo4Pi4mJkZ2djx44dMDU1VfUtPjO+090p2V1zUbFFNEZWVhZCQ0Pb5NOMGzdO1bf2l02fPh0AcOfOHVy9ehUjR45Ely5dUFRUhCFDhiA+Pp5pf9evX8fevXvbTIuxWIyrOMrYHtbFlre3N/P/P48TKl7k+vXrWLFihdrvEq2pqUFcXBzKysogEolgbm4OLy8vtVxHCbR/BNHEiRORmpra4bYfD5u1t7enZHcNQsUW0Siakk/j5+eHFStWYMCAAQBaH74hISH47rvvmPbj6ekJS0vLNtNiLDKphLZmzRrcvHkTY8eOVdrNxXLLvFDxInzuEiV/H5+j5yYmJtDX1+dGsB6fplan2A/SFq3ZImrvSfk0OTk5ANQzn6aqqoortIDWkRM+jjhpbm5GcHAw83ZVob6+Hnp6ejh79qzSdZY/fz4XSCvic5co+fv4THenYkqzUbFF1N6SJUuemk+jjsXW0KFDERwcDCcnJ8hkMqSmpsLS0pJ5P2+++SYyMzNha2vLyy5EIa1btw5NTU34/fff0dLSAmNjY6ZnygHCLZAWapcoeTZ2dnawsLDgRs9Xr17NbPRcnTLUyLOjaUSi9srKyjQun6axsRF79+7Ff/7zHwCAjY0NfHx8mBcPtra2uHXrFgBwR+uIRCKUlZUx7UcIFy5cQFBQEPT19SGVSnHr1i1ER0fDwsKCWR98L5CWO3/+PFauXInKykr079+f2yU6YsQIpv0IRSKR4Ny5c2hpacGIESPUbs0WpbuTjqJii2gUeT5Nfn4+5dOo2PTp058aj8A6xsDb2xtLly7liqtffvkF4eHhOHjwILM++Fwg/Ti+dokKLScnB8uWLcOIESMglUpRVFSEiIgItQptpXR30lE0jUg0CuXTPJvGxkbs3LkTv//+O1auXIndu3dj9uzZTB7sgYGBAFq3s69cuRLh4eEdbvNpHj58qDSKNWLECDx69IhpHyKRCNXV1UoLpFmONm7duhWBgYFYunRpu6+r40N98+bN2LdvH/r16wegNQ8vICBArYotSncnHUXFFtEI7eXTTJ8+Xa0+0FVhzZo1MDAwQElJCbS0tHDlyhUsW7aszTErf4divtWLL77INO+qPT179kRGRgbee+89AEBGRgbzw475XCANtK7VA9hmg6lac3MzV2gBQL9+/SCVSlV4R8+O0t1JR9E0IlF7mppPc+fOHdTX10Mmk6GlpQXXrl1jfi6fh4cHkpKS4O7ujuTkZMhkMkycOBFHjhzhpR8+Xb58GYsXL0ZlZSWA1of6xo0bMXDgQKb9CBEvsnbtWri6usLc3Jx520Lz9/eHlZUVpkyZAgA4ePAg8vLyEBMTo+I76xj56HlFRQWNnpM/RSNbRO0lJCRAX18fpaWlKC0txVdffaX0ujpuqd6yZQu+//57NDc3Q19fHzU1NTA3N2eeXi4SidDY2MitraqtrRXkaB0+GBkZ4cCBA3j48CGkUin09PSYtS10vEj//v0RERGBuro6TJw4ERMnTlS7QFO5iIgIhIWFISYmBjKZDFZWVkxHA4VCo+ekI2hki6i969evP/V1ddxSbW9vj8OHDyMiIgJz587FpUuXsG/fPnz77bdM+0lOTsaBAwdw5coVODk5ISMjA/PmzeNGITpCcd1RZmZmm0BQVuuPZDIZtm7dirfeeosb+QsODsarr76KoKAgJn2oaoF0dXU1xGIxDh8+jJdeegn79u3jpR8+5ebmYsyYMUrXjh07BgcHBxXd0bPT1NFzIhwqtgjphORHz+zcuROvvfYaHBwceNv19uuvvyI/Px8tLS14++23YWJiwqTdP5s2ZJVSHxUVhfLycqxatQovv/wygNYpxfXr18Pc3JzJsUCqiBe5d+8ejh49CrFYjJqaGjg5OWHevHm89ceaWCxGY2MjtmzZolT0Njc3IzY2FsePH1fh3T0bSncnHUXTiIR0Qnp6ekhOTsbQoUOxd+9e9O3bFw0NDbz09frrr+PevXv45ZdfUFdXx6xdeTEllUq5okQikcDAwIBZH0DrQvjExESlHZRGRkbYtGkTvLy8mBRbQi+Q9vf3R0lJCRwcHDB//nymWWFCefDgAc6ePYsHDx4gPz+fu66lpYWFCxeq8M6eHRVTpKNoZIuQTuiPP/5AWloafH19sX79epw+fRr+/v5wdnZm0n5+fj4+++wz9O7dGx9//DEiIyMxatQolJaWwsvLC3PmzOlwH7W1tQgMDISPjw9334GBgZBIJIiOjma2U1C+uP9ZX+soPhdIZ2ZmYuzYscxDbFXhzJkzzDd2EKJu1P+dTIgGOn36NHx9fQG0HkcEgOnZeGvXrsV3332Hu3fvwtfXF6mpqRg4cCDu3r0LHx8fJsVWREQE3nnnHTg6OnLXtmzZgujoaKxduxZffvllh/sAgG7dunFJ64quXLnCdJpPiAXS8pyt48ePtzvNpo45Wz179kRQUBDq6uqg+Lc961BbQjozKrYI6UR2796N+/fvIz4+Xmnhf0tLC1JTUzF16lRmfcnXZvXv35+LR+jRowezpPKLFy+2yesSiUQICAjAhAkTmPQBAHPmzIGvry/mzp0LMzMzvPDCC7hw4QKio6OxYMECJn08vkB68eLFvCyQlkc9aFLOVnBwMLy8vGBsbKy2O10J6SgqtgjpRIyMjHDhwoU211944QWsX7+eWT+KIz46OjpKr7FaWfC0ByvLEadx48ahS5cuiI2NRXh4OLp06YJhw4Zh5cqVeOedd5j0IVS8yNatW3Hjxg1MmDAB3bt3Z9Kmqunq6mLatGmqvg1CVIqKLUI6kXHjxmHcuHGoqKhgsrD7SW7evIlvvvmmzb/lX7NgaGiIU6dO4d1331W6np2dzXyR/NixYzF27FimbSoSaoH00qVLkZycjOjoaFhZWWHy5Mlqv97J1tYWe/bsga2trVJhb2hoqMK7IkRYtECekE7I1dUVKSkpvE27KBZX7WFR6F26dAkzZ86EtbU1zMzMoKOjg+LiYmRnZ2PHjh0wNTXtcB+aqrGxERkZGUhJScHly5fh6uqKSZMmcWcyqpPH89WA1lFP2uFHnidUbBHSCc2YMQN//PEHhg4dqjQaoG4LpGtqahAXF4eysjKIRCKYm5vDy8sL//jHP1R9a2rj9u3biIqKwqFDh9qdYiaEdH5UbBHSCT0pEJRVEKgmu3//PqRSKXr06KHqW+mQy5cv48iRIxCLxXjllVcwefJkuLi4qPq2nplEIsGaNWtw5swZtLS0wMrKCqtWraKCmzxXqNgipJMS4iBqTVJZWYnPPvsMlZWVkMlkePXVV7F582bmB1Hzqaamhjue5/79+3B3d4eHh4daTh/KBQQEYOTIkfDy8oJUKkVCQgIKCwsRGxur6lsjRDBUbBHSCW3duhW7d+9Gc3MzevXqhT/++IOXg6iB1pGHc+fOoaWlBSNGjFDbEYdPPvkEXl5eXK6XWCxGXFwc9uzZo+I7++tGjRoFBwcHeHh4ME+lVxU3NzekpKQoXePr6ClCOiv+DvYihPxtSUlJOHXqFJydnfHDDz9g+/bt6NWrF/N+cnJy4ObmhkOHDiEpKQmurq7Iyspi3o9EIkFWVhYyMjJw69Yt5u0DrYn1igGqzs7OuHPnDi998SU7Oxvr16/XmEILaF0MX11dzX1dVVWlEcn4hDwL+o0npBPq27cv9PT0YGxsjPLycjg4OGDTpk3M+9m8eTP27duHfv36AQCuXr2KgIAApqnoOTk5WLZsGUaMGAGpVIqQkBBEREQw7QNozSIrKSnB0KFDAQAXLlzgJXiUT3p6eqq+Bebmz58PLy8vWFhYQCaT4dy5cwgLC1P1bREiKCq2COmEhDqIurm5mSu0AKBfv36QSqVM+xCioAOAZcuWITAwEPr6+pDJZKirq8PmzZuZ9kGenZ2dHSwsLHD+/HlIpVKsXr0avXv3VvVtESIoKrYI6YQiIiKQlpYGd3d3ZGVlISQkhNnRM4oMDQ2xe/duTJkyBQBw8OBBvPrqq0z7EKKgA4CBAwfi6NGjuHz5MqRSKQYOHMgsoFUV1H0t3ZMOAM/JyQHQekg4Ic8LWiBPSCcklUq5I20kEgnzxHW527dvIywsDHl5eZDJZLCyssKKFSvQp08fZn34+/vDyspKqaDLy8tDTEwMk/arq6shk8kwe/Zs7NixgztuqKWlBX5+fkhPT2fSj5Aen3otKiriZeqVTyYmJujduzesra3RtWvXNq+rW2YcIR1BxRYhnUhtbS0CAwPh4+MDZ2dnAEBgYCBqa2vxzTffQF9fn2l/ubm5GDNmjNK1Y8eOwcHBgVkffBd0S5cuRX5+PmpqatC3b1/uura2NsaNG4dly5Yx6UdIkyZNQlRUVJup18d39XVmZWVlEIvFyM3NhYmJCZydnWFjY8P0XExC1AUVW4R0IosWLYKxsTH8/Py4h5JMJkN0dDQqKyvx5ZdfMulHLBajsbERW7ZsQVBQEHe9ubkZsbGxOH78OJN+AGEKOgD49ttvMXv2bKZtqoqrqysOHz6sdE2d4xKKi4shFouRn58Pc3NzuLi4aNSOS0L+DK3ZIqQTuXjxIiIjI5WuiUQiBAQEYMKECcz6efDgAc6ePYsHDx4gPz+fu66lpYWFCxcy6ePPCjpWxVZCQgK8vLzQ2NjY7pmPfB7ozRch1tIJadiwYRg2bBgKCwsRGRmJ1NRUFBUVqfq2CBEMFVuEdCJPO3ia5fSLp6cnPD09cebMGd5S6YUo6ABAEwfnIyIiEBYWhpiYGG7qVR3jEmQyGQoKCpCeno7s7GyYmppi+vTparX2jBAWaBqRkE5k7ty58Pb2xrvvvqt0PTs7Gzt37sTu3buZ9ldaWoqYmBjU1dUpFS0//PADsz74LOg0lVBTr3wKDQ1FTk4OzMzM4OTkBHt7e7XLPSOEFSq2COlELl26hJkzZ8La2hpmZmbQ0dFBcXExsrOzsWPHDpiamjLtb+LEifDy8oKxsbHSqNrbb7/NrA++CzoTExOle9fW1oaWlhYePXoEPT09FBQUMOlHCEKupeObiYkJ9PX18eKLLwJoO2p74sQJVdwWISpB04iEdCKDBg1CYmIi4uLikJeXB5FIBHNzcyQnJ/OSs6Srq4tp06Yxb1dRcHBwuwUdK+Xl5QBaR1JGjRoFV1dXiEQiHD16FNnZ2cz745NQU69CoGKKkP+jkS1CnmNRUVEwMDCAra0tdHR0uOuGhobM+vD09OTlAO3HeXh4ICkpSemau7v7E8M1OzOaeiVEs9DIFiHPMXlu065du7hrIpGI6aiEra0t9uzZw2tBBwDdunVDYmIinJycIJVKkZKSgp49ezLtQyg9e/ZEUFAQr2vpCCHCoZEtQgiv7O3t21xjXdABwPXr1xEWFob8/Hx06dIFNjY2WLFiBV5++WWm/QhBiLV0hBDhULFFSCclxNl4EokEa9aswZkzZ9DS0gIrKyusWrVK7c7hU3Tnzh3mSftCE2rqlRAiDDo3gZBOKCcnB25ubjh06BCSkpLg6uqKrKws5v2EhIRg2LBhOHHiBDIzM2FhYYHly5cz7UMikWDBggUYPXo0LC0tERAQgFu3bjHtA2g9HsbR0RHu7u74448/8P7776OkpIR5P0KQT73+/vvvqKqq4v4jhKgnGtkipBMS6mw8Nze3Nm2yPhYmICAAI0eOhJeXF6RSKRISElBYWIjY2Fgm7a9YsQLBwcHw9/fHmjVr8PnnnyM5ORm5ubnYvHkzDh48yKQfIQk19UoIEQYtkCekE2pubuYKLQDo168fpFIp835EIhGqq6vxz3/+EwBQVVUFbW22HwtXr15VOkbHz8+vzbl/HdG7d28EBwejvr4egwcP5q6PGTMGGzZsYNaPkDIzM1V9C4QQhqjYIqQTEupsvPnz58PLywsWFhaQyWQ4d+4c82Nh+C7o5PlTvr6+KC8v5xaUHz58WG13I2riWjpCnmc0jUhIJ3T79m2EhYUhLy+POxtvxYoV6NOnD/O+JBIJzp8/D6lUCgsLC/Tu3Ztp+1lZWQgNDW1T0I0bN45pP5WVlQgODkZxcTF0dXUxYMAAREZGYuDAgUz7EQLfU6+EEGFRsUVIJ8T32Xh/FvTp7u7OpB85vgs6AIiPj4e3tzcePnwIqVQKPT095n0IRYi1dIQQ4dA0IiGdyJ+djceq2FqyZAl69+4Na2trdO3atc3rLIqtJxV0OTk5zPpQtHfvXnh7e3Nn8akzIdbSEUKEQ+9eQjoRoc7GS0pKglgsRm5uLkxMTODs7AwbGxt06cIuDUaIgk7RK6+8ghkzZsDCwkIpqT4gIIBpP0IQYi0dIUQ4NI1ISCck5Nl4xcXFEIvFyM/Ph7m5OVxcXDB69OgOt1tWVsZ7QadIccejInUstgBhpl4JIcKgYouQTqi0tBQxMTGCno1XWFiIyMhIVFRUoKioiGnbfBV0ilpaWqClpQWgtVAxMDBg2r4QhF5LRwgRBhVbhHRCQpyNJ5PJUFBQgPT0dGRnZ8PU1BSOjo6ws7Pjbd0THwVdbW0tAgMD4ePjA2dnZwBAYGAgJBIJoqOj1eroHhMTk6dOva5bt04Fd0UI6SgqtgjphPg+Gy80NBQ5OTkwMzODk5MT7O3t0a1bN+b9CFHQLVq0CMbGxvDz8+OmKGUyGaKjo1FZWYkvv/ySST9CEHrqlRAiDCq2COmEoqKiYGBgAFtbW6XF3oaGhkzaNzExgb6+PlfwKI6eAWByLIxQBZ2rq+sTE+knTJiAI0eOMO9TCEJMvRJChEHFFiGdEN9n412/fv2pr7NIqxeioAPaz6SSe1ohpi74XEtHCBEGRT8Q0gnxfTYeH0f/PE6oQ5MNDQ1x6tQpvPvuu0rXs7Oz1XKRfHtTr9OnT4ednZ2qb40Q8jfRyBYhnRCdjffXXbp0CTNnzoS1tTXMzMygo6OD4uJiZGdnY8eOHTA1NVX1Lf5lQk29EkKERcUWIZ0QnY33bGpqahAXF4eysjKIRCKYm5vDy8tL7YpToaZeCSHComKLkE6IzsZ7Pgmxlo4QIjxas0VIJ0Rn4z2fqJgiRDPRpzchnRCdjUcIIZqDphEJ6aTobLxnJ5FIcO7cObS0tGDEiBFqt2aLEKKZqNgipBOhs/H+vpycHCxbtgwjRoyAVCpFUVERIiIiKDKBEKJyVGwR0onQ2Xh/36RJkxAVFYV+/foBAK5evYqAgIAnBp4SQohQaM0WIZ1IUlISnY33NzU3N3OFFgD069cPUqlUhXdECCGtaGSLkE6KzsZ7Nv7+/rCyssKUKVMAAAcPHkReXh5iYmJUfGeEkOcdFVuEdHJ0Nt5fc/v2bYSFhSEvLw8ymQxWVlZYsWIF+vTpo+pbI4Q856jYIqSTae9sPEdHR9jZ2XHJ4qSt3NxcjBkzRunasWPH4ODgoKI7IoSQVlRsEdKJ0Nl4z04sFqOxsRFbtmxBUFAQd725uRmxsbE4fvy4Cu+OEEKo2CKkU6Gz8Z7dgQMHcPbsWWRmZsLe3p67rqWlBRsbGzg7O6vw7gghhIotQjoVOhvv7ztz5gysra1VfRuEENIGFVuEEI1QWlqKmJgY1NXVQfFj7YcfflDhXRFCCOVsEUI0RHBwMLy8vGBsbNxm+pUQQlSJii1CiEbQ1dXFtGnTVH0bhBDSBk0jEkI0QlRUFAwMDGBrawsdHR3uuqGhoQrvihBCqNgihGgIxZ2IciKRiHZwEkJUjootQgghhBAe0em2hBCNIJFIsGDBAowePRqWlpYICAjArVu3VH1bhBBCxRYhRDOEhIRg2LBhOHHiBDIzM2FhYYHly5er+rYIIYSKLUKIZrh69SpmzZoFPT099OjRA35+fqiqqlL1bRFCCBVbhBDNIBKJUF1dzX1dVVUFbW1KtyGEqB59EhFCNML8+fPh5eUFCwsLyGQynDt3DmFhYaq+LUIIod2IhBDNIZFIcP78eUilUlhYWKB3796qviVCCKFiixCi3pKTk5/6uru7uyD3QQghT0LFFiFErZmYmKB3796wtrZG165d27y+bt06FdwVIYT8HxVbhBC1VlZWBrFYjNzcXJiYmMDZ2Rk2Njbo0oX2/xBCOgcqtgghGqO4uBhisRj5+fkwNzeHi4sLRo8ererbIoQ856jYIoRonMLCQkRGRqKiogJFRUWqvh1CyHOOii1CiNqTyWQoKChAeno6srOzYWpqCkdHR9jZ2eHFF19U9e0RQp5zVGwRQtRaaGgocnJyYGZmBicnJ9jb26Nbt26qvi1CCOFQsUUIUWsmJibQ19fnRrBEIpHS6ydOnFDFbRFCCIeKLUKIWrt+/fpTX3/11VcFuhNCCGkfFVuEEEIIITyiIBpCCCGEEB5RsUUIIYQQwiMqtgghhBBCeETFFiGEEEIIj6jYIoQQQgjh0f8AD6CFbgHWyGoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_show = min(len(numeric_features),25)\n",
    "\n",
    "corrmat = df_prepro[numeric_features[:n_show]].corr()\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(corrmat, vmax=.9, square=True, ax=ax,cmap=\"Blues\")\n",
    "ax.set_title(\"Correlación Lineal Predictores vs Predictores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_show = min(len(numeric_features),16)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(16,4*int(np.ceil(n_show/4))), nrows=int(np.ceil(n_show/4)), ncols=4)\n",
    "for i,  column in enumerate(numeric_features[:n_show]):\n",
    "    sns.distplot(df_prepro[column], hist=True, rug=False, kde=True, ax=axes[i//4,i%4],label=column)\n",
    "  #sns.kdeplot(x=data[column], y=data['Age'],ax=axes[i//4,i%4])\n",
    "  #sns.scatterplot(data=data,x=column, y='SalePrice',ax=axes[i//4,i%4])\n",
    "    i = i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train[get_columns(0, 'Monto')])\n",
    "\n",
    "X_train_1[get_columns(0, 'Monto')] = scaler.transform(X_train_1[get_columns(0, 'Monto')])\n",
    "X_test_1[get_columns(0, 'Monto')] = scaler.transform(X_test_1[get_columns(0, 'Monto')])\n",
    "\n",
    "X_train[get_columns(0, 'Monto')] = scaler.transform(X_train[get_columns(0, 'Monto')])\n",
    "X_test[get_columns(0, 'Monto')] = scaler.transform(X_test[get_columns(0, 'Monto')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[   0  483]\n",
      " [   4 4436]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.00000   0.00000   0.00000       483\n",
      "   Escritura    0.90181   0.99910   0.94796      4440\n",
      "\n",
      "    accuracy                        0.90108      4923\n",
      "   macro avg    0.45090   0.49955   0.47398      4923\n",
      "weighted avg    0.81333   0.90108   0.85496      4923\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[ 343  140]\n",
      " [1652 2788]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.17193   0.71014   0.27684       483\n",
      "   Escritura    0.95219   0.62793   0.75679      4440\n",
      "\n",
      "    accuracy                        0.63599      4923\n",
      "   macro avg    0.56206   0.66904   0.51681      4923\n",
      "weighted avg    0.87563   0.63599   0.70970      4923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='l2',random_state=1,solver=\"newton-cg\",class_weight=\"balanced\").fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Predicho Probabilidad\n",
      "Escritura Escritura [0.30893327 0.69106673]\n",
      "Escritura Desiste [0.69702287 0.30297713]\n",
      "Escritura Escritura [0.36746659 0.63253341]\n",
      "Escritura Escritura [0.2301537 0.7698463]\n",
      "Escritura Escritura [0.11410932 0.88589068]\n",
      "Escritura Desiste [0.62789116 0.37210884]\n",
      "Escritura Escritura [0.3235042 0.6764958]\n",
      "Escritura Desiste [0.72514232 0.27485768]\n",
      "Escritura Escritura [0.41722941 0.58277059]\n",
      "Escritura Desiste [0.56414692 0.43585308]\n",
      "Escritura Escritura [0.35786977 0.64213023]\n",
      "Escritura Desiste [0.56478022 0.43521978]\n",
      "Escritura Escritura [0.40004606 0.59995394]\n",
      "Escritura Escritura [0.41841624 0.58158376]\n",
      "Escritura Escritura [0.30235729 0.69764271]\n",
      "Escritura Escritura [0.4551372 0.5448628]\n",
      "Escritura Desiste [0.71952534 0.28047466]\n",
      "Escritura Desiste [0.53900126 0.46099874]\n",
      "Escritura Escritura [0.11005515 0.88994485]\n",
      "Escritura Escritura [0.28682183 0.71317817]\n",
      "Escritura Escritura [0.25213293 0.74786707]\n",
      "Escritura Desiste [0.6224088 0.3775912]\n",
      "Escritura Escritura [0.31626501 0.68373499]\n",
      "Escritura Escritura [0.32380054 0.67619946]\n",
      "Escritura Escritura [0.24015447 0.75984553]\n",
      "Escritura Escritura [0.20731601 0.79268399]\n",
      "Escritura Escritura [0.33112704 0.66887296]\n",
      "Escritura Escritura [0.46344891 0.53655109]\n",
      "Escritura Escritura [0.29177729 0.70822271]\n",
      "Escritura Escritura [0.16070938 0.83929062]\n",
      "Escritura Escritura [0.28347949 0.71652051]\n",
      "Escritura Desiste [0.71183466 0.28816534]\n",
      "Escritura Escritura [0.2318131 0.7681869]\n",
      "Escritura Escritura [0.15358654 0.84641346]\n",
      "Escritura Desiste [0.72096442 0.27903558]\n",
      "Escritura Escritura [0.16516532 0.83483468]\n",
      "Escritura Escritura [0.23151101 0.76848899]\n",
      "Escritura Escritura [0.39547827 0.60452173]\n",
      "Escritura Escritura [0.46682801 0.53317199]\n",
      "Escritura Desiste [0.63355542 0.36644458]\n",
      "Escritura Escritura [0.46644484 0.53355516]\n",
      "Escritura Escritura [0.17973008 0.82026992]\n",
      "Escritura Escritura [0.28636069 0.71363931]\n",
      "Escritura Escritura [0.36273666 0.63726334]\n",
      "Escritura Desiste [0.55492446 0.44507554]\n",
      "Escritura Desiste [0.55449525 0.44550475]\n",
      "Escritura Escritura [0.30082175 0.69917825]\n",
      "Escritura Escritura [0.20265607 0.79734393]\n",
      "Escritura Desiste [0.68879736 0.31120264]\n",
      "Escritura Escritura [0.17337202 0.82662798]\n",
      "Escritura Escritura [0.18524202 0.81475798]\n",
      "Escritura Escritura [0.30671158 0.69328842]\n",
      "Escritura Escritura [0.49896881 0.50103119]\n",
      "Escritura Escritura [0.25257539 0.74742461]\n",
      "Escritura Desiste [0.67765612 0.32234388]\n",
      "Escritura Escritura [0.2585136 0.7414864]\n",
      "Escritura Escritura [0.25625979 0.74374021]\n",
      "Escritura Desiste [0.58078329 0.41921671]\n",
      "Escritura Escritura [0.37645506 0.62354494]\n",
      "Escritura Desiste [0.60645448 0.39354552]\n",
      "Escritura Escritura [0.42505605 0.57494395]\n",
      "Escritura Escritura [0.18580189 0.81419811]\n",
      "Escritura Escritura [0.36107016 0.63892984]\n",
      "Escritura Escritura [0.41740997 0.58259003]\n",
      "Escritura Escritura [0.30152505 0.69847495]\n",
      "Escritura Escritura [0.36920404 0.63079596]\n",
      "Escritura Escritura [0.19729266 0.80270734]\n",
      "Escritura Escritura [0.06169456 0.93830544]\n",
      "Escritura Escritura [0.32346324 0.67653676]\n",
      "Escritura Escritura [0.17093882 0.82906118]\n",
      "Escritura Desiste [0.61218997 0.38781003]\n",
      "Escritura Desiste [0.63525003 0.36474997]\n",
      "Escritura Escritura [0.18736816 0.81263184]\n",
      "Escritura Desiste [0.64991441 0.35008559]\n",
      "Escritura Escritura [0.46032106 0.53967894]\n",
      "Escritura Escritura [0.28848181 0.71151819]\n",
      "Escritura Desiste [0.71388886 0.28611114]\n",
      "Escritura Escritura [0.14568212 0.85431788]\n",
      "Escritura Escritura [0.30525455 0.69474545]\n",
      "Escritura Escritura [0.11693934 0.88306066]\n",
      "Escritura Desiste [0.6931739 0.3068261]\n",
      "Escritura Escritura [0.19470925 0.80529075]\n",
      "Escritura Escritura [0.44017371 0.55982629]\n",
      "Escritura Desiste [0.6803942 0.3196058]\n",
      "Escritura Desiste [0.65244151 0.34755849]\n",
      "Escritura Desiste [0.68534351 0.31465649]\n",
      "Escritura Escritura [0.15608883 0.84391117]\n",
      "Escritura Escritura [0.43969712 0.56030288]\n",
      "Escritura Desiste [0.50626339 0.49373661]\n",
      "Escritura Desiste [0.69007804 0.30992196]\n",
      "Escritura Desiste [0.70249911 0.29750089]\n",
      "Escritura Escritura [0.27928615 0.72071385]\n",
      "Escritura Escritura [0.24834526 0.75165474]\n",
      "Escritura Desiste [0.70866643 0.29133357]\n",
      "Escritura Desiste [0.57160014 0.42839986]\n",
      "Escritura Escritura [0.25763919 0.74236081]\n",
      "Escritura Escritura [0.19012248 0.80987752]\n",
      "Escritura Escritura [0.33980353 0.66019647]\n",
      "Escritura Escritura [0.19395841 0.80604159]\n",
      "Escritura Escritura [0.34766905 0.65233095]\n",
      "Escritura Escritura [0.39783799 0.60216201]\n",
      "Escritura Escritura [0.08276047 0.91723953]\n",
      "Escritura Desiste [0.70560893 0.29439107]\n",
      "Escritura Desiste [0.67161599 0.32838401]\n",
      "Escritura Desiste [0.69732342 0.30267658]\n",
      "Escritura Escritura [0.22417079 0.77582921]\n",
      "Escritura Desiste [0.65488805 0.34511195]\n",
      "Escritura Escritura [0.21060709 0.78939291]\n",
      "Escritura Desiste [0.56342578 0.43657422]\n",
      "Escritura Escritura [0.26490507 0.73509493]\n",
      "Escritura Escritura [0.1801362 0.8198638]\n",
      "Escritura Escritura [0.33246216 0.66753784]\n",
      "Escritura Escritura [0.19315681 0.80684319]\n",
      "Escritura Escritura [0.20450499 0.79549501]\n",
      "Escritura Escritura [0.31911033 0.68088967]\n",
      "Escritura Escritura [0.40948167 0.59051833]\n",
      "Escritura Escritura [0.49180036 0.50819964]\n",
      "Escritura Escritura [0.19498952 0.80501048]\n",
      "Escritura Desiste [0.59448767 0.40551233]\n",
      "Escritura Escritura [0.32000604 0.67999396]\n",
      "Escritura Desiste [0.60278604 0.39721396]\n",
      "Escritura Escritura [0.45707841 0.54292159]\n",
      "Escritura Escritura [0.44728195 0.55271805]\n",
      "Escritura Escritura [0.26218876 0.73781124]\n",
      "Escritura Desiste [0.7558023 0.2441977]\n",
      "Escritura Escritura [0.21601806 0.78398194]\n",
      "Escritura Desiste [0.59699175 0.40300825]\n",
      "Escritura Escritura [0.38533976 0.61466024]\n",
      "Escritura Escritura [0.3993884 0.6006116]\n",
      "Escritura Escritura [0.31007839 0.68992161]\n",
      "Escritura Desiste [0.66225883 0.33774117]\n",
      "Escritura Escritura [0.28582396 0.71417604]\n",
      "Escritura Desiste [0.76270808 0.23729192]\n",
      "Escritura Desiste [0.64381334 0.35618666]\n",
      "Escritura Desiste [0.67125502 0.32874498]\n",
      "Escritura Desiste [0.77502003 0.22497997]\n",
      "Escritura Escritura [0.22657743 0.77342257]\n",
      "Escritura Escritura [0.49483296 0.50516704]\n",
      "Escritura Escritura [0.34520022 0.65479978]\n",
      "Escritura Desiste [0.63458408 0.36541592]\n",
      "Escritura Escritura [0.24489458 0.75510542]\n",
      "Escritura Escritura [0.41307753 0.58692247]\n",
      "Escritura Escritura [0.26131218 0.73868782]\n",
      "Escritura Escritura [0.41836697 0.58163303]\n",
      "Escritura Desiste [0.73072526 0.26927474]\n",
      "Escritura Desiste [0.6247245 0.3752755]\n",
      "Escritura Escritura [0.31942265 0.68057735]\n",
      "Escritura Escritura [0.32948742 0.67051258]\n",
      "Escritura Escritura [0.19661915 0.80338085]\n",
      "Escritura Escritura [0.34135509 0.65864491]\n",
      "Escritura Desiste [0.61221026 0.38778974]\n",
      "Escritura Desiste [0.6370805 0.3629195]\n",
      "Escritura Escritura [0.4457636 0.5542364]\n",
      "Escritura Escritura [0.44404814 0.55595186]\n",
      "Escritura Escritura [0.3678615 0.6321385]\n",
      "Escritura Desiste [0.75110333 0.24889667]\n",
      "Escritura Escritura [0.46878901 0.53121099]\n",
      "Escritura Escritura [0.19716938 0.80283062]\n",
      "Escritura Desiste [0.73480264 0.26519736]\n",
      "Escritura Desiste [0.6435308 0.3564692]\n",
      "Escritura Escritura [0.17620155 0.82379845]\n",
      "Escritura Escritura [0.25394389 0.74605611]\n",
      "Escritura Escritura [0.37077823 0.62922177]\n",
      "Escritura Escritura [0.31694186 0.68305814]\n",
      "Escritura Escritura [0.40470214 0.59529786]\n",
      "Escritura Desiste [0.60977235 0.39022765]\n",
      "Escritura Desiste [0.68536412 0.31463588]\n",
      "Escritura Escritura [0.39579924 0.60420076]\n",
      "Escritura Escritura [0.10780557 0.89219443]\n",
      "Escritura Desiste [0.61232967 0.38767033]\n",
      "Escritura Desiste [0.62152645 0.37847355]\n",
      "Escritura Escritura [0.27056599 0.72943401]\n",
      "Escritura Desiste [0.69195685 0.30804315]\n",
      "Escritura Escritura [0.31758486 0.68241514]\n",
      "Escritura Desiste [0.58649547 0.41350453]\n",
      "Escritura Desiste [0.62695241 0.37304759]\n",
      "Escritura Desiste [0.64941131 0.35058869]\n",
      "Escritura Desiste [0.59213329 0.40786671]\n",
      "Escritura Desiste [0.55563485 0.44436515]\n",
      "Escritura Escritura [0.27695534 0.72304466]\n",
      "Escritura Escritura [0.09507976 0.90492024]\n",
      "Escritura Desiste [0.5595354 0.4404646]\n",
      "Escritura Escritura [0.36584605 0.63415395]\n",
      "Escritura Escritura [0.39523392 0.60476608]\n",
      "Escritura Escritura [0.25564518 0.74435482]\n",
      "Escritura Escritura [0.29965238 0.70034762]\n",
      "Escritura Escritura [0.27642459 0.72357541]\n",
      "Escritura Escritura [0.34326693 0.65673307]\n",
      "Escritura Escritura [0.35189752 0.64810248]\n",
      "Escritura Desiste [0.63834741 0.36165259]\n",
      "Escritura Desiste [0.64136897 0.35863103]\n",
      "Escritura Escritura [0.4162023 0.5837977]\n",
      "Escritura Desiste [0.72010444 0.27989556]\n",
      "Escritura Escritura [0.29168603 0.70831397]\n",
      "Escritura Escritura [0.3013836 0.6986164]\n",
      "Escritura Desiste [0.69953075 0.30046925]\n",
      "Escritura Desiste [0.73713743 0.26286257]\n",
      "Escritura Escritura [0.0559874 0.9440126]\n",
      "Escritura Desiste [0.69669214 0.30330786]\n",
      "Escritura Escritura [0.30853355 0.69146645]\n",
      "Escritura Desiste [0.69469457 0.30530543]\n",
      "Escritura Escritura [0.28651539 0.71348461]\n",
      "Escritura Escritura [0.1886964 0.8113036]\n",
      "Escritura Desiste [0.67365447 0.32634553]\n",
      "Escritura Escritura [0.27962733 0.72037267]\n",
      "Escritura Escritura [0.30521284 0.69478716]\n",
      "Escritura Desiste [0.62902356 0.37097644]\n",
      "Escritura Escritura [0.14722903 0.85277097]\n",
      "Escritura Desiste [0.57141046 0.42858954]\n",
      "Escritura Escritura [0.43394846 0.56605154]\n",
      "Escritura Escritura [0.22349977 0.77650023]\n",
      "Escritura Escritura [0.3458427 0.6541573]\n",
      "Escritura Escritura [0.33451373 0.66548627]\n",
      "Escritura Escritura [0.31448086 0.68551914]\n",
      "Escritura Desiste [0.78564609 0.21435391]\n",
      "Escritura Escritura [0.22194727 0.77805273]\n",
      "Escritura Desiste [0.69911243 0.30088757]\n",
      "Escritura Escritura [0.05955302 0.94044698]\n",
      "Escritura Desiste [0.61092554 0.38907446]\n",
      "Escritura Escritura [0.28343461 0.71656539]\n",
      "Escritura Escritura [0.1177169 0.8822831]\n",
      "Escritura Desiste [0.72335475 0.27664525]\n",
      "Escritura Escritura [0.13027617 0.86972383]\n",
      "Escritura Escritura [0.49443767 0.50556233]\n",
      "Escritura Desiste [0.74788253 0.25211747]\n",
      "Escritura Desiste [0.89630315 0.10369685]\n",
      "Escritura Escritura [0.19970419 0.80029581]\n",
      "Escritura Escritura [0.24494857 0.75505143]\n",
      "Escritura Desiste [0.55100421 0.44899579]\n",
      "Escritura Escritura [0.16591769 0.83408231]\n",
      "Escritura Escritura [0.21377239 0.78622761]\n",
      "Escritura Escritura [0.37613164 0.62386836]\n",
      "Escritura Escritura [0.33863789 0.66136211]\n",
      "Escritura Desiste [0.61514411 0.38485589]\n",
      "Escritura Desiste [0.71249361 0.28750639]\n",
      "Escritura Desiste [0.59310123 0.40689877]\n",
      "Escritura Desiste [0.62113184 0.37886816]\n",
      "Escritura Desiste [0.62963256 0.37036744]\n",
      "Escritura Desiste [0.62251262 0.37748738]\n",
      "Escritura Escritura [0.33176273 0.66823727]\n",
      "Escritura Desiste [0.65777563 0.34222437]\n",
      "Escritura Escritura [0.3575445 0.6424555]\n",
      "Escritura Desiste [0.76316547 0.23683453]\n",
      "Escritura Escritura [0.29932424 0.70067576]\n",
      "Escritura Desiste [0.62533359 0.37466641]\n",
      "Escritura Desiste [0.55840658 0.44159342]\n",
      "Escritura Desiste [0.55420192 0.44579808]\n",
      "Escritura Escritura [0.45547061 0.54452939]\n",
      "Escritura Escritura [0.20953764 0.79046236]\n",
      "Escritura Escritura [0.30532919 0.69467081]\n",
      "Escritura Desiste [0.8265779 0.1734221]\n",
      "Escritura Escritura [0.26947673 0.73052327]\n",
      "Escritura Escritura [0.30332949 0.69667051]\n",
      "Escritura Desiste [0.65166668 0.34833332]\n",
      "Escritura Desiste [0.67373317 0.32626683]\n",
      "Escritura Desiste [0.56891937 0.43108063]\n",
      "Escritura Desiste [0.56773169 0.43226831]\n",
      "Escritura Desiste [0.50496709 0.49503291]\n",
      "Escritura Escritura [0.44164474 0.55835526]\n",
      "Escritura Desiste [0.59944239 0.40055761]\n",
      "Escritura Escritura [0.23659819 0.76340181]\n",
      "Escritura Escritura [0.3790972 0.6209028]\n",
      "Escritura Escritura [0.34686936 0.65313064]\n",
      "Escritura Desiste [0.51460245 0.48539755]\n",
      "Escritura Desiste [0.64043102 0.35956898]\n",
      "Escritura Escritura [0.41173392 0.58826608]\n",
      "Escritura Escritura [0.35501449 0.64498551]\n",
      "Escritura Escritura [0.28209586 0.71790414]\n",
      "Escritura Escritura [0.30625095 0.69374905]\n",
      "Escritura Escritura [0.33416907 0.66583093]\n",
      "Escritura Desiste [0.74224735 0.25775265]\n",
      "Escritura Escritura [0.2021949 0.7978051]\n",
      "Escritura Escritura [0.48471113 0.51528887]\n",
      "Escritura Desiste [0.65352574 0.34647426]\n",
      "Escritura Escritura [0.34525335 0.65474665]\n",
      "Escritura Escritura [0.31285182 0.68714818]\n",
      "Escritura Escritura [0.2291313 0.7708687]\n",
      "Escritura Escritura [0.39252311 0.60747689]\n",
      "Escritura Escritura [0.11090869 0.88909131]\n",
      "Escritura Desiste [0.59340288 0.40659712]\n",
      "Escritura Desiste [0.55802026 0.44197974]\n",
      "Escritura Escritura [0.29693446 0.70306554]\n",
      "Escritura Escritura [0.25338727 0.74661273]\n",
      "Escritura Desiste [0.57016141 0.42983859]\n",
      "Escritura Desiste [0.53723298 0.46276702]\n",
      "Escritura Escritura [0.3106421 0.6893579]\n",
      "Escritura Escritura [0.04906509 0.95093491]\n",
      "Escritura Desiste [0.64939696 0.35060304]\n",
      "Escritura Desiste [0.6042217 0.3957783]\n",
      "Escritura Escritura [0.37694856 0.62305144]\n",
      "Escritura Escritura [0.3503815 0.6496185]\n",
      "Escritura Escritura [0.36873546 0.63126454]\n",
      "Escritura Escritura [0.11847338 0.88152662]\n",
      "Escritura Escritura [0.34522514 0.65477486]\n",
      "Escritura Desiste [0.78430948 0.21569052]\n",
      "Escritura Escritura [0.06280676 0.93719324]\n",
      "Escritura Desiste [0.52236952 0.47763048]\n",
      "Escritura Escritura [0.44686231 0.55313769]\n",
      "Escritura Escritura [0.3235073 0.6764927]\n",
      "Escritura Escritura [0.35671932 0.64328068]\n",
      "Escritura Escritura [0.17887977 0.82112023]\n",
      "Escritura Escritura [0.1087467 0.8912533]\n",
      "Escritura Escritura [0.34880575 0.65119425]\n",
      "Escritura Escritura [0.31956968 0.68043032]\n",
      "Escritura Desiste [0.63322275 0.36677725]\n",
      "Escritura Escritura [0.16869877 0.83130123]\n",
      "Escritura Desiste [0.83888081 0.16111919]\n",
      "Escritura Escritura [0.15658113 0.84341887]\n",
      "Escritura Desiste [0.68270842 0.31729158]\n",
      "Escritura Escritura [0.29232632 0.70767368]\n",
      "Escritura Desiste [0.53141664 0.46858336]\n",
      "Escritura Escritura [0.3621524 0.6378476]\n",
      "Escritura Escritura [0.34991914 0.65008086]\n",
      "Escritura Escritura [0.24546515 0.75453485]\n",
      "Escritura Desiste [0.56932272 0.43067728]\n",
      "Escritura Escritura [0.29065518 0.70934482]\n",
      "Escritura Escritura [0.41834659 0.58165341]\n",
      "Escritura Desiste [0.72035911 0.27964089]\n",
      "Escritura Escritura [0.48425617 0.51574383]\n",
      "Escritura Escritura [0.25177011 0.74822989]\n",
      "Escritura Escritura [0.19983703 0.80016297]\n",
      "Escritura Desiste [0.58714169 0.41285831]\n",
      "Escritura Desiste [0.67313592 0.32686408]\n",
      "Escritura Desiste [0.51578979 0.48421021]\n",
      "Escritura Escritura [0.27501118 0.72498882]\n",
      "Escritura Escritura [0.33157935 0.66842065]\n",
      "Escritura Escritura [0.28814818 0.71185182]\n",
      "Escritura Desiste [0.74171019 0.25828981]\n",
      "Escritura Escritura [0.24744465 0.75255535]\n",
      "Escritura Escritura [0.43972193 0.56027807]\n",
      "Escritura Escritura [0.47156937 0.52843063]\n",
      "Escritura Escritura [0.2724892 0.7275108]\n",
      "Escritura Escritura [0.28505967 0.71494033]\n",
      "Escritura Escritura [0.36367148 0.63632852]\n",
      "Escritura Desiste [0.54183432 0.45816568]\n",
      "Escritura Escritura [0.2112586 0.7887414]\n",
      "Escritura Escritura [0.29513857 0.70486143]\n",
      "Escritura Escritura [0.19303073 0.80696927]\n",
      "Escritura Desiste [0.64740563 0.35259437]\n",
      "Escritura Escritura [0.28595669 0.71404331]\n",
      "Escritura Escritura [0.24897158 0.75102842]\n",
      "Escritura Escritura [0.25190464 0.74809536]\n",
      "Escritura Desiste [0.5055365 0.4944635]\n",
      "Escritura Escritura [0.38522781 0.61477219]\n",
      "Escritura Desiste [0.59460328 0.40539672]\n",
      "Escritura Escritura [0.19914275 0.80085725]\n",
      "Escritura Escritura [0.1955809 0.8044191]\n",
      "Escritura Escritura [0.2964706 0.7035294]\n",
      "Escritura Desiste [0.62701263 0.37298737]\n",
      "Escritura Escritura [0.06169976 0.93830024]\n",
      "Escritura Desiste [0.67511642 0.32488358]\n",
      "Escritura Escritura [0.45310103 0.54689897]\n",
      "Escritura Desiste [0.74647082 0.25352918]\n",
      "Escritura Escritura [0.45074734 0.54925266]\n",
      "Escritura Escritura [0.45804136 0.54195864]\n",
      "Escritura Desiste [0.57616781 0.42383219]\n",
      "Escritura Desiste [0.5680375 0.4319625]\n",
      "Escritura Escritura [0.22441888 0.77558112]\n",
      "Escritura Escritura [0.30669311 0.69330689]\n",
      "Escritura Escritura [0.31278409 0.68721591]\n",
      "Escritura Desiste [0.57843369 0.42156631]\n",
      "Escritura Escritura [0.30892488 0.69107512]\n",
      "Escritura Escritura [0.35730445 0.64269555]\n",
      "Escritura Escritura [0.20593228 0.79406772]\n",
      "Escritura Escritura [0.34095453 0.65904547]\n",
      "Escritura Escritura [0.33861753 0.66138247]\n",
      "Escritura Desiste [0.57025744 0.42974256]\n",
      "Escritura Desiste [0.57966612 0.42033388]\n",
      "Escritura Escritura [0.36429173 0.63570827]\n",
      "Escritura Escritura [0.1854577 0.8145423]\n",
      "Escritura Escritura [0.3633138 0.6366862]\n",
      "Escritura Escritura [0.23258452 0.76741548]\n",
      "Escritura Escritura [0.38983014 0.61016986]\n",
      "Escritura Desiste [0.57147461 0.42852539]\n",
      "Escritura Desiste [0.62449545 0.37550455]\n",
      "Escritura Escritura [0.33615226 0.66384774]\n",
      "Escritura Escritura [0.2945149 0.7054851]\n",
      "Escritura Desiste [0.52255386 0.47744614]\n",
      "Escritura Desiste [0.59362535 0.40637465]\n",
      "Escritura Desiste [0.61471411 0.38528589]\n",
      "Escritura Escritura [0.28017704 0.71982296]\n",
      "Escritura Desiste [0.65874064 0.34125936]\n",
      "Escritura Desiste [0.54582536 0.45417464]\n",
      "Escritura Desiste [0.63144021 0.36855979]\n",
      "Escritura Escritura [0.27023263 0.72976737]\n",
      "Escritura Escritura [0.35564601 0.64435399]\n",
      "Escritura Desiste [0.52761359 0.47238641]\n",
      "Escritura Escritura [0.29592051 0.70407949]\n",
      "Escritura Escritura [0.22613944 0.77386056]\n",
      "Escritura Desiste [0.66635806 0.33364194]\n",
      "Escritura Escritura [0.3137867 0.6862133]\n",
      "Escritura Escritura [0.47548355 0.52451645]\n",
      "Escritura Desiste [0.61226646 0.38773354]\n",
      "Escritura Desiste [0.53736969 0.46263031]\n",
      "Escritura Desiste [0.84237478 0.15762522]\n",
      "Escritura Desiste [0.61956652 0.38043348]\n",
      "Escritura Escritura [0.07635972 0.92364028]\n",
      "Escritura Escritura [0.12562912 0.87437088]\n",
      "Escritura Desiste [0.71567221 0.28432779]\n",
      "Escritura Desiste [0.55444389 0.44555611]\n",
      "Escritura Escritura [0.4777878 0.5222122]\n",
      "Escritura Escritura [0.31860497 0.68139503]\n",
      "Escritura Escritura [0.47636426 0.52363574]\n",
      "Escritura Escritura [0.26997948 0.73002052]\n",
      "Escritura Desiste [0.81275215 0.18724785]\n",
      "Escritura Desiste [0.69834404 0.30165596]\n",
      "Escritura Escritura [0.21421569 0.78578431]\n",
      "Escritura Desiste [0.58129695 0.41870305]\n",
      "Escritura Escritura [0.30326032 0.69673968]\n",
      "Escritura Escritura [0.20369095 0.79630905]\n",
      "Escritura Escritura [0.28793535 0.71206465]\n",
      "Escritura Escritura [0.1961884 0.8038116]\n",
      "Escritura Escritura [0.39373452 0.60626548]\n",
      "Escritura Escritura [0.21753892 0.78246108]\n",
      "Escritura Escritura [0.25895786 0.74104214]\n",
      "Escritura Desiste [0.52025144 0.47974856]\n",
      "Escritura Desiste [0.60385734 0.39614266]\n",
      "Escritura Desiste [0.70052283 0.29947717]\n",
      "Escritura Desiste [0.57235941 0.42764059]\n",
      "Escritura Desiste [0.67367371 0.32632629]\n",
      "Escritura Escritura [0.32804142 0.67195858]\n",
      "Escritura Escritura [0.34902869 0.65097131]\n",
      "Escritura Escritura [0.35866502 0.64133498]\n",
      "Escritura Escritura [0.37338756 0.62661244]\n",
      "Escritura Desiste [0.75697637 0.24302363]\n",
      "Escritura Escritura [0.21332381 0.78667619]\n",
      "Escritura Desiste [0.76352354 0.23647646]\n",
      "Escritura Escritura [0.19389847 0.80610153]\n",
      "Escritura Escritura [0.41279735 0.58720265]\n",
      "Escritura Escritura [0.17286729 0.82713271]\n",
      "Escritura Desiste [0.68029971 0.31970029]\n",
      "Escritura Desiste [0.73283674 0.26716326]\n",
      "Escritura Escritura [0.38268438 0.61731562]\n",
      "Escritura Escritura [0.24034614 0.75965386]\n",
      "Escritura Escritura [0.26869212 0.73130788]\n",
      "Escritura Desiste [0.70404 0.29596]\n",
      "Escritura Desiste [0.63415132 0.36584868]\n",
      "Escritura Escritura [0.2399762 0.7600238]\n",
      "Escritura Desiste [0.70777992 0.29222008]\n",
      "Escritura Escritura [0.26037874 0.73962126]\n",
      "Escritura Desiste [0.5797755 0.4202245]\n",
      "Escritura Desiste [0.60708518 0.39291482]\n",
      "Escritura Escritura [0.3271625 0.6728375]\n",
      "Escritura Desiste [0.68123351 0.31876649]\n",
      "Escritura Escritura [0.2759599 0.7240401]\n",
      "Escritura Escritura [0.44771469 0.55228531]\n",
      "Escritura Desiste [0.67620621 0.32379379]\n",
      "Escritura Escritura [0.23799617 0.76200383]\n",
      "Escritura Desiste [0.67427373 0.32572627]\n",
      "Escritura Escritura [0.33677528 0.66322472]\n",
      "Escritura Escritura [0.24127449 0.75872551]\n",
      "Escritura Escritura [0.46184004 0.53815996]\n",
      "Escritura Escritura [0.2424472 0.7575528]\n",
      "Escritura Escritura [0.20066395 0.79933605]\n",
      "Escritura Escritura [0.22684311 0.77315689]\n",
      "Escritura Escritura [0.39814115 0.60185885]\n",
      "Escritura Escritura [0.23838391 0.76161609]\n",
      "Escritura Escritura [0.20462498 0.79537502]\n",
      "Escritura Escritura [0.33250077 0.66749923]\n",
      "Escritura Desiste [0.62641818 0.37358182]\n",
      "Escritura Desiste [0.69783516 0.30216484]\n",
      "Escritura Escritura [0.17214335 0.82785665]\n",
      "Escritura Escritura [0.30172994 0.69827006]\n",
      "Escritura Escritura [0.24251831 0.75748169]\n",
      "Escritura Escritura [0.1681959 0.8318041]\n",
      "Escritura Escritura [0.41466948 0.58533052]\n",
      "Escritura Escritura [0.27306524 0.72693476]\n",
      "Escritura Escritura [0.27433034 0.72566966]\n",
      "Escritura Desiste [0.7875858 0.2124142]\n",
      "Escritura Escritura [0.25185734 0.74814266]\n",
      "Escritura Escritura [0.39332923 0.60667077]\n",
      "Escritura Escritura [0.27199156 0.72800844]\n",
      "Escritura Desiste [0.67340628 0.32659372]\n",
      "Escritura Desiste [0.62200742 0.37799258]\n",
      "Escritura Escritura [0.34108165 0.65891835]\n",
      "Escritura Desiste [0.51041369 0.48958631]\n",
      "Escritura Desiste [0.65957225 0.34042775]\n",
      "Escritura Escritura [0.20765463 0.79234537]\n",
      "Escritura Escritura [0.30296049 0.69703951]\n",
      "Escritura Escritura [0.46965448 0.53034552]\n",
      "Escritura Escritura [0.15305187 0.84694813]\n",
      "Escritura Escritura [0.44708679 0.55291321]\n",
      "Escritura Escritura [0.24609999 0.75390001]\n",
      "Escritura Escritura [0.28042253 0.71957747]\n",
      "Escritura Escritura [0.14188836 0.85811164]\n",
      "Escritura Desiste [0.61257097 0.38742903]\n",
      "Escritura Escritura [0.29412135 0.70587865]\n",
      "Escritura Desiste [0.6822551 0.3177449]\n",
      "Escritura Desiste [0.5030009 0.4969991]\n",
      "Escritura Escritura [0.25585201 0.74414799]\n",
      "Escritura Escritura [0.45396839 0.54603161]\n",
      "Escritura Escritura [0.24846931 0.75153069]\n",
      "Escritura Escritura [0.36304411 0.63695589]\n",
      "Escritura Escritura [0.3117579 0.6882421]\n",
      "Escritura Desiste [0.64191994 0.35808006]\n",
      "Escritura Desiste [0.72659159 0.27340841]\n",
      "Escritura Desiste [0.55518645 0.44481355]\n",
      "Escritura Escritura [0.26294135 0.73705865]\n",
      "Escritura Desiste [0.60865564 0.39134436]\n",
      "Escritura Escritura [0.10859555 0.89140445]\n",
      "Escritura Desiste [0.66290412 0.33709588]\n",
      "Escritura Escritura [0.1617945 0.8382055]\n",
      "Escritura Desiste [0.64465027 0.35534973]\n",
      "Escritura Desiste [0.50989009 0.49010991]\n",
      "Escritura Desiste [0.70485211 0.29514789]\n",
      "Escritura Escritura [0.27742801 0.72257199]\n",
      "Escritura Escritura [0.31990049 0.68009951]\n",
      "Escritura Escritura [0.26942188 0.73057812]\n",
      "Escritura Desiste [0.59579588 0.40420412]\n",
      "Escritura Desiste [0.6337796 0.3662204]\n",
      "Escritura Escritura [0.3090769 0.6909231]\n",
      "Escritura Desiste [0.69672443 0.30327557]\n",
      "Escritura Desiste [0.69219746 0.30780254]\n",
      "Escritura Desiste [0.67670364 0.32329636]\n",
      "Escritura Escritura [0.16465871 0.83534129]\n",
      "Escritura Escritura [0.26035409 0.73964591]\n",
      "Escritura Desiste [0.68436187 0.31563813]\n",
      "Escritura Desiste [0.62570803 0.37429197]\n",
      "Escritura Desiste [0.74537665 0.25462335]\n",
      "Escritura Escritura [0.24207374 0.75792626]\n",
      "Escritura Desiste [0.68950146 0.31049854]\n",
      "Escritura Escritura [0.21900551 0.78099449]\n",
      "Escritura Desiste [0.75913696 0.24086304]\n",
      "Escritura Desiste [0.54567395 0.45432605]\n",
      "Escritura Desiste [0.64641153 0.35358847]\n",
      "Escritura Escritura [0.30798294 0.69201706]\n",
      "Escritura Escritura [0.31354664 0.68645336]\n",
      "Escritura Desiste [0.60411845 0.39588155]\n",
      "Escritura Desiste [0.63868298 0.36131702]\n",
      "Escritura Escritura [0.23421014 0.76578986]\n",
      "Escritura Escritura [0.49214733 0.50785267]\n",
      "Escritura Desiste [0.54224701 0.45775299]\n",
      "Escritura Escritura [0.28141132 0.71858868]\n",
      "Escritura Escritura [0.3068528 0.6931472]\n",
      "Escritura Escritura [0.36085109 0.63914891]\n",
      "Escritura Escritura [0.21150159 0.78849841]\n",
      "Escritura Desiste [0.58139421 0.41860579]\n",
      "Escritura Desiste [0.57064488 0.42935512]\n",
      "Escritura Escritura [0.16702388 0.83297612]\n",
      "Escritura Desiste [0.50789104 0.49210896]\n",
      "Escritura Escritura [0.22616909 0.77383091]\n",
      "Escritura Escritura [0.21058579 0.78941421]\n",
      "Escritura Desiste [0.75783573 0.24216427]\n",
      "Escritura Desiste [0.70518189 0.29481811]\n",
      "Escritura Escritura [0.29631184 0.70368816]\n",
      "Escritura Escritura [0.43277938 0.56722062]\n",
      "Escritura Escritura [0.37134815 0.62865185]\n",
      "Escritura Desiste [0.61736834 0.38263166]\n",
      "Escritura Escritura [0.19030578 0.80969422]\n",
      "Escritura Desiste [0.65637218 0.34362782]\n",
      "Escritura Escritura [0.34785248 0.65214752]\n",
      "Escritura Escritura [0.15326935 0.84673065]\n",
      "Escritura Escritura [0.2114774 0.7885226]\n",
      "Escritura Desiste [0.57551384 0.42448616]\n",
      "Escritura Escritura [0.34372176 0.65627824]\n",
      "Escritura Escritura [0.23791211 0.76208789]\n",
      "Escritura Escritura [0.24942614 0.75057386]\n",
      "Escritura Desiste [0.62227964 0.37772036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Desiste [0.67608726 0.32391274]\n",
      "Escritura Escritura [0.30428075 0.69571925]\n",
      "Escritura Escritura [0.37542883 0.62457117]\n",
      "Escritura Escritura [0.33088718 0.66911282]\n",
      "Escritura Escritura [0.36170974 0.63829026]\n",
      "Escritura Escritura [0.25862144 0.74137856]\n",
      "Escritura Desiste [0.68877691 0.31122309]\n",
      "Escritura Escritura [0.25066423 0.74933577]\n",
      "Escritura Escritura [0.23962757 0.76037243]\n",
      "Escritura Escritura [0.3371546 0.6628454]\n",
      "Escritura Escritura [0.48660242 0.51339758]\n",
      "Escritura Escritura [0.21864797 0.78135203]\n",
      "Escritura Escritura [0.32730266 0.67269734]\n",
      "Escritura Escritura [0.34588136 0.65411864]\n",
      "Escritura Desiste [0.75787723 0.24212277]\n",
      "Escritura Desiste [0.60263529 0.39736471]\n",
      "Escritura Desiste [0.70809625 0.29190375]\n",
      "Escritura Escritura [0.25752452 0.74247548]\n",
      "Escritura Desiste [0.60535771 0.39464229]\n",
      "Escritura Escritura [0.24509901 0.75490099]\n",
      "Escritura Escritura [0.22212551 0.77787449]\n",
      "Escritura Desiste [0.74922473 0.25077527]\n",
      "Escritura Desiste [0.52955045 0.47044955]\n",
      "Escritura Escritura [0.22530089 0.77469911]\n",
      "Escritura Escritura [0.22890784 0.77109216]\n",
      "Escritura Desiste [0.73262185 0.26737815]\n",
      "Escritura Escritura [0.29744921 0.70255079]\n",
      "Escritura Escritura [0.37608373 0.62391627]\n",
      "Escritura Escritura [0.30685261 0.69314739]\n",
      "Escritura Escritura [0.20712244 0.79287756]\n",
      "Escritura Desiste [0.74688645 0.25311355]\n",
      "Escritura Desiste [0.64424103 0.35575897]\n",
      "Escritura Escritura [0.25958057 0.74041943]\n",
      "Escritura Desiste [0.69808682 0.30191318]\n",
      "Escritura Escritura [0.07005691 0.92994309]\n",
      "Escritura Desiste [0.60051753 0.39948247]\n",
      "Escritura Escritura [0.3231399 0.6768601]\n",
      "Escritura Desiste [0.68209525 0.31790475]\n",
      "Escritura Escritura [0.21831579 0.78168421]\n",
      "Escritura Escritura [0.33789981 0.66210019]\n",
      "Escritura Escritura [0.26451637 0.73548363]\n",
      "Escritura Desiste [0.73671995 0.26328005]\n",
      "Escritura Desiste [0.61063792 0.38936208]\n",
      "Escritura Escritura [0.30921349 0.69078651]\n",
      "Escritura Desiste [0.5902161 0.4097839]\n",
      "Escritura Escritura [0.23647979 0.76352021]\n",
      "Escritura Desiste [0.57745128 0.42254872]\n",
      "Escritura Escritura [0.36198896 0.63801104]\n",
      "Escritura Escritura [0.33599958 0.66400042]\n",
      "Escritura Desiste [0.62948412 0.37051588]\n",
      "Escritura Escritura [0.42618157 0.57381843]\n",
      "Escritura Escritura [0.24357826 0.75642174]\n",
      "Escritura Escritura [0.09378016 0.90621984]\n",
      "Escritura Escritura [0.2740534 0.7259466]\n",
      "Escritura Escritura [0.11741442 0.88258558]\n",
      "Escritura Escritura [0.24377174 0.75622826]\n",
      "Escritura Escritura [0.24255715 0.75744285]\n",
      "Escritura Desiste [0.6125496 0.3874504]\n",
      "Escritura Desiste [0.64997445 0.35002555]\n",
      "Escritura Escritura [0.09495275 0.90504725]\n",
      "Escritura Escritura [0.32892478 0.67107522]\n",
      "Escritura Escritura [0.23085703 0.76914297]\n",
      "Escritura Escritura [0.30576548 0.69423452]\n",
      "Escritura Escritura [0.30454079 0.69545921]\n",
      "Escritura Desiste [0.61569586 0.38430414]\n",
      "Escritura Desiste [0.61863099 0.38136901]\n",
      "Escritura Desiste [0.50332875 0.49667125]\n",
      "Escritura Escritura [0.18392974 0.81607026]\n",
      "Escritura Escritura [0.23721773 0.76278227]\n",
      "Escritura Escritura [0.26107221 0.73892779]\n",
      "Escritura Escritura [0.30515341 0.69484659]\n",
      "Escritura Desiste [0.79510244 0.20489756]\n",
      "Escritura Escritura [0.38250589 0.61749411]\n",
      "Escritura Escritura [0.22929387 0.77070613]\n",
      "Escritura Escritura [0.19447589 0.80552411]\n",
      "Escritura Escritura [0.41482716 0.58517284]\n",
      "Escritura Desiste [0.60940368 0.39059632]\n",
      "Escritura Escritura [0.24543221 0.75456779]\n",
      "Escritura Escritura [0.27024765 0.72975235]\n",
      "Escritura Desiste [0.63949749 0.36050251]\n",
      "Escritura Escritura [0.22069147 0.77930853]\n",
      "Escritura Desiste [0.77430889 0.22569111]\n",
      "Escritura Escritura [0.35225685 0.64774315]\n",
      "Escritura Escritura [0.43565798 0.56434202]\n",
      "Escritura Escritura [0.27168855 0.72831145]\n",
      "Escritura Escritura [0.37922808 0.62077192]\n",
      "Escritura Escritura [0.25804421 0.74195579]\n",
      "Escritura Escritura [0.3146419 0.6853581]\n",
      "Escritura Escritura [0.21100819 0.78899181]\n",
      "Escritura Escritura [0.28414024 0.71585976]\n",
      "Escritura Escritura [0.13901218 0.86098782]\n",
      "Escritura Escritura [0.22195466 0.77804534]\n",
      "Escritura Escritura [0.29839897 0.70160103]\n",
      "Escritura Escritura [0.11254467 0.88745533]\n",
      "Escritura Desiste [0.65380661 0.34619339]\n",
      "Escritura Escritura [0.15561928 0.84438072]\n",
      "Escritura Escritura [0.14982494 0.85017506]\n",
      "Escritura Escritura [0.41679134 0.58320866]\n",
      "Escritura Desiste [0.59360845 0.40639155]\n",
      "Escritura Escritura [0.2409888 0.7590112]\n",
      "Escritura Escritura [0.26767606 0.73232394]\n",
      "Escritura Escritura [0.33439606 0.66560394]\n",
      "Escritura Escritura [0.2778732 0.7221268]\n",
      "Escritura Escritura [0.07098585 0.92901415]\n",
      "Escritura Desiste [0.62959014 0.37040986]\n",
      "Escritura Escritura [0.2354093 0.7645907]\n",
      "Escritura Desiste [0.62115306 0.37884694]\n",
      "Escritura Escritura [0.27087266 0.72912734]\n",
      "Escritura Escritura [0.23511947 0.76488053]\n",
      "Escritura Desiste [0.53940778 0.46059222]\n",
      "Escritura Escritura [0.15410202 0.84589798]\n",
      "Escritura Escritura [0.21304694 0.78695306]\n",
      "Escritura Escritura [0.40176347 0.59823653]\n",
      "Escritura Escritura [0.31285844 0.68714156]\n",
      "Escritura Escritura [0.43376234 0.56623766]\n",
      "Escritura Escritura [0.31212968 0.68787032]\n",
      "Escritura Desiste [0.66368646 0.33631354]\n",
      "Escritura Escritura [0.30365704 0.69634296]\n",
      "Escritura Escritura [0.24030035 0.75969965]\n",
      "Escritura Escritura [0.29542584 0.70457416]\n",
      "Escritura Desiste [0.62995956 0.37004044]\n",
      "Escritura Desiste [0.66485917 0.33514083]\n",
      "Escritura Desiste [0.54606106 0.45393894]\n",
      "Escritura Desiste [0.63412172 0.36587828]\n",
      "Escritura Desiste [0.71617046 0.28382954]\n",
      "Escritura Escritura [0.17499141 0.82500859]\n",
      "Escritura Escritura [0.04281194 0.95718806]\n",
      "Escritura Escritura [0.08958081 0.91041919]\n",
      "Escritura Escritura [0.25094181 0.74905819]\n",
      "Escritura Desiste [0.67983831 0.32016169]\n",
      "Escritura Escritura [0.30417296 0.69582704]\n",
      "Escritura Escritura [0.39898165 0.60101835]\n",
      "Escritura Escritura [0.47236459 0.52763541]\n",
      "Escritura Desiste [0.86557664 0.13442336]\n",
      "Escritura Escritura [0.24475858 0.75524142]\n",
      "Escritura Desiste [0.59897151 0.40102849]\n",
      "Escritura Escritura [0.31027543 0.68972457]\n",
      "Escritura Escritura [0.37394311 0.62605689]\n",
      "Escritura Escritura [0.22906896 0.77093104]\n",
      "Escritura Desiste [0.75133117 0.24866883]\n",
      "Escritura Desiste [0.65856033 0.34143967]\n",
      "Escritura Desiste [0.76551798 0.23448202]\n",
      "Escritura Escritura [0.30481812 0.69518188]\n",
      "Escritura Desiste [0.64969323 0.35030677]\n",
      "Escritura Escritura [0.22286894 0.77713106]\n",
      "Escritura Desiste [0.67378225 0.32621775]\n",
      "Escritura Escritura [0.30507732 0.69492268]\n",
      "Escritura Desiste [0.58455197 0.41544803]\n",
      "Escritura Desiste [0.85377896 0.14622104]\n",
      "Escritura Escritura [0.22845831 0.77154169]\n",
      "Escritura Escritura [0.22671264 0.77328736]\n",
      "Escritura Desiste [0.66325774 0.33674226]\n",
      "Escritura Desiste [0.57251376 0.42748624]\n",
      "Escritura Escritura [0.27372561 0.72627439]\n",
      "Escritura Escritura [0.26803447 0.73196553]\n",
      "Escritura Escritura [0.24350547 0.75649453]\n",
      "Escritura Escritura [0.13711404 0.86288596]\n",
      "Escritura Desiste [0.65001393 0.34998607]\n",
      "Escritura Desiste [0.65379272 0.34620728]\n",
      "Escritura Desiste [0.72679906 0.27320094]\n",
      "Escritura Escritura [0.2225828 0.7774172]\n",
      "Escritura Desiste [0.77149347 0.22850653]\n",
      "Escritura Escritura [0.29082324 0.70917676]\n",
      "Escritura Desiste [0.64519797 0.35480203]\n",
      "Escritura Escritura [0.25877094 0.74122906]\n",
      "Escritura Escritura [0.46635741 0.53364259]\n",
      "Escritura Desiste [0.68049063 0.31950937]\n",
      "Escritura Desiste [0.67455428 0.32544572]\n",
      "Escritura Escritura [0.38989887 0.61010113]\n",
      "Escritura Escritura [0.23490439 0.76509561]\n",
      "Escritura Escritura [0.30277252 0.69722748]\n",
      "Escritura Escritura [0.28435701 0.71564299]\n",
      "Escritura Desiste [0.68166314 0.31833686]\n",
      "Escritura Escritura [0.35021802 0.64978198]\n",
      "Escritura Escritura [0.3485841 0.6514159]\n",
      "Escritura Desiste [0.66933265 0.33066735]\n",
      "Escritura Escritura [0.43470687 0.56529313]\n",
      "Escritura Desiste [0.61527742 0.38472258]\n",
      "Escritura Desiste [0.63942256 0.36057744]\n",
      "Escritura Escritura [0.21170027 0.78829973]\n",
      "Escritura Escritura [0.26056529 0.73943471]\n",
      "Escritura Desiste [0.59422881 0.40577119]\n",
      "Escritura Escritura [0.28277899 0.71722101]\n",
      "Escritura Escritura [0.33805392 0.66194608]\n",
      "Escritura Escritura [0.15998877 0.84001123]\n",
      "Escritura Desiste [0.60579819 0.39420181]\n",
      "Escritura Escritura [0.31828576 0.68171424]\n",
      "Escritura Escritura [0.19328253 0.80671747]\n",
      "Escritura Desiste [0.69634299 0.30365701]\n",
      "Escritura Desiste [0.77320449 0.22679551]\n",
      "Escritura Escritura [0.38396839 0.61603161]\n",
      "Escritura Escritura [0.2501774 0.7498226]\n",
      "Escritura Escritura [0.0349997 0.9650003]\n",
      "Escritura Escritura [0.19343404 0.80656596]\n",
      "Escritura Desiste [0.64202845 0.35797155]\n",
      "Escritura Desiste [0.73962095 0.26037905]\n",
      "Escritura Escritura [0.35567842 0.64432158]\n",
      "Escritura Escritura [0.34125327 0.65874673]\n",
      "Escritura Desiste [0.72188412 0.27811588]\n",
      "Escritura Escritura [0.344067 0.655933]\n",
      "Escritura Escritura [0.22524444 0.77475556]\n",
      "Escritura Escritura [0.36459583 0.63540417]\n",
      "Escritura Escritura [0.1308673 0.8691327]\n",
      "Escritura Escritura [0.27981822 0.72018178]\n",
      "Escritura Desiste [0.64898715 0.35101285]\n",
      "Escritura Desiste [0.52737292 0.47262708]\n",
      "Escritura Escritura [0.15516526 0.84483474]\n",
      "Escritura Escritura [0.27152589 0.72847411]\n",
      "Escritura Escritura [0.40928228 0.59071772]\n",
      "Escritura Escritura [0.32073741 0.67926259]\n",
      "Escritura Escritura [0.24284508 0.75715492]\n",
      "Escritura Escritura [0.23393643 0.76606357]\n",
      "Escritura Desiste [0.52491849 0.47508151]\n",
      "Escritura Escritura [0.11453109 0.88546891]\n",
      "Escritura Desiste [0.66520213 0.33479787]\n",
      "Escritura Escritura [0.30205893 0.69794107]\n",
      "Escritura Escritura [0.23042046 0.76957954]\n",
      "Escritura Escritura [0.2910256 0.7089744]\n",
      "Escritura Desiste [0.64733487 0.35266513]\n",
      "Escritura Escritura [0.33690253 0.66309747]\n",
      "Escritura Desiste [0.5560122 0.4439878]\n",
      "Escritura Desiste [0.57728031 0.42271969]\n",
      "Escritura Escritura [0.41065319 0.58934681]\n",
      "Escritura Desiste [0.74474838 0.25525162]\n",
      "Escritura Escritura [0.21953905 0.78046095]\n",
      "Escritura Escritura [0.28406045 0.71593955]\n",
      "Escritura Escritura [0.2751472 0.7248528]\n",
      "Escritura Desiste [0.50624749 0.49375251]\n",
      "Escritura Desiste [0.67473086 0.32526914]\n",
      "Escritura Escritura [0.40120946 0.59879054]\n",
      "Escritura Escritura [0.2599465 0.7400535]\n",
      "Escritura Escritura [0.46135401 0.53864599]\n",
      "Escritura Escritura [0.28585091 0.71414909]\n",
      "Escritura Escritura [0.20276233 0.79723767]\n",
      "Escritura Escritura [0.18922499 0.81077501]\n",
      "Escritura Escritura [0.00308454 0.99691546]\n",
      "Escritura Desiste [0.56544335 0.43455665]\n",
      "Escritura Desiste [0.63173812 0.36826188]\n",
      "Escritura Escritura [0.37775448 0.62224552]\n",
      "Escritura Escritura [0.31262048 0.68737952]\n",
      "Escritura Desiste [0.73080652 0.26919348]\n",
      "Escritura Escritura [0.37213054 0.62786946]\n",
      "Escritura Escritura [0.21397651 0.78602349]\n",
      "Escritura Escritura [0.33015991 0.66984009]\n",
      "Escritura Escritura [0.41858996 0.58141004]\n",
      "Escritura Escritura [0.28856025 0.71143975]\n",
      "Escritura Escritura [0.30638077 0.69361923]\n",
      "Escritura Escritura [0.24252816 0.75747184]\n",
      "Escritura Escritura [0.18799772 0.81200228]\n",
      "Escritura Escritura [0.21669844 0.78330156]\n",
      "Escritura Escritura [0.2245787 0.7754213]\n",
      "Escritura Escritura [0.2481161 0.7518839]\n",
      "Escritura Escritura [0.26372956 0.73627044]\n",
      "Escritura Desiste [0.60419138 0.39580862]\n",
      "Escritura Desiste [0.70185694 0.29814306]\n",
      "Escritura Escritura [0.29837702 0.70162298]\n",
      "Escritura Desiste [0.63328716 0.36671284]\n",
      "Escritura Escritura [0.34287611 0.65712389]\n",
      "Escritura Escritura [0.45450731 0.54549269]\n",
      "Escritura Escritura [0.18210193 0.81789807]\n",
      "Escritura Escritura [0.36909887 0.63090113]\n",
      "Escritura Escritura [0.18703305 0.81296695]\n",
      "Escritura Escritura [0.20113609 0.79886391]\n",
      "Escritura Desiste [0.62475097 0.37524903]\n",
      "Escritura Escritura [0.37190605 0.62809395]\n",
      "Escritura Escritura [0.23588033 0.76411967]\n",
      "Escritura Desiste [0.52873218 0.47126782]\n",
      "Escritura Escritura [0.21923262 0.78076738]\n",
      "Escritura Escritura [0.22739348 0.77260652]\n",
      "Escritura Escritura [0.32757843 0.67242157]\n",
      "Escritura Escritura [0.25642127 0.74357873]\n",
      "Escritura Escritura [0.21075399 0.78924601]\n",
      "Escritura Escritura [0.25131445 0.74868555]\n",
      "Escritura Desiste [0.57478485 0.42521515]\n",
      "Escritura Escritura [0.38094963 0.61905037]\n",
      "Escritura Escritura [0.47482766 0.52517234]\n",
      "Escritura Escritura [0.37136236 0.62863764]\n",
      "Escritura Desiste [0.64894236 0.35105764]\n",
      "Escritura Escritura [0.21808694 0.78191306]\n",
      "Escritura Desiste [0.73394018 0.26605982]\n",
      "Escritura Desiste [0.57627154 0.42372846]\n",
      "Escritura Escritura [0.10822308 0.89177692]\n",
      "Escritura Escritura [0.35250993 0.64749007]\n",
      "Escritura Desiste [0.65772695 0.34227305]\n",
      "Escritura Desiste [0.68591285 0.31408715]\n",
      "Escritura Escritura [0.32246704 0.67753296]\n",
      "Escritura Desiste [0.59126762 0.40873238]\n",
      "Escritura Escritura [0.47278161 0.52721839]\n",
      "Escritura Escritura [0.21368936 0.78631064]\n",
      "Escritura Desiste [0.73338088 0.26661912]\n",
      "Escritura Desiste [0.64406773 0.35593227]\n",
      "Escritura Escritura [0.32572896 0.67427104]\n",
      "Escritura Desiste [0.73737299 0.26262701]\n",
      "Escritura Escritura [0.34026157 0.65973843]\n",
      "Escritura Escritura [0.14824727 0.85175273]\n",
      "Escritura Escritura [0.3076367 0.6923633]\n",
      "Escritura Desiste [0.66169857 0.33830143]\n",
      "Escritura Escritura [0.29491378 0.70508622]\n",
      "Escritura Escritura [0.35339762 0.64660238]\n",
      "Escritura Escritura [0.32376382 0.67623618]\n",
      "Escritura Desiste [0.70432583 0.29567417]\n",
      "Escritura Escritura [0.39007655 0.60992345]\n",
      "Escritura Escritura [0.10459142 0.89540858]\n",
      "Escritura Escritura [0.1974347 0.8025653]\n",
      "Escritura Escritura [0.29169989 0.70830011]\n",
      "Escritura Desiste [0.62433729 0.37566271]\n",
      "Escritura Desiste [0.6356352 0.3643648]\n",
      "Escritura Escritura [0.24651201 0.75348799]\n",
      "Escritura Escritura [0.36155457 0.63844543]\n",
      "Escritura Escritura [0.23539114 0.76460886]\n",
      "Escritura Desiste [0.62576073 0.37423927]\n",
      "Escritura Escritura [0.2443916 0.7556084]\n",
      "Escritura Escritura [0.28236132 0.71763868]\n",
      "Escritura Escritura [0.49784302 0.50215698]\n",
      "Escritura Escritura [0.24035465 0.75964535]\n",
      "Escritura Desiste [0.6514597 0.3485403]\n",
      "Escritura Escritura [0.22153247 0.77846753]\n",
      "Escritura Escritura [0.45585877 0.54414123]\n",
      "Escritura Desiste [0.70097724 0.29902276]\n",
      "Escritura Desiste [0.77177099 0.22822901]\n",
      "Escritura Escritura [0.19367131 0.80632869]\n",
      "Escritura Desiste [0.67087998 0.32912002]\n",
      "Escritura Desiste [0.72378028 0.27621972]\n",
      "Escritura Escritura [0.19678394 0.80321606]\n",
      "Escritura Desiste [0.6278529 0.3721471]\n",
      "Escritura Escritura [0.18522808 0.81477192]\n",
      "Escritura Escritura [0.4318629 0.5681371]\n",
      "Escritura Desiste [0.59852933 0.40147067]\n",
      "Escritura Escritura [0.33216933 0.66783067]\n",
      "Escritura Escritura [0.15181923 0.84818077]\n",
      "Escritura Escritura [0.26753016 0.73246984]\n",
      "Escritura Escritura [0.31561893 0.68438107]\n",
      "Escritura Desiste [0.74589015 0.25410985]\n",
      "Escritura Escritura [0.2116537 0.7883463]\n",
      "Escritura Desiste [0.61888891 0.38111109]\n",
      "Escritura Escritura [0.2454991 0.7545009]\n",
      "Escritura Desiste [0.76245769 0.23754231]\n",
      "Escritura Escritura [0.20771901 0.79228099]\n",
      "Escritura Desiste [0.77453156 0.22546844]\n",
      "Escritura Escritura [0.24380792 0.75619208]\n",
      "Escritura Escritura [0.27196898 0.72803102]\n",
      "Escritura Escritura [0.4149097 0.5850903]\n",
      "Escritura Escritura [0.22913782 0.77086218]\n",
      "Escritura Escritura [0.47421779 0.52578221]\n",
      "Escritura Desiste [0.68685963 0.31314037]\n",
      "Escritura Desiste [0.82015083 0.17984917]\n",
      "Escritura Desiste [0.67914115 0.32085885]\n",
      "Escritura Desiste [0.57615614 0.42384386]\n",
      "Escritura Escritura [0.20515704 0.79484296]\n",
      "Escritura Desiste [0.68702391 0.31297609]\n",
      "Escritura Escritura [0.24030568 0.75969432]\n",
      "Escritura Desiste [0.63649057 0.36350943]\n",
      "Escritura Escritura [0.25150684 0.74849316]\n",
      "Escritura Escritura [0.03336473 0.96663527]\n",
      "Escritura Desiste [0.71678802 0.28321198]\n",
      "Escritura Escritura [0.23359214 0.76640786]\n",
      "Escritura Desiste [0.79432979 0.20567021]\n",
      "Escritura Escritura [0.22625842 0.77374158]\n",
      "Escritura Escritura [0.32037062 0.67962938]\n",
      "Escritura Escritura [0.29723287 0.70276713]\n",
      "Escritura Escritura [0.31510272 0.68489728]\n",
      "Escritura Escritura [0.30768776 0.69231224]\n",
      "Escritura Desiste [0.51354783 0.48645217]\n",
      "Escritura Escritura [0.1683789 0.8316211]\n",
      "Escritura Desiste [0.75571074 0.24428926]\n",
      "Escritura Escritura [0.40956843 0.59043157]\n",
      "Escritura Desiste [0.56397884 0.43602116]\n",
      "Escritura Escritura [0.25586452 0.74413548]\n",
      "Escritura Desiste [0.73647856 0.26352144]\n",
      "Escritura Escritura [0.17123818 0.82876182]\n",
      "Escritura Desiste [0.82897969 0.17102031]\n",
      "Escritura Desiste [0.55858434 0.44141566]\n",
      "Escritura Escritura [0.23458448 0.76541552]\n",
      "Escritura Escritura [0.22292125 0.77707875]\n",
      "Escritura Escritura [0.32274257 0.67725743]\n",
      "Escritura Escritura [0.22755828 0.77244172]\n",
      "Escritura Escritura [0.12479324 0.87520676]\n",
      "Escritura Escritura [0.25554678 0.74445322]\n",
      "Escritura Escritura [0.16895937 0.83104063]\n",
      "Escritura Escritura [0.30821408 0.69178592]\n",
      "Escritura Escritura [0.44698653 0.55301347]\n",
      "Escritura Desiste [0.7171881 0.2828119]\n",
      "Escritura Escritura [0.19532285 0.80467715]\n",
      "Escritura Escritura [0.2479524 0.7520476]\n",
      "Escritura Escritura [0.3527454 0.6472546]\n",
      "Escritura Desiste [0.68636945 0.31363055]\n",
      "Escritura Desiste [0.6828753 0.3171247]\n",
      "Escritura Desiste [0.62490397 0.37509603]\n",
      "Escritura Escritura [0.28966263 0.71033737]\n",
      "Escritura Escritura [0.2959729 0.7040271]\n",
      "Escritura Escritura [0.29392922 0.70607078]\n",
      "Escritura Escritura [0.20758035 0.79241965]\n",
      "Escritura Escritura [0.35746383 0.64253617]\n",
      "Escritura Desiste [0.71324546 0.28675454]\n",
      "Escritura Desiste [0.59611062 0.40388938]\n",
      "Escritura Escritura [0.25111938 0.74888062]\n",
      "Escritura Escritura [0.46215745 0.53784255]\n",
      "Escritura Desiste [0.64595258 0.35404742]\n",
      "Escritura Escritura [0.28847057 0.71152943]\n",
      "Escritura Desiste [0.83150257 0.16849743]\n",
      "Escritura Escritura [0.41985053 0.58014947]\n",
      "Escritura Escritura [0.30659941 0.69340059]\n",
      "Escritura Escritura [0.33632844 0.66367156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Desiste [0.62991961 0.37008039]\n",
      "Escritura Desiste [0.76368624 0.23631376]\n",
      "Escritura Desiste [0.55747976 0.44252024]\n",
      "Escritura Escritura [0.25423813 0.74576187]\n",
      "Escritura Escritura [0.28352868 0.71647132]\n",
      "Escritura Desiste [0.76121593 0.23878407]\n",
      "Escritura Escritura [0.18524425 0.81475575]\n",
      "Escritura Escritura [0.22734267 0.77265733]\n",
      "Escritura Desiste [0.66784098 0.33215902]\n",
      "Escritura Desiste [0.58899875 0.41100125]\n",
      "Escritura Escritura [0.38843432 0.61156568]\n",
      "Escritura Escritura [0.28352675 0.71647325]\n",
      "Escritura Desiste [0.68124598 0.31875402]\n",
      "Escritura Escritura [0.36626315 0.63373685]\n",
      "Escritura Escritura [0.3785375 0.6214625]\n",
      "Escritura Escritura [0.22577346 0.77422654]\n",
      "Escritura Escritura [0.27515095 0.72484905]\n",
      "Escritura Escritura [0.2094071 0.7905929]\n",
      "Escritura Desiste [0.6365583 0.3634417]\n",
      "Escritura Desiste [0.69942544 0.30057456]\n",
      "Escritura Desiste [0.6977285 0.3022715]\n",
      "Escritura Desiste [0.70555071 0.29444929]\n",
      "Escritura Escritura [0.31967587 0.68032413]\n",
      "Escritura Escritura [0.39313926 0.60686074]\n",
      "Escritura Desiste [0.67847238 0.32152762]\n",
      "Escritura Escritura [0.33214158 0.66785842]\n",
      "Escritura Escritura [0.13335719 0.86664281]\n",
      "Escritura Escritura [0.2000944 0.7999056]\n",
      "Escritura Escritura [0.25084389 0.74915611]\n",
      "Escritura Escritura [0.28163624 0.71836376]\n",
      "Escritura Escritura [0.15641252 0.84358748]\n",
      "Escritura Desiste [0.5959036 0.4040964]\n",
      "Escritura Desiste [0.53418618 0.46581382]\n",
      "Escritura Escritura [0.30534635 0.69465365]\n",
      "Escritura Desiste [0.55883463 0.44116537]\n",
      "Escritura Escritura [0.24710221 0.75289779]\n",
      "Escritura Escritura [0.32835106 0.67164894]\n",
      "Escritura Escritura [0.22090595 0.77909405]\n",
      "Escritura Escritura [0.37781924 0.62218076]\n",
      "Escritura Escritura [0.22871335 0.77128665]\n",
      "Escritura Escritura [0.34732183 0.65267817]\n",
      "Escritura Escritura [0.16656677 0.83343323]\n",
      "Escritura Desiste [0.63637643 0.36362357]\n",
      "Escritura Desiste [0.73945331 0.26054669]\n",
      "Escritura Desiste [0.70791888 0.29208112]\n",
      "Escritura Escritura [0.30686302 0.69313698]\n",
      "Escritura Escritura [0.24603485 0.75396515]\n",
      "Escritura Escritura [0.31240719 0.68759281]\n",
      "Escritura Escritura [0.30548929 0.69451071]\n",
      "Escritura Escritura [0.25147259 0.74852741]\n",
      "Escritura Desiste [0.69025455 0.30974545]\n",
      "Escritura Escritura [0.31065412 0.68934588]\n",
      "Escritura Desiste [0.51211963 0.48788037]\n",
      "Escritura Escritura [0.19974786 0.80025214]\n",
      "Escritura Desiste [0.63404279 0.36595721]\n",
      "Escritura Desiste [0.68571209 0.31428791]\n",
      "Escritura Desiste [0.58305966 0.41694034]\n",
      "Escritura Desiste [0.62363292 0.37636708]\n",
      "Escritura Desiste [0.53298727 0.46701273]\n",
      "Escritura Escritura [0.30250835 0.69749165]\n",
      "Escritura Escritura [0.25345468 0.74654532]\n",
      "Escritura Escritura [0.29278458 0.70721542]\n",
      "Escritura Desiste [0.56744088 0.43255912]\n",
      "Escritura Escritura [0.16253432 0.83746568]\n",
      "Escritura Desiste [0.60594319 0.39405681]\n",
      "Escritura Escritura [0.39080205 0.60919795]\n",
      "Escritura Desiste [0.63305949 0.36694051]\n",
      "Escritura Escritura [0.01081603 0.98918397]\n",
      "Escritura Desiste [0.52434262 0.47565738]\n",
      "Escritura Escritura [0.39586889 0.60413111]\n",
      "Escritura Escritura [0.32767331 0.67232669]\n",
      "Escritura Escritura [0.23019036 0.76980964]\n",
      "Escritura Escritura [0.25862588 0.74137412]\n",
      "Escritura Escritura [0.45503973 0.54496027]\n",
      "Escritura Escritura [0.35136386 0.64863614]\n",
      "Escritura Escritura [0.32472858 0.67527142]\n",
      "Escritura Escritura [0.32663516 0.67336484]\n",
      "Escritura Escritura [0.29608533 0.70391467]\n",
      "Escritura Desiste [0.55609272 0.44390728]\n",
      "Escritura Escritura [0.28982726 0.71017274]\n",
      "Escritura Escritura [0.43026096 0.56973904]\n",
      "Escritura Escritura [0.3023391 0.6976609]\n",
      "Escritura Escritura [0.21177136 0.78822864]\n",
      "Escritura Escritura [0.44653042 0.55346958]\n",
      "Escritura Escritura [0.36058367 0.63941633]\n",
      "Escritura Escritura [0.40288871 0.59711129]\n",
      "Escritura Desiste [0.61318264 0.38681736]\n",
      "Escritura Desiste [0.75372996 0.24627004]\n",
      "Escritura Escritura [0.29344639 0.70655361]\n",
      "Escritura Escritura [0.23902369 0.76097631]\n",
      "Escritura Desiste [0.58477038 0.41522962]\n",
      "Escritura Desiste [0.64789616 0.35210384]\n",
      "Escritura Desiste [0.69910291 0.30089709]\n",
      "Escritura Escritura [0.11977346 0.88022654]\n",
      "Escritura Escritura [0.30435734 0.69564266]\n",
      "Escritura Desiste [0.52254177 0.47745823]\n",
      "Escritura Desiste [0.73192471 0.26807529]\n",
      "Escritura Desiste [0.61512211 0.38487789]\n",
      "Escritura Escritura [0.26116269 0.73883731]\n",
      "Escritura Desiste [0.63898453 0.36101547]\n",
      "Escritura Desiste [0.63357446 0.36642554]\n",
      "Escritura Escritura [0.19871286 0.80128714]\n",
      "Escritura Escritura [0.16275303 0.83724697]\n",
      "Escritura Escritura [0.20235567 0.79764433]\n",
      "Escritura Escritura [0.38755288 0.61244712]\n",
      "Escritura Desiste [0.72386614 0.27613386]\n",
      "Escritura Escritura [0.25977751 0.74022249]\n",
      "Escritura Escritura [0.31948599 0.68051401]\n",
      "Escritura Escritura [0.27913942 0.72086058]\n",
      "Escritura Escritura [0.15136485 0.84863515]\n",
      "Escritura Escritura [0.30856996 0.69143004]\n",
      "Escritura Escritura [0.26750504 0.73249496]\n",
      "Escritura Escritura [0.21897466 0.78102534]\n",
      "Escritura Escritura [0.44630025 0.55369975]\n",
      "Escritura Escritura [0.15545946 0.84454054]\n",
      "Escritura Escritura [0.27816093 0.72183907]\n",
      "Escritura Desiste [0.67444189 0.32555811]\n",
      "Escritura Escritura [0.20208561 0.79791439]\n",
      "Escritura Escritura [0.18620425 0.81379575]\n",
      "Escritura Escritura [0.39347903 0.60652097]\n",
      "Escritura Escritura [0.21865807 0.78134193]\n",
      "Escritura Desiste [0.66476228 0.33523772]\n",
      "Escritura Desiste [0.71641645 0.28358355]\n",
      "Escritura Desiste [0.69058234 0.30941766]\n",
      "Escritura Desiste [0.80394148 0.19605852]\n",
      "Escritura Escritura [0.20283166 0.79716834]\n",
      "Escritura Escritura [0.36965256 0.63034744]\n",
      "Escritura Desiste [0.58172551 0.41827449]\n",
      "Escritura Escritura [0.2684811 0.7315189]\n",
      "Escritura Escritura [0.26741544 0.73258456]\n",
      "Escritura Escritura [0.24332994 0.75667006]\n",
      "Escritura Escritura [0.38079403 0.61920597]\n",
      "Escritura Desiste [0.56935821 0.43064179]\n",
      "Escritura Escritura [0.48997724 0.51002276]\n",
      "Escritura Escritura [0.25815774 0.74184226]\n",
      "Escritura Escritura [0.35346006 0.64653994]\n",
      "Escritura Desiste [0.50662098 0.49337902]\n",
      "Escritura Escritura [0.22165577 0.77834423]\n",
      "Escritura Escritura [0.26441385 0.73558615]\n",
      "Escritura Desiste [0.58693196 0.41306804]\n",
      "Escritura Escritura [0.33315508 0.66684492]\n",
      "Escritura Escritura [0.20697561 0.79302439]\n",
      "Escritura Escritura [0.24689309 0.75310691]\n",
      "Escritura Escritura [0.24097338 0.75902662]\n",
      "Escritura Desiste [0.71097807 0.28902193]\n",
      "Escritura Escritura [0.2704822 0.7295178]\n",
      "Escritura Escritura [0.18062486 0.81937514]\n",
      "Escritura Escritura [0.31141676 0.68858324]\n",
      "Escritura Escritura [0.16563445 0.83436555]\n",
      "Escritura Desiste [0.70794557 0.29205443]\n",
      "Escritura Desiste [0.66241261 0.33758739]\n",
      "Escritura Desiste [0.61328781 0.38671219]\n",
      "Escritura Escritura [0.24324341 0.75675659]\n",
      "Escritura Escritura [0.21474667 0.78525333]\n",
      "Escritura Desiste [0.57231786 0.42768214]\n",
      "Escritura Escritura [0.06912096 0.93087904]\n",
      "Escritura Desiste [0.60218277 0.39781723]\n",
      "Escritura Desiste [0.52571078 0.47428922]\n",
      "Escritura Escritura [0.36717954 0.63282046]\n",
      "Escritura Escritura [0.27134525 0.72865475]\n",
      "Escritura Desiste [0.61367674 0.38632326]\n",
      "Escritura Escritura [0.19444979 0.80555021]\n",
      "Escritura Escritura [0.25930106 0.74069894]\n",
      "Escritura Escritura [0.40254907 0.59745093]\n",
      "Escritura Desiste [0.69456516 0.30543484]\n",
      "Escritura Desiste [0.60084105 0.39915895]\n",
      "Escritura Desiste [0.66591981 0.33408019]\n",
      "Escritura Escritura [0.28727562 0.71272438]\n",
      "Escritura Escritura [0.32203118 0.67796882]\n",
      "Escritura Escritura [0.28278043 0.71721957]\n",
      "Escritura Escritura [0.26769592 0.73230408]\n",
      "Escritura Escritura [0.30506683 0.69493317]\n",
      "Escritura Escritura [0.21186539 0.78813461]\n",
      "Escritura Desiste [0.65733008 0.34266992]\n",
      "Escritura Escritura [0.2782114 0.7217886]\n",
      "Escritura Desiste [0.52049195 0.47950805]\n",
      "Escritura Escritura [0.30898835 0.69101165]\n",
      "Escritura Escritura [0.19287837 0.80712163]\n",
      "Escritura Desiste [0.72306969 0.27693031]\n",
      "Escritura Escritura [0.37775467 0.62224533]\n",
      "Escritura Escritura [0.2189773 0.7810227]\n",
      "Escritura Escritura [0.07155167 0.92844833]\n",
      "Escritura Desiste [0.55912989 0.44087011]\n",
      "Escritura Escritura [0.30510239 0.69489761]\n",
      "Escritura Escritura [0.29148632 0.70851368]\n",
      "Escritura Escritura [0.30431254 0.69568746]\n",
      "Escritura Desiste [0.69879073 0.30120927]\n",
      "Escritura Desiste [0.77139645 0.22860355]\n",
      "Escritura Escritura [0.08812658 0.91187342]\n",
      "Escritura Escritura [0.31583593 0.68416407]\n",
      "Escritura Escritura [0.17497305 0.82502695]\n",
      "Escritura Escritura [0.17474227 0.82525773]\n",
      "Escritura Desiste [0.7389747 0.2610253]\n",
      "Escritura Desiste [0.590348 0.409652]\n",
      "Escritura Desiste [0.69141625 0.30858375]\n",
      "Escritura Desiste [0.60309415 0.39690585]\n",
      "Escritura Desiste [0.53377652 0.46622348]\n",
      "Escritura Escritura [0.224109 0.775891]\n",
      "Escritura Escritura [0.25058332 0.74941668]\n",
      "Escritura Escritura [0.33247588 0.66752412]\n",
      "Escritura Desiste [0.72155126 0.27844874]\n",
      "Escritura Desiste [0.69580679 0.30419321]\n",
      "Escritura Escritura [0.31360693 0.68639307]\n",
      "Escritura Desiste [0.80343654 0.19656346]\n",
      "Escritura Desiste [0.64190877 0.35809123]\n",
      "Escritura Escritura [0.21271791 0.78728209]\n",
      "Escritura Escritura [0.12561741 0.87438259]\n",
      "Escritura Escritura [0.36824477 0.63175523]\n",
      "Escritura Escritura [0.37208238 0.62791762]\n",
      "Escritura Desiste [0.67659291 0.32340709]\n",
      "Escritura Escritura [0.28208743 0.71791257]\n",
      "Escritura Escritura [0.34985358 0.65014642]\n",
      "Escritura Escritura [0.20115418 0.79884582]\n",
      "Escritura Escritura [0.39022743 0.60977257]\n",
      "Escritura Escritura [0.46767319 0.53232681]\n",
      "Escritura Escritura [0.34526482 0.65473518]\n",
      "Escritura Desiste [0.60501725 0.39498275]\n",
      "Escritura Escritura [0.21295075 0.78704925]\n",
      "Escritura Desiste [0.73256941 0.26743059]\n",
      "Escritura Escritura [0.12049319 0.87950681]\n",
      "Escritura Desiste [0.62171564 0.37828436]\n",
      "Escritura Desiste [0.5095026 0.4904974]\n",
      "Escritura Escritura [0.25201306 0.74798694]\n",
      "Escritura Escritura [0.18050733 0.81949267]\n",
      "Escritura Escritura [0.33756082 0.66243918]\n",
      "Escritura Escritura [0.25143462 0.74856538]\n",
      "Escritura Escritura [0.22028464 0.77971536]\n",
      "Escritura Escritura [0.3597336 0.6402664]\n",
      "Escritura Escritura [0.32099621 0.67900379]\n",
      "Escritura Escritura [0.38215148 0.61784852]\n",
      "Escritura Desiste [0.6060083 0.3939917]\n",
      "Escritura Escritura [0.40278316 0.59721684]\n",
      "Escritura Escritura [0.30997377 0.69002623]\n",
      "Escritura Escritura [0.38844858 0.61155142]\n",
      "Escritura Desiste [0.61414883 0.38585117]\n",
      "Escritura Escritura [0.47118084 0.52881916]\n",
      "Escritura Desiste [0.67708277 0.32291723]\n",
      "Escritura Escritura [0.32091289 0.67908711]\n",
      "Escritura Desiste [0.67629165 0.32370835]\n",
      "Escritura Escritura [0.28234499 0.71765501]\n",
      "Escritura Escritura [0.22246552 0.77753448]\n",
      "Escritura Desiste [0.64222725 0.35777275]\n",
      "Escritura Escritura [0.33418863 0.66581137]\n",
      "Escritura Escritura [0.33769417 0.66230583]\n",
      "Escritura Escritura [0.46004871 0.53995129]\n",
      "Escritura Escritura [0.24320198 0.75679802]\n",
      "Escritura Desiste [0.60782509 0.39217491]\n",
      "Escritura Desiste [0.80672147 0.19327853]\n",
      "Escritura Escritura [0.38044584 0.61955416]\n",
      "Escritura Escritura [0.245659 0.754341]\n",
      "Escritura Escritura [0.27490317 0.72509683]\n",
      "Escritura Escritura [0.25371574 0.74628426]\n",
      "Escritura Desiste [0.67188055 0.32811945]\n",
      "Escritura Desiste [0.81265876 0.18734124]\n",
      "Escritura Escritura [0.00308336 0.99691664]\n",
      "Escritura Escritura [0.17219412 0.82780588]\n",
      "Escritura Escritura [0.21806578 0.78193422]\n",
      "Escritura Escritura [0.2641838 0.7358162]\n",
      "Escritura Desiste [0.54521301 0.45478699]\n",
      "Escritura Escritura [0.25173119 0.74826881]\n",
      "Escritura Escritura [0.29204795 0.70795205]\n",
      "Escritura Desiste [0.6805157 0.3194843]\n",
      "Escritura Escritura [0.12651382 0.87348618]\n",
      "Escritura Escritura [0.39629594 0.60370406]\n",
      "Escritura Escritura [0.33204169 0.66795831]\n",
      "Escritura Escritura [0.43112389 0.56887611]\n",
      "Escritura Escritura [0.2974787 0.7025213]\n",
      "Escritura Escritura [0.16477687 0.83522313]\n",
      "Escritura Escritura [0.2604635 0.7395365]\n",
      "Escritura Escritura [0.31193529 0.68806471]\n",
      "Escritura Desiste [0.63065129 0.36934871]\n",
      "Escritura Escritura [0.38392749 0.61607251]\n",
      "Escritura Escritura [0.3267606 0.6732394]\n",
      "Escritura Escritura [0.18832558 0.81167442]\n",
      "Escritura Desiste [0.67104652 0.32895348]\n",
      "Escritura Desiste [0.68448461 0.31551539]\n",
      "Escritura Escritura [0.22336924 0.77663076]\n",
      "Escritura Escritura [0.32228239 0.67771761]\n",
      "Escritura Escritura [0.29473915 0.70526085]\n",
      "Escritura Desiste [0.62706831 0.37293169]\n",
      "Escritura Escritura [0.38072959 0.61927041]\n",
      "Escritura Escritura [0.24411943 0.75588057]\n",
      "Escritura Desiste [0.645991 0.354009]\n",
      "Escritura Escritura [0.14729577 0.85270423]\n",
      "Escritura Desiste [0.78129854 0.21870146]\n",
      "Escritura Desiste [0.74036947 0.25963053]\n",
      "Escritura Escritura [0.19594272 0.80405728]\n",
      "Escritura Desiste [0.61548347 0.38451653]\n",
      "Escritura Escritura [0.24289765 0.75710235]\n",
      "Escritura Desiste [0.62966731 0.37033269]\n",
      "Escritura Escritura [0.32105209 0.67894791]\n",
      "Escritura Escritura [0.24376585 0.75623415]\n",
      "Escritura Escritura [0.27309395 0.72690605]\n",
      "Escritura Desiste [0.65588534 0.34411466]\n",
      "Escritura Desiste [0.80327604 0.19672396]\n",
      "Escritura Desiste [0.51737857 0.48262143]\n",
      "Escritura Escritura [0.40203379 0.59796621]\n",
      "Escritura Escritura [0.20325992 0.79674008]\n",
      "Escritura Escritura [0.31931113 0.68068887]\n",
      "Escritura Escritura [0.27793584 0.72206416]\n",
      "Escritura Escritura [0.28211657 0.71788343]\n",
      "Escritura Desiste [0.7881992 0.2118008]\n",
      "Escritura Escritura [0.23443337 0.76556663]\n",
      "Escritura Escritura [0.27485002 0.72514998]\n",
      "Escritura Desiste [0.65611455 0.34388545]\n",
      "Escritura Desiste [0.56999249 0.43000751]\n",
      "Escritura Desiste [0.53930275 0.46069725]\n",
      "Escritura Escritura [0.25282802 0.74717198]\n",
      "Escritura Desiste [0.51881736 0.48118264]\n",
      "Escritura Desiste [0.55061412 0.44938588]\n",
      "Escritura Desiste [0.56700766 0.43299234]\n",
      "Escritura Desiste [0.66349869 0.33650131]\n",
      "Escritura Escritura [0.49822349 0.50177651]\n",
      "Escritura Escritura [0.29036755 0.70963245]\n",
      "Escritura Escritura [0.28139883 0.71860117]\n",
      "Escritura Escritura [0.31826867 0.68173133]\n",
      "Escritura Escritura [0.18387183 0.81612817]\n",
      "Escritura Desiste [0.62827572 0.37172428]\n",
      "Escritura Desiste [0.61664546 0.38335454]\n",
      "Escritura Desiste [0.66205707 0.33794293]\n",
      "Escritura Desiste [0.662305 0.337695]\n",
      "Escritura Desiste [0.62729307 0.37270693]\n",
      "Escritura Escritura [0.27514749 0.72485251]\n",
      "Escritura Escritura [0.00168376 0.99831624]\n",
      "Escritura Escritura [0.14924149 0.85075851]\n",
      "Escritura Escritura [0.36718732 0.63281268]\n",
      "Escritura Desiste [0.62066467 0.37933533]\n",
      "Escritura Desiste [0.59423828 0.40576172]\n",
      "Escritura Desiste [0.66322983 0.33677017]\n",
      "Escritura Escritura [0.17452555 0.82547445]\n",
      "Escritura Escritura [0.26845907 0.73154093]\n",
      "Escritura Escritura [0.48690772 0.51309228]\n",
      "Escritura Escritura [0.41349718 0.58650282]\n",
      "Escritura Escritura [0.15019845 0.84980155]\n",
      "Escritura Escritura [0.38033545 0.61966455]\n",
      "Escritura Escritura [0.38584356 0.61415644]\n",
      "Escritura Escritura [0.32137652 0.67862348]\n",
      "Escritura Desiste [0.762878 0.237122]\n",
      "Escritura Desiste [0.70517477 0.29482523]\n",
      "Escritura Escritura [0.37220846 0.62779154]\n",
      "Escritura Desiste [0.60210741 0.39789259]\n",
      "Escritura Desiste [0.65415876 0.34584124]\n",
      "Escritura Escritura [0.16915566 0.83084434]\n",
      "Escritura Escritura [0.31951004 0.68048996]\n",
      "Escritura Desiste [0.55460732 0.44539268]\n",
      "Escritura Escritura [0.25102845 0.74897155]\n",
      "Escritura Desiste [0.52490943 0.47509057]\n",
      "Escritura Desiste [0.52564287 0.47435713]\n",
      "Escritura Escritura [0.27050253 0.72949747]\n",
      "Escritura Escritura [0.33987679 0.66012321]\n",
      "Escritura Escritura [0.22500127 0.77499873]\n",
      "Escritura Escritura [0.46980696 0.53019304]\n",
      "Escritura Escritura [0.45285927 0.54714073]\n",
      "Escritura Desiste [0.55284141 0.44715859]\n",
      "Escritura Escritura [0.27618319 0.72381681]\n",
      "Escritura Desiste [0.66411077 0.33588923]\n",
      "Escritura Desiste [0.55010944 0.44989056]\n",
      "Escritura Desiste [0.69413389 0.30586611]\n",
      "Escritura Escritura [0.22682745 0.77317255]\n",
      "Escritura Desiste [0.77388427 0.22611573]\n",
      "Escritura Desiste [0.62437373 0.37562627]\n",
      "Escritura Escritura [0.18889442 0.81110558]\n",
      "Escritura Escritura [0.41194647 0.58805353]\n",
      "Escritura Escritura [0.36565756 0.63434244]\n",
      "Escritura Desiste [0.6385592 0.3614408]\n",
      "Escritura Escritura [0.1979238 0.8020762]\n",
      "Escritura Desiste [0.61727875 0.38272125]\n",
      "Escritura Escritura [0.24354679 0.75645321]\n",
      "Escritura Escritura [0.34542458 0.65457542]\n",
      "Escritura Desiste [0.65259116 0.34740884]\n",
      "Escritura Escritura [0.25447548 0.74552452]\n",
      "Escritura Escritura [0.08007671 0.91992329]\n",
      "Escritura Desiste [0.72559966 0.27440034]\n",
      "Escritura Escritura [0.37665797 0.62334203]\n",
      "Escritura Desiste [0.72518752 0.27481248]\n",
      "Escritura Escritura [0.27263426 0.72736574]\n",
      "Escritura Escritura [0.43073262 0.56926738]\n",
      "Escritura Escritura [0.33214671 0.66785329]\n",
      "Escritura Escritura [0.30093061 0.69906939]\n",
      "Escritura Desiste [0.6035138 0.3964862]\n",
      "Escritura Desiste [0.6510502 0.3489498]\n",
      "Escritura Escritura [0.03840854 0.96159146]\n",
      "Escritura Desiste [0.70641287 0.29358713]\n",
      "Escritura Escritura [0.30525226 0.69474774]\n",
      "Escritura Desiste [0.56472137 0.43527863]\n",
      "Escritura Desiste [0.56888571 0.43111429]\n",
      "Escritura Escritura [0.36104914 0.63895086]\n",
      "Escritura Escritura [0.12363266 0.87636734]\n",
      "Escritura Escritura [0.40035921 0.59964079]\n",
      "Escritura Desiste [0.78206483 0.21793517]\n",
      "Escritura Escritura [0.33787974 0.66212026]\n",
      "Escritura Escritura [0.38817096 0.61182904]\n",
      "Escritura Escritura [0.29944586 0.70055414]\n",
      "Escritura Escritura [0.23032909 0.76967091]\n",
      "Escritura Desiste [0.68445112 0.31554888]\n",
      "Escritura Escritura [0.2891939 0.7108061]\n",
      "Escritura Desiste [0.62642489 0.37357511]\n",
      "Escritura Escritura [0.331327 0.668673]\n",
      "Escritura Escritura [0.30294153 0.69705847]\n",
      "Escritura Escritura [0.25824372 0.74175628]\n",
      "Escritura Escritura [0.34168397 0.65831603]\n",
      "Escritura Desiste [0.64139223 0.35860777]\n",
      "Escritura Desiste [0.78047208 0.21952792]\n",
      "Escritura Desiste [0.68866863 0.31133137]\n",
      "Escritura Escritura [0.26073926 0.73926074]\n",
      "Escritura Escritura [0.14450279 0.85549721]\n",
      "Escritura Escritura [0.28337251 0.71662749]\n",
      "Escritura Desiste [0.7110493 0.2889507]\n",
      "Escritura Desiste [0.64923054 0.35076946]\n",
      "Escritura Escritura [0.34116051 0.65883949]\n",
      "Escritura Desiste [0.7537227 0.2462773]\n",
      "Escritura Escritura [0.2967392 0.7032608]\n",
      "Escritura Escritura [0.30623313 0.69376687]\n",
      "Escritura Desiste [0.69171336 0.30828664]\n",
      "Escritura Escritura [0.23768322 0.76231678]\n",
      "Escritura Escritura [0.25941719 0.74058281]\n",
      "Escritura Desiste [0.70424272 0.29575728]\n",
      "Escritura Escritura [0.20608627 0.79391373]\n",
      "Escritura Escritura [0.24290518 0.75709482]\n",
      "Escritura Escritura [0.16111122 0.83888878]\n",
      "Escritura Escritura [0.14255437 0.85744563]\n",
      "Escritura Desiste [0.61805911 0.38194089]\n",
      "Escritura Escritura [0.31674101 0.68325899]\n",
      "Escritura Escritura [0.27761902 0.72238098]\n",
      "Escritura Escritura [0.23511821 0.76488179]\n",
      "Escritura Desiste [0.5252094 0.4747906]\n",
      "Escritura Escritura [0.30927903 0.69072097]\n",
      "Escritura Desiste [0.6117269 0.3882731]\n",
      "Escritura Escritura [0.2855135 0.7144865]\n",
      "Escritura Escritura [0.22649987 0.77350013]\n",
      "Escritura Escritura [0.29783951 0.70216049]\n",
      "Escritura Desiste [0.62825169 0.37174831]\n",
      "Escritura Escritura [0.34165454 0.65834546]\n",
      "Escritura Desiste [0.56176106 0.43823894]\n",
      "Escritura Escritura [0.22929632 0.77070368]\n",
      "Escritura Escritura [0.29092461 0.70907539]\n",
      "Escritura Desiste [0.51955365 0.48044635]\n",
      "Escritura Escritura [0.29882954 0.70117046]\n",
      "Escritura Escritura [0.25937023 0.74062977]\n",
      "Escritura Desiste [0.51468704 0.48531296]\n",
      "Escritura Desiste [0.62725789 0.37274211]\n",
      "Escritura Escritura [0.35221162 0.64778838]\n",
      "Escritura Escritura [0.2889006 0.7110994]\n",
      "Escritura Escritura [0.33298393 0.66701607]\n",
      "Escritura Escritura [0.20560482 0.79439518]\n",
      "Escritura Desiste [0.56371589 0.43628411]\n",
      "Escritura Escritura [0.31602027 0.68397973]\n",
      "Escritura Escritura [0.27144155 0.72855845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Escritura [0.33239548 0.66760452]\n",
      "Escritura Escritura [0.21285601 0.78714399]\n",
      "Escritura Desiste [0.80396998 0.19603002]\n",
      "Escritura Desiste [0.51055439 0.48944561]\n",
      "Escritura Escritura [0.46908065 0.53091935]\n",
      "Escritura Desiste [0.60240789 0.39759211]\n",
      "Escritura Escritura [0.28250002 0.71749998]\n",
      "Escritura Desiste [0.5983773 0.4016227]\n",
      "Escritura Escritura [0.20165255 0.79834745]\n",
      "Escritura Escritura [0.12599571 0.87400429]\n",
      "Escritura Escritura [0.39722499 0.60277501]\n",
      "Escritura Escritura [0.32548362 0.67451638]\n",
      "Escritura Escritura [0.27912928 0.72087072]\n",
      "Escritura Desiste [0.5845504 0.4154496]\n",
      "Escritura Escritura [0.29746538 0.70253462]\n",
      "Escritura Escritura [0.26644673 0.73355327]\n",
      "Escritura Escritura [0.25886852 0.74113148]\n",
      "Escritura Escritura [0.27311133 0.72688867]\n",
      "Escritura Escritura [0.19659261 0.80340739]\n",
      "Escritura Desiste [0.69273785 0.30726215]\n",
      "Escritura Escritura [0.20406471 0.79593529]\n",
      "Escritura Escritura [0.29956103 0.70043897]\n",
      "Escritura Escritura [0.17668468 0.82331532]\n",
      "Escritura Escritura [0.17301665 0.82698335]\n",
      "Escritura Escritura [0.12826458 0.87173542]\n",
      "Escritura Desiste [0.7556536 0.2443464]\n",
      "Escritura Desiste [0.59133572 0.40866428]\n",
      "Escritura Escritura [0.46089197 0.53910803]\n",
      "Escritura Escritura [0.48028237 0.51971763]\n",
      "Escritura Escritura [0.26021649 0.73978351]\n",
      "Escritura Escritura [0.26436048 0.73563952]\n",
      "Escritura Desiste [0.69157372 0.30842628]\n",
      "Escritura Desiste [0.76398629 0.23601371]\n",
      "Escritura Escritura [0.20332376 0.79667624]\n",
      "Escritura Escritura [0.25583665 0.74416335]\n",
      "Escritura Desiste [0.52194874 0.47805126]\n",
      "Escritura Escritura [0.28333881 0.71666119]\n",
      "Escritura Desiste [0.6397612 0.3602388]\n",
      "Escritura Escritura [0.3481604 0.6518396]\n",
      "Escritura Escritura [0.39718377 0.60281623]\n",
      "Escritura Escritura [0.1829124 0.8170876]\n",
      "Escritura Desiste [0.58736718 0.41263282]\n",
      "Escritura Desiste [0.54580944 0.45419056]\n",
      "Escritura Escritura [0.27106599 0.72893401]\n",
      "Escritura Escritura [0.21004053 0.78995947]\n",
      "Escritura Desiste [0.67505223 0.32494777]\n",
      "Escritura Escritura [0.21065403 0.78934597]\n",
      "Escritura Escritura [0.30152493 0.69847507]\n",
      "Escritura Desiste [0.54778901 0.45221099]\n",
      "Escritura Escritura [0.19529743 0.80470257]\n",
      "Escritura Desiste [0.60579209 0.39420791]\n",
      "Escritura Escritura [0.27851279 0.72148721]\n",
      "Escritura Escritura [0.23745508 0.76254492]\n",
      "Escritura Desiste [0.62588902 0.37411098]\n",
      "Escritura Escritura [0.31124303 0.68875697]\n",
      "Escritura Escritura [0.28731377 0.71268623]\n",
      "Escritura Escritura [0.32364087 0.67635913]\n",
      "Escritura Desiste [0.50303069 0.49696931]\n",
      "Escritura Desiste [0.66452001 0.33547999]\n",
      "Escritura Escritura [0.25500456 0.74499544]\n",
      "Escritura Escritura [0.49437635 0.50562365]\n",
      "Escritura Escritura [0.14008822 0.85991178]\n",
      "Escritura Escritura [0.18798722 0.81201278]\n",
      "Escritura Desiste [0.6490981 0.3509019]\n",
      "Escritura Desiste [0.58056037 0.41943963]\n",
      "Escritura Escritura [0.25349546 0.74650454]\n",
      "Escritura Desiste [0.50652765 0.49347235]\n",
      "Escritura Desiste [0.74700888 0.25299112]\n",
      "Escritura Desiste [0.68342654 0.31657346]\n",
      "Escritura Escritura [0.14174215 0.85825785]\n",
      "Escritura Escritura [0.20489215 0.79510785]\n",
      "Escritura Escritura [0.25782807 0.74217193]\n",
      "Escritura Escritura [0.28471624 0.71528376]\n",
      "Escritura Escritura [0.31456368 0.68543632]\n",
      "Escritura Escritura [0.21922322 0.78077678]\n",
      "Escritura Desiste [0.60438795 0.39561205]\n",
      "Escritura Escritura [0.3443369 0.6556631]\n",
      "Escritura Escritura [0.33147837 0.66852163]\n",
      "Escritura Escritura [0.13316806 0.86683194]\n",
      "Escritura Escritura [0.32528774 0.67471226]\n",
      "Escritura Desiste [0.57822454 0.42177546]\n",
      "Escritura Desiste [0.66397441 0.33602559]\n",
      "Escritura Escritura [0.21073416 0.78926584]\n",
      "Escritura Desiste [0.56075009 0.43924991]\n",
      "Escritura Desiste [0.63603153 0.36396847]\n",
      "Escritura Escritura [0.28257564 0.71742436]\n",
      "Escritura Desiste [0.690218 0.309782]\n",
      "Escritura Escritura [0.30143459 0.69856541]\n",
      "Escritura Escritura [0.22677608 0.77322392]\n",
      "Escritura Desiste [0.6295962 0.3704038]\n",
      "Escritura Desiste [0.62354916 0.37645084]\n",
      "Escritura Escritura [0.25294875 0.74705125]\n",
      "Escritura Desiste [0.54100258 0.45899742]\n",
      "Escritura Escritura [0.36576894 0.63423106]\n",
      "Escritura Escritura [0.23720165 0.76279835]\n",
      "Escritura Escritura [0.32391619 0.67608381]\n",
      "Escritura Escritura [0.28257014 0.71742986]\n",
      "Escritura Escritura [0.22093379 0.77906621]\n",
      "Escritura Escritura [0.18690463 0.81309537]\n",
      "Escritura Escritura [0.09161842 0.90838158]\n",
      "Escritura Escritura [0.22819902 0.77180098]\n",
      "Escritura Escritura [0.13540444 0.86459556]\n",
      "Escritura Escritura [0.4603224 0.5396776]\n",
      "Escritura Escritura [0.36365224 0.63634776]\n",
      "Escritura Escritura [0.23017803 0.76982197]\n",
      "Escritura Desiste [0.50618059 0.49381941]\n",
      "Escritura Desiste [0.5615309 0.4384691]\n",
      "Escritura Escritura [0.34162767 0.65837233]\n",
      "Escritura Desiste [0.68045333 0.31954667]\n",
      "Escritura Escritura [0.30074224 0.69925776]\n",
      "Escritura Desiste [0.6514597 0.3485403]\n",
      "Escritura Escritura [0.27337509 0.72662491]\n",
      "Escritura Escritura [0.33329797 0.66670203]\n",
      "Escritura Escritura [0.21514755 0.78485245]\n",
      "Escritura Desiste [0.66132448 0.33867552]\n",
      "Escritura Escritura [0.33036282 0.66963718]\n",
      "Escritura Escritura [0.25632379 0.74367621]\n",
      "Escritura Desiste [0.52520236 0.47479764]\n",
      "Escritura Escritura [0.19390788 0.80609212]\n",
      "Escritura Escritura [0.29571443 0.70428557]\n",
      "Escritura Escritura [0.26637337 0.73362663]\n",
      "Escritura Escritura [0.29650022 0.70349978]\n",
      "Escritura Escritura [0.35043385 0.64956615]\n",
      "Escritura Desiste [0.58303874 0.41696126]\n",
      "Escritura Escritura [0.400976 0.599024]\n",
      "Escritura Desiste [0.63299839 0.36700161]\n",
      "Escritura Escritura [0.31576025 0.68423975]\n",
      "Escritura Escritura [0.19171231 0.80828769]\n",
      "Escritura Desiste [0.52465968 0.47534032]\n",
      "Escritura Escritura [0.2282567 0.7717433]\n",
      "Escritura Desiste [0.64468237 0.35531763]\n",
      "Escritura Escritura [0.27354034 0.72645966]\n",
      "Escritura Desiste [0.65726234 0.34273766]\n",
      "Escritura Escritura [0.20547183 0.79452817]\n",
      "Escritura Escritura [0.35391841 0.64608159]\n",
      "Escritura Desiste [0.7354356 0.2645644]\n",
      "Escritura Escritura [0.36000049 0.63999951]\n",
      "Escritura Desiste [0.73238081 0.26761919]\n",
      "Escritura Escritura [0.26223061 0.73776939]\n",
      "Escritura Desiste [0.60264703 0.39735297]\n",
      "Escritura Escritura [0.28954977 0.71045023]\n",
      "Escritura Escritura [0.26956769 0.73043231]\n",
      "Escritura Escritura [0.31784595 0.68215405]\n",
      "Escritura Desiste [0.57721751 0.42278249]\n",
      "Escritura Escritura [0.20826847 0.79173153]\n",
      "Escritura Escritura [0.25285118 0.74714882]\n",
      "Escritura Desiste [0.65406727 0.34593273]\n",
      "Escritura Escritura [0.26571991 0.73428009]\n",
      "Escritura Escritura [0.29980194 0.70019806]\n",
      "Escritura Escritura [0.23800996 0.76199004]\n",
      "Escritura Escritura [0.25499316 0.74500684]\n",
      "Escritura Escritura [0.36920718 0.63079282]\n",
      "Escritura Desiste [0.58421041 0.41578959]\n",
      "Escritura Escritura [0.35295395 0.64704605]\n",
      "Escritura Escritura [0.34253729 0.65746271]\n",
      "Escritura Escritura [0.16987475 0.83012525]\n",
      "Escritura Escritura [0.14818254 0.85181746]\n",
      "Escritura Desiste [0.69713489 0.30286511]\n",
      "Escritura Desiste [0.65726502 0.34273498]\n",
      "Escritura Escritura [0.48693206 0.51306794]\n",
      "Escritura Escritura [0.29733778 0.70266222]\n",
      "Escritura Desiste [0.91314121 0.08685879]\n",
      "Escritura Escritura [0.26720983 0.73279017]\n",
      "Escritura Escritura [0.24595814 0.75404186]\n",
      "Escritura Desiste [0.69719203 0.30280797]\n",
      "Escritura Desiste [0.58784735 0.41215265]\n",
      "Escritura Escritura [0.25325962 0.74674038]\n",
      "Escritura Desiste [0.55580229 0.44419771]\n",
      "Escritura Escritura [0.30259129 0.69740871]\n",
      "Escritura Escritura [0.23923864 0.76076136]\n",
      "Escritura Escritura [0.4111666 0.5888334]\n",
      "Escritura Escritura [0.23944544 0.76055456]\n",
      "Escritura Desiste [0.55272138 0.44727862]\n",
      "Escritura Desiste [0.5664707 0.4335293]\n",
      "Escritura Desiste [0.70405735 0.29594265]\n",
      "Escritura Escritura [0.26516968 0.73483032]\n",
      "Escritura Desiste [0.61104567 0.38895433]\n",
      "Escritura Escritura [0.23898046 0.76101954]\n",
      "Escritura Escritura [0.12087806 0.87912194]\n",
      "Escritura Desiste [0.62109527 0.37890473]\n",
      "Escritura Escritura [0.2402081 0.7597919]\n",
      "Escritura Desiste [0.6540738 0.3459262]\n",
      "Escritura Escritura [0.3662423 0.6337577]\n",
      "Escritura Escritura [0.24043363 0.75956637]\n",
      "Escritura Escritura [0.22467035 0.77532965]\n",
      "Escritura Desiste [0.53123154 0.46876846]\n",
      "Escritura Escritura [0.20166042 0.79833958]\n",
      "Escritura Escritura [0.31010878 0.68989122]\n",
      "Escritura Escritura [0.4765583 0.5234417]\n",
      "Escritura Escritura [0.37967089 0.62032911]\n",
      "Escritura Desiste [0.74396421 0.25603579]\n",
      "Escritura Escritura [0.36738138 0.63261862]\n",
      "Escritura Escritura [0.18384973 0.81615027]\n",
      "Escritura Escritura [0.22854235 0.77145765]\n",
      "Escritura Escritura [0.20236444 0.79763556]\n",
      "Escritura Escritura [0.47988867 0.52011133]\n",
      "Escritura Desiste [0.62074815 0.37925185]\n",
      "Escritura Desiste [0.69405234 0.30594766]\n",
      "Escritura Escritura [0.34516729 0.65483271]\n",
      "Escritura Escritura [0.42031251 0.57968749]\n",
      "Escritura Escritura [0.40709714 0.59290286]\n",
      "Escritura Escritura [0.31714716 0.68285284]\n",
      "Escritura Escritura [0.33226765 0.66773235]\n",
      "Escritura Escritura [0.41485284 0.58514716]\n",
      "Escritura Desiste [0.66075982 0.33924018]\n",
      "Escritura Desiste [0.56540743 0.43459257]\n",
      "Escritura Escritura [0.17036265 0.82963735]\n",
      "Escritura Escritura [0.30373277 0.69626723]\n",
      "Escritura Escritura [0.26453753 0.73546247]\n",
      "Escritura Escritura [0.28053511 0.71946489]\n",
      "Escritura Desiste [0.71581904 0.28418096]\n",
      "Escritura Escritura [0.25781514 0.74218486]\n",
      "Escritura Desiste [0.62365307 0.37634693]\n",
      "Escritura Escritura [0.30560199 0.69439801]\n",
      "Escritura Escritura [0.10747382 0.89252618]\n",
      "Escritura Escritura [0.26701624 0.73298376]\n",
      "Escritura Escritura [0.38126573 0.61873427]\n",
      "Escritura Escritura [0.26008868 0.73991132]\n",
      "Escritura Escritura [0.1664031 0.8335969]\n",
      "Escritura Escritura [0.38184947 0.61815053]\n",
      "Escritura Desiste [0.75118717 0.24881283]\n",
      "Escritura Desiste [0.57118908 0.42881092]\n",
      "Escritura Desiste [0.56224752 0.43775248]\n",
      "Escritura Escritura [0.14038968 0.85961032]\n",
      "Escritura Desiste [0.64107132 0.35892868]\n",
      "Escritura Desiste [0.55651137 0.44348863]\n",
      "Escritura Escritura [0.37623248 0.62376752]\n",
      "Escritura Desiste [0.5548146 0.4451854]\n",
      "Escritura Desiste [0.75960249 0.24039751]\n",
      "Escritura Escritura [0.28981104 0.71018896]\n",
      "Escritura Escritura [0.31098362 0.68901638]\n",
      "Escritura Desiste [0.70088583 0.29911417]\n",
      "Escritura Escritura [0.2011377 0.7988623]\n",
      "Escritura Desiste [0.53335433 0.46664567]\n",
      "Escritura Escritura [0.2395716 0.7604284]\n",
      "Escritura Desiste [0.5820935 0.4179065]\n",
      "Escritura Escritura [0.23064133 0.76935867]\n",
      "Escritura Escritura [0.30612438 0.69387562]\n",
      "Escritura Escritura [0.17837291 0.82162709]\n",
      "Escritura Escritura [0.22290942 0.77709058]\n",
      "Escritura Desiste [0.68473353 0.31526647]\n",
      "Escritura Escritura [0.25320438 0.74679562]\n",
      "Escritura Escritura [0.26773631 0.73226369]\n",
      "Escritura Escritura [0.19435392 0.80564608]\n",
      "Escritura Escritura [0.27446754 0.72553246]\n",
      "Escritura Escritura [0.30860067 0.69139933]\n",
      "Escritura Desiste [0.61952863 0.38047137]\n",
      "Escritura Escritura [0.26330599 0.73669401]\n",
      "Escritura Desiste [0.80673226 0.19326774]\n",
      "Escritura Desiste [0.69553059 0.30446941]\n",
      "Escritura Desiste [0.78016991 0.21983009]\n",
      "Escritura Escritura [0.48658231 0.51341769]\n",
      "Escritura Desiste [0.70814628 0.29185372]\n",
      "Escritura Escritura [0.37597761 0.62402239]\n",
      "Escritura Escritura [0.31424805 0.68575195]\n",
      "Escritura Escritura [0.30039971 0.69960029]\n",
      "Escritura Escritura [0.26712332 0.73287668]\n",
      "Escritura Escritura [0.34278332 0.65721668]\n",
      "Escritura Escritura [0.45698282 0.54301718]\n",
      "Escritura Desiste [0.73482934 0.26517066]\n",
      "Escritura Escritura [0.36226152 0.63773848]\n",
      "Escritura Escritura [0.40140519 0.59859481]\n",
      "Escritura Escritura [0.31274614 0.68725386]\n",
      "Escritura Escritura [0.29044521 0.70955479]\n",
      "Escritura Escritura [0.24399106 0.75600894]\n",
      "Escritura Escritura [0.27694274 0.72305726]\n",
      "Escritura Escritura [0.34466233 0.65533767]\n",
      "Escritura Escritura [0.37252119 0.62747881]\n",
      "Escritura Escritura [0.28475979 0.71524021]\n",
      "Escritura Escritura [0.18424668 0.81575332]\n",
      "Escritura Desiste [0.58545806 0.41454194]\n",
      "Escritura Escritura [0.30823531 0.69176469]\n",
      "Escritura Escritura [0.22529714 0.77470286]\n",
      "Escritura Escritura [0.33024278 0.66975722]\n",
      "Escritura Escritura [0.40566539 0.59433461]\n",
      "Escritura Escritura [0.09750443 0.90249557]\n",
      "Escritura Desiste [0.57507031 0.42492969]\n",
      "Escritura Desiste [0.7195772 0.2804228]\n",
      "Escritura Desiste [0.64321396 0.35678604]\n",
      "Escritura Escritura [0.25514997 0.74485003]\n",
      "Escritura Desiste [0.70729954 0.29270046]\n",
      "Escritura Escritura [0.44939174 0.55060826]\n",
      "Escritura Escritura [0.2938701 0.7061299]\n",
      "Escritura Escritura [0.15113184 0.84886816]\n",
      "Escritura Escritura [0.25910572 0.74089428]\n",
      "Escritura Escritura [0.25676304 0.74323696]\n",
      "Escritura Escritura [0.35701124 0.64298876]\n",
      "Escritura Desiste [0.69075909 0.30924091]\n",
      "Escritura Desiste [0.62044685 0.37955315]\n",
      "Escritura Escritura [0.4767445 0.5232555]\n",
      "Escritura Desiste [0.58550781 0.41449219]\n",
      "Escritura Escritura [0.23152374 0.76847626]\n",
      "Escritura Escritura [0.22201984 0.77798016]\n",
      "Escritura Escritura [0.41008974 0.58991026]\n",
      "Escritura Desiste [0.65456925 0.34543075]\n",
      "Escritura Escritura [0.23063723 0.76936277]\n",
      "Escritura Escritura [0.257056 0.742944]\n",
      "Escritura Desiste [0.5583029 0.4416971]\n",
      "Escritura Desiste [0.5530422 0.4469578]\n",
      "Escritura Escritura [0.33153566 0.66846434]\n",
      "Escritura Escritura [0.14862515 0.85137485]\n",
      "Escritura Desiste [0.66255899 0.33744101]\n",
      "Escritura Escritura [0.22515866 0.77484134]\n",
      "Escritura Escritura [0.34145123 0.65854877]\n",
      "Escritura Desiste [0.61339352 0.38660648]\n",
      "Escritura Desiste [0.50481836 0.49518164]\n",
      "Escritura Desiste [0.73603866 0.26396134]\n",
      "Escritura Desiste [0.83306966 0.16693034]\n",
      "Escritura Escritura [0.3730421 0.6269579]\n",
      "Escritura Escritura [0.3840993 0.6159007]\n",
      "Escritura Desiste [0.75002816 0.24997184]\n",
      "Escritura Escritura [0.29989735 0.70010265]\n",
      "Escritura Escritura [0.22559376 0.77440624]\n",
      "Escritura Escritura [0.26289341 0.73710659]\n",
      "Escritura Escritura [0.27482252 0.72517748]\n",
      "Escritura Escritura [0.35308778 0.64691222]\n",
      "Escritura Desiste [0.81494033 0.18505967]\n",
      "Escritura Escritura [0.35386566 0.64613434]\n",
      "Escritura Escritura [0.13880308 0.86119692]\n",
      "Escritura Desiste [0.55772073 0.44227927]\n",
      "Escritura Escritura [0.24470046 0.75529954]\n",
      "Escritura Escritura [0.13508997 0.86491003]\n",
      "Escritura Escritura [0.35178434 0.64821566]\n",
      "Escritura Escritura [0.21233774 0.78766226]\n",
      "Escritura Desiste [0.67102559 0.32897441]\n",
      "Escritura Escritura [0.39646257 0.60353743]\n",
      "Escritura Escritura [0.1567431 0.8432569]\n",
      "Escritura Escritura [0.39070262 0.60929738]\n",
      "Escritura Escritura [0.28949865 0.71050135]\n",
      "Escritura Escritura [0.25481985 0.74518015]\n",
      "Escritura Desiste [0.64305135 0.35694865]\n",
      "Escritura Escritura [0.33366894 0.66633106]\n",
      "Escritura Escritura [0.27348199 0.72651801]\n",
      "Escritura Escritura [0.30001129 0.69998871]\n",
      "Escritura Escritura [0.22310487 0.77689513]\n",
      "Escritura Desiste [0.58505496 0.41494504]\n",
      "Escritura Escritura [0.18560389 0.81439611]\n",
      "Escritura Desiste [0.51762793 0.48237207]\n",
      "Escritura Escritura [0.18183449 0.81816551]\n",
      "Escritura Escritura [0.11319794 0.88680206]\n",
      "Escritura Escritura [0.46215745 0.53784255]\n",
      "Escritura Escritura [0.36513628 0.63486372]\n",
      "Escritura Desiste [0.67914791 0.32085209]\n",
      "Escritura Desiste [0.69875153 0.30124847]\n",
      "Escritura Desiste [0.66942874 0.33057126]\n",
      "Escritura Escritura [0.27207848 0.72792152]\n",
      "Escritura Escritura [0.38335283 0.61664717]\n",
      "Escritura Desiste [0.56622623 0.43377377]\n",
      "Escritura Desiste [0.52396743 0.47603257]\n",
      "Escritura Escritura [0.35929739 0.64070261]\n",
      "Escritura Desiste [0.63072676 0.36927324]\n",
      "Escritura Escritura [0.36053025 0.63946975]\n",
      "Escritura Escritura [0.19974398 0.80025602]\n",
      "Escritura Escritura [0.22605963 0.77394037]\n",
      "Escritura Escritura [0.29187724 0.70812276]\n",
      "Escritura Escritura [0.06902018 0.93097982]\n",
      "Escritura Escritura [0.27927549 0.72072451]\n",
      "Escritura Escritura [0.24691513 0.75308487]\n",
      "Escritura Escritura [0.20410804 0.79589196]\n",
      "Escritura Escritura [0.2793568 0.7206432]\n",
      "Escritura Escritura [0.30494944 0.69505056]\n",
      "Escritura Escritura [0.20785417 0.79214583]\n",
      "Escritura Escritura [0.20597135 0.79402865]\n",
      "Escritura Desiste [0.69822668 0.30177332]\n",
      "Escritura Desiste [0.59017108 0.40982892]\n",
      "Escritura Desiste [0.62484679 0.37515321]\n",
      "Escritura Escritura [0.13290073 0.86709927]\n",
      "Escritura Escritura [0.17308211 0.82691789]\n",
      "Escritura Escritura [0.38552552 0.61447448]\n",
      "Escritura Escritura [0.36521301 0.63478699]\n",
      "Escritura Escritura [0.25762379 0.74237621]\n",
      "Escritura Escritura [0.28509908 0.71490092]\n",
      "Escritura Escritura [0.35174106 0.64825894]\n",
      "Escritura Desiste [0.74725677 0.25274323]\n",
      "Escritura Escritura [0.35342695 0.64657305]\n",
      "Escritura Desiste [0.68517971 0.31482029]\n",
      "Escritura Desiste [0.51020741 0.48979259]\n",
      "Escritura Escritura [0.27931576 0.72068424]\n",
      "Escritura Escritura [0.33808297 0.66191703]\n",
      "Escritura Escritura [0.32181459 0.67818541]\n",
      "Escritura Escritura [0.48560681 0.51439319]\n",
      "Escritura Escritura [0.27245529 0.72754471]\n",
      "Escritura Escritura [0.27784078 0.72215922]\n",
      "Escritura Escritura [0.33798778 0.66201222]\n",
      "Escritura Escritura [0.20973355 0.79026645]\n",
      "Escritura Desiste [0.69797256 0.30202744]\n",
      "Escritura Desiste [0.55740354 0.44259646]\n",
      "Escritura Desiste [0.68736398 0.31263602]\n",
      "Escritura Escritura [0.1645219 0.8354781]\n",
      "Escritura Escritura [0.32923285 0.67076715]\n",
      "Escritura Desiste [0.61532673 0.38467327]\n",
      "Escritura Desiste [0.62442871 0.37557129]\n",
      "Escritura Escritura [0.22293271 0.77706729]\n",
      "Escritura Desiste [0.53477256 0.46522744]\n",
      "Escritura Escritura [0.3427143 0.6572857]\n",
      "Escritura Desiste [0.69024161 0.30975839]\n",
      "Escritura Desiste [0.64704808 0.35295192]\n",
      "Escritura Desiste [0.50463501 0.49536499]\n",
      "Escritura Escritura [0.22110819 0.77889181]\n",
      "Escritura Desiste [0.63249974 0.36750026]\n",
      "Escritura Desiste [0.67901177 0.32098823]\n",
      "Escritura Desiste [0.64212331 0.35787669]\n",
      "Escritura Desiste [0.64286479 0.35713521]\n",
      "Escritura Escritura [0.34474375 0.65525625]\n",
      "Escritura Escritura [0.38794794 0.61205206]\n",
      "Escritura Escritura [0.25013414 0.74986586]\n",
      "Escritura Desiste [0.53026686 0.46973314]\n",
      "Escritura Escritura [0.2593895 0.7406105]\n",
      "Escritura Desiste [0.68038789 0.31961211]\n",
      "Escritura Escritura [0.36063743 0.63936257]\n",
      "Escritura Escritura [0.29146817 0.70853183]\n",
      "Escritura Desiste [0.50847796 0.49152204]\n",
      "Escritura Escritura [0.25580877 0.74419123]\n",
      "Escritura Escritura [0.13575026 0.86424974]\n",
      "Escritura Escritura [0.22460616 0.77539384]\n",
      "Escritura Escritura [0.35210298 0.64789702]\n",
      "Escritura Escritura [0.22351204 0.77648796]\n",
      "Escritura Escritura [0.2200037 0.7799963]\n",
      "Escritura Escritura [0.35344077 0.64655923]\n",
      "Escritura Escritura [0.33809137 0.66190863]\n",
      "Escritura Escritura [0.19511992 0.80488008]\n",
      "Escritura Escritura [0.14097616 0.85902384]\n",
      "Escritura Escritura [0.32026795 0.67973205]\n",
      "Escritura Desiste [0.62442696 0.37557304]\n",
      "Escritura Desiste [0.62281359 0.37718641]\n",
      "Escritura Escritura [0.40394119 0.59605881]\n",
      "Escritura Desiste [0.77440743 0.22559257]\n",
      "Escritura Escritura [0.23509543 0.76490457]\n",
      "Escritura Desiste [0.64091213 0.35908787]\n",
      "Escritura Desiste [0.81196024 0.18803976]\n",
      "Escritura Desiste [0.53438627 0.46561373]\n",
      "Escritura Desiste [0.57041871 0.42958129]\n",
      "Escritura Escritura [0.38031564 0.61968436]\n",
      "Escritura Escritura [0.27422817 0.72577183]\n",
      "Escritura Escritura [0.11206605 0.88793395]\n",
      "Escritura Desiste [0.6296313 0.3703687]\n",
      "Escritura Escritura [0.27699099 0.72300901]\n",
      "Escritura Escritura [0.33539887 0.66460113]\n",
      "Escritura Escritura [0.39626689 0.60373311]\n",
      "Escritura Desiste [0.55120128 0.44879872]\n",
      "Escritura Escritura [0.31961293 0.68038707]\n",
      "Escritura Escritura [0.33533862 0.66466138]\n",
      "Escritura Escritura [0.16622356 0.83377644]\n",
      "Escritura Desiste [0.63821211 0.36178789]\n",
      "Escritura Desiste [0.5171698 0.4828302]\n",
      "Escritura Escritura [0.18800687 0.81199313]\n",
      "Escritura Escritura [0.08887811 0.91112189]\n",
      "Escritura Escritura [0.41991223 0.58008777]\n",
      "Escritura Escritura [0.41323151 0.58676849]\n",
      "Escritura Desiste [0.75545503 0.24454497]\n",
      "Escritura Escritura [0.4550502 0.5449498]\n",
      "Escritura Desiste [0.63812976 0.36187024]\n",
      "Escritura Escritura [0.21673092 0.78326908]\n",
      "Escritura Escritura [0.21273724 0.78726276]\n",
      "Escritura Escritura [0.28268744 0.71731256]\n",
      "Escritura Escritura [0.4882894 0.5117106]\n",
      "Escritura Escritura [0.32447184 0.67552816]\n",
      "Escritura Escritura [0.27397311 0.72602689]\n",
      "Escritura Escritura [0.19300074 0.80699926]\n",
      "Escritura Escritura [0.35769283 0.64230717]\n",
      "Escritura Escritura [0.34731884 0.65268116]\n",
      "Escritura Desiste [0.54264086 0.45735914]\n",
      "Escritura Escritura [0.21745 0.78255]\n",
      "Escritura Escritura [0.30187521 0.69812479]\n",
      "Escritura Escritura [0.29676939 0.70323061]\n",
      "Escritura Escritura [0.24495942 0.75504058]\n",
      "Escritura Desiste [0.54389568 0.45610432]\n",
      "Escritura Escritura [0.2960167 0.7039833]\n",
      "Escritura Escritura [0.17207313 0.82792687]\n",
      "Escritura Escritura [0.14337745 0.85662255]\n",
      "Escritura Escritura [0.16826411 0.83173589]\n",
      "Escritura Escritura [0.34137352 0.65862648]\n",
      "Escritura Desiste [0.61724245 0.38275755]\n",
      "Escritura Desiste [0.78050945 0.21949055]\n",
      "Escritura Desiste [0.65921957 0.34078043]\n",
      "Escritura Escritura [0.29383833 0.70616167]\n",
      "Escritura Escritura [0.23448222 0.76551778]\n",
      "Escritura Escritura [0.33635095 0.66364905]\n",
      "Escritura Escritura [0.30676506 0.69323494]\n",
      "Escritura Desiste [0.51952509 0.48047491]\n",
      "Escritura Escritura [0.49547348 0.50452652]\n",
      "Escritura Escritura [0.28128299 0.71871701]\n",
      "Escritura Escritura [0.30861202 0.69138798]\n",
      "Escritura Desiste [0.65450337 0.34549663]\n",
      "Escritura Desiste [0.55034687 0.44965313]\n",
      "Escritura Escritura [0.26672899 0.73327101]\n",
      "Escritura Desiste [0.57555166 0.42444834]\n",
      "Escritura Escritura [0.37572092 0.62427908]\n",
      "Escritura Desiste [0.70952238 0.29047762]\n",
      "Escritura Escritura [0.29693361 0.70306639]\n",
      "Escritura Desiste [0.74038626 0.25961374]\n",
      "Escritura Escritura [0.36552156 0.63447844]\n",
      "Escritura Escritura [0.30542413 0.69457587]\n",
      "Escritura Escritura [0.2532527 0.7467473]\n",
      "Escritura Escritura [0.23889787 0.76110213]\n",
      "Escritura Escritura [0.3006565 0.6993435]\n",
      "Escritura Escritura [0.28049178 0.71950822]\n",
      "Escritura Desiste [0.70171234 0.29828766]\n",
      "Escritura Escritura [0.28417074 0.71582926]\n",
      "Escritura Desiste [0.57466201 0.42533799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Desiste [0.65257813 0.34742187]\n",
      "Escritura Escritura [0.30319397 0.69680603]\n",
      "Escritura Escritura [0.27830731 0.72169269]\n",
      "Escritura Escritura [0.44691715 0.55308285]\n",
      "Escritura Desiste [0.65372335 0.34627665]\n",
      "Escritura Desiste [0.56888057 0.43111943]\n",
      "Escritura Desiste [0.55428161 0.44571839]\n",
      "Escritura Escritura [0.2708928 0.7291072]\n",
      "Escritura Desiste [0.64845509 0.35154491]\n",
      "Escritura Desiste [0.6315902 0.3684098]\n",
      "Escritura Desiste [0.63233142 0.36766858]\n",
      "Escritura Escritura [0.18193619 0.81806381]\n",
      "Escritura Desiste [0.67462499 0.32537501]\n",
      "Escritura Escritura [0.2645698 0.7354302]\n",
      "Escritura Escritura [0.11178553 0.88821447]\n",
      "Escritura Desiste [0.63718448 0.36281552]\n",
      "Escritura Escritura [0.32999806 0.67000194]\n",
      "Escritura Escritura [0.1882755 0.8117245]\n",
      "Escritura Desiste [0.62297106 0.37702894]\n",
      "Escritura Escritura [0.41240627 0.58759373]\n",
      "Escritura Desiste [0.68576658 0.31423342]\n",
      "Escritura Desiste [0.63633688 0.36366312]\n",
      "Escritura Escritura [0.43045481 0.56954519]\n",
      "Escritura Desiste [0.56823486 0.43176514]\n",
      "Escritura Desiste [0.65161364 0.34838636]\n",
      "Escritura Escritura [0.34539827 0.65460173]\n",
      "Escritura Escritura [0.14322888 0.85677112]\n",
      "Escritura Desiste [0.56813282 0.43186718]\n",
      "Escritura Escritura [0.40247391 0.59752609]\n",
      "Escritura Desiste [0.68280287 0.31719713]\n",
      "Escritura Escritura [0.27005399 0.72994601]\n",
      "Escritura Desiste [0.79599495 0.20400505]\n",
      "Escritura Escritura [0.26076467 0.73923533]\n",
      "Escritura Escritura [0.46906794 0.53093206]\n",
      "Escritura Desiste [0.59123147 0.40876853]\n",
      "Escritura Escritura [0.23313371 0.76686629]\n",
      "Escritura Desiste [0.67850321 0.32149679]\n",
      "Escritura Escritura [0.33961665 0.66038335]\n",
      "Escritura Escritura [0.23574353 0.76425647]\n",
      "Escritura Escritura [0.25583664 0.74416336]\n",
      "Escritura Desiste [0.51506635 0.48493365]\n",
      "Escritura Escritura [0.30065668 0.69934332]\n",
      "Escritura Desiste [0.70990413 0.29009587]\n",
      "Escritura Escritura [0.21090789 0.78909211]\n",
      "Escritura Desiste [0.69485832 0.30514168]\n",
      "Escritura Escritura [0.47254072 0.52745928]\n",
      "Escritura Desiste [0.55882652 0.44117348]\n",
      "Escritura Desiste [0.60732569 0.39267431]\n",
      "Escritura Desiste [0.69510086 0.30489914]\n",
      "Escritura Escritura [0.31703598 0.68296402]\n",
      "Escritura Escritura [0.29299129 0.70700871]\n",
      "Escritura Escritura [0.3478935 0.6521065]\n",
      "Escritura Escritura [0.2579763 0.7420237]\n",
      "Escritura Escritura [0.1081201 0.8918799]\n",
      "Escritura Desiste [0.80415172 0.19584828]\n",
      "Escritura Desiste [0.62239673 0.37760327]\n",
      "Escritura Desiste [0.68921819 0.31078181]\n",
      "Escritura Desiste [0.52721499 0.47278501]\n",
      "Escritura Desiste [0.65035067 0.34964933]\n",
      "Escritura Escritura [0.22749401 0.77250599]\n",
      "Escritura Escritura [0.27735845 0.72264155]\n",
      "Escritura Escritura [0.16920513 0.83079487]\n",
      "Escritura Desiste [0.69692648 0.30307352]\n",
      "Escritura Desiste [0.75762073 0.24237927]\n",
      "Escritura Escritura [0.39964654 0.60035346]\n",
      "Escritura Escritura [0.25959784 0.74040216]\n",
      "Escritura Escritura [0.47904392 0.52095608]\n",
      "Escritura Escritura [0.15333352 0.84666648]\n",
      "Escritura Escritura [0.22106365 0.77893635]\n",
      "Escritura Escritura [0.2906765 0.7093235]\n",
      "Escritura Escritura [0.19562529 0.80437471]\n",
      "Escritura Escritura [0.26063989 0.73936011]\n",
      "Escritura Escritura [0.3293364 0.6706636]\n",
      "Escritura Escritura [0.30992699 0.69007301]\n",
      "Escritura Escritura [0.2217125 0.7782875]\n",
      "Escritura Escritura [0.16235255 0.83764745]\n",
      "Escritura Escritura [0.27689549 0.72310451]\n",
      "Escritura Escritura [0.44700985 0.55299015]\n",
      "Escritura Escritura [0.32511467 0.67488533]\n",
      "Escritura Escritura [0.3909089 0.6090911]\n",
      "Escritura Desiste [0.63660241 0.36339759]\n",
      "Escritura Escritura [0.38537117 0.61462883]\n",
      "Escritura Escritura [0.25924418 0.74075582]\n",
      "Escritura Desiste [0.64588393 0.35411607]\n",
      "Escritura Escritura [0.18567813 0.81432187]\n",
      "Escritura Escritura [0.07519387 0.92480613]\n",
      "Escritura Desiste [0.55776908 0.44223092]\n",
      "Escritura Desiste [0.59248542 0.40751458]\n",
      "Escritura Escritura [0.29601134 0.70398866]\n",
      "Escritura Escritura [0.36732317 0.63267683]\n",
      "Escritura Desiste [0.6740668 0.3259332]\n",
      "Escritura Escritura [0.29775843 0.70224157]\n",
      "Escritura Escritura [0.41804418 0.58195582]\n",
      "Escritura Escritura [0.21864817 0.78135183]\n",
      "Escritura Escritura [0.30899348 0.69100652]\n",
      "Escritura Escritura [0.16779755 0.83220245]\n",
      "Escritura Desiste [0.56025333 0.43974667]\n",
      "Escritura Escritura [0.32869114 0.67130886]\n",
      "Escritura Escritura [0.34137711 0.65862289]\n",
      "Escritura Escritura [0.29071271 0.70928729]\n",
      "Escritura Desiste [0.68194502 0.31805498]\n",
      "Escritura Desiste [0.64223641 0.35776359]\n",
      "Escritura Desiste [0.50408859 0.49591141]\n",
      "Escritura Desiste [0.64210995 0.35789005]\n",
      "Escritura Desiste [0.68393568 0.31606432]\n",
      "Escritura Escritura [0.30999313 0.69000687]\n",
      "Escritura Desiste [0.57565449 0.42434551]\n",
      "Escritura Desiste [0.636307 0.363693]\n",
      "Escritura Desiste [0.66068232 0.33931768]\n",
      "Escritura Escritura [0.18011904 0.81988096]\n",
      "Escritura Escritura [0.25152958 0.74847042]\n",
      "Escritura Escritura [0.14883566 0.85116434]\n",
      "Escritura Escritura [0.27105132 0.72894868]\n",
      "Escritura Escritura [0.2596054 0.7403946]\n",
      "Escritura Desiste [0.64103459 0.35896541]\n",
      "Escritura Escritura [0.4985964 0.5014036]\n",
      "Escritura Desiste [0.70418229 0.29581771]\n",
      "Escritura Escritura [0.35230329 0.64769671]\n",
      "Escritura Escritura [0.20710995 0.79289005]\n",
      "Escritura Escritura [0.49819224 0.50180776]\n",
      "Escritura Escritura [0.24046792 0.75953208]\n",
      "Escritura Escritura [0.29994583 0.70005417]\n",
      "Escritura Desiste [0.65539406 0.34460594]\n",
      "Escritura Escritura [0.31139058 0.68860942]\n",
      "Escritura Escritura [0.34326583 0.65673417]\n",
      "Escritura Desiste [0.54762515 0.45237485]\n",
      "Escritura Escritura [0.06257374 0.93742626]\n",
      "Escritura Escritura [0.16747064 0.83252936]\n",
      "Escritura Desiste [0.7841557 0.2158443]\n",
      "Escritura Desiste [0.60443103 0.39556897]\n",
      "Escritura Escritura [0.21642924 0.78357076]\n",
      "Escritura Escritura [0.3063895 0.6936105]\n",
      "Escritura Escritura [0.23374466 0.76625534]\n",
      "Escritura Desiste [0.77978783 0.22021217]\n",
      "Escritura Desiste [0.65117971 0.34882029]\n",
      "Escritura Escritura [0.22215696 0.77784304]\n",
      "Escritura Escritura [0.22047669 0.77952331]\n",
      "Escritura Desiste [0.62217022 0.37782978]\n",
      "Escritura Escritura [0.39349226 0.60650774]\n",
      "Escritura Desiste [0.70999112 0.29000888]\n",
      "Escritura Escritura [0.3118049 0.6881951]\n",
      "Escritura Escritura [0.30494657 0.69505343]\n",
      "Escritura Escritura [0.1840279 0.8159721]\n",
      "Escritura Desiste [0.63556653 0.36443347]\n",
      "Escritura Desiste [0.51655335 0.48344665]\n",
      "Escritura Escritura [0.26252329 0.73747671]\n",
      "Escritura Desiste [0.71695695 0.28304305]\n",
      "Escritura Escritura [0.16013385 0.83986615]\n",
      "Escritura Escritura [0.08457948 0.91542052]\n",
      "Escritura Escritura [0.2362914 0.7637086]\n",
      "Escritura Escritura [0.20872806 0.79127194]\n",
      "Escritura Escritura [0.26262748 0.73737252]\n",
      "Escritura Escritura [0.37860175 0.62139825]\n",
      "Escritura Desiste [0.57881385 0.42118615]\n",
      "Escritura Desiste [0.72139792 0.27860208]\n",
      "Escritura Desiste [0.65310022 0.34689978]\n",
      "Escritura Desiste [0.63652251 0.36347749]\n",
      "Escritura Escritura [0.39451466 0.60548534]\n",
      "Escritura Desiste [0.74920483 0.25079517]\n",
      "Escritura Desiste [0.67502762 0.32497238]\n",
      "Escritura Escritura [0.34743617 0.65256383]\n",
      "Escritura Escritura [0.30615626 0.69384374]\n",
      "Escritura Escritura [0.39139247 0.60860753]\n",
      "Escritura Escritura [0.12093357 0.87906643]\n",
      "Escritura Escritura [0.21341657 0.78658343]\n",
      "Escritura Escritura [0.19526241 0.80473759]\n",
      "Escritura Desiste [0.71732353 0.28267647]\n",
      "Escritura Escritura [0.1566109 0.8433891]\n",
      "Escritura Escritura [0.23304202 0.76695798]\n",
      "Escritura Desiste [0.7298382 0.2701618]\n",
      "Escritura Desiste [0.83140358 0.16859642]\n",
      "Escritura Desiste [0.52312383 0.47687617]\n",
      "Escritura Escritura [0.22539125 0.77460875]\n",
      "Escritura Desiste [0.66361725 0.33638275]\n",
      "Escritura Desiste [0.70032415 0.29967585]\n",
      "Escritura Escritura [0.14940501 0.85059499]\n",
      "Escritura Desiste [0.74348545 0.25651455]\n",
      "Escritura Escritura [0.13915913 0.86084087]\n",
      "Escritura Escritura [0.10548446 0.89451554]\n",
      "Escritura Escritura [0.353429 0.646571]\n",
      "Escritura Desiste [0.57166132 0.42833868]\n",
      "Escritura Escritura [0.27241648 0.72758352]\n",
      "Escritura Desiste [0.57544343 0.42455657]\n",
      "Escritura Escritura [0.17080264 0.82919736]\n",
      "Escritura Desiste [0.52496534 0.47503466]\n",
      "Escritura Escritura [0.1878897 0.8121103]\n",
      "Escritura Desiste [0.67567889 0.32432111]\n",
      "Escritura Escritura [0.23254016 0.76745984]\n",
      "Escritura Escritura [0.39829382 0.60170618]\n",
      "Escritura Desiste [0.55984927 0.44015073]\n",
      "Escritura Escritura [0.27994863 0.72005137]\n",
      "Escritura Desiste [0.62045321 0.37954679]\n",
      "Escritura Escritura [0.39679952 0.60320048]\n",
      "Escritura Desiste [0.54169501 0.45830499]\n",
      "Escritura Desiste [0.75614389 0.24385611]\n",
      "Escritura Desiste [0.69595884 0.30404116]\n",
      "Escritura Desiste [0.51544007 0.48455993]\n",
      "Escritura Desiste [0.69598313 0.30401687]\n",
      "Escritura Desiste [0.73871868 0.26128132]\n",
      "Escritura Escritura [0.08818342 0.91181658]\n",
      "Escritura Desiste [0.58925033 0.41074967]\n",
      "Escritura Desiste [0.73947995 0.26052005]\n",
      "Escritura Desiste [0.57113109 0.42886891]\n",
      "Escritura Desiste [0.62801357 0.37198643]\n",
      "Escritura Escritura [0.17437428 0.82562572]\n",
      "Escritura Desiste [0.57065313 0.42934687]\n",
      "Escritura Escritura [0.3892845 0.6107155]\n",
      "Escritura Escritura [0.22713839 0.77286161]\n",
      "Escritura Desiste [0.60834017 0.39165983]\n",
      "Escritura Escritura [0.38942992 0.61057008]\n",
      "Escritura Desiste [0.6181083 0.3818917]\n",
      "Escritura Escritura [0.27012777 0.72987223]\n",
      "Escritura Desiste [0.75728608 0.24271392]\n",
      "Escritura Escritura [0.39082833 0.60917167]\n",
      "Escritura Desiste [0.59993274 0.40006726]\n",
      "Escritura Escritura [0.12110981 0.87889019]\n",
      "Escritura Escritura [0.38212847 0.61787153]\n",
      "Escritura Escritura [0.40977469 0.59022531]\n",
      "Escritura Escritura [0.33618251 0.66381749]\n",
      "Escritura Escritura [0.3220316 0.6779684]\n",
      "Escritura Escritura [0.01601391 0.98398609]\n",
      "Escritura Desiste [0.60022887 0.39977113]\n",
      "Escritura Escritura [0.28407742 0.71592258]\n",
      "Escritura Desiste [0.62723138 0.37276862]\n",
      "Escritura Desiste [0.74201688 0.25798312]\n",
      "Escritura Escritura [0.2878738 0.7121262]\n",
      "Escritura Desiste [0.66923854 0.33076146]\n",
      "Escritura Escritura [0.29205428 0.70794572]\n",
      "Escritura Escritura [0.1917099 0.8082901]\n",
      "Escritura Escritura [0.3681394 0.6318606]\n",
      "Escritura Escritura [0.19248649 0.80751351]\n",
      "Escritura Desiste [0.55895999 0.44104001]\n",
      "Escritura Desiste [0.66163778 0.33836222]\n",
      "Escritura Escritura [0.26689013 0.73310987]\n",
      "Escritura Escritura [0.21277189 0.78722811]\n",
      "Escritura Escritura [0.21134307 0.78865693]\n",
      "Escritura Desiste [0.61300024 0.38699976]\n",
      "Escritura Desiste [0.58155551 0.41844449]\n",
      "Escritura Escritura [0.27539364 0.72460636]\n",
      "Escritura Escritura [0.36258505 0.63741495]\n",
      "Escritura Escritura [0.45584683 0.54415317]\n",
      "Escritura Desiste [0.59330371 0.40669629]\n",
      "Escritura Escritura [0.33323654 0.66676346]\n",
      "Escritura Escritura [0.42766606 0.57233394]\n",
      "Escritura Escritura [0.24300244 0.75699756]\n",
      "Escritura Desiste [0.67097186 0.32902814]\n",
      "Escritura Desiste [0.56437512 0.43562488]\n",
      "Escritura Desiste [0.68272042 0.31727958]\n",
      "Escritura Desiste [0.65171675 0.34828325]\n",
      "Escritura Desiste [0.58326402 0.41673598]\n",
      "Escritura Desiste [0.6196294 0.3803706]\n",
      "Escritura Escritura [0.34565398 0.65434602]\n",
      "Escritura Desiste [0.71733279 0.28266721]\n",
      "Escritura Desiste [0.7009662 0.2990338]\n",
      "Escritura Escritura [0.29610687 0.70389313]\n",
      "Escritura Escritura [0.27401963 0.72598037]\n",
      "Escritura Escritura [0.10979183 0.89020817]\n",
      "Escritura Desiste [0.67749862 0.32250138]\n",
      "Escritura Desiste [0.69899124 0.30100876]\n",
      "Escritura Escritura [0.31683776 0.68316224]\n",
      "Escritura Desiste [0.66142514 0.33857486]\n",
      "Escritura Escritura [0.44003905 0.55996095]\n",
      "Escritura Desiste [0.60255057 0.39744943]\n",
      "Escritura Desiste [0.69528556 0.30471444]\n",
      "Escritura Escritura [0.24825176 0.75174824]\n",
      "Escritura Escritura [0.29473555 0.70526445]\n",
      "Escritura Escritura [0.20064113 0.79935887]\n",
      "Escritura Escritura [0.43184094 0.56815906]\n",
      "Escritura Escritura [0.22929564 0.77070436]\n",
      "Escritura Desiste [0.72190814 0.27809186]\n",
      "Escritura Desiste [0.6647701 0.3352299]\n",
      "Escritura Escritura [0.29500464 0.70499536]\n",
      "Escritura Escritura [0.2725005 0.7274995]\n",
      "Escritura Desiste [0.56667306 0.43332694]\n",
      "Escritura Desiste [0.68371189 0.31628811]\n",
      "Escritura Escritura [0.25009007 0.74990993]\n",
      "Escritura Desiste [0.72946005 0.27053995]\n",
      "Escritura Escritura [0.38166824 0.61833176]\n",
      "Escritura Escritura [0.1350031 0.8649969]\n",
      "Escritura Escritura [0.34520003 0.65479997]\n",
      "Escritura Desiste [0.63205994 0.36794006]\n",
      "Escritura Escritura [0.34738559 0.65261441]\n",
      "Escritura Escritura [0.07974965 0.92025035]\n",
      "Escritura Escritura [0.2842517 0.7157483]\n",
      "Escritura Escritura [0.36973045 0.63026955]\n",
      "Escritura Desiste [0.62319811 0.37680189]\n",
      "Escritura Escritura [0.19842727 0.80157273]\n",
      "Escritura Escritura [0.34036475 0.65963525]\n",
      "Escritura Escritura [0.32880173 0.67119827]\n",
      "Escritura Escritura [0.25201697 0.74798303]\n",
      "Escritura Desiste [0.69289019 0.30710981]\n",
      "Escritura Escritura [0.26510614 0.73489386]\n",
      "Escritura Desiste [0.62641654 0.37358346]\n",
      "Escritura Desiste [0.74776015 0.25223985]\n",
      "Escritura Escritura [0.43715229 0.56284771]\n",
      "Escritura Escritura [0.25246318 0.74753682]\n",
      "Escritura Desiste [0.59385442 0.40614558]\n",
      "Escritura Desiste [0.57585984 0.42414016]\n",
      "Escritura Escritura [0.23727091 0.76272909]\n",
      "Escritura Escritura [0.49294389 0.50705611]\n",
      "Escritura Desiste [0.58453107 0.41546893]\n",
      "Escritura Escritura [0.31507614 0.68492386]\n",
      "Escritura Escritura [0.14805661 0.85194339]\n",
      "Escritura Desiste [0.70432138 0.29567862]\n",
      "Escritura Escritura [0.27471223 0.72528777]\n",
      "Escritura Escritura [0.26769744 0.73230256]\n",
      "Escritura Escritura [0.1491646 0.8508354]\n",
      "Escritura Desiste [0.59856839 0.40143161]\n",
      "Escritura Escritura [0.19286421 0.80713579]\n",
      "Escritura Escritura [0.32780956 0.67219044]\n",
      "Escritura Desiste [0.50755132 0.49244868]\n",
      "Escritura Desiste [0.66389625 0.33610375]\n",
      "Escritura Escritura [0.29348819 0.70651181]\n",
      "Escritura Desiste [0.66276962 0.33723038]\n",
      "Escritura Desiste [0.72252282 0.27747718]\n",
      "Escritura Desiste [0.57930351 0.42069649]\n",
      "Escritura Escritura [0.37777395 0.62222605]\n",
      "Escritura Desiste [0.67166597 0.32833403]\n",
      "Escritura Escritura [0.30815511 0.69184489]\n",
      "Escritura Desiste [0.53936839 0.46063161]\n",
      "Escritura Escritura [0.36747094 0.63252906]\n",
      "Escritura Escritura [0.14517292 0.85482708]\n",
      "Escritura Escritura [0.38501887 0.61498113]\n",
      "Escritura Desiste [0.61992222 0.38007778]\n",
      "Escritura Escritura [0.49705879 0.50294121]\n",
      "Escritura Desiste [0.73484286 0.26515714]\n",
      "Escritura Escritura [0.34985496 0.65014504]\n",
      "Escritura Desiste [0.65542258 0.34457742]\n",
      "Escritura Escritura [0.19532657 0.80467343]\n",
      "Escritura Escritura [0.25279264 0.74720736]\n",
      "Escritura Desiste [0.70217952 0.29782048]\n",
      "Escritura Escritura [0.21481798 0.78518202]\n",
      "Escritura Escritura [0.3733311 0.6266689]\n",
      "Escritura Escritura [0.12960046 0.87039954]\n",
      "Escritura Escritura [0.37298249 0.62701751]\n",
      "Escritura Escritura [0.32837932 0.67162068]\n",
      "Escritura Desiste [0.60777187 0.39222813]\n",
      "Escritura Escritura [0.15538697 0.84461303]\n",
      "Escritura Desiste [0.69197895 0.30802105]\n",
      "Escritura Desiste [0.58316785 0.41683215]\n",
      "Escritura Escritura [0.25509245 0.74490755]\n",
      "Escritura Desiste [0.60821597 0.39178403]\n",
      "Escritura Escritura [0.24266942 0.75733058]\n",
      "Escritura Escritura [0.2031045 0.7968955]\n",
      "Escritura Escritura [0.41248463 0.58751537]\n",
      "Escritura Escritura [0.37729545 0.62270455]\n",
      "Escritura Escritura [0.2049874 0.7950126]\n",
      "Escritura Desiste [0.71111348 0.28888652]\n",
      "Escritura Desiste [0.73062908 0.26937092]\n",
      "Escritura Escritura [0.2412393 0.7587607]\n",
      "Escritura Escritura [0.36693454 0.63306546]\n",
      "Escritura Escritura [0.36580841 0.63419159]\n",
      "Escritura Desiste [0.70114528 0.29885472]\n",
      "Escritura Escritura [0.31130326 0.68869674]\n",
      "Escritura Escritura [0.00213498 0.99786502]\n",
      "Escritura Escritura [0.30507624 0.69492376]\n",
      "Escritura Escritura [0.4643213 0.5356787]\n",
      "Escritura Escritura [0.3379474 0.6620526]\n",
      "Escritura Escritura [0.22322339 0.77677661]\n",
      "Escritura Escritura [0.31168331 0.68831669]\n",
      "Escritura Desiste [0.53803804 0.46196196]\n",
      "Escritura Escritura [0.22875771 0.77124229]\n",
      "Escritura Escritura [0.28383304 0.71616696]\n",
      "Escritura Desiste [0.57349614 0.42650386]\n",
      "Escritura Escritura [0.18499525 0.81500475]\n",
      "Escritura Desiste [0.6504617 0.3495383]\n",
      "Escritura Escritura [0.27699509 0.72300491]\n",
      "Escritura Escritura [0.10622708 0.89377292]\n",
      "Escritura Escritura [0.33674877 0.66325123]\n",
      "Escritura Escritura [0.23571598 0.76428402]\n",
      "Escritura Escritura [0.40576278 0.59423722]\n",
      "Escritura Desiste [0.57452331 0.42547669]\n",
      "Escritura Desiste [0.5884051 0.4115949]\n",
      "Escritura Desiste [0.58332871 0.41667129]\n",
      "Escritura Escritura [0.12832385 0.87167615]\n",
      "Escritura Desiste [0.62368902 0.37631098]\n",
      "Escritura Desiste [0.5996137 0.4003863]\n",
      "Escritura Escritura [0.33545357 0.66454643]\n",
      "Escritura Escritura [0.23338372 0.76661628]\n",
      "Escritura Desiste [0.65531367 0.34468633]\n",
      "Escritura Escritura [0.25958885 0.74041115]\n",
      "Escritura Desiste [0.61267055 0.38732945]\n",
      "Escritura Desiste [0.73755309 0.26244691]\n",
      "Escritura Escritura [0.29756144 0.70243856]\n",
      "Escritura Escritura [0.23082174 0.76917826]\n",
      "Escritura Desiste [0.60841896 0.39158104]\n",
      "Escritura Desiste [0.58768689 0.41231311]\n",
      "Escritura Escritura [0.3285034 0.6714966]\n",
      "Escritura Escritura [0.20181109 0.79818891]\n",
      "Escritura Escritura [0.1889889 0.8110111]\n",
      "Escritura Escritura [0.35992937 0.64007063]\n",
      "Escritura Escritura [0.42503232 0.57496768]\n",
      "Escritura Escritura [0.12496078 0.87503922]\n",
      "Escritura Escritura [0.32732368 0.67267632]\n",
      "Escritura Escritura [0.24511174 0.75488826]\n",
      "Escritura Escritura [0.19656838 0.80343162]\n",
      "Escritura Escritura [0.23478466 0.76521534]\n",
      "Escritura Desiste [0.62524225 0.37475775]\n",
      "Escritura Desiste [0.59262906 0.40737094]\n",
      "Escritura Escritura [0.1268654 0.8731346]\n",
      "Escritura Desiste [0.66143752 0.33856248]\n",
      "Escritura Escritura [0.23464013 0.76535987]\n",
      "Escritura Desiste [0.68584875 0.31415125]\n",
      "Escritura Desiste [0.50067024 0.49932976]\n",
      "Escritura Escritura [0.32730938 0.67269062]\n",
      "Escritura Escritura [0.15147565 0.84852435]\n",
      "Escritura Desiste [0.67943574 0.32056426]\n",
      "Escritura Desiste [0.67512854 0.32487146]\n",
      "Escritura Desiste [0.6141291 0.3858709]\n",
      "Escritura Escritura [0.00867565 0.99132435]\n",
      "Escritura Desiste [0.7209281 0.2790719]\n",
      "Escritura Escritura [0.18037717 0.81962283]\n",
      "Escritura Escritura [0.21921783 0.78078217]\n",
      "Escritura Escritura [0.29465922 0.70534078]\n",
      "Escritura Escritura [0.28157438 0.71842562]\n",
      "Escritura Desiste [0.6084122 0.3915878]\n",
      "Escritura Desiste [0.64252989 0.35747011]\n",
      "Escritura Escritura [0.49561221 0.50438779]\n",
      "Escritura Escritura [0.25099831 0.74900169]\n",
      "Escritura Escritura [0.20286926 0.79713074]\n",
      "Escritura Desiste [0.67340767 0.32659233]\n",
      "Escritura Desiste [0.6269292 0.3730708]\n",
      "Escritura Escritura [0.40309804 0.59690196]\n",
      "Escritura Escritura [0.28208546 0.71791454]\n",
      "Escritura Escritura [0.23626175 0.76373825]\n",
      "Escritura Desiste [0.71744907 0.28255093]\n",
      "Escritura Desiste [0.63807035 0.36192965]\n",
      "Escritura Desiste [0.6806605 0.3193395]\n",
      "Escritura Desiste [0.62483763 0.37516237]\n",
      "Escritura Desiste [0.58727099 0.41272901]\n",
      "Escritura Escritura [0.20999466 0.79000534]\n",
      "Escritura Escritura [0.25266537 0.74733463]\n",
      "Escritura Escritura [0.32884749 0.67115251]\n",
      "Escritura Escritura [0.40437934 0.59562066]\n",
      "Escritura Desiste [0.61583475 0.38416525]\n",
      "Escritura Escritura [0.27135623 0.72864377]\n",
      "Escritura Escritura [0.38591889 0.61408111]\n",
      "Escritura Escritura [0.1905341 0.8094659]\n",
      "Escritura Escritura [0.27096463 0.72903537]\n",
      "Escritura Desiste [0.69514057 0.30485943]\n",
      "Escritura Escritura [0.18871071 0.81128929]\n",
      "Escritura Desiste [0.99032803 0.00967197]\n",
      "Escritura Escritura [0.37714698 0.62285302]\n",
      "Escritura Escritura [0.20427424 0.79572576]\n",
      "Escritura Escritura [0.41407022 0.58592978]\n",
      "Escritura Escritura [0.31267239 0.68732761]\n",
      "Escritura Desiste [0.7916861 0.2083139]\n",
      "Escritura Escritura [0.31995614 0.68004386]\n",
      "Escritura Escritura [0.38299582 0.61700418]\n",
      "Escritura Escritura [0.26246592 0.73753408]\n",
      "Escritura Escritura [0.31599399 0.68400601]\n",
      "Escritura Escritura [0.30436759 0.69563241]\n",
      "Escritura Escritura [0.32372227 0.67627773]\n",
      "Escritura Escritura [0.28997809 0.71002191]\n",
      "Escritura Escritura [0.16421391 0.83578609]\n",
      "Escritura Desiste [0.63787867 0.36212133]\n",
      "Escritura Escritura [0.3155635 0.6844365]\n",
      "Escritura Escritura [0.28760093 0.71239907]\n",
      "Escritura Escritura [0.27643056 0.72356944]\n",
      "Escritura Escritura [0.40146892 0.59853108]\n",
      "Escritura Escritura [0.21100732 0.78899268]\n",
      "Escritura Desiste [0.64808 0.35192]\n",
      "Escritura Escritura [0.15951388 0.84048612]\n",
      "Escritura Desiste [0.67554699 0.32445301]\n",
      "Escritura Desiste [0.76731079 0.23268921]\n",
      "Escritura Desiste [0.55757059 0.44242941]\n",
      "Escritura Desiste [0.61372655 0.38627345]\n",
      "Escritura Escritura [0.154717 0.845283]\n",
      "Escritura Escritura [0.40843138 0.59156862]\n",
      "Escritura Escritura [0.30890438 0.69109562]\n",
      "Escritura Escritura [0.30258408 0.69741592]\n",
      "Escritura Desiste [0.55649394 0.44350606]\n",
      "Escritura Escritura [0.31673158 0.68326842]\n",
      "Escritura Escritura [0.45068494 0.54931506]\n",
      "Escritura Escritura [0.15305329 0.84694671]\n",
      "Escritura Escritura [0.2555007 0.7444993]\n",
      "Escritura Escritura [0.25583796 0.74416204]\n",
      "Escritura Escritura [0.24990897 0.75009103]\n",
      "Escritura Escritura [0.27088288 0.72911712]\n",
      "Escritura Escritura [0.40750713 0.59249287]\n",
      "Escritura Escritura [0.18191406 0.81808594]\n",
      "Escritura Escritura [0.31835745 0.68164255]\n",
      "Escritura Escritura [0.22338069 0.77661931]\n",
      "Escritura Escritura [0.31719259 0.68280741]\n",
      "Escritura Escritura [0.11172844 0.88827156]\n",
      "Escritura Escritura [0.27094939 0.72905061]\n",
      "Escritura Escritura [0.15876818 0.84123182]\n",
      "Escritura Escritura [0.24919634 0.75080366]\n",
      "Escritura Escritura [0.36865703 0.63134297]\n",
      "Escritura Desiste [0.61183585 0.38816415]\n",
      "Escritura Escritura [0.27084527 0.72915473]\n",
      "Escritura Desiste [0.77825224 0.22174776]\n",
      "Escritura Desiste [0.70466931 0.29533069]\n",
      "Escritura Escritura [0.25622892 0.74377108]\n",
      "Escritura Escritura [0.18737841 0.81262159]\n",
      "Escritura Escritura [0.28433926 0.71566074]\n",
      "Escritura Desiste [0.66536805 0.33463195]\n",
      "Escritura Escritura [0.20581294 0.79418706]\n",
      "Escritura Escritura [0.23546832 0.76453168]\n",
      "Escritura Desiste [0.75988267 0.24011733]\n",
      "Escritura Escritura [0.18570264 0.81429736]\n",
      "Escritura Desiste [0.72787726 0.27212274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Escritura [0.3376971 0.6623029]\n",
      "Escritura Desiste [0.65016336 0.34983664]\n",
      "Escritura Escritura [0.23470492 0.76529508]\n",
      "Escritura Escritura [0.23115655 0.76884345]\n",
      "Escritura Escritura [0.31150358 0.68849642]\n",
      "Escritura Desiste [0.62435458 0.37564542]\n",
      "Escritura Escritura [0.33503537 0.66496463]\n",
      "Escritura Escritura [0.20465327 0.79534673]\n",
      "Escritura Escritura [0.32678895 0.67321105]\n",
      "Escritura Escritura [0.47211928 0.52788072]\n",
      "Escritura Desiste [0.63671805 0.36328195]\n",
      "Escritura Desiste [0.57428612 0.42571388]\n",
      "Escritura Escritura [0.26884494 0.73115506]\n",
      "Escritura Escritura [0.45532205 0.54467795]\n",
      "Escritura Desiste [0.65480652 0.34519348]\n",
      "Escritura Escritura [0.29300843 0.70699157]\n",
      "Escritura Escritura [0.29075318 0.70924682]\n",
      "Escritura Escritura [0.40671881 0.59328119]\n",
      "Escritura Escritura [0.36516666 0.63483334]\n",
      "Escritura Desiste [0.80495835 0.19504165]\n",
      "Escritura Desiste [0.67840791 0.32159209]\n",
      "Escritura Desiste [0.62401802 0.37598198]\n",
      "Escritura Escritura [0.3294183 0.6705817]\n",
      "Escritura Desiste [0.62419977 0.37580023]\n",
      "Escritura Desiste [0.51488423 0.48511577]\n",
      "Escritura Desiste [0.535971 0.464029]\n",
      "Escritura Escritura [0.39001904 0.60998096]\n",
      "Escritura Desiste [0.59961796 0.40038204]\n",
      "Escritura Escritura [0.303855 0.696145]\n",
      "Escritura Desiste [0.51969602 0.48030398]\n",
      "Escritura Escritura [0.19846097 0.80153903]\n",
      "Escritura Escritura [0.34783843 0.65216157]\n",
      "Escritura Escritura [0.18839712 0.81160288]\n",
      "Escritura Desiste [0.68302586 0.31697414]\n",
      "Escritura Escritura [0.32814057 0.67185943]\n",
      "Escritura Escritura [0.36315541 0.63684459]\n",
      "Escritura Desiste [0.75241497 0.24758503]\n",
      "Escritura Desiste [0.62340548 0.37659452]\n",
      "Escritura Desiste [0.56638438 0.43361562]\n",
      "Escritura Escritura [0.19227213 0.80772787]\n",
      "Escritura Desiste [0.66685088 0.33314912]\n",
      "Escritura Escritura [0.43401614 0.56598386]\n",
      "Escritura Escritura [0.23802674 0.76197326]\n",
      "Escritura Desiste [0.76600283 0.23399717]\n",
      "Escritura Escritura [0.22492513 0.77507487]\n",
      "Escritura Escritura [0.30842553 0.69157447]\n",
      "Escritura Desiste [0.64360153 0.35639847]\n",
      "Escritura Desiste [0.60234654 0.39765346]\n",
      "Escritura Escritura [0.49437635 0.50562365]\n",
      "Escritura Escritura [0.353092 0.646908]\n",
      "Escritura Escritura [0.37714698 0.62285302]\n",
      "Escritura Escritura [0.29663493 0.70336507]\n",
      "Escritura Escritura [0.1574198 0.8425802]\n",
      "Escritura Desiste [0.63417635 0.36582365]\n",
      "Escritura Escritura [0.04699947 0.95300053]\n",
      "Escritura Desiste [0.62382303 0.37617697]\n",
      "Escritura Desiste [0.61266273 0.38733727]\n",
      "Escritura Escritura [0.30217512 0.69782488]\n",
      "Escritura Escritura [0.35327698 0.64672302]\n",
      "Escritura Escritura [0.44692521 0.55307479]\n",
      "Escritura Desiste [0.59426841 0.40573159]\n",
      "Escritura Desiste [0.7838366 0.2161634]\n",
      "Escritura Escritura [0.12703662 0.87296338]\n",
      "Escritura Escritura [0.30986118 0.69013882]\n",
      "Escritura Escritura [0.24890397 0.75109603]\n",
      "Escritura Escritura [0.26547293 0.73452707]\n",
      "Escritura Desiste [0.52341573 0.47658427]\n",
      "Escritura Escritura [0.2612133 0.7387867]\n",
      "Escritura Escritura [0.24290766 0.75709234]\n",
      "Escritura Desiste [0.73093091 0.26906909]\n",
      "Escritura Escritura [0.32894637 0.67105363]\n",
      "Escritura Escritura [0.30396665 0.69603335]\n",
      "Escritura Desiste [0.58175371 0.41824629]\n",
      "Escritura Escritura [0.21373963 0.78626037]\n",
      "Escritura Escritura [0.351027 0.648973]\n",
      "Escritura Desiste [0.59302291 0.40697709]\n",
      "Escritura Escritura [0.47099937 0.52900063]\n",
      "Escritura Escritura [0.23365615 0.76634385]\n",
      "Escritura Escritura [0.31142316 0.68857684]\n",
      "Escritura Escritura [0.25809067 0.74190933]\n",
      "Escritura Desiste [0.57177594 0.42822406]\n",
      "Escritura Desiste [0.69990813 0.30009187]\n",
      "Escritura Desiste [0.61528158 0.38471842]\n",
      "Escritura Escritura [0.31062652 0.68937348]\n",
      "Escritura Desiste [0.68418525 0.31581475]\n",
      "Escritura Escritura [0.41037767 0.58962233]\n",
      "Escritura Desiste [0.52883957 0.47116043]\n",
      "Escritura Escritura [0.1805315 0.8194685]\n",
      "Escritura Escritura [0.06391505 0.93608495]\n",
      "Escritura Desiste [0.7487717 0.2512283]\n",
      "Escritura Desiste [0.58909026 0.41090974]\n",
      "Escritura Escritura [0.16263249 0.83736751]\n",
      "Escritura Escritura [0.28595269 0.71404731]\n",
      "Escritura Escritura [0.38365927 0.61634073]\n",
      "Escritura Escritura [0.24745318 0.75254682]\n",
      "Escritura Escritura [0.39265364 0.60734636]\n",
      "Escritura Escritura [0.17915613 0.82084387]\n",
      "Escritura Escritura [0.30254411 0.69745589]\n",
      "Escritura Escritura [0.19531521 0.80468479]\n",
      "Escritura Escritura [0.37736524 0.62263476]\n",
      "Escritura Escritura [0.26205766 0.73794234]\n",
      "Escritura Desiste [0.62113054 0.37886946]\n",
      "Escritura Desiste [0.68113741 0.31886259]\n",
      "Escritura Escritura [0.23205194 0.76794806]\n",
      "Escritura Escritura [0.13155343 0.86844657]\n",
      "Escritura Escritura [0.23719004 0.76280996]\n",
      "Escritura Desiste [0.6049373 0.3950627]\n",
      "Escritura Desiste [0.63958917 0.36041083]\n",
      "Escritura Desiste [0.64138489 0.35861511]\n",
      "Escritura Escritura [0.25366541 0.74633459]\n",
      "Escritura Escritura [0.27605085 0.72394915]\n",
      "Escritura Desiste [0.58180472 0.41819528]\n",
      "Escritura Desiste [0.5370208 0.4629792]\n",
      "Escritura Escritura [0.2728037 0.7271963]\n",
      "Escritura Escritura [0.19194456 0.80805544]\n",
      "Escritura Escritura [0.40727979 0.59272021]\n",
      "Escritura Desiste [0.55845032 0.44154968]\n",
      "Escritura Escritura [0.27610399 0.72389601]\n",
      "Escritura Desiste [0.6204197 0.3795803]\n",
      "Escritura Escritura [0.4114008 0.5885992]\n",
      "Escritura Desiste [0.62284181 0.37715819]\n",
      "Escritura Escritura [0.30066279 0.69933721]\n",
      "Escritura Escritura [0.278703 0.721297]\n",
      "Escritura Escritura [0.43150677 0.56849323]\n",
      "Escritura Escritura [0.32161549 0.67838451]\n",
      "Escritura Escritura [0.32877978 0.67122022]\n",
      "Escritura Escritura [0.26880144 0.73119856]\n",
      "Escritura Desiste [0.59738731 0.40261269]\n",
      "Escritura Escritura [0.14328261 0.85671739]\n",
      "Escritura Escritura [0.27433238 0.72566762]\n",
      "Escritura Escritura [0.05101164 0.94898836]\n",
      "Escritura Escritura [0.35727725 0.64272275]\n",
      "Escritura Escritura [0.3820373 0.6179627]\n",
      "Escritura Escritura [0.2155832 0.7844168]\n",
      "Escritura Desiste [0.62638323 0.37361677]\n",
      "Escritura Desiste [0.58168033 0.41831967]\n",
      "Escritura Escritura [0.24599409 0.75400591]\n",
      "Escritura Escritura [0.28172159 0.71827841]\n",
      "Escritura Desiste [0.72308028 0.27691972]\n",
      "Escritura Escritura [0.17265859 0.82734141]\n",
      "Escritura Desiste [0.56811414 0.43188586]\n",
      "Escritura Escritura [0.30369756 0.69630244]\n",
      "Escritura Desiste [0.6192183 0.3807817]\n",
      "Escritura Desiste [0.70989744 0.29010256]\n",
      "Escritura Escritura [0.33660602 0.66339398]\n",
      "Escritura Desiste [0.59782458 0.40217542]\n",
      "Escritura Escritura [0.26583992 0.73416008]\n",
      "Escritura Escritura [0.22330995 0.77669005]\n",
      "Escritura Escritura [0.19924498 0.80075502]\n",
      "Escritura Escritura [0.45378868 0.54621132]\n",
      "Escritura Desiste [0.60780343 0.39219657]\n",
      "Escritura Escritura [0.24164924 0.75835076]\n",
      "Escritura Escritura [0.26924147 0.73075853]\n",
      "Escritura Desiste [0.58600269 0.41399731]\n",
      "Escritura Desiste [0.6537835 0.3462165]\n",
      "Escritura Desiste [0.74377006 0.25622994]\n",
      "Escritura Escritura [0.40018135 0.59981865]\n",
      "Escritura Escritura [0.33765469 0.66234531]\n",
      "Escritura Escritura [0.32851272 0.67148728]\n",
      "Escritura Desiste [0.57474987 0.42525013]\n",
      "Escritura Escritura [0.29253721 0.70746279]\n",
      "Escritura Escritura [0.27391072 0.72608928]\n",
      "Escritura Escritura [0.41108542 0.58891458]\n",
      "Escritura Desiste [0.65389734 0.34610266]\n",
      "Escritura Escritura [0.2629744 0.7370256]\n",
      "Escritura Desiste [0.71012241 0.28987759]\n",
      "Escritura Desiste [0.79721259 0.20278741]\n",
      "Escritura Desiste [0.74832442 0.25167558]\n",
      "Escritura Escritura [0.36115373 0.63884627]\n",
      "Escritura Escritura [0.30373227 0.69626773]\n",
      "Escritura Escritura [0.17582624 0.82417376]\n",
      "Escritura Desiste [0.73356464 0.26643536]\n",
      "Escritura Desiste [0.60508409 0.39491591]\n",
      "Escritura Escritura [0.27750357 0.72249643]\n",
      "Escritura Escritura [0.34865255 0.65134745]\n",
      "Escritura Escritura [0.26396902 0.73603098]\n",
      "Escritura Desiste [0.6208783 0.3791217]\n",
      "Escritura Escritura [0.17552033 0.82447967]\n",
      "Escritura Escritura [0.32845194 0.67154806]\n",
      "Escritura Desiste [0.70048036 0.29951964]\n",
      "Escritura Escritura [0.38477696 0.61522304]\n",
      "Escritura Escritura [0.02228744 0.97771256]\n",
      "Escritura Escritura [0.26170109 0.73829891]\n",
      "Escritura Desiste [0.65751859 0.34248141]\n",
      "Escritura Desiste [0.65092643 0.34907357]\n",
      "Escritura Escritura [0.29138673 0.70861327]\n",
      "Escritura Escritura [0.21960313 0.78039687]\n",
      "Escritura Desiste [0.65761603 0.34238397]\n",
      "Escritura Escritura [0.20053345 0.79946655]\n",
      "Escritura Desiste [0.5830486 0.4169514]\n",
      "Escritura Desiste [0.53107735 0.46892265]\n",
      "Escritura Escritura [0.40249422 0.59750578]\n",
      "Escritura Escritura [0.34882959 0.65117041]\n",
      "Escritura Escritura [0.24003178 0.75996822]\n",
      "Escritura Escritura [0.43276966 0.56723034]\n",
      "Escritura Desiste [0.62576041 0.37423959]\n",
      "Escritura Escritura [0.27130419 0.72869581]\n",
      "Escritura Escritura [0.11998036 0.88001964]\n",
      "Escritura Escritura [0.00270377 0.99729623]\n",
      "Escritura Escritura [0.21165847 0.78834153]\n",
      "Escritura Escritura [0.29310046 0.70689954]\n",
      "Escritura Escritura [0.26044786 0.73955214]\n",
      "Escritura Escritura [0.2301037 0.7698963]\n",
      "Escritura Desiste [0.70000715 0.29999285]\n",
      "Escritura Desiste [0.68565526 0.31434474]\n",
      "Escritura Escritura [0.22917948 0.77082052]\n",
      "Escritura Desiste [0.56146485 0.43853515]\n",
      "Escritura Escritura [0.15370988 0.84629012]\n",
      "Escritura Escritura [0.49003666 0.50996334]\n",
      "Escritura Escritura [0.33836321 0.66163679]\n",
      "Escritura Desiste [0.60413805 0.39586195]\n",
      "Escritura Escritura [0.26420516 0.73579484]\n",
      "Escritura Desiste [0.69045676 0.30954324]\n",
      "Escritura Escritura [0.28767664 0.71232336]\n",
      "Escritura Desiste [0.54146856 0.45853144]\n",
      "Escritura Desiste [0.64432333 0.35567667]\n",
      "Escritura Desiste [0.82927452 0.17072548]\n",
      "Escritura Escritura [0.453116 0.546884]\n",
      "Escritura Escritura [0.243614 0.756386]\n",
      "Escritura Desiste [0.67441391 0.32558609]\n",
      "Escritura Desiste [0.59380997 0.40619003]\n",
      "Escritura Desiste [0.56928935 0.43071065]\n",
      "Escritura Escritura [0.29127937 0.70872063]\n",
      "Escritura Escritura [0.20900166 0.79099834]\n",
      "Escritura Escritura [0.41484241 0.58515759]\n",
      "Escritura Desiste [0.63927275 0.36072725]\n",
      "Escritura Escritura [0.36845022 0.63154978]\n",
      "Escritura Escritura [0.27095389 0.72904611]\n",
      "Escritura Desiste [0.58088154 0.41911846]\n",
      "Escritura Desiste [0.62980226 0.37019774]\n",
      "Escritura Desiste [0.64908216 0.35091784]\n",
      "Escritura Escritura [0.37629949 0.62370051]\n",
      "Escritura Escritura [0.39442665 0.60557335]\n",
      "Escritura Desiste [0.56932272 0.43067728]\n",
      "Escritura Escritura [0.36648698 0.63351302]\n",
      "Escritura Escritura [0.43798142 0.56201858]\n",
      "Escritura Escritura [0.26164366 0.73835634]\n",
      "Escritura Escritura [0.23570434 0.76429566]\n",
      "Escritura Desiste [0.66557188 0.33442812]\n",
      "Escritura Escritura [0.2015705 0.7984295]\n",
      "Escritura Escritura [0.33288475 0.66711525]\n",
      "Escritura Desiste [0.68876808 0.31123192]\n",
      "Escritura Escritura [0.10121853 0.89878147]\n",
      "Escritura Escritura [0.36943239 0.63056761]\n",
      "Escritura Desiste [0.64904185 0.35095815]\n",
      "Escritura Escritura [0.27139244 0.72860756]\n",
      "Escritura Escritura [0.35233945 0.64766055]\n",
      "Escritura Escritura [0.28026777 0.71973223]\n",
      "Escritura Desiste [0.64272418 0.35727582]\n",
      "Escritura Escritura [0.27218508 0.72781492]\n",
      "Escritura Escritura [0.28460487 0.71539513]\n",
      "Escritura Desiste [0.61370365 0.38629635]\n",
      "Escritura Escritura [0.23006311 0.76993689]\n",
      "Escritura Desiste [0.85060001 0.14939999]\n",
      "Escritura Escritura [0.33743654 0.66256346]\n",
      "Escritura Desiste [0.6306123 0.3693877]\n",
      "Escritura Desiste [0.64732399 0.35267601]\n",
      "Escritura Escritura [0.2582534 0.7417466]\n",
      "Escritura Desiste [0.62682591 0.37317409]\n",
      "Escritura Escritura [0.49217526 0.50782474]\n",
      "Escritura Escritura [0.28493482 0.71506518]\n",
      "Escritura Escritura [0.21510382 0.78489618]\n",
      "Escritura Escritura [0.28484555 0.71515445]\n",
      "Escritura Escritura [0.42754763 0.57245237]\n",
      "Escritura Desiste [0.83809445 0.16190555]\n",
      "Escritura Escritura [0.39218157 0.60781843]\n",
      "Escritura Escritura [0.21583824 0.78416176]\n",
      "Escritura Escritura [0.25418213 0.74581787]\n",
      "Escritura Escritura [0.20179656 0.79820344]\n",
      "Escritura Escritura [0.31729475 0.68270525]\n",
      "Escritura Escritura [0.13721726 0.86278274]\n",
      "Escritura Escritura [0.31322525 0.68677475]\n",
      "Escritura Escritura [0.22945516 0.77054484]\n",
      "Escritura Desiste [0.68608394 0.31391606]\n",
      "Escritura Escritura [0.32427297 0.67572703]\n",
      "Escritura Escritura [0.34780306 0.65219694]\n",
      "Escritura Desiste [0.6999425 0.3000575]\n",
      "Escritura Escritura [0.29366725 0.70633275]\n",
      "Escritura Desiste [0.64159614 0.35840386]\n",
      "Escritura Escritura [0.31147522 0.68852478]\n",
      "Escritura Escritura [0.21188907 0.78811093]\n",
      "Escritura Desiste [0.70994857 0.29005143]\n",
      "Escritura Escritura [0.22040969 0.77959031]\n",
      "Escritura Escritura [0.21598846 0.78401154]\n",
      "Escritura Desiste [0.65529996 0.34470004]\n",
      "Escritura Escritura [0.195493 0.804507]\n",
      "Escritura Escritura [0.14545784 0.85454216]\n",
      "Escritura Escritura [0.21380885 0.78619115]\n",
      "Escritura Escritura [0.22987753 0.77012247]\n",
      "Escritura Escritura [0.37368646 0.62631354]\n",
      "Escritura Escritura [0.23072032 0.76927968]\n",
      "Escritura Desiste [0.52286161 0.47713839]\n",
      "Escritura Desiste [0.56988947 0.43011053]\n",
      "Escritura Escritura [0.2837926 0.7162074]\n",
      "Escritura Escritura [0.18490869 0.81509131]\n",
      "Escritura Escritura [0.32671323 0.67328677]\n",
      "Escritura Escritura [0.32614187 0.67385813]\n",
      "Escritura Desiste [0.52583015 0.47416985]\n",
      "Escritura Escritura [0.31133366 0.68866634]\n",
      "Escritura Escritura [0.20344324 0.79655676]\n",
      "Escritura Desiste [0.77031541 0.22968459]\n",
      "Escritura Escritura [0.22386008 0.77613992]\n",
      "Escritura Escritura [0.29333125 0.70666875]\n",
      "Escritura Escritura [0.4338626 0.5661374]\n",
      "Escritura Escritura [0.3089812 0.6910188]\n",
      "Escritura Escritura [0.36466855 0.63533145]\n",
      "Escritura Escritura [0.1947632 0.8052368]\n",
      "Escritura Escritura [0.28152487 0.71847513]\n",
      "Escritura Desiste [0.68406332 0.31593668]\n",
      "Escritura Escritura [0.27722788 0.72277212]\n",
      "Escritura Escritura [0.40766092 0.59233908]\n",
      "Escritura Desiste [0.67885404 0.32114596]\n",
      "Escritura Escritura [0.07160142 0.92839858]\n",
      "Escritura Escritura [0.37111018 0.62888982]\n",
      "Escritura Desiste [0.560063 0.439937]\n",
      "Escritura Escritura [0.2589473 0.7410527]\n",
      "Escritura Escritura [0.40731448 0.59268552]\n",
      "Escritura Escritura [0.44807786 0.55192214]\n",
      "Escritura Escritura [0.44591279 0.55408721]\n",
      "Escritura Desiste [0.63371982 0.36628018]\n",
      "Escritura Desiste [0.69496039 0.30503961]\n",
      "Escritura Escritura [0.24024328 0.75975672]\n",
      "Escritura Desiste [0.66657144 0.33342856]\n",
      "Escritura Escritura [0.32927602 0.67072398]\n",
      "Escritura Desiste [0.60151037 0.39848963]\n",
      "Escritura Escritura [0.07070931 0.92929069]\n",
      "Escritura Escritura [0.27294209 0.72705791]\n",
      "Escritura Escritura [0.2972413 0.7027587]\n",
      "Escritura Desiste [0.71053849 0.28946151]\n",
      "Escritura Escritura [0.22828347 0.77171653]\n",
      "Escritura Escritura [0.26232681 0.73767319]\n",
      "Escritura Desiste [0.61675514 0.38324486]\n",
      "Escritura Escritura [0.22907436 0.77092564]\n",
      "Escritura Escritura [0.20413322 0.79586678]\n",
      "Escritura Escritura [0.33613887 0.66386113]\n",
      "Escritura Escritura [0.17073679 0.82926321]\n",
      "Escritura Escritura [0.10616248 0.89383752]\n",
      "Escritura Desiste [0.6446569 0.3553431]\n",
      "Escritura Escritura [0.3178974 0.6821026]\n",
      "Escritura Escritura [0.10162013 0.89837987]\n",
      "Escritura Desiste [0.57710855 0.42289145]\n",
      "Escritura Desiste [0.56650855 0.43349145]\n",
      "Escritura Escritura [0.21569158 0.78430842]\n",
      "Escritura Escritura [0.24982135 0.75017865]\n",
      "Escritura Desiste [0.64530224 0.35469776]\n",
      "Escritura Desiste [0.50241488 0.49758512]\n",
      "Escritura Desiste [0.6039359 0.3960641]\n",
      "Escritura Escritura [0.09170449 0.90829551]\n",
      "Escritura Desiste [0.55602444 0.44397556]\n",
      "Escritura Desiste [0.70099559 0.29900441]\n",
      "Escritura Desiste [0.55171998 0.44828002]\n",
      "Escritura Escritura [0.4140655 0.5859345]\n",
      "Escritura Escritura [0.31692703 0.68307297]\n",
      "Escritura Escritura [0.22237541 0.77762459]\n",
      "Escritura Escritura [0.26695217 0.73304783]\n",
      "Escritura Escritura [0.40974639 0.59025361]\n",
      "Escritura Escritura [0.3708387 0.6291613]\n",
      "Escritura Desiste [0.68487444 0.31512556]\n",
      "Escritura Escritura [0.21377392 0.78622608]\n",
      "Escritura Escritura [0.40176777 0.59823223]\n",
      "Escritura Escritura [0.30759744 0.69240256]\n",
      "Escritura Escritura [0.21109113 0.78890887]\n",
      "Escritura Escritura [0.33113685 0.66886315]\n",
      "Escritura Escritura [0.28847075 0.71152925]\n",
      "Escritura Desiste [0.63427467 0.36572533]\n",
      "Escritura Desiste [0.74897619 0.25102381]\n",
      "Escritura Escritura [0.36089994 0.63910006]\n",
      "Escritura Escritura [0.32594274 0.67405726]\n",
      "Escritura Escritura [0.23730341 0.76269659]\n",
      "Escritura Desiste [0.66169471 0.33830529]\n",
      "Escritura Desiste [0.77425624 0.22574376]\n",
      "Escritura Escritura [0.24146868 0.75853132]\n",
      "Escritura Escritura [0.32206465 0.67793535]\n",
      "Escritura Escritura [0.33580019 0.66419981]\n",
      "Escritura Escritura [0.27980681 0.72019319]\n",
      "Escritura Desiste [0.6536493 0.3463507]\n",
      "Escritura Desiste [0.62831683 0.37168317]\n",
      "Escritura Desiste [0.53575316 0.46424684]\n",
      "Escritura Desiste [0.53870665 0.46129335]\n",
      "Escritura Escritura [0.35544707 0.64455293]\n",
      "Escritura Escritura [0.34568303 0.65431697]\n",
      "Escritura Desiste [0.65609459 0.34390541]\n",
      "Escritura Desiste [0.63944837 0.36055163]\n",
      "Escritura Escritura [0.08159649 0.91840351]\n",
      "Escritura Escritura [0.36630541 0.63369459]\n",
      "Escritura Desiste [0.63882997 0.36117003]\n",
      "Escritura Escritura [0.17039934 0.82960066]\n",
      "Escritura Desiste [0.55122825 0.44877175]\n",
      "Escritura Desiste [0.5292442 0.4707558]\n",
      "Escritura Escritura [0.49027467 0.50972533]\n",
      "Escritura Escritura [0.28016621 0.71983379]\n",
      "Escritura Escritura [0.31487826 0.68512174]\n",
      "Escritura Desiste [0.62525735 0.37474265]\n",
      "Escritura Desiste [0.57320681 0.42679319]\n",
      "Escritura Escritura [0.39634575 0.60365425]\n",
      "Escritura Escritura [0.32857067 0.67142933]\n",
      "Escritura Escritura [0.3778619 0.6221381]\n",
      "Escritura Desiste [0.65810182 0.34189818]\n",
      "Escritura Escritura [0.17012668 0.82987332]\n",
      "Escritura Desiste [0.56911525 0.43088475]\n",
      "Escritura Desiste [0.53738125 0.46261875]\n",
      "Escritura Escritura [0.23402644 0.76597356]\n",
      "Escritura Desiste [0.67374351 0.32625649]\n",
      "Escritura Escritura [0.30573614 0.69426386]\n",
      "Escritura Escritura [0.34705369 0.65294631]\n",
      "Escritura Desiste [0.73872441 0.26127559]\n",
      "Escritura Escritura [0.22047546 0.77952454]\n",
      "Escritura Desiste [0.72472111 0.27527889]\n",
      "Escritura Escritura [0.25133994 0.74866006]\n",
      "Escritura Desiste [0.5190429 0.4809571]\n",
      "Escritura Desiste [0.6245027 0.3754973]\n",
      "Escritura Escritura [0.38706274 0.61293726]\n",
      "Escritura Escritura [0.45805551 0.54194449]\n",
      "Escritura Escritura [0.33552819 0.66447181]\n",
      "Escritura Desiste [0.68932696 0.31067304]\n",
      "Escritura Desiste [0.53470532 0.46529468]\n",
      "Escritura Desiste [0.64311841 0.35688159]\n",
      "Escritura Escritura [0.46109716 0.53890284]\n",
      "Escritura Desiste [0.75100967 0.24899033]\n",
      "Escritura Escritura [0.21007207 0.78992793]\n",
      "Escritura Desiste [0.63895267 0.36104733]\n",
      "Escritura Desiste [0.7167425 0.2832575]\n",
      "Escritura Escritura [0.31454315 0.68545685]\n",
      "Escritura Escritura [0.3015032 0.6984968]\n",
      "Escritura Escritura [0.21055983 0.78944017]\n",
      "Escritura Escritura [0.1149129 0.8850871]\n",
      "Escritura Desiste [0.78603893 0.21396107]\n",
      "Escritura Escritura [0.23056925 0.76943075]\n",
      "Escritura Escritura [0.39411925 0.60588075]\n",
      "Escritura Escritura [0.31939652 0.68060348]\n",
      "Escritura Desiste [0.62400208 0.37599792]\n",
      "Escritura Escritura [0.27082886 0.72917114]\n",
      "Escritura Escritura [0.40205144 0.59794856]\n",
      "Escritura Escritura [0.37036311 0.62963689]\n",
      "Escritura Escritura [0.45169411 0.54830589]\n",
      "Escritura Escritura [0.33320612 0.66679388]\n",
      "Escritura Escritura [0.22193396 0.77806604]\n",
      "Escritura Desiste [0.7497861 0.2502139]\n",
      "Escritura Escritura [0.30783021 0.69216979]\n",
      "Escritura Desiste [0.52756203 0.47243797]\n",
      "Escritura Escritura [0.05005815 0.94994185]\n",
      "Escritura Desiste [0.73182924 0.26817076]\n",
      "Escritura Escritura [0.26691651 0.73308349]\n",
      "Escritura Desiste [0.63513871 0.36486129]\n",
      "Escritura Desiste [0.70036798 0.29963202]\n",
      "Escritura Desiste [0.62270914 0.37729086]\n",
      "Escritura Desiste [0.63218451 0.36781549]\n",
      "Escritura Escritura [0.26384852 0.73615148]\n",
      "Escritura Desiste [0.61441674 0.38558326]\n",
      "Escritura Escritura [0.06222675 0.93777325]\n",
      "Escritura Escritura [0.3046533 0.6953467]\n",
      "Escritura Escritura [0.26482364 0.73517636]\n",
      "Escritura Escritura [0.26298595 0.73701405]\n",
      "Escritura Escritura [0.26988409 0.73011591]\n",
      "Escritura Escritura [0.20443304 0.79556696]\n",
      "Escritura Escritura [0.27082076 0.72917924]\n",
      "Escritura Escritura [0.00749311 0.99250689]\n",
      "Escritura Escritura [0.38750263 0.61249737]\n",
      "Escritura Escritura [0.24177125 0.75822875]\n",
      "Escritura Escritura [0.29968934 0.70031066]\n",
      "Escritura Escritura [0.21921224 0.78078776]\n",
      "Escritura Escritura [0.23191976 0.76808024]\n",
      "Escritura Escritura [0.23956809 0.76043191]\n",
      "Escritura Escritura [0.18084681 0.81915319]\n",
      "Escritura Escritura [0.13632607 0.86367393]\n",
      "Escritura Desiste [0.65673343 0.34326657]\n",
      "Escritura Escritura [0.12344798 0.87655202]\n",
      "Escritura Desiste [0.71940143 0.28059857]\n",
      "Escritura Escritura [0.33438366 0.66561634]\n",
      "Escritura Escritura [0.40826389 0.59173611]\n",
      "Escritura Desiste [0.79305424 0.20694576]\n",
      "Escritura Desiste [0.75977744 0.24022256]\n",
      "Escritura Desiste [0.76850202 0.23149798]\n",
      "Escritura Escritura [0.12384502 0.87615498]\n",
      "Escritura Desiste [0.73191782 0.26808218]\n",
      "Escritura Escritura [0.26695766 0.73304234]\n",
      "Escritura Desiste [0.75014998 0.24985002]\n",
      "Escritura Desiste [0.59026113 0.40973887]\n",
      "Escritura Escritura [0.1214021 0.8785979]\n",
      "Escritura Desiste [0.64049738 0.35950262]\n",
      "Escritura Desiste [0.58050511 0.41949489]\n",
      "Escritura Escritura [0.24086777 0.75913223]\n",
      "Escritura Escritura [0.31896718 0.68103282]\n",
      "Escritura Escritura [0.38941136 0.61058864]\n",
      "Escritura Escritura [0.1900572 0.8099428]\n",
      "Escritura Escritura [0.27835396 0.72164604]\n",
      "Escritura Escritura [0.22563935 0.77436065]\n",
      "Escritura Escritura [0.29098081 0.70901919]\n",
      "Escritura Escritura [0.30752963 0.69247037]\n",
      "Escritura Escritura [0.33312918 0.66687082]\n",
      "Escritura Escritura [0.33745537 0.66254463]\n",
      "Escritura Escritura [0.31473947 0.68526053]\n",
      "Escritura Escritura [0.43027116 0.56972884]\n",
      "Escritura Escritura [0.32525662 0.67474338]\n",
      "Escritura Desiste [0.5908883 0.4091117]\n",
      "Escritura Desiste [0.63848787 0.36151213]\n",
      "Escritura Escritura [0.26024782 0.73975218]\n",
      "Escritura Escritura [0.24202133 0.75797867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Escritura [0.2621887 0.7378113]\n",
      "Escritura Escritura [0.20019951 0.79980049]\n",
      "Escritura Escritura [0.42059059 0.57940941]\n",
      "Escritura Desiste [0.63042371 0.36957629]\n",
      "Escritura Escritura [0.24322739 0.75677261]\n",
      "Escritura Desiste [0.70982052 0.29017948]\n",
      "Escritura Escritura [0.22214709 0.77785291]\n",
      "Escritura Escritura [0.2428466 0.7571534]\n",
      "Escritura Escritura [0.21308146 0.78691854]\n",
      "Escritura Escritura [0.18008883 0.81991117]\n",
      "Escritura Desiste [0.61164829 0.38835171]\n",
      "Escritura Escritura [0.1486178 0.8513822]\n",
      "Escritura Escritura [0.3550484 0.6449516]\n",
      "Escritura Escritura [0.25357336 0.74642664]\n",
      "Escritura Escritura [0.3064929 0.6935071]\n",
      "Escritura Escritura [0.24170613 0.75829387]\n",
      "Escritura Escritura [0.27436887 0.72563113]\n",
      "Escritura Escritura [0.33689786 0.66310214]\n",
      "Escritura Desiste [0.58885357 0.41114643]\n",
      "Escritura Escritura [0.17457431 0.82542569]\n",
      "Escritura Desiste [0.57373656 0.42626344]\n",
      "Escritura Desiste [0.56289508 0.43710492]\n",
      "Escritura Escritura [0.23275438 0.76724562]\n",
      "Escritura Escritura [0.20188263 0.79811737]\n",
      "Escritura Desiste [0.58196062 0.41803938]\n",
      "Escritura Desiste [0.74150538 0.25849462]\n",
      "Escritura Escritura [0.17641484 0.82358516]\n",
      "Escritura Desiste [0.65853765 0.34146235]\n",
      "Escritura Desiste [0.61496234 0.38503766]\n",
      "Escritura Escritura [0.18687321 0.81312679]\n",
      "Escritura Escritura [0.31363106 0.68636894]\n",
      "Escritura Escritura [0.29967915 0.70032085]\n",
      "Escritura Escritura [0.31240328 0.68759672]\n",
      "Escritura Escritura [0.28937872 0.71062128]\n",
      "Escritura Escritura [0.27618091 0.72381909]\n",
      "Escritura Escritura [0.212734 0.787266]\n",
      "Escritura Escritura [0.16285905 0.83714095]\n",
      "Escritura Escritura [0.34313424 0.65686576]\n",
      "Escritura Desiste [0.54032289 0.45967711]\n",
      "Escritura Escritura [0.26260436 0.73739564]\n",
      "Escritura Desiste [0.77881527 0.22118473]\n",
      "Escritura Escritura [0.47992332 0.52007668]\n",
      "Escritura Escritura [0.26559424 0.73440576]\n",
      "Escritura Desiste [0.65967712 0.34032288]\n",
      "Escritura Desiste [0.8282679 0.1717321]\n",
      "Escritura Desiste [0.84237478 0.15762522]\n",
      "Escritura Escritura [0.3833438 0.6166562]\n",
      "Escritura Desiste [0.86136553 0.13863447]\n",
      "Escritura Escritura [0.27096453 0.72903547]\n",
      "Escritura Desiste [0.74105266 0.25894734]\n",
      "Escritura Escritura [0.21733016 0.78266984]\n",
      "Escritura Escritura [0.34222345 0.65777655]\n",
      "Escritura Escritura [0.26294903 0.73705097]\n",
      "Escritura Escritura [0.31468173 0.68531827]\n",
      "Escritura Desiste [0.53238499 0.46761501]\n",
      "Escritura Escritura [0.29337983 0.70662017]\n",
      "Escritura Escritura [0.30156807 0.69843193]\n",
      "Escritura Escritura [0.26202421 0.73797579]\n",
      "Escritura Escritura [0.26275819 0.73724181]\n",
      "Escritura Desiste [0.68827399 0.31172601]\n",
      "Escritura Escritura [0.05247286 0.94752714]\n",
      "Escritura Escritura [0.34522945 0.65477055]\n",
      "Escritura Desiste [0.56922131 0.43077869]\n",
      "Escritura Escritura [0.28361181 0.71638819]\n",
      "Escritura Escritura [0.22475351 0.77524649]\n",
      "Escritura Desiste [0.64632762 0.35367238]\n",
      "Escritura Desiste [0.72744913 0.27255087]\n",
      "Escritura Escritura [0.18025894 0.81974106]\n",
      "Escritura Escritura [0.24162236 0.75837764]\n",
      "Escritura Escritura [0.29065076 0.70934924]\n",
      "Escritura Escritura [0.24218634 0.75781366]\n",
      "Escritura Escritura [0.35243124 0.64756876]\n",
      "Escritura Desiste [0.80093996 0.19906004]\n",
      "Escritura Escritura [0.22231651 0.77768349]\n",
      "Escritura Escritura [0.32441143 0.67558857]\n",
      "Escritura Escritura [0.20862424 0.79137576]\n",
      "Escritura Escritura [0.15741753 0.84258247]\n",
      "Escritura Desiste [0.53418204 0.46581796]\n",
      "Escritura Escritura [0.32793489 0.67206511]\n",
      "Escritura Desiste [0.51488312 0.48511688]\n",
      "Escritura Escritura [0.2012092 0.7987908]\n",
      "Escritura Escritura [0.22884861 0.77115139]\n",
      "Escritura Desiste [0.66379275 0.33620725]\n",
      "Escritura Desiste [0.53729009 0.46270991]\n",
      "Escritura Desiste [0.57276376 0.42723624]\n",
      "Escritura Escritura [0.3003325 0.6996675]\n",
      "Escritura Escritura [0.23771294 0.76228706]\n",
      "Escritura Desiste [0.56966772 0.43033228]\n",
      "Escritura Desiste [0.62126292 0.37873708]\n",
      "Escritura Desiste [0.74543067 0.25456933]\n",
      "Escritura Desiste [0.70717784 0.29282216]\n",
      "Escritura Escritura [0.31778202 0.68221798]\n",
      "Escritura Desiste [0.58382782 0.41617218]\n",
      "Escritura Escritura [0.11073231 0.88926769]\n",
      "Escritura Escritura [0.48053913 0.51946087]\n",
      "Escritura Escritura [0.23811531 0.76188469]\n",
      "Escritura Escritura [0.31420125 0.68579875]\n",
      "Escritura Escritura [0.36780149 0.63219851]\n",
      "Escritura Escritura [0.35840535 0.64159465]\n",
      "Escritura Escritura [0.39205323 0.60794677]\n",
      "Escritura Desiste [0.53093908 0.46906092]\n",
      "Escritura Escritura [0.10311084 0.89688916]\n",
      "Escritura Escritura [0.23375731 0.76624269]\n",
      "Escritura Escritura [0.38587438 0.61412562]\n",
      "Escritura Desiste [0.60940505 0.39059495]\n",
      "Escritura Escritura [0.24641961 0.75358039]\n",
      "Escritura Escritura [0.35675769 0.64324231]\n",
      "Escritura Escritura [0.18747596 0.81252404]\n",
      "Escritura Desiste [0.65828223 0.34171777]\n",
      "Escritura Escritura [0.22936246 0.77063754]\n",
      "Escritura Desiste [0.74411911 0.25588089]\n",
      "Escritura Escritura [0.27169697 0.72830303]\n",
      "Escritura Desiste [0.51776634 0.48223366]\n",
      "Escritura Escritura [0.37212368 0.62787632]\n",
      "Escritura Desiste [0.56486574 0.43513426]\n",
      "Escritura Escritura [0.24841344 0.75158656]\n",
      "Escritura Desiste [0.66734912 0.33265088]\n",
      "Escritura Desiste [0.52018649 0.47981351]\n",
      "Escritura Escritura [0.17138365 0.82861635]\n",
      "Escritura Escritura [0.2138893 0.7861107]\n",
      "Escritura Desiste [0.61447951 0.38552049]\n",
      "Escritura Desiste [0.54121424 0.45878576]\n",
      "Escritura Desiste [0.80665714 0.19334286]\n",
      "Escritura Desiste [0.6693777 0.3306223]\n",
      "Escritura Escritura [0.29415605 0.70584395]\n",
      "Escritura Escritura [0.46857476 0.53142524]\n",
      "Escritura Desiste [0.58303874 0.41696126]\n",
      "Escritura Escritura [0.2420587 0.7579413]\n",
      "Escritura Escritura [0.20953364 0.79046636]\n",
      "Escritura Escritura [0.31231402 0.68768598]\n",
      "Escritura Desiste [0.52456027 0.47543973]\n",
      "Escritura Desiste [0.58675195 0.41324805]\n",
      "Escritura Escritura [0.39084433 0.60915567]\n",
      "Escritura Escritura [0.40518801 0.59481199]\n",
      "Escritura Desiste [0.6316529 0.3683471]\n",
      "Escritura Escritura [0.3690266 0.6309734]\n",
      "Escritura Escritura [0.30261996 0.69738004]\n",
      "Escritura Escritura [0.3772591 0.6227409]\n",
      "Escritura Desiste [0.69561861 0.30438139]\n",
      "Escritura Desiste [0.71722038 0.28277962]\n",
      "Escritura Desiste [0.70861504 0.29138496]\n",
      "Escritura Desiste [0.5290677 0.4709323]\n",
      "Escritura Escritura [0.29566377 0.70433623]\n",
      "Escritura Desiste [0.63910377 0.36089623]\n",
      "Escritura Escritura [0.41552548 0.58447452]\n",
      "Escritura Escritura [0.24699614 0.75300386]\n",
      "Escritura Desiste [0.70401663 0.29598337]\n",
      "Escritura Escritura [0.45424131 0.54575869]\n",
      "Escritura Desiste [0.63952278 0.36047722]\n",
      "Escritura Escritura [0.22400828 0.77599172]\n",
      "Escritura Desiste [0.76318005 0.23681995]\n",
      "Escritura Desiste [0.51781182 0.48218818]\n",
      "Escritura Desiste [0.73522876 0.26477124]\n",
      "Escritura Desiste [0.63645666 0.36354334]\n",
      "Escritura Escritura [0.32882609 0.67117391]\n",
      "Escritura Desiste [0.74721668 0.25278332]\n",
      "Escritura Desiste [0.68471889 0.31528111]\n",
      "Escritura Desiste [0.68733286 0.31266714]\n",
      "Escritura Desiste [0.54236736 0.45763264]\n",
      "Escritura Desiste [0.66692366 0.33307634]\n",
      "Escritura Desiste [0.67610428 0.32389572]\n",
      "Escritura Desiste [0.72712777 0.27287223]\n",
      "Escritura Desiste [0.55508939 0.44491061]\n",
      "Escritura Escritura [0.20713824 0.79286176]\n",
      "Escritura Desiste [0.76008972 0.23991028]\n",
      "Escritura Escritura [0.11280792 0.88719208]\n",
      "Escritura Escritura [0.33713466 0.66286534]\n",
      "Escritura Desiste [0.7268534 0.2731466]\n",
      "Escritura Desiste [0.58493894 0.41506106]\n",
      "Escritura Desiste [0.69899099 0.30100901]\n",
      "Escritura Escritura [0.2921984 0.7078016]\n",
      "Escritura Desiste [0.55020057 0.44979943]\n",
      "Escritura Desiste [0.61350164 0.38649836]\n",
      "Escritura Desiste [0.56509249 0.43490751]\n",
      "Escritura Escritura [0.23460104 0.76539896]\n",
      "Escritura Escritura [0.24545681 0.75454319]\n",
      "Escritura Escritura [0.4255121 0.5744879]\n",
      "Escritura Escritura [0.36044052 0.63955948]\n",
      "Escritura Desiste [0.72002581 0.27997419]\n",
      "Escritura Escritura [0.2435331 0.7564669]\n",
      "Escritura Escritura [0.26000658 0.73999342]\n",
      "Escritura Desiste [0.59877843 0.40122157]\n",
      "Escritura Desiste [0.6732162 0.3267838]\n",
      "Escritura Escritura [0.29997273 0.70002727]\n",
      "Escritura Desiste [0.53352791 0.46647209]\n",
      "Escritura Desiste [0.5995971 0.4004029]\n",
      "Escritura Escritura [0.16612388 0.83387612]\n",
      "Escritura Escritura [0.2835797 0.7164203]\n",
      "Escritura Escritura [0.46620425 0.53379575]\n",
      "Escritura Escritura [0.32118153 0.67881847]\n",
      "Escritura Desiste [0.74177054 0.25822946]\n",
      "Escritura Desiste [0.81813861 0.18186139]\n",
      "Escritura Desiste [0.64265857 0.35734143]\n",
      "Escritura Escritura [0.22488214 0.77511786]\n",
      "Escritura Desiste [0.70140176 0.29859824]\n",
      "Escritura Desiste [0.56138531 0.43861469]\n",
      "Escritura Escritura [0.45396887 0.54603113]\n",
      "Escritura Escritura [0.27760953 0.72239047]\n",
      "Escritura Escritura [0.27938645 0.72061355]\n",
      "Escritura Desiste [0.70478971 0.29521029]\n",
      "Escritura Escritura [0.32870954 0.67129046]\n",
      "Escritura Escritura [0.26228936 0.73771064]\n",
      "Escritura Desiste [0.74994154 0.25005846]\n",
      "Escritura Escritura [0.18943197 0.81056803]\n",
      "Escritura Escritura [0.47849407 0.52150593]\n",
      "Escritura Desiste [0.75073265 0.24926735]\n",
      "Escritura Desiste [0.72825118 0.27174882]\n",
      "Escritura Desiste [0.58691707 0.41308293]\n",
      "Escritura Escritura [0.48166554 0.51833446]\n",
      "Escritura Desiste [0.64674334 0.35325666]\n",
      "Escritura Escritura [0.41966839 0.58033161]\n",
      "Escritura Escritura [0.20200535 0.79799465]\n",
      "Escritura Desiste [0.6497772 0.3502228]\n",
      "Escritura Desiste [0.65902786 0.34097214]\n",
      "Escritura Escritura [0.1633518 0.8366482]\n",
      "Escritura Escritura [0.23120031 0.76879969]\n",
      "Escritura Escritura [0.200619 0.799381]\n",
      "Escritura Escritura [0.3954444 0.6045556]\n",
      "Escritura Desiste [0.59992847 0.40007153]\n",
      "Escritura Desiste [0.56099921 0.43900079]\n",
      "Escritura Escritura [0.19115034 0.80884966]\n",
      "Escritura Escritura [0.30272552 0.69727448]\n",
      "Escritura Desiste [0.6169864 0.3830136]\n",
      "Escritura Desiste [0.58170093 0.41829907]\n",
      "Escritura Escritura [0.28770333 0.71229667]\n",
      "Escritura Desiste [0.53578209 0.46421791]\n",
      "Escritura Escritura [0.31768454 0.68231546]\n",
      "Escritura Escritura [0.27041506 0.72958494]\n",
      "Escritura Escritura [0.32624907 0.67375093]\n",
      "Escritura Desiste [0.68950375 0.31049625]\n",
      "Escritura Desiste [0.62137755 0.37862245]\n",
      "Escritura Escritura [0.26911849 0.73088151]\n",
      "Escritura Escritura [0.21873947 0.78126053]\n",
      "Escritura Escritura [0.24950621 0.75049379]\n",
      "Escritura Escritura [0.24895031 0.75104969]\n",
      "Escritura Escritura [0.35762486 0.64237514]\n",
      "Escritura Escritura [0.22596144 0.77403856]\n",
      "Escritura Escritura [0.13944019 0.86055981]\n",
      "Escritura Escritura [0.15026807 0.84973193]\n",
      "Escritura Desiste [0.694995 0.305005]\n",
      "Escritura Escritura [0.34318793 0.65681207]\n",
      "Escritura Escritura [0.28244161 0.71755839]\n",
      "Escritura Escritura [0.43780412 0.56219588]\n",
      "Escritura Escritura [0.14683835 0.85316165]\n",
      "Escritura Escritura [0.21349015 0.78650985]\n",
      "Escritura Escritura [0.15118351 0.84881649]\n",
      "Escritura Desiste [0.6364508 0.3635492]\n",
      "Escritura Desiste [0.63278237 0.36721763]\n",
      "Escritura Desiste [0.57099342 0.42900658]\n",
      "Escritura Escritura [0.30592627 0.69407373]\n",
      "Escritura Desiste [0.60039474 0.39960526]\n",
      "Escritura Escritura [0.29932338 0.70067662]\n",
      "Escritura Escritura [0.36175746 0.63824254]\n",
      "Escritura Escritura [0.24552752 0.75447248]\n",
      "Escritura Escritura [0.30477907 0.69522093]\n",
      "Escritura Desiste [0.70277626 0.29722374]\n",
      "Escritura Escritura [0.32303778 0.67696222]\n",
      "Escritura Desiste [0.59130146 0.40869854]\n",
      "Escritura Escritura [0.22408999 0.77591001]\n",
      "Escritura Desiste [0.65667847 0.34332153]\n",
      "Escritura Escritura [0.30205905 0.69794095]\n",
      "Escritura Escritura [0.3558347 0.6441653]\n",
      "Escritura Desiste [0.66842819 0.33157181]\n",
      "Escritura Desiste [0.63354738 0.36645262]\n",
      "Escritura Escritura [0.19323587 0.80676413]\n",
      "Escritura Escritura [0.22543231 0.77456769]\n",
      "Escritura Escritura [0.22703013 0.77296987]\n",
      "Escritura Desiste [0.70170661 0.29829339]\n",
      "Escritura Escritura [0.26165416 0.73834584]\n",
      "Escritura Escritura [0.33233741 0.66766259]\n",
      "Escritura Desiste [0.60264531 0.39735469]\n",
      "Escritura Desiste [0.50042175 0.49957825]\n",
      "Escritura Desiste [0.63854121 0.36145879]\n",
      "Escritura Escritura [0.07210429 0.92789571]\n",
      "Escritura Desiste [0.64069715 0.35930285]\n",
      "Escritura Desiste [0.63671805 0.36328195]\n",
      "Escritura Desiste [0.76526719 0.23473281]\n",
      "Escritura Escritura [0.18458701 0.81541299]\n",
      "Escritura Escritura [0.35894174 0.64105826]\n",
      "Escritura Desiste [0.95319211 0.04680789]\n",
      "Escritura Desiste [0.53783995 0.46216005]\n",
      "Escritura Escritura [0.2743977 0.7256023]\n",
      "Escritura Escritura [0.28047309 0.71952691]\n",
      "Escritura Escritura [0.27767482 0.72232518]\n",
      "Escritura Escritura [0.2602941 0.7397059]\n",
      "Escritura Desiste [0.63399215 0.36600785]\n",
      "Escritura Escritura [0.23893419 0.76106581]\n",
      "Escritura Escritura [0.24389103 0.75610897]\n",
      "Escritura Desiste [0.66890441 0.33109559]\n",
      "Escritura Escritura [0.23386371 0.76613629]\n",
      "Escritura Desiste [0.76037556 0.23962444]\n",
      "Escritura Escritura [0.23546236 0.76453764]\n",
      "Escritura Escritura [0.19676006 0.80323994]\n",
      "Escritura Desiste [0.58597896 0.41402104]\n",
      "Escritura Escritura [0.32229933 0.67770067]\n",
      "Escritura Desiste [0.65283024 0.34716976]\n",
      "Escritura Escritura [0.31525465 0.68474535]\n",
      "Escritura Desiste [0.67381087 0.32618913]\n",
      "Escritura Desiste [0.68502887 0.31497113]\n",
      "Escritura Escritura [0.23572076 0.76427924]\n",
      "Escritura Escritura [0.17027818 0.82972182]\n",
      "Escritura Desiste [0.76222255 0.23777745]\n",
      "Escritura Desiste [0.65712654 0.34287346]\n",
      "Escritura Escritura [0.16034332 0.83965668]\n",
      "Escritura Escritura [0.31414717 0.68585283]\n",
      "Escritura Escritura [0.35517875 0.64482125]\n",
      "Escritura Desiste [0.62265109 0.37734891]\n",
      "Escritura Escritura [0.28139202 0.71860798]\n",
      "Escritura Escritura [0.43489711 0.56510289]\n",
      "Escritura Escritura [0.39974009 0.60025991]\n",
      "Escritura Escritura [0.38790032 0.61209968]\n",
      "Escritura Escritura [0.24933813 0.75066187]\n",
      "Escritura Escritura [0.32828327 0.67171673]\n",
      "Escritura Escritura [0.38265422 0.61734578]\n",
      "Escritura Escritura [0.39334653 0.60665347]\n",
      "Escritura Desiste [0.71085929 0.28914071]\n",
      "Escritura Escritura [0.29553878 0.70446122]\n",
      "Escritura Escritura [0.16490567 0.83509433]\n",
      "Escritura Escritura [0.22830379 0.77169621]\n",
      "Escritura Escritura [0.31176829 0.68823171]\n",
      "Escritura Escritura [0.18044586 0.81955414]\n",
      "Escritura Escritura [0.48375705 0.51624295]\n",
      "Escritura Desiste [0.67965269 0.32034731]\n",
      "Escritura Escritura [0.33005639 0.66994361]\n",
      "Escritura Desiste [0.65116087 0.34883913]\n",
      "Escritura Desiste [0.67972204 0.32027796]\n",
      "Escritura Escritura [0.37097317 0.62902683]\n",
      "Escritura Desiste [0.70832181 0.29167819]\n",
      "Escritura Desiste [0.66696645 0.33303355]\n",
      "Escritura Escritura [0.24463643 0.75536357]\n",
      "Escritura Escritura [0.49309668 0.50690332]\n",
      "Escritura Desiste [0.75758786 0.24241214]\n",
      "Escritura Desiste [0.56097354 0.43902646]\n",
      "Escritura Escritura [0.26841989 0.73158011]\n",
      "Escritura Escritura [0.35247169 0.64752831]\n",
      "Escritura Desiste [0.66711069 0.33288931]\n",
      "Escritura Escritura [0.2250191 0.7749809]\n",
      "Escritura Escritura [0.34721401 0.65278599]\n",
      "Escritura Escritura [0.20800009 0.79199991]\n",
      "Escritura Escritura [0.20704034 0.79295966]\n",
      "Escritura Desiste [0.69488706 0.30511294]\n",
      "Escritura Escritura [0.46405573 0.53594427]\n",
      "Escritura Desiste [0.61740534 0.38259466]\n",
      "Escritura Escritura [0.33346263 0.66653737]\n",
      "Escritura Desiste [0.64523732 0.35476268]\n",
      "Escritura Desiste [0.51686928 0.48313072]\n",
      "Escritura Escritura [0.27964729 0.72035271]\n",
      "Escritura Desiste [0.6928862 0.3071138]\n",
      "Escritura Desiste [0.57047527 0.42952473]\n",
      "Escritura Escritura [0.26412479 0.73587521]\n",
      "Escritura Escritura [0.21328166 0.78671834]\n",
      "Escritura Escritura [0.29063032 0.70936968]\n",
      "Escritura Escritura [0.33360329 0.66639671]\n",
      "Escritura Escritura [0.29595389 0.70404611]\n",
      "Escritura Desiste [0.68264467 0.31735533]\n",
      "Escritura Escritura [0.30443704 0.69556296]\n",
      "Escritura Desiste [0.56138279 0.43861721]\n",
      "Escritura Desiste [0.57529088 0.42470912]\n",
      "Escritura Escritura [0.26666635 0.73333365]\n",
      "Escritura Desiste [0.62297151 0.37702849]\n",
      "Escritura Escritura [0.29060699 0.70939301]\n",
      "Escritura Desiste [0.58981076 0.41018924]\n",
      "Escritura Desiste [0.52922377 0.47077623]\n",
      "Escritura Desiste [0.54178583 0.45821417]\n",
      "Escritura Desiste [0.66343412 0.33656588]\n",
      "Escritura Desiste [0.6633504 0.3366496]\n",
      "Escritura Desiste [0.60812263 0.39187737]\n",
      "Escritura Desiste [0.65423515 0.34576485]\n",
      "Escritura Escritura [0.20023822 0.79976178]\n",
      "Escritura Desiste [0.71049849 0.28950151]\n",
      "Escritura Escritura [0.3709142 0.6290858]\n",
      "Escritura Desiste [0.60717765 0.39282235]\n",
      "Escritura Escritura [0.43811464 0.56188536]\n",
      "Escritura Escritura [0.20969773 0.79030227]\n",
      "Escritura Desiste [0.67891037 0.32108963]\n",
      "Escritura Desiste [0.65199777 0.34800223]\n",
      "Escritura Escritura [0.39122147 0.60877853]\n",
      "Escritura Escritura [0.31210404 0.68789596]\n",
      "Escritura Escritura [0.18838932 0.81161068]\n",
      "Escritura Desiste [0.59907801 0.40092199]\n",
      "Escritura Escritura [0.16569618 0.83430382]\n",
      "Escritura Desiste [0.52505 0.47495]\n",
      "Escritura Escritura [0.17177616 0.82822384]\n",
      "Escritura Desiste [0.66627918 0.33372082]\n",
      "Escritura Desiste [0.65297045 0.34702955]\n",
      "Escritura Escritura [0.14904944 0.85095056]\n",
      "Escritura Desiste [0.72072563 0.27927437]\n",
      "Escritura Escritura [0.11517663 0.88482337]\n",
      "Escritura Escritura [0.25142598 0.74857402]\n",
      "Escritura Escritura [0.26821333 0.73178667]\n",
      "Escritura Desiste [0.61487663 0.38512337]\n",
      "Escritura Escritura [0.45998381 0.54001619]\n",
      "Escritura Desiste [0.71569316 0.28430684]\n",
      "Escritura Escritura [0.28331651 0.71668349]\n",
      "Escritura Escritura [0.28755523 0.71244477]\n",
      "Escritura Desiste [0.57667199 0.42332801]\n",
      "Escritura Desiste [0.82057639 0.17942361]\n",
      "Escritura Desiste [0.51001947 0.48998053]\n",
      "Escritura Desiste [0.64482293 0.35517707]\n",
      "Escritura Escritura [0.33411265 0.66588735]\n",
      "Escritura Desiste [0.55503515 0.44496485]\n",
      "Escritura Escritura [0.38191319 0.61808681]\n",
      "Escritura Desiste [0.83771415 0.16228585]\n",
      "Escritura Desiste [0.52883315 0.47116685]\n",
      "Escritura Escritura [0.1871223 0.8128777]\n",
      "Escritura Escritura [0.47961916 0.52038084]\n",
      "Escritura Desiste [0.55020247 0.44979753]\n",
      "Escritura Escritura [0.35783518 0.64216482]\n",
      "Escritura Escritura [0.20947092 0.79052908]\n",
      "Escritura Escritura [0.28400349 0.71599651]\n",
      "Escritura Desiste [0.75563142 0.24436858]\n",
      "Escritura Escritura [0.31894774 0.68105226]\n",
      "Escritura Desiste [0.57410591 0.42589409]\n",
      "Escritura Escritura [0.14155921 0.85844079]\n",
      "Escritura Desiste [0.65444201 0.34555799]\n",
      "Escritura Escritura [0.21109074 0.78890926]\n",
      "Escritura Escritura [0.38729942 0.61270058]\n",
      "Escritura Escritura [0.32882242 0.67117758]\n",
      "Escritura Desiste [0.58684967 0.41315033]\n",
      "Escritura Desiste [0.65578504 0.34421496]\n",
      "Escritura Escritura [0.29776956 0.70223044]\n",
      "Escritura Desiste [0.77692253 0.22307747]\n",
      "Escritura Desiste [0.60143742 0.39856258]\n",
      "Escritura Escritura [0.46059134 0.53940866]\n",
      "Escritura Desiste [0.55346104 0.44653896]\n",
      "Escritura Desiste [0.7039428 0.2960572]\n",
      "Escritura Escritura [0.13996587 0.86003413]\n",
      "Escritura Escritura [0.33696659 0.66303341]\n",
      "Escritura Escritura [0.24694994 0.75305006]\n",
      "Escritura Desiste [0.67453835 0.32546165]\n",
      "Escritura Escritura [0.21675651 0.78324349]\n",
      "Escritura Escritura [0.21948944 0.78051056]\n",
      "Escritura Desiste [0.58387316 0.41612684]\n",
      "Escritura Escritura [0.31595493 0.68404507]\n",
      "Escritura Desiste [0.63097776 0.36902224]\n",
      "Escritura Desiste [0.66433997 0.33566003]\n",
      "Escritura Desiste [0.71266137 0.28733863]\n",
      "Escritura Desiste [0.72723895 0.27276105]\n",
      "Escritura Escritura [0.24486403 0.75513597]\n",
      "Escritura Escritura [0.27658114 0.72341886]\n",
      "Escritura Escritura [0.34572707 0.65427293]\n",
      "Escritura Escritura [0.31126634 0.68873366]\n",
      "Escritura Desiste [0.53403399 0.46596601]\n",
      "Escritura Escritura [0.31331202 0.68668798]\n",
      "Escritura Escritura [0.45361616 0.54638384]\n",
      "Escritura Escritura [0.253005 0.746995]\n",
      "Escritura Desiste [0.60091285 0.39908715]\n",
      "Escritura Desiste [0.66125188 0.33874812]\n",
      "Escritura Desiste [0.72916424 0.27083576]\n",
      "Escritura Escritura [0.27688817 0.72311183]\n",
      "Escritura Desiste [0.5571696 0.4428304]\n",
      "Escritura Desiste [0.62459264 0.37540736]\n",
      "Escritura Escritura [0.40141029 0.59858971]\n",
      "Escritura Desiste [0.63093617 0.36906383]\n",
      "Escritura Desiste [0.67477346 0.32522654]\n",
      "Escritura Escritura [0.29648938 0.70351062]\n",
      "Escritura Desiste [0.76653229 0.23346771]\n",
      "Escritura Desiste [0.63923366 0.36076634]\n",
      "Escritura Desiste [0.67048146 0.32951854]\n",
      "Escritura Desiste [0.60703015 0.39296985]\n",
      "Escritura Desiste [0.71508822 0.28491178]\n",
      "Escritura Desiste [0.76651434 0.23348566]\n",
      "Escritura Escritura [0.28498527 0.71501473]\n",
      "Escritura Escritura [0.0849366 0.9150634]\n",
      "Escritura Desiste [0.59298867 0.40701133]\n",
      "Escritura Escritura [0.24658798 0.75341202]\n",
      "Escritura Desiste [0.67147705 0.32852295]\n",
      "Escritura Desiste [0.5236588 0.4763412]\n",
      "Escritura Escritura [0.23625821 0.76374179]\n",
      "Escritura Escritura [0.28741163 0.71258837]\n",
      "Escritura Desiste [0.54673864 0.45326136]\n",
      "Escritura Desiste [0.62431509 0.37568491]\n",
      "Escritura Desiste [0.68378619 0.31621381]\n",
      "Escritura Escritura [0.22663278 0.77336722]\n",
      "Escritura Escritura [0.31412854 0.68587146]\n",
      "Escritura Escritura [0.27112686 0.72887314]\n",
      "Escritura Desiste [0.67451713 0.32548287]\n",
      "Escritura Escritura [0.2395622 0.7604378]\n",
      "Escritura Escritura [0.40131684 0.59868316]\n",
      "Escritura Desiste [0.65951311 0.34048689]\n",
      "Escritura Escritura [0.40574094 0.59425906]\n",
      "Escritura Desiste [0.68821797 0.31178203]\n",
      "Escritura Desiste [0.61694448 0.38305552]\n",
      "Escritura Desiste [0.6240208 0.3759792]\n",
      "Escritura Desiste [0.76432298 0.23567702]\n",
      "Escritura Desiste [0.62594137 0.37405863]\n",
      "Escritura Desiste [0.53982978 0.46017022]\n",
      "Escritura Escritura [0.35818772 0.64181228]\n",
      "Escritura Desiste [0.77973235 0.22026765]\n",
      "Escritura Escritura [0.3521343 0.6478657]\n",
      "Escritura Desiste [0.65669146 0.34330854]\n",
      "Escritura Escritura [0.01877295 0.98122705]\n",
      "Escritura Escritura [0.23283108 0.76716892]\n",
      "Escritura Escritura [0.35805951 0.64194049]\n",
      "Escritura Desiste [0.62492518 0.37507482]\n",
      "Escritura Escritura [0.23070513 0.76929487]\n",
      "Escritura Escritura [0.26392219 0.73607781]\n",
      "Escritura Escritura [0.2471706 0.7528294]\n",
      "Escritura Escritura [0.18522572 0.81477428]\n",
      "Escritura Escritura [0.2790008 0.7209992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Desiste [0.67496126 0.32503874]\n",
      "Escritura Escritura [0.35407474 0.64592526]\n",
      "Escritura Escritura [0.30417106 0.69582894]\n",
      "Escritura Escritura [0.1430113 0.8569887]\n",
      "Escritura Escritura [0.36738138 0.63261862]\n",
      "Escritura Escritura [0.34465239 0.65534761]\n",
      "Escritura Escritura [0.35770644 0.64229356]\n",
      "Escritura Desiste [0.60491157 0.39508843]\n",
      "Escritura Escritura [0.3054137 0.6945863]\n",
      "Escritura Escritura [0.22905898 0.77094102]\n",
      "Escritura Escritura [0.40436251 0.59563749]\n",
      "Escritura Escritura [0.49843579 0.50156421]\n",
      "Escritura Escritura [0.30339877 0.69660123]\n",
      "Escritura Escritura [0.07796226 0.92203774]\n",
      "Escritura Desiste [0.51044605 0.48955395]\n",
      "Escritura Escritura [0.46924108 0.53075892]\n",
      "Escritura Escritura [0.02060389 0.97939611]\n",
      "Escritura Escritura [0.2274016 0.7725984]\n",
      "Escritura Escritura [0.15839382 0.84160618]\n",
      "Escritura Escritura [0.34750727 0.65249273]\n",
      "Escritura Escritura [0.25544424 0.74455576]\n",
      "Escritura Desiste [0.8729885 0.1270115]\n",
      "Escritura Escritura [0.24774396 0.75225604]\n",
      "Escritura Escritura [0.43108473 0.56891527]\n",
      "Escritura Desiste [0.59752553 0.40247447]\n",
      "Escritura Desiste [0.59123846 0.40876154]\n",
      "Escritura Escritura [0.49031807 0.50968193]\n",
      "Escritura Desiste [0.71658205 0.28341795]\n",
      "Escritura Desiste [0.77980518 0.22019482]\n",
      "Escritura Desiste [0.6931739 0.3068261]\n",
      "Escritura Escritura [0.30910666 0.69089334]\n",
      "Escritura Escritura [0.31877308 0.68122692]\n",
      "Escritura Escritura [0.21892931 0.78107069]\n",
      "Escritura Escritura [0.40468618 0.59531382]\n",
      "Escritura Escritura [0.29413589 0.70586411]\n",
      "Escritura Escritura [0.30193087 0.69806913]\n",
      "Escritura Escritura [0.29658934 0.70341066]\n",
      "Escritura Desiste [0.55303036 0.44696964]\n",
      "Escritura Escritura [0.43793926 0.56206074]\n",
      "Escritura Escritura [0.24075103 0.75924897]\n",
      "Escritura Desiste [0.73475737 0.26524263]\n",
      "Escritura Escritura [0.25858946 0.74141054]\n",
      "Escritura Escritura [0.4174333 0.5825667]\n",
      "Escritura Escritura [0.3208102 0.6791898]\n",
      "Escritura Escritura [0.43801985 0.56198015]\n",
      "Escritura Desiste [0.5112098 0.4887902]\n",
      "Escritura Escritura [0.21094953 0.78905047]\n",
      "Escritura Desiste [0.61950357 0.38049643]\n",
      "Escritura Escritura [0.2824373 0.7175627]\n",
      "Escritura Escritura [0.48463724 0.51536276]\n",
      "Escritura Desiste [0.55116955 0.44883045]\n",
      "Escritura Escritura [0.23685834 0.76314166]\n",
      "Escritura Escritura [0.27328557 0.72671443]\n",
      "Escritura Escritura [0.32978829 0.67021171]\n",
      "Escritura Desiste [0.5714428 0.4285572]\n",
      "Escritura Escritura [0.25250383 0.74749617]\n",
      "Escritura Escritura [0.1418915 0.8581085]\n",
      "Escritura Desiste [0.75769995 0.24230005]\n",
      "Escritura Escritura [0.24334153 0.75665847]\n",
      "Escritura Desiste [0.67901929 0.32098071]\n",
      "Escritura Escritura [0.37829523 0.62170477]\n",
      "Escritura Desiste [0.59006881 0.40993119]\n",
      "Escritura Desiste [0.51660349 0.48339651]\n",
      "Escritura Escritura [0.27143754 0.72856246]\n",
      "Escritura Desiste [0.74404522 0.25595478]\n",
      "Escritura Escritura [0.36073675 0.63926325]\n",
      "Escritura Desiste [0.54414938 0.45585062]\n",
      "Escritura Escritura [0.35013116 0.64986884]\n",
      "Escritura Escritura [0.31659417 0.68340583]\n",
      "Escritura Escritura [0.26244075 0.73755925]\n",
      "Escritura Escritura [0.48542886 0.51457114]\n",
      "Escritura Desiste [0.72810901 0.27189099]\n",
      "Escritura Desiste [0.54855706 0.45144294]\n",
      "Escritura Escritura [0.3945392 0.6054608]\n",
      "Escritura Desiste [0.60060668 0.39939332]\n",
      "Escritura Escritura [0.20610663 0.79389337]\n",
      "Escritura Escritura [0.30731828 0.69268172]\n",
      "Escritura Desiste [0.57781286 0.42218714]\n",
      "Escritura Desiste [0.60137015 0.39862985]\n",
      "Escritura Escritura [0.22012129 0.77987871]\n",
      "Escritura Escritura [0.11080033 0.88919967]\n",
      "Escritura Escritura [0.32613904 0.67386096]\n",
      "Escritura Desiste [0.74640299 0.25359701]\n",
      "Escritura Desiste [0.76096827 0.23903173]\n",
      "Escritura Escritura [0.24120856 0.75879144]\n",
      "Escritura Escritura [0.23323421 0.76676579]\n",
      "Escritura Escritura [0.17119766 0.82880234]\n",
      "Escritura Escritura [0.33334362 0.66665638]\n",
      "Escritura Escritura [0.31500198 0.68499802]\n",
      "Escritura Desiste [0.62221947 0.37778053]\n",
      "Escritura Desiste [0.67838752 0.32161248]\n",
      "Escritura Escritura [0.22460321 0.77539679]\n",
      "Escritura Desiste [0.60204286 0.39795714]\n",
      "Escritura Escritura [0.38023756 0.61976244]\n",
      "Escritura Escritura [0.21061899 0.78938101]\n",
      "Escritura Desiste [0.50760182 0.49239818]\n",
      "Escritura Escritura [0.21681169 0.78318831]\n",
      "Escritura Desiste [0.51706161 0.48293839]\n",
      "Escritura Escritura [0.26294641 0.73705359]\n",
      "Escritura Escritura [0.18925283 0.81074717]\n",
      "Escritura Escritura [0.24394581 0.75605419]\n",
      "Escritura Escritura [0.20805082 0.79194918]\n",
      "Escritura Escritura [0.33208903 0.66791097]\n",
      "Escritura Escritura [0.31843075 0.68156925]\n",
      "Escritura Escritura [0.37961381 0.62038619]\n",
      "Escritura Escritura [0.26642062 0.73357938]\n",
      "Escritura Escritura [0.27758248 0.72241752]\n",
      "Escritura Escritura [0.34263828 0.65736172]\n",
      "Escritura Desiste [0.65921988 0.34078012]\n",
      "Escritura Escritura [0.44126089 0.55873911]\n",
      "Escritura Escritura [0.10215567 0.89784433]\n",
      "Escritura Desiste [0.64129129 0.35870871]\n",
      "Escritura Desiste [0.69135726 0.30864274]\n",
      "Escritura Desiste [0.58460839 0.41539161]\n",
      "Escritura Escritura [0.35147328 0.64852672]\n",
      "Escritura Desiste [0.67186639 0.32813361]\n",
      "Escritura Escritura [0.46720682 0.53279318]\n",
      "Escritura Escritura [0.29456521 0.70543479]\n",
      "Escritura Desiste [0.75682063 0.24317937]\n",
      "Escritura Escritura [0.19312986 0.80687014]\n",
      "Escritura Desiste [0.53118586 0.46881414]\n",
      "Escritura Desiste [0.55994529 0.44005471]\n",
      "Escritura Desiste [0.76040813 0.23959187]\n",
      "Escritura Escritura [0.20466295 0.79533705]\n",
      "Escritura Escritura [0.27938539 0.72061461]\n",
      "Escritura Desiste [0.71183603 0.28816397]\n",
      "Escritura Escritura [0.02182122 0.97817878]\n",
      "Escritura Desiste [0.69335607 0.30664393]\n",
      "Escritura Escritura [0.2144622 0.7855378]\n",
      "Escritura Desiste [0.66274946 0.33725054]\n",
      "Escritura Escritura [0.18340412 0.81659588]\n",
      "Escritura Escritura [0.40735967 0.59264033]\n",
      "Escritura Desiste [0.74483408 0.25516592]\n",
      "Escritura Desiste [0.61635408 0.38364592]\n",
      "Escritura Escritura [0.48373171 0.51626829]\n",
      "Escritura Desiste [0.64091443 0.35908557]\n",
      "Escritura Escritura [0.12766601 0.87233399]\n",
      "Escritura Desiste [0.65989752 0.34010248]\n",
      "Escritura Escritura [0.25148807 0.74851193]\n",
      "Escritura Escritura [0.38466626 0.61533374]\n",
      "Escritura Escritura [0.23215456 0.76784544]\n",
      "Escritura Desiste [0.72533563 0.27466437]\n",
      "Escritura Escritura [0.26783949 0.73216051]\n",
      "Escritura Escritura [0.1366385 0.8633615]\n",
      "Escritura Escritura [0.21019591 0.78980409]\n",
      "Escritura Desiste [0.64472695 0.35527305]\n",
      "Escritura Desiste [0.66175312 0.33824688]\n",
      "Escritura Escritura [0.25544791 0.74455209]\n",
      "Escritura Desiste [0.57260059 0.42739941]\n",
      "Escritura Desiste [0.68526113 0.31473887]\n",
      "Escritura Escritura [0.41398614 0.58601386]\n",
      "Escritura Desiste [0.62685188 0.37314812]\n",
      "Escritura Escritura [0.25065258 0.74934742]\n",
      "Escritura Desiste [0.57150389 0.42849611]\n",
      "Escritura Escritura [0.26517204 0.73482796]\n",
      "Escritura Escritura [0.3526419 0.6473581]\n",
      "Escritura Desiste [0.68509659 0.31490341]\n",
      "Escritura Desiste [0.76340574 0.23659426]\n",
      "Escritura Escritura [0.33219595 0.66780405]\n",
      "Escritura Desiste [0.50662098 0.49337902]\n",
      "Escritura Escritura [0.4546615 0.5453385]\n",
      "Escritura Escritura [0.28617854 0.71382146]\n",
      "Escritura Escritura [0.38668175 0.61331825]\n",
      "Escritura Desiste [0.56138085 0.43861915]\n",
      "Escritura Desiste [0.63952585 0.36047415]\n",
      "Escritura Escritura [0.33533862 0.66466138]\n",
      "Escritura Desiste [0.70738965 0.29261035]\n",
      "Escritura Escritura [0.27826426 0.72173574]\n",
      "Escritura Escritura [0.24035922 0.75964078]\n",
      "Escritura Escritura [0.23174252 0.76825748]\n",
      "Escritura Desiste [0.54458871 0.45541129]\n",
      "Escritura Escritura [0.46747909 0.53252091]\n",
      "Escritura Escritura [0.3441129 0.6558871]\n",
      "Escritura Escritura [0.29624653 0.70375347]\n",
      "Escritura Escritura [0.24499875 0.75500125]\n",
      "Escritura Desiste [0.68734057 0.31265943]\n",
      "Escritura Escritura [0.37238017 0.62761983]\n",
      "Escritura Desiste [0.67471533 0.32528467]\n",
      "Escritura Desiste [0.64338777 0.35661223]\n",
      "Escritura Desiste [0.59726212 0.40273788]\n",
      "Escritura Escritura [0.22564767 0.77435233]\n",
      "Escritura Desiste [0.50377232 0.49622768]\n",
      "Escritura Escritura [0.33849661 0.66150339]\n",
      "Escritura Escritura [0.29340714 0.70659286]\n",
      "Escritura Escritura [0.27898375 0.72101625]\n",
      "Escritura Escritura [0.30853616 0.69146384]\n",
      "Escritura Escritura [0.45489072 0.54510928]\n",
      "Escritura Escritura [0.22615125 0.77384875]\n",
      "Escritura Desiste [0.63175087 0.36824913]\n",
      "Escritura Escritura [0.24536 0.75464]\n",
      "Escritura Escritura [0.30032519 0.69967481]\n",
      "Escritura Desiste [0.55254189 0.44745811]\n",
      "Escritura Desiste [0.58544253 0.41455747]\n",
      "Escritura Desiste [0.56803529 0.43196471]\n",
      "Escritura Escritura [0.20422758 0.79577242]\n",
      "Escritura Escritura [0.46083171 0.53916829]\n",
      "Escritura Escritura [0.41915882 0.58084118]\n",
      "Escritura Desiste [0.66194146 0.33805854]\n",
      "Escritura Escritura [0.41974977 0.58025023]\n",
      "Escritura Desiste [0.6039359 0.3960641]\n",
      "Escritura Escritura [0.26161345 0.73838655]\n",
      "Escritura Desiste [0.68515402 0.31484598]\n",
      "Escritura Escritura [0.25881875 0.74118125]\n",
      "Escritura Desiste [0.79037252 0.20962748]\n",
      "Escritura Desiste [0.62565872 0.37434128]\n",
      "Escritura Escritura [0.32691323 0.67308677]\n",
      "Escritura Escritura [0.33698302 0.66301698]\n",
      "Escritura Escritura [0.27239765 0.72760235]\n",
      "Escritura Escritura [0.26163389 0.73836611]\n",
      "Escritura Escritura [0.22802257 0.77197743]\n",
      "Escritura Escritura [0.31978501 0.68021499]\n",
      "Escritura Escritura [0.24940349 0.75059651]\n",
      "Escritura Escritura [0.41366162 0.58633838]\n",
      "Escritura Escritura [0.18880007 0.81119993]\n",
      "Escritura Desiste [0.55092216 0.44907784]\n",
      "Escritura Escritura [0.43056235 0.56943765]\n",
      "Escritura Escritura [0.31783485 0.68216515]\n",
      "Escritura Desiste [0.64566423 0.35433577]\n",
      "Escritura Escritura [0.10508256 0.89491744]\n",
      "Escritura Escritura [0.27996773 0.72003227]\n",
      "Escritura Escritura [0.47803249 0.52196751]\n",
      "Escritura Escritura [0.35667616 0.64332384]\n",
      "Escritura Escritura [0.38312045 0.61687955]\n",
      "Escritura Desiste [0.72210983 0.27789017]\n",
      "Escritura Escritura [0.18309386 0.81690614]\n",
      "Escritura Desiste [0.60493429 0.39506571]\n",
      "Escritura Desiste [0.70622458 0.29377542]\n",
      "Escritura Escritura [0.318814 0.681186]\n",
      "Escritura Escritura [0.19227314 0.80772686]\n",
      "Escritura Escritura [0.30008287 0.69991713]\n",
      "Escritura Escritura [0.27242788 0.72757212]\n",
      "Escritura Desiste [0.63745221 0.36254779]\n",
      "Escritura Escritura [0.42626041 0.57373959]\n",
      "Escritura Escritura [0.47824667 0.52175333]\n",
      "Escritura Escritura [0.27160475 0.72839525]\n",
      "Escritura Escritura [0.12190459 0.87809541]\n",
      "Escritura Desiste [0.68739691 0.31260309]\n",
      "Escritura Desiste [0.62004117 0.37995883]\n",
      "Escritura Escritura [0.33973889 0.66026111]\n",
      "Escritura Desiste [0.6609766 0.3390234]\n",
      "Escritura Desiste [0.76427662 0.23572338]\n",
      "Escritura Escritura [0.41692762 0.58307238]\n",
      "Escritura Desiste [0.66543688 0.33456312]\n",
      "Escritura Escritura [0.16625442 0.83374558]\n",
      "Escritura Desiste [0.73213737 0.26786263]\n",
      "Escritura Escritura [0.15513648 0.84486352]\n",
      "Escritura Escritura [0.24462339 0.75537661]\n",
      "Escritura Escritura [0.34924569 0.65075431]\n",
      "Escritura Escritura [0.31311121 0.68688879]\n",
      "Escritura Escritura [0.29820244 0.70179756]\n",
      "Escritura Desiste [0.52513843 0.47486157]\n",
      "Escritura Escritura [0.2824272 0.7175728]\n",
      "Escritura Escritura [0.36638823 0.63361177]\n",
      "Escritura Escritura [0.23569335 0.76430665]\n",
      "Escritura Escritura [0.29934271 0.70065729]\n",
      "Escritura Desiste [0.68832808 0.31167192]\n",
      "Escritura Escritura [0.27018998 0.72981002]\n",
      "Escritura Escritura [0.20352325 0.79647675]\n",
      "Escritura Escritura [0.4196818 0.5803182]\n",
      "Escritura Escritura [0.37551063 0.62448937]\n",
      "Escritura Escritura [0.09825905 0.90174095]\n",
      "Escritura Desiste [0.75762746 0.24237254]\n",
      "Escritura Escritura [0.3596729 0.6403271]\n",
      "Escritura Escritura [0.20921565 0.79078435]\n",
      "Escritura Escritura [0.31868235 0.68131765]\n",
      "Escritura Escritura [0.34832188 0.65167812]\n",
      "Escritura Desiste [0.62969436 0.37030564]\n",
      "Escritura Desiste [0.68333642 0.31666358]\n",
      "Escritura Escritura [0.30429663 0.69570337]\n",
      "Escritura Desiste [0.57849073 0.42150927]\n",
      "Escritura Desiste [0.54696868 0.45303132]\n",
      "Escritura Escritura [0.45077734 0.54922266]\n",
      "Escritura Desiste [0.57095871 0.42904129]\n",
      "Escritura Desiste [0.69016059 0.30983941]\n",
      "Escritura Desiste [0.67087233 0.32912767]\n",
      "Escritura Desiste [0.72220894 0.27779106]\n",
      "Escritura Desiste [0.65801997 0.34198003]\n",
      "Escritura Desiste [0.67560573 0.32439427]\n",
      "Escritura Escritura [0.32492878 0.67507122]\n",
      "Escritura Escritura [2.38284104e-04 9.99761716e-01]\n",
      "Escritura Escritura [0.49686357 0.50313643]\n",
      "Escritura Escritura [0.22540453 0.77459547]\n",
      "Escritura Escritura [0.27959607 0.72040393]\n",
      "Escritura Escritura [0.28903202 0.71096798]\n",
      "Escritura Desiste [0.58328385 0.41671615]\n",
      "Escritura Desiste [0.54460201 0.45539799]\n",
      "Escritura Escritura [0.44369053 0.55630947]\n",
      "Escritura Escritura [0.33371734 0.66628266]\n",
      "Escritura Escritura [0.30099788 0.69900212]\n",
      "Escritura Desiste [0.59720723 0.40279277]\n",
      "Escritura Escritura [0.28476485 0.71523515]\n",
      "Escritura Desiste [0.62676455 0.37323545]\n",
      "Escritura Desiste [0.70084489 0.29915511]\n",
      "Escritura Desiste [0.70272856 0.29727144]\n",
      "Escritura Escritura [0.39910568 0.60089432]\n",
      "Escritura Escritura [0.28671735 0.71328265]\n",
      "Escritura Desiste [0.69341744 0.30658256]\n",
      "Escritura Desiste [0.57530808 0.42469192]\n",
      "Escritura Escritura [0.15235146 0.84764854]\n",
      "Escritura Escritura [0.42891072 0.57108928]\n",
      "Escritura Escritura [0.32156255 0.67843745]\n",
      "Escritura Escritura [0.29284289 0.70715711]\n",
      "Escritura Desiste [0.61193347 0.38806653]\n",
      "Escritura Escritura [0.13541067 0.86458933]\n",
      "Escritura Desiste [0.73504574 0.26495426]\n",
      "Escritura Escritura [0.29984342 0.70015658]\n",
      "Escritura Escritura [0.34469658 0.65530342]\n",
      "Escritura Escritura [0.27835383 0.72164617]\n",
      "Escritura Desiste [0.53554623 0.46445377]\n",
      "Escritura Desiste [0.69007549 0.30992451]\n",
      "Escritura Escritura [0.20151938 0.79848062]\n",
      "Escritura Desiste [0.66551146 0.33448854]\n",
      "Escritura Desiste [0.54570269 0.45429731]\n",
      "Escritura Escritura [0.32225735 0.67774265]\n",
      "Escritura Escritura [0.3037601 0.6962399]\n",
      "Escritura Desiste [0.6443176 0.3556824]\n",
      "Escritura Escritura [0.18361649 0.81638351]\n",
      "Escritura Escritura [0.15556589 0.84443411]\n",
      "Escritura Escritura [0.41735712 0.58264288]\n",
      "Escritura Escritura [0.11199639 0.88800361]\n",
      "Escritura Escritura [0.25518838 0.74481162]\n",
      "Escritura Escritura [0.32552677 0.67447323]\n",
      "Escritura Desiste [0.68196795 0.31803205]\n",
      "Escritura Escritura [0.19599518 0.80400482]\n",
      "Escritura Escritura [0.09231476 0.90768524]\n",
      "Escritura Desiste [0.55356872 0.44643128]\n",
      "Escritura Escritura [0.17886894 0.82113106]\n",
      "Escritura Desiste [0.64947753 0.35052247]\n",
      "Escritura Escritura [0.28950124 0.71049876]\n",
      "Escritura Escritura [0.49165591 0.50834409]\n",
      "Escritura Desiste [0.52721499 0.47278501]\n",
      "Escritura Escritura [0.28517136 0.71482864]\n",
      "Escritura Escritura [0.29733159 0.70266841]\n",
      "Escritura Escritura [0.43254619 0.56745381]\n",
      "Escritura Desiste [0.92234545 0.07765455]\n",
      "Escritura Escritura [0.23061383 0.76938617]\n",
      "Escritura Desiste [0.59842292 0.40157708]\n",
      "Escritura Escritura [0.1591616 0.8408384]\n",
      "Escritura Escritura [0.19639781 0.80360219]\n",
      "Escritura Desiste [0.6089546 0.3910454]\n",
      "Escritura Desiste [0.57582117 0.42417883]\n",
      "Escritura Escritura [0.24803977 0.75196023]\n",
      "Escritura Escritura [0.29271175 0.70728825]\n",
      "Escritura Escritura [0.29774819 0.70225181]\n",
      "Escritura Escritura [0.3756387 0.6243613]\n",
      "Escritura Escritura [0.2781702 0.7218298]\n",
      "Escritura Desiste [0.58348726 0.41651274]\n",
      "Escritura Desiste [0.62218675 0.37781325]\n",
      "Escritura Escritura [0.25779065 0.74220935]\n",
      "Escritura Escritura [0.2782441 0.7217559]\n",
      "Escritura Desiste [0.66150481 0.33849519]\n",
      "Escritura Escritura [0.24393394 0.75606606]\n",
      "Escritura Escritura [0.22079047 0.77920953]\n",
      "Escritura Escritura [0.17199599 0.82800401]\n",
      "Escritura Desiste [0.57389229 0.42610771]\n",
      "Escritura Desiste [0.74271168 0.25728832]\n",
      "Escritura Desiste [0.67986165 0.32013835]\n",
      "Escritura Desiste [0.63991908 0.36008092]\n",
      "Escritura Escritura [0.14470772 0.85529228]\n",
      "Escritura Desiste [0.56554518 0.43445482]\n",
      "Escritura Escritura [0.27042429 0.72957571]\n",
      "Escritura Desiste [0.61447822 0.38552178]\n",
      "Escritura Desiste [0.58163783 0.41836217]\n",
      "Escritura Desiste [0.65939031 0.34060969]\n",
      "Escritura Escritura [0.29617285 0.70382715]\n",
      "Escritura Escritura [0.26229337 0.73770663]\n",
      "Escritura Desiste [0.6153218 0.3846782]\n",
      "Escritura Escritura [0.22305457 0.77694543]\n",
      "Escritura Desiste [0.6694553 0.3305447]\n",
      "Escritura Escritura [0.14269844 0.85730156]\n",
      "Escritura Escritura [0.33136114 0.66863886]\n",
      "Escritura Desiste [0.55406755 0.44593245]\n",
      "Escritura Escritura [0.25541899 0.74458101]\n",
      "Escritura Escritura [0.20940083 0.79059917]\n",
      "Escritura Desiste [0.62397348 0.37602652]\n",
      "Escritura Desiste [0.55321801 0.44678199]\n",
      "Escritura Escritura [0.24653238 0.75346762]\n",
      "Escritura Desiste [0.60484448 0.39515552]\n",
      "Escritura Desiste [0.61538375 0.38461625]\n",
      "Escritura Desiste [0.59550791 0.40449209]\n",
      "Escritura Escritura [0.34426998 0.65573002]\n",
      "Escritura Escritura [0.10025055 0.89974945]\n",
      "Escritura Desiste [0.67976593 0.32023407]\n",
      "Escritura Escritura [0.36669268 0.63330732]\n",
      "Escritura Desiste [0.63034224 0.36965776]\n",
      "Escritura Escritura [0.41546931 0.58453069]\n",
      "Escritura Escritura [0.31160311 0.68839689]\n",
      "Escritura Desiste [0.51699607 0.48300393]\n",
      "Escritura Desiste [0.55798115 0.44201885]\n",
      "Escritura Escritura [0.20650514 0.79349486]\n",
      "Escritura Escritura [0.30827305 0.69172695]\n",
      "Escritura Desiste [0.72118601 0.27881399]\n",
      "Escritura Desiste [0.63233979 0.36766021]\n",
      "Escritura Escritura [0.38369309 0.61630691]\n",
      "Escritura Desiste [0.55693708 0.44306292]\n",
      "Escritura Escritura [0.22208617 0.77791383]\n",
      "Escritura Escritura [0.31044943 0.68955057]\n",
      "Escritura Escritura [0.27874625 0.72125375]\n",
      "Escritura Desiste [0.70232063 0.29767937]\n",
      "Escritura Desiste [0.67827682 0.32172318]\n",
      "Escritura Escritura [0.25345829 0.74654171]\n",
      "Escritura Escritura [0.13916703 0.86083297]\n",
      "Escritura Desiste [0.65406727 0.34593273]\n",
      "Escritura Desiste [0.67811129 0.32188871]\n",
      "Escritura Escritura [0.24850051 0.75149949]\n",
      "Escritura Desiste [0.73211503 0.26788497]\n",
      "Escritura Escritura [0.27768348 0.72231652]\n",
      "Escritura Desiste [0.5546144 0.4453856]\n",
      "Escritura Desiste [0.59151876 0.40848124]\n",
      "Escritura Escritura [0.3273936 0.6726064]\n",
      "Escritura Escritura [0.26263202 0.73736798]\n",
      "Escritura Desiste [0.64044135 0.35955865]\n",
      "Escritura Escritura [0.28954734 0.71045266]\n",
      "Escritura Escritura [0.43072749 0.56927251]\n",
      "Escritura Escritura [0.39062297 0.60937703]\n",
      "Escritura Escritura [0.29641383 0.70358617]\n",
      "Escritura Desiste [0.80331221 0.19668779]\n",
      "Escritura Escritura [0.4629786 0.5370214]\n",
      "Escritura Escritura [0.35924158 0.64075842]\n",
      "Escritura Escritura [0.33726309 0.66273691]\n",
      "Escritura Escritura [0.26166927 0.73833073]\n",
      "Escritura Desiste [0.52675099 0.47324901]\n",
      "Escritura Escritura [0.32875529 0.67124471]\n",
      "Escritura Escritura [0.36069144 0.63930856]\n",
      "Escritura Desiste [0.65157345 0.34842655]\n",
      "Escritura Desiste [0.55478756 0.44521244]\n",
      "Escritura Escritura [0.15751853 0.84248147]\n",
      "Escritura Escritura [0.34341556 0.65658444]\n",
      "Escritura Escritura [0.30380827 0.69619173]\n",
      "Escritura Escritura [0.37083466 0.62916534]\n",
      "Escritura Escritura [0.42056808 0.57943192]\n",
      "Escritura Escritura [0.31833828 0.68166172]\n",
      "Escritura Escritura [0.17033178 0.82966822]\n",
      "Escritura Desiste [0.59943284 0.40056716]\n",
      "Escritura Escritura [0.27183892 0.72816108]\n",
      "Escritura Escritura [0.34194949 0.65805051]\n",
      "Escritura Escritura [0.25244502 0.74755498]\n",
      "Escritura Escritura [0.28629905 0.71370095]\n",
      "Escritura Desiste [0.5153744 0.4846256]\n",
      "Escritura Escritura [0.27544286 0.72455714]\n",
      "Escritura Desiste [0.65635005 0.34364995]\n",
      "Escritura Escritura [0.31640504 0.68359496]\n",
      "Escritura Desiste [0.66961415 0.33038585]\n",
      "Escritura Desiste [0.62219204 0.37780796]\n",
      "Escritura Escritura [0.26609361 0.73390639]\n",
      "Escritura Desiste [0.70763367 0.29236633]\n",
      "Escritura Escritura [0.29765994 0.70234006]\n",
      "Escritura Escritura [0.25179447 0.74820553]\n",
      "Escritura Escritura [0.26749063 0.73250937]\n",
      "Escritura Desiste [0.73330396 0.26669604]\n",
      "Escritura Desiste [0.76143434 0.23856566]\n",
      "Escritura Desiste [0.63835935 0.36164065]\n",
      "Escritura Desiste [0.59167398 0.40832602]\n",
      "Escritura Desiste [0.60001389 0.39998611]\n",
      "Escritura Escritura [0.2757374 0.7242626]\n",
      "Escritura Escritura [0.22971643 0.77028357]\n",
      "Escritura Escritura [0.26455258 0.73544742]\n",
      "Escritura Desiste [0.59968036 0.40031964]\n",
      "Escritura Desiste [0.61739922 0.38260078]\n",
      "Escritura Desiste [0.73803637 0.26196363]\n",
      "Escritura Escritura [0.16964367 0.83035633]\n",
      "Escritura Escritura [0.33431055 0.66568945]\n",
      "Escritura Escritura [0.42025069 0.57974931]\n",
      "Escritura Escritura [0.41310242 0.58689758]\n",
      "Escritura Desiste [0.52536223 0.47463777]\n",
      "Escritura Escritura [0.26329778 0.73670222]\n",
      "Escritura Desiste [0.68742138 0.31257862]\n",
      "Escritura Escritura [0.21665211 0.78334789]\n",
      "Escritura Escritura [0.28055362 0.71944638]\n",
      "Escritura Escritura [0.22167245 0.77832755]\n",
      "Escritura Escritura [0.24575669 0.75424331]\n",
      "Escritura Desiste [0.72459847 0.27540153]\n",
      "Escritura Escritura [0.43483138 0.56516862]\n",
      "Escritura Escritura [0.22418742 0.77581258]\n",
      "Escritura Escritura [0.37175043 0.62824957]\n",
      "Escritura Desiste [0.70654014 0.29345986]\n",
      "Escritura Escritura [0.21615836 0.78384164]\n",
      "Escritura Escritura [0.25413459 0.74586541]\n",
      "Escritura Desiste [0.61773036 0.38226964]\n",
      "Escritura Escritura [0.25338991 0.74661009]\n",
      "Escritura Escritura [0.17324239 0.82675761]\n",
      "Escritura Escritura [0.3341417 0.6658583]\n",
      "Escritura Escritura [0.09725515 0.90274485]\n",
      "Escritura Escritura [0.25301865 0.74698135]\n",
      "Escritura Escritura [0.23612509 0.76387491]\n",
      "Escritura Escritura [0.18706111 0.81293889]\n",
      "Escritura Escritura [0.25510645 0.74489355]\n",
      "Escritura Desiste [0.57140241 0.42859759]\n",
      "Escritura Desiste [0.59127102 0.40872898]\n",
      "Escritura Escritura [0.36177349 0.63822651]\n",
      "Escritura Escritura [0.44824634 0.55175366]\n",
      "Escritura Escritura [0.32662085 0.67337915]\n",
      "Escritura Escritura [0.21550065 0.78449935]\n",
      "Escritura Escritura [0.20997072 0.79002928]\n",
      "Escritura Desiste [0.65632761 0.34367239]\n",
      "Escritura Escritura [0.20304501 0.79695499]\n",
      "Escritura Escritura [0.29628516 0.70371484]\n",
      "Escritura Escritura [0.21459048 0.78540952]\n",
      "Escritura Escritura [0.46190742 0.53809258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Escritura [0.38655208 0.61344792]\n",
      "Escritura Escritura [0.22484938 0.77515062]\n",
      "Escritura Desiste [0.63536548 0.36463452]\n",
      "Escritura Escritura [0.39371792 0.60628208]\n",
      "Escritura Escritura [0.28815538 0.71184462]\n",
      "Escritura Desiste [0.60531561 0.39468439]\n",
      "Escritura Escritura [0.28993851 0.71006149]\n",
      "Escritura Escritura [0.26562123 0.73437877]\n",
      "Escritura Escritura [0.366404 0.633596]\n",
      "Escritura Desiste [0.53654822 0.46345178]\n",
      "Escritura Desiste [0.64057231 0.35942769]\n",
      "Escritura Desiste [0.54654353 0.45345647]\n",
      "Escritura Escritura [0.31143392 0.68856608]\n",
      "Escritura Escritura [0.14660136 0.85339864]\n",
      "Escritura Escritura [0.26090286 0.73909714]\n",
      "Escritura Desiste [0.62072569 0.37927431]\n",
      "Escritura Escritura [0.25151979 0.74848021]\n",
      "Escritura Escritura [0.2721433 0.7278567]\n",
      "Escritura Escritura [0.33314071 0.66685929]\n",
      "Escritura Desiste [0.74250244 0.25749756]\n",
      "Escritura Escritura [0.4826217 0.5173783]\n",
      "Escritura Desiste [0.72540865 0.27459135]\n",
      "Escritura Escritura [0.28254701 0.71745299]\n",
      "Escritura Escritura [0.17272756 0.82727244]\n",
      "Escritura Escritura [0.3504943 0.6495057]\n",
      "Escritura Desiste [0.60262447 0.39737553]\n",
      "Escritura Desiste [0.76217458 0.23782542]\n",
      "Escritura Escritura [0.13245622 0.86754378]\n",
      "Escritura Desiste [0.65821806 0.34178194]\n",
      "Escritura Escritura [0.24597654 0.75402346]\n",
      "Escritura Escritura [0.26117772 0.73882228]\n",
      "Escritura Escritura [0.31404251 0.68595749]\n",
      "Escritura Desiste [0.63767154 0.36232846]\n",
      "Escritura Escritura [0.39768116 0.60231884]\n",
      "Escritura Escritura [0.34447226 0.65552774]\n",
      "Escritura Escritura [0.19926078 0.80073922]\n",
      "Escritura Escritura [0.33397664 0.66602336]\n",
      "Escritura Escritura [0.3744296 0.6255704]\n",
      "Escritura Escritura [0.21277552 0.78722448]\n",
      "Escritura Desiste [0.68113613 0.31886387]\n",
      "Escritura Escritura [0.26953615 0.73046385]\n",
      "Escritura Escritura [0.38228739 0.61771261]\n",
      "Escritura Escritura [0.24728546 0.75271454]\n",
      "Escritura Escritura [0.16411035 0.83588965]\n",
      "Escritura Escritura [0.33008409 0.66991591]\n",
      "Escritura Desiste [0.66230189 0.33769811]\n",
      "Escritura Escritura [0.31910722 0.68089278]\n",
      "Escritura Escritura [0.18971867 0.81028133]\n",
      "Escritura Escritura [0.46327654 0.53672346]\n",
      "Escritura Desiste [0.67772449 0.32227551]\n",
      "Escritura Desiste [0.55615131 0.44384869]\n",
      "Escritura Desiste [0.70553342 0.29446658]\n",
      "Escritura Desiste [0.67323893 0.32676107]\n",
      "Escritura Desiste [0.64732741 0.35267259]\n",
      "Escritura Escritura [0.2063762 0.7936238]\n",
      "Escritura Escritura [0.27665878 0.72334122]\n",
      "Escritura Escritura [0.34326192 0.65673808]\n",
      "Escritura Escritura [0.28311457 0.71688543]\n",
      "Escritura Desiste [0.68649876 0.31350124]\n",
      "Escritura Escritura [0.16842469 0.83157531]\n",
      "Escritura Escritura [0.06505434 0.93494566]\n",
      "Escritura Escritura [0.46352779 0.53647221]\n",
      "Escritura Desiste [0.72682429 0.27317571]\n",
      "Escritura Escritura [0.48771809 0.51228191]\n",
      "Escritura Desiste [0.54709606 0.45290394]\n",
      "Escritura Escritura [0.27849091 0.72150909]\n",
      "Escritura Escritura [0.23784821 0.76215179]\n",
      "Escritura Desiste [0.5465637 0.4534363]\n",
      "Escritura Desiste [0.71788549 0.28211451]\n",
      "Escritura Desiste [0.73999347 0.26000653]\n",
      "Escritura Escritura [0.3279006 0.6720994]\n",
      "Escritura Desiste [0.54144492 0.45855508]\n",
      "Escritura Escritura [0.1846593 0.8153407]\n",
      "Escritura Escritura [0.19787915 0.80212085]\n",
      "Escritura Escritura [0.25749795 0.74250205]\n",
      "Escritura Escritura [0.21340584 0.78659416]\n",
      "Escritura Escritura [0.00977792 0.99022208]\n",
      "Escritura Desiste [0.71911848 0.28088152]\n",
      "Escritura Desiste [0.62404249 0.37595751]\n",
      "Escritura Escritura [0.19534368 0.80465632]\n",
      "Escritura Escritura [0.312276 0.687724]\n",
      "Escritura Desiste [0.67693287 0.32306713]\n",
      "Escritura Escritura [0.39279669 0.60720331]\n",
      "Escritura Escritura [0.30001161 0.69998839]\n",
      "Escritura Escritura [0.23651903 0.76348097]\n",
      "Escritura Escritura [0.30516494 0.69483506]\n",
      "Escritura Escritura [0.3121026 0.6878974]\n",
      "Escritura Escritura [0.22830286 0.77169714]\n",
      "Escritura Desiste [0.59035983 0.40964017]\n",
      "Escritura Desiste [0.622108 0.377892]\n",
      "Escritura Escritura [0.31925883 0.68074117]\n",
      "Escritura Desiste [0.76004727 0.23995273]\n",
      "Escritura Escritura [0.36318308 0.63681692]\n",
      "Escritura Desiste [0.71772781 0.28227219]\n",
      "Escritura Desiste [0.69225718 0.30774282]\n",
      "Escritura Escritura [0.2840208 0.7159792]\n",
      "Escritura Escritura [0.27611886 0.72388114]\n",
      "Escritura Escritura [0.3263465 0.6736535]\n",
      "Escritura Escritura [0.3273753 0.6726247]\n",
      "Escritura Escritura [0.31501603 0.68498397]\n",
      "Escritura Escritura [0.34434547 0.65565453]\n",
      "Escritura Escritura [0.23616873 0.76383127]\n",
      "Escritura Escritura [0.15808877 0.84191123]\n",
      "Escritura Escritura [0.17618196 0.82381804]\n",
      "Escritura Escritura [0.19939295 0.80060705]\n",
      "Escritura Escritura [0.4203616 0.5796384]\n",
      "Escritura Escritura [0.26695669 0.73304331]\n",
      "Escritura Escritura [0.39180885 0.60819115]\n",
      "Escritura Escritura [0.35001426 0.64998574]\n",
      "Escritura Escritura [0.28123443 0.71876557]\n",
      "Escritura Escritura [0.2144914 0.7855086]\n",
      "Escritura Desiste [0.56361128 0.43638872]\n",
      "Escritura Desiste [0.68980729 0.31019271]\n",
      "Escritura Escritura [0.2766903 0.7233097]\n",
      "Escritura Escritura [0.23991883 0.76008117]\n",
      "Escritura Escritura [0.38379824 0.61620176]\n",
      "Escritura Escritura [0.12596684 0.87403316]\n",
      "Escritura Desiste [0.58130556 0.41869444]\n",
      "Escritura Escritura [0.21882816 0.78117184]\n",
      "Escritura Escritura [0.35868462 0.64131538]\n",
      "Escritura Escritura [0.46682219 0.53317781]\n",
      "Escritura Escritura [0.3598989 0.6401011]\n",
      "Escritura Escritura [0.24882986 0.75117014]\n",
      "Escritura Escritura [0.39978199 0.60021801]\n",
      "Escritura Escritura [0.24647285 0.75352715]\n",
      "Escritura Escritura [0.4588934 0.5411066]\n",
      "Escritura Desiste [0.75702574 0.24297426]\n",
      "Escritura Desiste [0.77265999 0.22734001]\n",
      "Escritura Desiste [0.58236006 0.41763994]\n",
      "Escritura Desiste [0.58358686 0.41641314]\n",
      "Escritura Escritura [0.1592923 0.8407077]\n",
      "Escritura Desiste [0.60339548 0.39660452]\n",
      "Escritura Desiste [0.69482651 0.30517349]\n",
      "Escritura Escritura [0.29471892 0.70528108]\n",
      "Escritura Desiste [0.71433594 0.28566406]\n",
      "Escritura Escritura [0.28712498 0.71287502]\n",
      "Escritura Desiste [0.75953477 0.24046523]\n",
      "Escritura Escritura [0.26601979 0.73398021]\n",
      "Escritura Desiste [0.61809515 0.38190485]\n",
      "Escritura Escritura [0.34385902 0.65614098]\n",
      "Escritura Escritura [0.21864819 0.78135181]\n",
      "Escritura Escritura [0.30008231 0.69991769]\n",
      "Escritura Escritura [0.16270035 0.83729965]\n",
      "Escritura Desiste [0.80997181 0.19002819]\n",
      "Escritura Escritura [0.20254334 0.79745666]\n",
      "Escritura Escritura [0.28240962 0.71759038]\n",
      "Escritura Desiste [0.67215634 0.32784366]\n",
      "Escritura Desiste [0.8798062 0.1201938]\n",
      "Escritura Desiste [0.63626356 0.36373644]\n",
      "Escritura Escritura [0.25367136 0.74632864]\n",
      "Escritura Escritura [0.2364104 0.7635896]\n",
      "Escritura Escritura [0.21453532 0.78546468]\n",
      "Escritura Desiste [0.62645505 0.37354495]\n",
      "Escritura Desiste [0.59509763 0.40490237]\n",
      "Escritura Desiste [0.70395975 0.29604025]\n",
      "Escritura Desiste [0.55783771 0.44216229]\n",
      "Escritura Escritura [0.20581952 0.79418048]\n",
      "Escritura Escritura [0.36127642 0.63872358]\n",
      "Escritura Escritura [0.31611194 0.68388806]\n",
      "Escritura Desiste [0.71621665 0.28378335]\n",
      "Escritura Escritura [0.24085281 0.75914719]\n",
      "Escritura Desiste [0.65094305 0.34905695]\n",
      "Escritura Escritura [0.28514347 0.71485653]\n",
      "Escritura Desiste [0.76787217 0.23212783]\n",
      "Escritura Escritura [0.23797328 0.76202672]\n",
      "Escritura Escritura [0.45150955 0.54849045]\n",
      "Escritura Escritura [0.13487438 0.86512562]\n",
      "Escritura Desiste [0.60346169 0.39653831]\n",
      "Escritura Escritura [0.28886857 0.71113143]\n",
      "Escritura Escritura [0.24460237 0.75539763]\n",
      "Escritura Escritura [0.25900153 0.74099847]\n",
      "Escritura Escritura [0.19224374 0.80775626]\n",
      "Escritura Desiste [0.74661717 0.25338283]\n",
      "Escritura Escritura [0.24622689 0.75377311]\n",
      "Escritura Escritura [0.20879416 0.79120584]\n",
      "Escritura Escritura [0.42386596 0.57613404]\n",
      "Escritura Desiste [0.53148254 0.46851746]\n",
      "Escritura Escritura [0.2835125 0.7164875]\n",
      "Escritura Escritura [0.22892806 0.77107194]\n",
      "Escritura Escritura [0.10913039 0.89086961]\n",
      "Escritura Escritura [0.42098838 0.57901162]\n",
      "Escritura Desiste [0.69237389 0.30762611]\n",
      "Escritura Desiste [0.51527697 0.48472303]\n",
      "Escritura Escritura [0.34041848 0.65958152]\n",
      "Escritura Desiste [0.56924622 0.43075378]\n",
      "Escritura Desiste [0.65582743 0.34417257]\n",
      "Escritura Desiste [0.536822 0.463178]\n",
      "Escritura Escritura [0.20110679 0.79889321]\n",
      "Escritura Escritura [0.26705575 0.73294425]\n",
      "Escritura Desiste [0.61407165 0.38592835]\n",
      "Escritura Escritura [0.26950133 0.73049867]\n",
      "Escritura Desiste [0.67899931 0.32100069]\n",
      "Escritura Escritura [0.30196349 0.69803651]\n",
      "Escritura Escritura [0.32370039 0.67629961]\n",
      "Escritura Escritura [0.27060305 0.72939695]\n",
      "Escritura Escritura [0.36339773 0.63660227]\n",
      "Escritura Desiste [0.6001384 0.3998616]\n",
      "Escritura Escritura [0.45003664 0.54996336]\n",
      "Escritura Escritura [0.08438935 0.91561065]\n",
      "Escritura Escritura [0.28365335 0.71634665]\n",
      "Escritura Escritura [0.30893125 0.69106875]\n",
      "Escritura Desiste [0.51683465 0.48316535]\n",
      "Escritura Desiste [0.5806632 0.4193368]\n",
      "Escritura Escritura [0.48943156 0.51056844]\n",
      "Escritura Escritura [0.20929347 0.79070653]\n",
      "Escritura Desiste [0.77061651 0.22938349]\n",
      "Escritura Escritura [0.25453845 0.74546155]\n",
      "Escritura Escritura [0.24451745 0.75548255]\n",
      "Escritura Escritura [0.31898637 0.68101363]\n",
      "Escritura Desiste [0.62080712 0.37919288]\n",
      "Escritura Escritura [0.25543549 0.74456451]\n",
      "Escritura Desiste [0.5753697 0.4246303]\n",
      "Escritura Escritura [0.32258333 0.67741667]\n",
      "Escritura Escritura [0.47575159 0.52424841]\n",
      "Escritura Desiste [0.71243637 0.28756363]\n",
      "Escritura Escritura [0.39133246 0.60866754]\n",
      "Escritura Escritura [0.14287508 0.85712492]\n",
      "Escritura Escritura [0.18664847 0.81335153]\n",
      "Escritura Escritura [0.22543271 0.77456729]\n",
      "Escritura Escritura [0.2190379 0.7809621]\n",
      "Escritura Escritura [0.30189053 0.69810947]\n",
      "Escritura Desiste [0.61094459 0.38905541]\n",
      "Escritura Escritura [0.18156592 0.81843408]\n",
      "Escritura Desiste [0.53753258 0.46246742]\n",
      "Escritura Escritura [0.25359235 0.74640765]\n",
      "Escritura Escritura [0.29338143 0.70661857]\n",
      "Escritura Desiste [0.59886543 0.40113457]\n",
      "Escritura Desiste [0.63954463 0.36045537]\n",
      "Escritura Escritura [0.26141361 0.73858639]\n",
      "Escritura Escritura [0.36245045 0.63754955]\n",
      "Escritura Escritura [0.35886189 0.64113811]\n",
      "Escritura Escritura [0.2198244 0.7801756]\n",
      "Escritura Desiste [0.73784113 0.26215887]\n",
      "Escritura Escritura [0.18637454 0.81362546]\n",
      "Escritura Desiste [0.53513434 0.46486566]\n",
      "Escritura Escritura [0.13652798 0.86347202]\n",
      "Escritura Escritura [0.18667728 0.81332272]\n",
      "Escritura Desiste [0.68777912 0.31222088]\n",
      "Escritura Desiste [0.70858444 0.29141556]\n",
      "Escritura Escritura [0.25074266 0.74925734]\n",
      "Escritura Desiste [0.59412564 0.40587436]\n",
      "Escritura Escritura [0.28257389 0.71742611]\n",
      "Escritura Escritura [0.25234303 0.74765697]\n",
      "Escritura Escritura [0.20739495 0.79260505]\n",
      "Escritura Desiste [0.51052783 0.48947217]\n",
      "Escritura Desiste [0.62913162 0.37086838]\n",
      "Escritura Escritura [0.21754072 0.78245928]\n",
      "Escritura Escritura [0.35035824 0.64964176]\n",
      "Escritura Desiste [0.51137433 0.48862567]\n",
      "Escritura Escritura [0.1458343 0.8541657]\n",
      "Escritura Escritura [0.17230626 0.82769374]\n",
      "Escritura Escritura [0.27528065 0.72471935]\n",
      "Escritura Escritura [0.265185 0.734815]\n",
      "Escritura Desiste [0.77881527 0.22118473]\n",
      "Escritura Escritura [0.35861907 0.64138093]\n",
      "Escritura Escritura [0.25235629 0.74764371]\n",
      "Escritura Desiste [0.7832679 0.2167321]\n",
      "Escritura Desiste [0.76835199 0.23164801]\n",
      "Escritura Escritura [0.25875769 0.74124231]\n",
      "Escritura Desiste [0.5401583 0.4598417]\n",
      "Escritura Desiste [0.58283738 0.41716262]\n",
      "Escritura Desiste [0.6970812 0.3029188]\n",
      "Escritura Escritura [0.40167639 0.59832361]\n",
      "Escritura Desiste [0.63366795 0.36633205]\n",
      "Escritura Escritura [0.17399276 0.82600724]\n",
      "Escritura Desiste [0.65937499 0.34062501]\n",
      "Escritura Escritura [0.19608046 0.80391954]\n",
      "Escritura Escritura [0.39625058 0.60374942]\n",
      "Escritura Desiste [0.68167273 0.31832727]\n",
      "Escritura Desiste [0.58332871 0.41667129]\n",
      "Escritura Desiste [0.59967436 0.40032564]\n",
      "Escritura Desiste [0.72060633 0.27939367]\n",
      "Escritura Escritura [0.23574218 0.76425782]\n",
      "Escritura Desiste [0.68861779 0.31138221]\n",
      "Escritura Escritura [0.05924271 0.94075729]\n",
      "Escritura Escritura [0.39904949 0.60095051]\n",
      "Escritura Escritura [0.17896259 0.82103741]\n",
      "Escritura Escritura [0.2950555 0.7049445]\n",
      "Escritura Escritura [0.15826893 0.84173107]\n",
      "Escritura Desiste [0.62576041 0.37423959]\n",
      "Escritura Desiste [0.67376573 0.32623427]\n",
      "Escritura Desiste [0.79121004 0.20878996]\n",
      "Escritura Desiste [0.70206561 0.29793439]\n",
      "Escritura Desiste [0.59166108 0.40833892]\n",
      "Escritura Escritura [0.29775141 0.70224859]\n",
      "Escritura Desiste [0.57574429 0.42425571]\n",
      "Escritura Desiste [0.78126864 0.21873136]\n",
      "Escritura Escritura [0.33048531 0.66951469]\n",
      "Escritura Escritura [0.31816305 0.68183695]\n",
      "Escritura Escritura [0.29518406 0.70481594]\n",
      "Escritura Escritura [0.29912196 0.70087804]\n",
      "Escritura Escritura [0.09582742 0.90417258]\n",
      "Escritura Escritura [0.30085584 0.69914416]\n",
      "Escritura Escritura [0.22625775 0.77374225]\n",
      "Escritura Desiste [0.62694338 0.37305662]\n",
      "Escritura Escritura [0.28556261 0.71443739]\n",
      "Escritura Escritura [0.34335121 0.65664879]\n",
      "Escritura Escritura [0.34141661 0.65858339]\n",
      "Escritura Escritura [0.12729151 0.87270849]\n",
      "Escritura Escritura [0.35562115 0.64437885]\n",
      "Escritura Desiste [0.73528688 0.26471312]\n",
      "Escritura Escritura [0.31922458 0.68077542]\n",
      "Escritura Escritura [0.38638315 0.61361685]\n",
      "Escritura Desiste [0.56645711 0.43354289]\n",
      "Escritura Escritura [0.29008434 0.70991566]\n",
      "Escritura Desiste [0.63915058 0.36084942]\n",
      "Escritura Escritura [0.19418176 0.80581824]\n",
      "Escritura Escritura [0.36304497 0.63695503]\n",
      "Escritura Desiste [0.55249145 0.44750855]\n",
      "Escritura Escritura [0.00165719 0.99834281]\n",
      "Escritura Desiste [0.59251384 0.40748616]\n",
      "Escritura Desiste [0.71197216 0.28802784]\n",
      "Escritura Desiste [0.56401689 0.43598311]\n",
      "Escritura Escritura [0.28373037 0.71626963]\n",
      "Escritura Desiste [0.503537 0.496463]\n",
      "Escritura Desiste [0.61353691 0.38646309]\n",
      "Escritura Escritura [0.34277455 0.65722545]\n",
      "Escritura Escritura [0.34249 0.65751]\n",
      "Escritura Desiste [0.56696761 0.43303239]\n",
      "Escritura Escritura [0.40645534 0.59354466]\n",
      "Escritura Escritura [0.26556577 0.73443423]\n",
      "Escritura Escritura [0.16290042 0.83709958]\n",
      "Escritura Escritura [0.25714535 0.74285465]\n",
      "Escritura Escritura [0.37985983 0.62014017]\n",
      "Escritura Escritura [0.18115672 0.81884328]\n",
      "Escritura Escritura [0.35044182 0.64955818]\n",
      "Escritura Escritura [0.33559597 0.66440403]\n",
      "Escritura Escritura [0.13616814 0.86383186]\n",
      "Escritura Desiste [0.69118201 0.30881799]\n",
      "Escritura Escritura [0.25457045 0.74542955]\n",
      "Escritura Escritura [0.35783813 0.64216187]\n",
      "Escritura Desiste [0.62644475 0.37355525]\n",
      "Escritura Desiste [0.58888653 0.41111347]\n",
      "Escritura Escritura [0.2338294 0.7661706]\n",
      "Escritura Desiste [0.67133003 0.32866997]\n",
      "Escritura Escritura [0.3295316 0.6704684]\n",
      "Escritura Desiste [0.65598815 0.34401185]\n",
      "Escritura Desiste [0.68288579 0.31711421]\n",
      "Escritura Desiste [0.71176979 0.28823021]\n",
      "Escritura Escritura [0.3518096 0.6481904]\n",
      "Escritura Desiste [0.6590028 0.3409972]\n",
      "Escritura Escritura [0.41236153 0.58763847]\n",
      "Escritura Escritura [0.22119424 0.77880576]\n",
      "Escritura Desiste [0.72295952 0.27704048]\n",
      "Escritura Escritura [0.26598921 0.73401079]\n",
      "Escritura Desiste [0.69526067 0.30473933]\n",
      "Escritura Escritura [0.1278625 0.8721375]\n",
      "Escritura Escritura [0.31242645 0.68757355]\n",
      "Escritura Escritura [0.2406026 0.7593974]\n",
      "Escritura Escritura [0.19905298 0.80094702]\n",
      "Escritura Escritura [0.27309009 0.72690991]\n",
      "Escritura Escritura [0.34078422 0.65921578]\n",
      "Escritura Escritura [0.30168247 0.69831753]\n",
      "Escritura Escritura [0.26897388 0.73102612]\n",
      "Escritura Desiste [0.70553034 0.29446966]\n",
      "Escritura Desiste [0.57822142 0.42177858]\n",
      "Escritura Escritura [0.30085334 0.69914666]\n",
      "Escritura Desiste [0.58202774 0.41797226]\n",
      "Escritura Escritura [0.2744343 0.7255657]\n",
      "Escritura Escritura [0.41694636 0.58305364]\n",
      "Escritura Escritura [0.26219935 0.73780065]\n",
      "Escritura Desiste [0.7760257 0.2239743]\n",
      "Escritura Escritura [0.25257014 0.74742986]\n",
      "Escritura Escritura [0.37567335 0.62432665]\n",
      "Escritura Desiste [0.78159728 0.21840272]\n",
      "Escritura Escritura [0.30702284 0.69297716]\n",
      "Escritura Desiste [0.6958143 0.3041857]\n",
      "Escritura Escritura [0.28902572 0.71097428]\n",
      "Escritura Escritura [0.1803767 0.8196233]\n",
      "Escritura Escritura [0.2366278 0.7633722]\n",
      "Escritura Escritura [0.46385168 0.53614832]\n",
      "Escritura Escritura [0.45197672 0.54802328]\n",
      "Escritura Desiste [0.52810649 0.47189351]\n",
      "Escritura Escritura [0.28154206 0.71845794]\n",
      "Escritura Escritura [0.16153013 0.83846987]\n",
      "Escritura Desiste [0.64451119 0.35548881]\n",
      "Escritura Escritura [0.41114044 0.58885956]\n",
      "Escritura Escritura [0.10771509 0.89228491]\n",
      "Escritura Escritura [0.43881432 0.56118568]\n",
      "Escritura Desiste [0.6296676 0.3703324]\n",
      "Escritura Desiste [0.68265745 0.31734255]\n",
      "Escritura Escritura [0.47091175 0.52908825]\n",
      "Escritura Desiste [0.53256621 0.46743379]\n",
      "Escritura Desiste [0.61447835 0.38552165]\n",
      "Escritura Escritura [0.2258314 0.7741686]\n",
      "Escritura Escritura [0.16422832 0.83577168]\n",
      "Escritura Escritura [0.40186403 0.59813597]\n",
      "Escritura Escritura [0.2344908 0.7655092]\n",
      "Escritura Desiste [0.7943305 0.2056695]\n",
      "Escritura Escritura [0.45348783 0.54651217]\n",
      "Escritura Desiste [0.69233268 0.30766732]\n",
      "Escritura Escritura [0.20740757 0.79259243]\n",
      "Escritura Escritura [0.35221446 0.64778554]\n",
      "Escritura Escritura [0.46741391 0.53258609]\n",
      "Escritura Desiste [0.83815835 0.16184165]\n",
      "Escritura Escritura [0.01375731 0.98624269]\n",
      "Escritura Escritura [0.0895745 0.9104255]\n",
      "Escritura Desiste [0.72028237 0.27971763]\n",
      "Escritura Desiste [0.54333978 0.45666022]\n",
      "Escritura Desiste [0.59869655 0.40130345]\n",
      "Escritura Escritura [0.31827382 0.68172618]\n",
      "Escritura Escritura [0.31909044 0.68090956]\n",
      "Escritura Desiste [0.71122549 0.28877451]\n",
      "Escritura Desiste [0.63338655 0.36661345]\n",
      "Escritura Desiste [0.62631579 0.37368421]\n",
      "Escritura Escritura [0.2715645 0.7284355]\n",
      "Escritura Escritura [0.3000326 0.6999674]\n",
      "Escritura Desiste [0.51609278 0.48390722]\n",
      "Escritura Desiste [0.53253429 0.46746571]\n",
      "Escritura Escritura [0.29259646 0.70740354]\n",
      "Escritura Escritura [0.17165826 0.82834174]\n",
      "Escritura Desiste [0.57547425 0.42452575]\n",
      "Escritura Escritura [0.34349569 0.65650431]\n",
      "Escritura Desiste [0.69700782 0.30299218]\n",
      "Escritura Desiste [0.72698896 0.27301104]\n",
      "Escritura Escritura [0.24190142 0.75809858]\n",
      "Escritura Escritura [0.18412231 0.81587769]\n",
      "Escritura Escritura [0.18925815 0.81074185]\n",
      "Escritura Escritura [0.25924965 0.74075035]\n",
      "Escritura Escritura [0.25321235 0.74678765]\n",
      "Escritura Escritura [0.17045748 0.82954252]\n",
      "Escritura Desiste [0.66203804 0.33796196]\n",
      "Escritura Escritura [0.30135684 0.69864316]\n",
      "Escritura Desiste [0.5447117 0.4552883]\n",
      "Escritura Desiste [0.67329414 0.32670586]\n",
      "Escritura Escritura [0.25898689 0.74101311]\n",
      "Escritura Escritura [0.23308138 0.76691862]\n",
      "Escritura Escritura [0.27011011 0.72988989]\n",
      "Escritura Escritura [0.20402857 0.79597143]\n",
      "Escritura Desiste [0.50613428 0.49386572]\n",
      "Escritura Desiste [0.62950564 0.37049436]\n",
      "Escritura Escritura [0.34495537 0.65504463]\n",
      "Escritura Escritura [0.34189496 0.65810504]\n",
      "Escritura Escritura [0.47177505 0.52822495]\n",
      "Escritura Desiste [0.66087426 0.33912574]\n",
      "Escritura Escritura [0.20927573 0.79072427]\n",
      "Escritura Desiste [0.85669139 0.14330861]\n",
      "Escritura Escritura [0.19752882 0.80247118]\n",
      "Escritura Escritura [0.35574296 0.64425704]\n",
      "Escritura Escritura [0.30684906 0.69315094]\n",
      "Escritura Desiste [0.71316757 0.28683243]\n",
      "Escritura Desiste [0.63649076 0.36350924]\n",
      "Escritura Escritura [0.24726333 0.75273667]\n",
      "Escritura Desiste [0.74135835 0.25864165]\n",
      "Escritura Desiste [0.69668122 0.30331878]\n",
      "Escritura Escritura [0.09525457 0.90474543]\n",
      "Escritura Escritura [0.32317919 0.67682081]\n",
      "Escritura Escritura [0.2378858 0.7621142]\n",
      "Escritura Desiste [0.74327039 0.25672961]\n",
      "Escritura Escritura [0.28827933 0.71172067]\n",
      "Escritura Desiste [0.5430088 0.4569912]\n",
      "Escritura Desiste [0.62982321 0.37017679]\n",
      "Escritura Escritura [0.14617993 0.85382007]\n",
      "Escritura Escritura [0.29059732 0.70940268]\n",
      "Escritura Escritura [0.42422534 0.57577466]\n",
      "Escritura Desiste [0.76446722 0.23553278]\n",
      "Escritura Escritura [0.32861679 0.67138321]\n",
      "Escritura Escritura [0.18574269 0.81425731]\n",
      "Escritura Escritura [0.21346423 0.78653577]\n",
      "Escritura Escritura [0.23990487 0.76009513]\n",
      "Escritura Desiste [0.55774059 0.44225941]\n",
      "Escritura Desiste [0.73448136 0.26551864]\n",
      "Escritura Escritura [0.42682696 0.57317304]\n",
      "Escritura Desiste [0.79567829 0.20432171]\n",
      "Escritura Escritura [0.34157854 0.65842146]\n",
      "Escritura Escritura [0.43008816 0.56991184]\n",
      "Escritura Escritura [0.30456631 0.69543369]\n",
      "Escritura Escritura [0.23369407 0.76630593]\n",
      "Escritura Desiste [0.59460187 0.40539813]\n",
      "Escritura Escritura [0.15202206 0.84797794]\n",
      "Escritura Escritura [0.25835543 0.74164457]\n",
      "Escritura Desiste [0.70494042 0.29505958]\n",
      "Escritura Desiste [0.69221899 0.30778101]\n",
      "Escritura Desiste [0.71842807 0.28157193]\n",
      "Escritura Desiste [0.54727072 0.45272928]\n",
      "Escritura Desiste [0.59935121 0.40064879]\n",
      "Escritura Desiste [0.58718502 0.41281498]\n",
      "Escritura Escritura [0.23265194 0.76734806]\n",
      "Escritura Escritura [0.33368604 0.66631396]\n",
      "Escritura Escritura [0.30384975 0.69615025]\n",
      "Escritura Escritura [0.3388802 0.6611198]\n",
      "Escritura Desiste [0.56035599 0.43964401]\n",
      "Escritura Escritura [0.2059338 0.7940662]\n",
      "Escritura Escritura [0.20761986 0.79238014]\n",
      "Escritura Desiste [0.55045258 0.44954742]\n",
      "Escritura Desiste [0.64441442 0.35558558]\n",
      "Escritura Escritura [0.29046065 0.70953935]\n",
      "Escritura Escritura [0.32741526 0.67258474]\n",
      "Escritura Desiste [0.50370591 0.49629409]\n",
      "Escritura Escritura [0.34338528 0.65661472]\n",
      "Escritura Escritura [0.35459448 0.64540552]\n",
      "Escritura Escritura [0.11558887 0.88441113]\n",
      "Escritura Escritura [0.41778747 0.58221253]\n",
      "Escritura Escritura [0.28678042 0.71321958]\n",
      "Escritura Escritura [0.24426048 0.75573952]\n",
      "Escritura Escritura [0.15997614 0.84002386]\n",
      "Escritura Escritura [0.28937316 0.71062684]\n",
      "Escritura Desiste [0.75660975 0.24339025]\n",
      "Escritura Desiste [0.58444037 0.41555963]\n",
      "Escritura Escritura [0.26052042 0.73947958]\n",
      "Escritura Escritura [0.27987228 0.72012772]\n",
      "Escritura Desiste [0.73357764 0.26642236]\n",
      "Escritura Desiste [0.66976325 0.33023675]\n",
      "Escritura Escritura [0.16980343 0.83019657]\n",
      "Escritura Escritura [0.27892211 0.72107789]\n",
      "Escritura Escritura [0.29210583 0.70789417]\n",
      "Escritura Desiste [0.64149649 0.35850351]\n",
      "Escritura Desiste [0.72733368 0.27266632]\n",
      "Escritura Desiste [0.55055658 0.44944342]\n",
      "Escritura Desiste [0.69950478 0.30049522]\n",
      "Escritura Desiste [0.52529449 0.47470551]\n",
      "Escritura Escritura [0.33536493 0.66463507]\n",
      "Escritura Escritura [0.30419663 0.69580337]\n",
      "Escritura Escritura [0.35399362 0.64600638]\n",
      "Escritura Escritura [0.2838356 0.7161644]\n",
      "Escritura Escritura [0.15914861 0.84085139]\n",
      "Escritura Escritura [0.37441344 0.62558656]\n",
      "Escritura Desiste [0.59085919 0.40914081]\n",
      "Escritura Escritura [0.27852033 0.72147967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Escritura [0.29961121 0.70038879]\n",
      "Escritura Desiste [0.69496956 0.30503044]\n",
      "Escritura Escritura [0.39404473 0.60595527]\n",
      "Escritura Desiste [0.59523632 0.40476368]\n",
      "Escritura Desiste [0.58310413 0.41689587]\n",
      "Escritura Escritura [0.412329 0.587671]\n",
      "Escritura Escritura [0.48839093 0.51160907]\n",
      "Escritura Escritura [0.24989444 0.75010556]\n",
      "Escritura Desiste [0.556181 0.443819]\n",
      "Escritura Desiste [0.53813359 0.46186641]\n",
      "Escritura Desiste [0.74419705 0.25580295]\n",
      "Escritura Desiste [0.55468979 0.44531021]\n",
      "Escritura Escritura [0.29235441 0.70764559]\n",
      "Desiste Desiste [0.64986692 0.35013308]\n",
      "Desiste Desiste [0.60611895 0.39388105]\n",
      "Desiste Escritura [0.4206619 0.5793381]\n",
      "Desiste Desiste [0.65138198 0.34861802]\n",
      "Desiste Desiste [0.58855226 0.41144774]\n",
      "Desiste Desiste [0.77367852 0.22632148]\n",
      "Desiste Escritura [0.31001462 0.68998538]\n",
      "Desiste Desiste [0.63921622 0.36078378]\n",
      "Desiste Desiste [0.61349913 0.38650087]\n",
      "Desiste Desiste [0.65191948 0.34808052]\n",
      "Desiste Desiste [0.60552615 0.39447385]\n",
      "Desiste Desiste [0.60334561 0.39665439]\n",
      "Desiste Escritura [0.36818419 0.63181581]\n",
      "Desiste Desiste [0.78295718 0.21704282]\n",
      "Desiste Escritura [0.38873005 0.61126995]\n",
      "Desiste Desiste [0.78016943 0.21983057]\n",
      "Desiste Desiste [0.54831687 0.45168313]\n",
      "Desiste Desiste [0.71847197 0.28152803]\n",
      "Desiste Desiste [0.66791076 0.33208924]\n",
      "Desiste Escritura [0.30149301 0.69850699]\n",
      "Desiste Desiste [0.72130015 0.27869985]\n",
      "Desiste Desiste [0.69710942 0.30289058]\n",
      "Desiste Desiste [0.60210836 0.39789164]\n",
      "Desiste Desiste [0.59540012 0.40459988]\n",
      "Desiste Desiste [0.70589424 0.29410576]\n",
      "Desiste Desiste [0.58430181 0.41569819]\n",
      "Desiste Desiste [0.58755073 0.41244927]\n",
      "Desiste Desiste [0.54748495 0.45251505]\n",
      "Desiste Desiste [0.5648792 0.4351208]\n",
      "Desiste Desiste [0.58245581 0.41754419]\n",
      "Desiste Desiste [0.65413657 0.34586343]\n",
      "Desiste Desiste [0.73764971 0.26235029]\n",
      "Desiste Desiste [0.75459914 0.24540086]\n",
      "Desiste Desiste [0.73958681 0.26041319]\n",
      "Desiste Escritura [0.40248231 0.59751769]\n",
      "Desiste Desiste [0.69276876 0.30723124]\n",
      "Desiste Escritura [0.32499312 0.67500688]\n",
      "Desiste Desiste [0.7898568 0.2101432]\n",
      "Desiste Desiste [0.69066836 0.30933164]\n",
      "Desiste Desiste [0.61510289 0.38489711]\n",
      "Desiste Escritura [0.24622346 0.75377654]\n",
      "Desiste Desiste [0.56984471 0.43015529]\n",
      "Desiste Escritura [0.35058377 0.64941623]\n",
      "Desiste Desiste [0.74588954 0.25411046]\n",
      "Desiste Desiste [0.66111093 0.33888907]\n",
      "Desiste Desiste [0.62690591 0.37309409]\n",
      "Desiste Desiste [0.76169935 0.23830065]\n",
      "Desiste Escritura [0.3217442 0.6782558]\n",
      "Desiste Escritura [0.34965486 0.65034514]\n",
      "Desiste Desiste [0.57539449 0.42460551]\n",
      "Desiste Desiste [0.54416781 0.45583219]\n",
      "Desiste Desiste [0.721346 0.278654]\n",
      "Desiste Desiste [0.67123537 0.32876463]\n",
      "Desiste Desiste [0.74219465 0.25780535]\n",
      "Desiste Desiste [0.66965543 0.33034457]\n",
      "Desiste Desiste [0.69606406 0.30393594]\n",
      "Desiste Desiste [0.6390632 0.3609368]\n",
      "Desiste Escritura [0.44988193 0.55011807]\n",
      "Desiste Escritura [0.42977182 0.57022818]\n",
      "Desiste Desiste [0.61211381 0.38788619]\n",
      "Desiste Desiste [0.6712602 0.3287398]\n",
      "Desiste Desiste [0.66526364 0.33473636]\n",
      "Desiste Desiste [0.78179133 0.21820867]\n",
      "Desiste Desiste [0.67639738 0.32360262]\n",
      "Desiste Desiste [0.58181015 0.41818985]\n",
      "Desiste Desiste [0.69725143 0.30274857]\n",
      "Desiste Escritura [0.28326686 0.71673314]\n",
      "Desiste Escritura [0.22925831 0.77074169]\n",
      "Desiste Desiste [0.57255712 0.42744288]\n",
      "Desiste Escritura [0.36130603 0.63869397]\n",
      "Desiste Desiste [0.74015835 0.25984165]\n",
      "Desiste Desiste [0.60303656 0.39696344]\n",
      "Desiste Desiste [0.53149852 0.46850148]\n",
      "Desiste Desiste [0.71217176 0.28782824]\n",
      "Desiste Escritura [0.33425356 0.66574644]\n",
      "Desiste Desiste [0.67172119 0.32827881]\n",
      "Desiste Escritura [0.40555265 0.59444735]\n",
      "Desiste Desiste [0.68516758 0.31483242]\n",
      "Desiste Desiste [0.76056744 0.23943256]\n",
      "Desiste Escritura [0.17993961 0.82006039]\n",
      "Desiste Desiste [0.67695427 0.32304573]\n",
      "Desiste Desiste [0.68334538 0.31665462]\n",
      "Desiste Desiste [0.69816814 0.30183186]\n",
      "Desiste Escritura [0.44622686 0.55377314]\n",
      "Desiste Escritura [0.16678639 0.83321361]\n",
      "Desiste Desiste [0.74555269 0.25444731]\n",
      "Desiste Desiste [0.64094736 0.35905264]\n",
      "Desiste Desiste [0.62082079 0.37917921]\n",
      "Desiste Desiste [0.62449998 0.37550002]\n",
      "Desiste Desiste [0.554049 0.445951]\n",
      "Desiste Desiste [0.70537688 0.29462312]\n",
      "Desiste Desiste [0.76770469 0.23229531]\n",
      "Desiste Desiste [0.54676599 0.45323401]\n",
      "Desiste Desiste [0.54610439 0.45389561]\n",
      "Desiste Desiste [0.56412129 0.43587871]\n",
      "Desiste Desiste [0.63217555 0.36782445]\n",
      "Desiste Desiste [0.56361128 0.43638872]\n",
      "Desiste Desiste [0.68247093 0.31752907]\n",
      "Desiste Desiste [0.50567497 0.49432503]\n",
      "Desiste Escritura [0.29328195 0.70671805]\n",
      "Desiste Desiste [0.63460151 0.36539849]\n",
      "Desiste Desiste [0.64327533 0.35672467]\n",
      "Desiste Escritura [0.01573739 0.98426261]\n",
      "Desiste Desiste [0.7360667 0.2639333]\n",
      "Desiste Desiste [0.63843469 0.36156531]\n",
      "Desiste Desiste [0.65299926 0.34700074]\n",
      "Desiste Desiste [0.55895999 0.44104001]\n",
      "Desiste Desiste [0.77881527 0.22118473]\n",
      "Desiste Desiste [0.5445373 0.4554627]\n",
      "Desiste Desiste [0.61531759 0.38468241]\n",
      "Desiste Escritura [0.24406392 0.75593608]\n",
      "Desiste Escritura [0.42395807 0.57604193]\n",
      "Desiste Escritura [0.12994267 0.87005733]\n",
      "Desiste Desiste [0.61737504 0.38262496]\n",
      "Desiste Desiste [0.69301207 0.30698793]\n",
      "Desiste Desiste [0.54027089 0.45972911]\n",
      "Desiste Desiste [0.72263837 0.27736163]\n",
      "Desiste Escritura [0.32375861 0.67624139]\n",
      "Desiste Escritura [0.00175609 0.99824391]\n",
      "Desiste Desiste [0.6965648 0.3034352]\n",
      "Desiste Desiste [0.54705637 0.45294363]\n",
      "Desiste Escritura [0.3326809 0.6673191]\n",
      "Desiste Desiste [0.67752495 0.32247505]\n",
      "Desiste Desiste [0.60263197 0.39736803]\n",
      "Desiste Desiste [0.55867547 0.44132453]\n",
      "Desiste Desiste [0.70535785 0.29464215]\n",
      "Desiste Desiste [0.78219751 0.21780249]\n",
      "Desiste Escritura [0.19200369 0.80799631]\n",
      "Desiste Desiste [0.63193252 0.36806748]\n",
      "Desiste Desiste [0.67996579 0.32003421]\n",
      "Desiste Desiste [0.65962507 0.34037493]\n",
      "Desiste Escritura [0.32357119 0.67642881]\n",
      "Desiste Desiste [0.63916276 0.36083724]\n",
      "Desiste Desiste [0.61394882 0.38605118]\n",
      "Desiste Desiste [0.65340366 0.34659634]\n",
      "Desiste Desiste [0.72650385 0.27349615]\n",
      "Desiste Desiste [0.61955231 0.38044769]\n",
      "Desiste Escritura [0.18697401 0.81302599]\n",
      "Desiste Desiste [0.75814656 0.24185344]\n",
      "Desiste Desiste [0.61327535 0.38672465]\n",
      "Desiste Desiste [0.63241513 0.36758487]\n",
      "Desiste Desiste [0.56002824 0.43997176]\n",
      "Desiste Escritura [0.25460492 0.74539508]\n",
      "Desiste Desiste [0.79731376 0.20268624]\n",
      "Desiste Desiste [0.59055548 0.40944452]\n",
      "Desiste Desiste [0.56137448 0.43862552]\n",
      "Desiste Desiste [0.60049846 0.39950154]\n",
      "Desiste Desiste [0.74511887 0.25488113]\n",
      "Desiste Escritura [0.30451428 0.69548572]\n",
      "Desiste Escritura [0.3948441 0.6051559]\n",
      "Desiste Escritura [0.31445893 0.68554107]\n",
      "Desiste Desiste [0.80329204 0.19670796]\n",
      "Desiste Desiste [0.75399748 0.24600252]\n",
      "Desiste Desiste [0.83988487 0.16011513]\n",
      "Desiste Desiste [0.61298534 0.38701466]\n",
      "Desiste Desiste [0.70567735 0.29432265]\n",
      "Desiste Desiste [0.61069867 0.38930133]\n",
      "Desiste Desiste [0.66386015 0.33613985]\n",
      "Desiste Desiste [0.55968056 0.44031944]\n",
      "Desiste Desiste [0.65004491 0.34995509]\n",
      "Desiste Escritura [0.26220194 0.73779806]\n",
      "Desiste Escritura [0.33957946 0.66042054]\n",
      "Desiste Desiste [0.55032687 0.44967313]\n",
      "Desiste Desiste [0.61126811 0.38873189]\n",
      "Desiste Desiste [0.76768312 0.23231688]\n",
      "Desiste Desiste [0.6655767 0.3344233]\n",
      "Desiste Desiste [0.7359105 0.2640895]\n",
      "Desiste Escritura [0.23049052 0.76950948]\n",
      "Desiste Desiste [0.65161829 0.34838171]\n",
      "Desiste Desiste [0.60909513 0.39090487]\n",
      "Desiste Desiste [0.7413603 0.2586397]\n",
      "Desiste Escritura [0.29255446 0.70744554]\n",
      "Desiste Desiste [0.56499265 0.43500735]\n",
      "Desiste Desiste [0.61003127 0.38996873]\n",
      "Desiste Escritura [0.45046467 0.54953533]\n",
      "Desiste Escritura [0.31368349 0.68631651]\n",
      "Desiste Desiste [0.73777573 0.26222427]\n",
      "Desiste Escritura [0.31810516 0.68189484]\n",
      "Desiste Desiste [0.69229619 0.30770381]\n",
      "Desiste Desiste [0.61710103 0.38289897]\n",
      "Desiste Escritura [0.25682331 0.74317669]\n",
      "Desiste Escritura [0.22913643 0.77086357]\n",
      "Desiste Escritura [0.09657699 0.90342301]\n",
      "Desiste Escritura [0.0921776 0.9078224]\n",
      "Desiste Desiste [0.76958501 0.23041499]\n",
      "Desiste Desiste [0.66727276 0.33272724]\n",
      "Desiste Escritura [0.09027416 0.90972584]\n",
      "Desiste Desiste [0.58706046 0.41293954]\n",
      "Desiste Desiste [0.69656175 0.30343825]\n",
      "Desiste Desiste [0.64931266 0.35068734]\n",
      "Desiste Desiste [0.5838921 0.4161079]\n",
      "Desiste Desiste [0.52992731 0.47007269]\n",
      "Desiste Desiste [0.69588405 0.30411595]\n",
      "Desiste Escritura [0.28087112 0.71912888]\n",
      "Desiste Desiste [0.75329467 0.24670533]\n",
      "Desiste Desiste [0.61918829 0.38081171]\n",
      "Desiste Desiste [0.53947774 0.46052226]\n",
      "Desiste Desiste [0.70671534 0.29328466]\n",
      "Desiste Desiste [0.68890899 0.31109101]\n",
      "Desiste Escritura [0.38349938 0.61650062]\n",
      "Desiste Desiste [0.78251665 0.21748335]\n",
      "Desiste Escritura [0.30475415 0.69524585]\n",
      "Desiste Escritura [0.36407166 0.63592834]\n",
      "Desiste Escritura [0.43402487 0.56597513]\n",
      "Desiste Desiste [0.63572324 0.36427676]\n",
      "Desiste Escritura [0.0494276 0.9505724]\n",
      "Desiste Desiste [0.55085138 0.44914862]\n",
      "Desiste Escritura [0.31675959 0.68324041]\n",
      "Desiste Desiste [0.80526822 0.19473178]\n",
      "Desiste Escritura [0.29783081 0.70216919]\n",
      "Desiste Desiste [0.63910729 0.36089271]\n",
      "Desiste Escritura [0.34730437 0.65269563]\n",
      "Desiste Escritura [0.25847798 0.74152202]\n",
      "Desiste Desiste [0.8222131 0.1777869]\n",
      "Desiste Desiste [0.74683895 0.25316105]\n",
      "Desiste Desiste [0.74030473 0.25969527]\n",
      "Desiste Desiste [0.62702642 0.37297358]\n",
      "Desiste Escritura [0.13775827 0.86224173]\n",
      "Desiste Escritura [0.41060835 0.58939165]\n",
      "Desiste Desiste [0.77719926 0.22280074]\n",
      "Desiste Desiste [0.600214 0.399786]\n",
      "Desiste Desiste [0.67127444 0.32872556]\n",
      "Desiste Desiste [0.61412475 0.38587525]\n",
      "Desiste Desiste [0.70461187 0.29538813]\n",
      "Desiste Desiste [0.60877127 0.39122873]\n",
      "Desiste Desiste [0.75162705 0.24837295]\n",
      "Desiste Desiste [0.71544854 0.28455146]\n",
      "Desiste Desiste [0.62120271 0.37879729]\n",
      "Desiste Escritura [0.23206138 0.76793862]\n",
      "Desiste Desiste [0.83021778 0.16978222]\n",
      "Desiste Desiste [0.64375743 0.35624257]\n",
      "Desiste Escritura [0.4990398 0.5009602]\n",
      "Desiste Desiste [0.69179335 0.30820665]\n",
      "Desiste Desiste [0.74787353 0.25212647]\n",
      "Desiste Desiste [0.6868394 0.3131606]\n",
      "Desiste Escritura [0.28692408 0.71307592]\n",
      "Desiste Desiste [0.78771871 0.21228129]\n",
      "Desiste Escritura [0.30455305 0.69544695]\n",
      "Desiste Escritura [0.32464932 0.67535068]\n",
      "Desiste Escritura [0.44619163 0.55380837]\n",
      "Desiste Desiste [0.6256069 0.3743931]\n",
      "Desiste Desiste [0.54681838 0.45318162]\n",
      "Desiste Desiste [0.52511543 0.47488457]\n",
      "Desiste Desiste [0.59593367 0.40406633]\n",
      "Desiste Desiste [0.58206927 0.41793073]\n",
      "Desiste Desiste [0.67814003 0.32185997]\n",
      "Desiste Escritura [0.44629239 0.55370761]\n",
      "Desiste Desiste [0.60676729 0.39323271]\n",
      "Desiste Desiste [0.53624231 0.46375769]\n",
      "Desiste Desiste [0.61908632 0.38091368]\n",
      "Desiste Desiste [0.63013597 0.36986403]\n",
      "Desiste Desiste [0.75127942 0.24872058]\n",
      "Desiste Escritura [0.34796288 0.65203712]\n",
      "Desiste Escritura [0.37818372 0.62181628]\n",
      "Desiste Desiste [0.58029138 0.41970862]\n",
      "Desiste Desiste [0.66678967 0.33321033]\n",
      "Desiste Desiste [0.66167433 0.33832567]\n",
      "Desiste Desiste [0.73585327 0.26414673]\n",
      "Desiste Escritura [0.17484607 0.82515393]\n",
      "Desiste Desiste [0.68295785 0.31704215]\n",
      "Desiste Desiste [0.77259404 0.22740596]\n",
      "Desiste Desiste [0.59539652 0.40460348]\n",
      "Desiste Escritura [0.34237489 0.65762511]\n",
      "Desiste Escritura [0.31387726 0.68612274]\n",
      "Desiste Desiste [0.72145417 0.27854583]\n",
      "Desiste Desiste [0.77113989 0.22886011]\n",
      "Desiste Desiste [0.59297677 0.40702323]\n",
      "Desiste Desiste [0.62140772 0.37859228]\n",
      "Desiste Desiste [0.64131662 0.35868338]\n",
      "Desiste Escritura [0.34141702 0.65858298]\n",
      "Desiste Escritura [0.43655954 0.56344046]\n",
      "Desiste Escritura [0.3515859 0.6484141]\n",
      "Desiste Escritura [0.27449571 0.72550429]\n",
      "Desiste Desiste [0.56380584 0.43619416]\n",
      "Desiste Escritura [0.19913409 0.80086591]\n",
      "Desiste Desiste [0.63890446 0.36109554]\n",
      "Desiste Escritura [0.35699691 0.64300309]\n",
      "Desiste Desiste [0.60051865 0.39948135]\n",
      "Desiste Desiste [0.68492771 0.31507229]\n",
      "Desiste Escritura [0.34102623 0.65897377]\n",
      "Desiste Desiste [0.7350439 0.2649561]\n",
      "Desiste Desiste [0.739004 0.260996]\n",
      "Desiste Desiste [0.54430398 0.45569602]\n",
      "Desiste Desiste [0.75249408 0.24750592]\n",
      "Desiste Escritura [0.38511764 0.61488236]\n",
      "Desiste Desiste [0.70645838 0.29354162]\n",
      "Desiste Desiste [0.76053522 0.23946478]\n",
      "Desiste Escritura [0.1252813 0.8747187]\n",
      "Desiste Desiste [0.71001802 0.28998198]\n",
      "Desiste Desiste [0.69396327 0.30603673]\n",
      "Desiste Desiste [0.70695969 0.29304031]\n",
      "Desiste Desiste [0.68517664 0.31482336]\n",
      "Desiste Desiste [0.75001148 0.24998852]\n",
      "Desiste Desiste [0.72523811 0.27476189]\n",
      "Desiste Desiste [0.54535164 0.45464836]\n",
      "Desiste Desiste [0.6404307 0.3595693]\n",
      "Desiste Escritura [0.26995412 0.73004588]\n",
      "Desiste Escritura [0.31215254 0.68784746]\n",
      "Desiste Desiste [0.71173064 0.28826936]\n",
      "Desiste Desiste [0.75587308 0.24412692]\n",
      "Desiste Escritura [0.26174877 0.73825123]\n",
      "Desiste Escritura [0.35241528 0.64758472]\n",
      "Desiste Desiste [0.65083596 0.34916404]\n",
      "Desiste Desiste [0.62873129 0.37126871]\n",
      "Desiste Escritura [0.25796383 0.74203617]\n",
      "Desiste Desiste [0.72572802 0.27427198]\n",
      "Desiste Desiste [0.68605597 0.31394403]\n",
      "Desiste Desiste [0.67013546 0.32986454]\n",
      "Desiste Escritura [0.27655535 0.72344465]\n",
      "Desiste Desiste [0.72965503 0.27034497]\n",
      "Desiste Desiste [0.70504802 0.29495198]\n",
      "Desiste Escritura [0.34168145 0.65831855]\n",
      "Desiste Desiste [0.68327192 0.31672808]\n",
      "Desiste Desiste [0.66012281 0.33987719]\n",
      "Desiste Desiste [0.74127191 0.25872809]\n",
      "Desiste Desiste [0.61386295 0.38613705]\n",
      "Desiste Desiste [0.65395184 0.34604816]\n",
      "Desiste Escritura [0.22593699 0.77406301]\n",
      "Desiste Escritura [0.24729037 0.75270963]\n",
      "Desiste Desiste [0.57009508 0.42990492]\n",
      "Desiste Desiste [0.73416446 0.26583554]\n",
      "Desiste Desiste [0.66792446 0.33207554]\n",
      "Desiste Escritura [0.23911076 0.76088924]\n",
      "Desiste Desiste [0.76032871 0.23967129]\n",
      "Desiste Desiste [0.7282435 0.2717565]\n",
      "Desiste Escritura [0.15219388 0.84780612]\n",
      "Desiste Desiste [0.67514761 0.32485239]\n",
      "Desiste Desiste [0.66641665 0.33358335]\n",
      "Desiste Desiste [0.7213836 0.2786164]\n",
      "Desiste Desiste [0.58367266 0.41632734]\n",
      "Desiste Desiste [0.59264816 0.40735184]\n",
      "Desiste Desiste [0.63020405 0.36979595]\n",
      "Desiste Escritura [0.28175603 0.71824397]\n",
      "Desiste Desiste [0.72321838 0.27678162]\n",
      "Desiste Desiste [0.74502488 0.25497512]\n",
      "Desiste Desiste [0.67421799 0.32578201]\n",
      "Desiste Desiste [0.62547799 0.37452201]\n",
      "Desiste Escritura [0.25172643 0.74827357]\n",
      "Desiste Escritura [0.28943223 0.71056777]\n",
      "Desiste Desiste [0.68567566 0.31432434]\n",
      "Desiste Desiste [0.60198794 0.39801206]\n",
      "Desiste Escritura [0.24977196 0.75022804]\n",
      "Desiste Escritura [0.41261495 0.58738505]\n",
      "Desiste Desiste [0.63671493 0.36328507]\n",
      "Desiste Escritura [0.3291053 0.6708947]\n",
      "Desiste Desiste [0.52182069 0.47817931]\n",
      "Desiste Desiste [0.59413113 0.40586887]\n",
      "Desiste Desiste [0.80664458 0.19335542]\n",
      "Desiste Desiste [0.69148004 0.30851996]\n",
      "Desiste Escritura [0.38329673 0.61670327]\n",
      "Desiste Desiste [0.72299076 0.27700924]\n",
      "Desiste Desiste [0.58551259 0.41448741]\n",
      "Desiste Desiste [0.55497166 0.44502834]\n",
      "Desiste Escritura [0.25848117 0.74151883]\n",
      "Desiste Escritura [0.32927238 0.67072762]\n",
      "Desiste Desiste [0.65411293 0.34588707]\n",
      "Desiste Desiste [0.56751607 0.43248393]\n",
      "Desiste Escritura [0.11087589 0.88912411]\n",
      "Desiste Desiste [0.7656067 0.2343933]\n",
      "Desiste Desiste [0.62406341 0.37593659]\n",
      "Desiste Desiste [0.50085499 0.49914501]\n",
      "Desiste Desiste [0.75465375 0.24534625]\n",
      "Desiste Desiste [0.67571741 0.32428259]\n",
      "Desiste Desiste [0.7282152 0.2717848]\n",
      "Desiste Escritura [0.32112928 0.67887072]\n",
      "Desiste Desiste [0.59878779 0.40121221]\n",
      "Desiste Desiste [0.68827399 0.31172601]\n",
      "Desiste Escritura [0.33019295 0.66980705]\n",
      "Desiste Desiste [0.75334488 0.24665512]\n",
      "Desiste Desiste [0.59880125 0.40119875]\n",
      "Desiste Desiste [0.65317613 0.34682387]\n",
      "Desiste Desiste [0.740849 0.259151]\n",
      "Desiste Desiste [0.69176425 0.30823575]\n",
      "Desiste Desiste [0.67542916 0.32457084]\n",
      "Desiste Desiste [0.60900457 0.39099543]\n",
      "Desiste Desiste [0.58340891 0.41659109]\n",
      "Desiste Desiste [0.63885165 0.36114835]\n",
      "Desiste Desiste [0.67274166 0.32725834]\n",
      "Desiste Desiste [0.6696726 0.3303274]\n",
      "Desiste Escritura [0.39992699 0.60007301]\n",
      "Desiste Desiste [0.71609741 0.28390259]\n",
      "Desiste Desiste [0.6644055 0.3355945]\n",
      "Desiste Desiste [0.66099703 0.33900297]\n",
      "Desiste Desiste [0.62512218 0.37487782]\n",
      "Desiste Desiste [0.5221943 0.4778057]\n",
      "Desiste Escritura [0.23368573 0.76631427]\n",
      "Desiste Desiste [0.69809413 0.30190587]\n",
      "Desiste Desiste [0.685346 0.314654]\n",
      "Desiste Desiste [0.6708631 0.3291369]\n",
      "Desiste Desiste [0.6467431 0.3532569]\n",
      "Desiste Desiste [0.65894915 0.34105085]\n",
      "Desiste Escritura [0.2880554 0.7119446]\n",
      "Desiste Desiste [0.50655504 0.49344496]\n",
      "Desiste Desiste [0.64142312 0.35857688]\n",
      "Desiste Desiste [0.67660868 0.32339132]\n",
      "Desiste Desiste [0.67964961 0.32035039]\n",
      "Desiste Escritura [0.09924977 0.90075023]\n",
      "Desiste Escritura [0.40480611 0.59519389]\n",
      "Desiste Desiste [0.64196884 0.35803116]\n",
      "Desiste Escritura [0.48959758 0.51040242]\n",
      "Desiste Escritura [0.17471619 0.82528381]\n",
      "Desiste Desiste [0.70346941 0.29653059]\n",
      "Desiste Desiste [0.74000359 0.25999641]\n",
      "Desiste Escritura [0.4482893 0.5517107]\n",
      "Desiste Escritura [0.42269241 0.57730759]\n",
      "Desiste Desiste [0.70837825 0.29162175]\n",
      "Desiste Desiste [0.80112475 0.19887525]\n",
      "Desiste Desiste [0.52271964 0.47728036]\n",
      "Desiste Escritura [0.3779456 0.6220544]\n",
      "Desiste Desiste [0.7461849 0.2538151]\n",
      "Desiste Desiste [0.57889852 0.42110148]\n",
      "Desiste Escritura [0.02696072 0.97303928]\n",
      "Desiste Escritura [0.28764067 0.71235933]\n",
      "Desiste Escritura [0.20658137 0.79341863]\n",
      "Desiste Escritura [0.34543019 0.65456981]\n",
      "Desiste Escritura [0.29420987 0.70579013]\n",
      "Desiste Desiste [0.55180506 0.44819494]\n",
      "Desiste Desiste [0.66127635 0.33872365]\n",
      "Desiste Desiste [0.66014605 0.33985395]\n",
      "Desiste Desiste [0.61028431 0.38971569]\n",
      "Desiste Desiste [0.71020322 0.28979678]\n",
      "Desiste Desiste [0.71000388 0.28999612]\n",
      "Desiste Escritura [0.14348128 0.85651872]\n",
      "Desiste Escritura [0.31262697 0.68737303]\n",
      "Desiste Desiste [0.68347732 0.31652268]\n",
      "Desiste Desiste [0.54214095 0.45785905]\n",
      "Desiste Desiste [0.6628089 0.3371911]\n",
      "Desiste Desiste [0.51416253 0.48583747]\n",
      "Desiste Desiste [0.56759687 0.43240313]\n",
      "Desiste Desiste [0.5129507 0.4870493]\n",
      "Desiste Desiste [0.65049311 0.34950689]\n",
      "Desiste Escritura [0.33036003 0.66963997]\n",
      "Desiste Desiste [0.53437966 0.46562034]\n",
      "Desiste Desiste [0.7181803 0.2818197]\n",
      "Desiste Desiste [0.69319992 0.30680008]\n",
      "Desiste Desiste [0.80844312 0.19155688]\n",
      "Desiste Escritura [0.25821606 0.74178394]\n",
      "Desiste Escritura [0.34683555 0.65316445]\n",
      "Desiste Desiste [0.7348543 0.2651457]\n",
      "Desiste Escritura [0.19498055 0.80501945]\n",
      "Desiste Desiste [0.68566862 0.31433138]\n",
      "Desiste Escritura [0.2222897 0.7777103]\n",
      "Desiste Escritura [0.20934748 0.79065252]\n",
      "Desiste Desiste [0.59647738 0.40352262]\n",
      "Desiste Desiste [0.54087946 0.45912054]\n",
      "Desiste Desiste [0.66171746 0.33828254]\n",
      "Desiste Desiste [0.62021562 0.37978438]\n",
      "Desiste Escritura [0.31271872 0.68728128]\n",
      "Desiste Escritura [0.33601667 0.66398333]\n",
      "Desiste Escritura [0.26257549 0.73742451]\n",
      "Desiste Desiste [0.70871666 0.29128334]\n",
      "Desiste Desiste [0.74747059 0.25252941]\n",
      "Desiste Desiste [0.65815004 0.34184996]\n",
      "Desiste Escritura [0.30521848 0.69478152]\n",
      "Desiste Desiste [0.62341272 0.37658728]\n",
      "Desiste Desiste [0.67059525 0.32940475]\n",
      "Desiste Desiste [0.63565471 0.36434529]\n",
      "Desiste Desiste [0.68794359 0.31205641]\n",
      "Desiste Escritura [0.20948569 0.79051431]\n",
      "Desiste Escritura [0.31171445 0.68828555]\n",
      "Desiste Desiste [0.57585984 0.42414016]\n",
      "Desiste Desiste [0.54823376 0.45176624]\n",
      "Desiste Escritura [0.4774174 0.5225826]\n",
      "Desiste Desiste [0.70178833 0.29821167]\n",
      "Desiste Desiste [0.74267844 0.25732156]\n",
      "Desiste Desiste [0.63179973 0.36820027]\n",
      "Desiste Escritura [0.31668795 0.68331205]\n",
      "Desiste Desiste [0.63396682 0.36603318]\n",
      "Desiste Escritura [0.32128103 0.67871897]\n",
      "Desiste Desiste [0.60639886 0.39360114]\n",
      "Desiste Desiste [0.61489526 0.38510474]\n",
      "Desiste Desiste [0.69555089 0.30444911]\n",
      "Desiste Desiste [0.63944483 0.36055517]\n",
      "Desiste Escritura [0.32363352 0.67636648]\n",
      "Desiste Escritura [0.32224043 0.67775957]\n",
      "Desiste Desiste [0.64140715 0.35859285]\n",
      "Desiste Escritura [0.22149334 0.77850666]\n",
      "Desiste Desiste [0.69149373 0.30850627]\n",
      "Desiste Desiste [0.64863336 0.35136664]\n",
      "Desiste Desiste [0.74417869 0.25582131]\n",
      "Desiste Escritura [0.3394244 0.6605756]\n",
      "Desiste Desiste [0.68876143 0.31123857]\n",
      "Desiste Desiste [0.74568304 0.25431696]\n"
     ]
    }
   ],
   "source": [
    "print('Real', 'Predicho', 'Probabilidad')\n",
    "prob = clf.predict_proba(X_test)\n",
    "for i in range(len(y_test)):\n",
    "    print(y_test.tolist()[i], y_pred[i], prob[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remuestreo Clase Minoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>...</th>\n",
       "      <th>Comuna Estandarizada_Vitacura</th>\n",
       "      <th>Comuna Estandarizada_X Región de Los Lagos</th>\n",
       "      <th>Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo</th>\n",
       "      <th>Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena</th>\n",
       "      <th>Comuna Estandarizada_XIV Región de Los Ríos</th>\n",
       "      <th>Comuna Estandarizada_XV Región de Arica y Parinacota</th>\n",
       "      <th>Comuna Estandarizada_XVI Región de Ñuble</th>\n",
       "      <th>Comuna Estandarizada_Ñuñoa</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18965</th>\n",
       "      <td>-0.237820</td>\n",
       "      <td>-0.014490</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.389774</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>0.112674</td>\n",
       "      <td>-0.377468</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.316448</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16701</th>\n",
       "      <td>0.126693</td>\n",
       "      <td>0.126346</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.474094</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19519</th>\n",
       "      <td>-0.167721</td>\n",
       "      <td>0.158317</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>1.035375</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021</th>\n",
       "      <td>-0.097623</td>\n",
       "      <td>-0.377468</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>0.091561</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17497</th>\n",
       "      <td>0.112674</td>\n",
       "      <td>1.321269</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>5.936189</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15198</th>\n",
       "      <td>-0.237820</td>\n",
       "      <td>-0.261315</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.299787</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15269</th>\n",
       "      <td>-0.213286</td>\n",
       "      <td>-0.261315</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>2.261301</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.424761</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15678</th>\n",
       "      <td>-0.167721</td>\n",
       "      <td>-0.282324</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.254120</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14662</th>\n",
       "      <td>-0.097623</td>\n",
       "      <td>0.048843</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>0.072543</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19688 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "18965      -0.237820  -0.014490                   -0.022313   \n",
       "1262        0.112674  -0.377468                   -0.022313   \n",
       "16701       0.126693   0.126346                   -0.022313   \n",
       "19519      -0.167721   0.158317                   -0.022313   \n",
       "8021       -0.097623  -0.377468                   -0.022313   \n",
       "...              ...        ...                         ...   \n",
       "17497       0.112674   1.321269                   -0.022313   \n",
       "15198      -0.237820  -0.261315                   -0.022313   \n",
       "15269      -0.213286  -0.261315                   -0.022313   \n",
       "15678      -0.167721  -0.282324                   -0.022313   \n",
       "14662      -0.097623   0.048843                   -0.022313   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "18965               -0.046823              -0.068004   \n",
       "1262                -0.046823              -0.068004   \n",
       "16701               -0.046823              -0.068004   \n",
       "19519               -0.046823              -0.068004   \n",
       "8021                -0.046823              -0.068004   \n",
       "...                       ...                    ...   \n",
       "17497               -0.046823              -0.068004   \n",
       "15198               -0.046823              -0.068004   \n",
       "15269               -0.046823              -0.068004   \n",
       "15678               -0.046823              -0.068004   \n",
       "14662               -0.046823              -0.068004   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "18965                     -0.091839       -0.02385                 -0.389774   \n",
       "1262                      -0.091839       -0.02385                 -0.316448   \n",
       "16701                     -0.091839       -0.02385                 -0.474094   \n",
       "19519                     -0.091839       -0.02385                  1.035375   \n",
       "8021                      -0.091839       -0.02385                  0.091561   \n",
       "...                             ...            ...                       ...   \n",
       "17497                     -0.091839       -0.02385                  5.936189   \n",
       "15198                     -0.091839       -0.02385                 -0.299787   \n",
       "15269                      2.261301       -0.02385                 -0.424761   \n",
       "15678                     -0.091839       -0.02385                 -0.254120   \n",
       "14662                     -0.091839       -0.02385                  0.072543   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  ...  Comuna Estandarizada_Vitacura  \\\n",
       "18965  -0.201095         -0.082281  ...                              0   \n",
       "1262   -0.201095         -0.082281  ...                              0   \n",
       "16701  -0.201095         -0.082281  ...                              0   \n",
       "19519  -0.201095         -0.082281  ...                              0   \n",
       "8021   -0.201095         -0.082281  ...                              0   \n",
       "...          ...               ...  ...                            ...   \n",
       "17497  -0.201095         -0.082281  ...                              1   \n",
       "15198  -0.201095         -0.082281  ...                              0   \n",
       "15269  -0.201095         -0.082281  ...                              0   \n",
       "15678  -0.201095         -0.082281  ...                              0   \n",
       "14662  -0.201095         -0.082281  ...                              0   \n",
       "\n",
       "       Comuna Estandarizada_X Región de Los Lagos  \\\n",
       "18965                                           0   \n",
       "1262                                            0   \n",
       "16701                                           0   \n",
       "19519                                           0   \n",
       "8021                                            0   \n",
       "...                                           ...   \n",
       "17497                                           0   \n",
       "15198                                           0   \n",
       "15269                                           0   \n",
       "15678                                           0   \n",
       "14662                                           0   \n",
       "\n",
       "       Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo  \\\n",
       "18965                                                  0                          \n",
       "1262                                                   0                          \n",
       "16701                                                  0                          \n",
       "19519                                                  0                          \n",
       "8021                                                   0                          \n",
       "...                                                  ...                          \n",
       "17497                                                  0                          \n",
       "15198                                                  0                          \n",
       "15269                                                  0                          \n",
       "15678                                                  0                          \n",
       "14662                                                  0                          \n",
       "\n",
       "       Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena  \\\n",
       "18965                                                  0                         \n",
       "1262                                                   0                         \n",
       "16701                                                  0                         \n",
       "19519                                                  0                         \n",
       "8021                                                   0                         \n",
       "...                                                  ...                         \n",
       "17497                                                  0                         \n",
       "15198                                                  0                         \n",
       "15269                                                  0                         \n",
       "15678                                                  0                         \n",
       "14662                                                  0                         \n",
       "\n",
       "       Comuna Estandarizada_XIV Región de Los Ríos  \\\n",
       "18965                                            0   \n",
       "1262                                             0   \n",
       "16701                                            0   \n",
       "19519                                            0   \n",
       "8021                                             0   \n",
       "...                                            ...   \n",
       "17497                                            0   \n",
       "15198                                            0   \n",
       "15269                                            0   \n",
       "15678                                            0   \n",
       "14662                                            0   \n",
       "\n",
       "       Comuna Estandarizada_XV Región de Arica y Parinacota  \\\n",
       "18965                                                  0      \n",
       "1262                                                   0      \n",
       "16701                                                  0      \n",
       "19519                                                  0      \n",
       "8021                                                   0      \n",
       "...                                                  ...      \n",
       "17497                                                  0      \n",
       "15198                                                  0      \n",
       "15269                                                  0      \n",
       "15678                                                  0      \n",
       "14662                                                  0      \n",
       "\n",
       "       Comuna Estandarizada_XVI Región de Ñuble  Comuna Estandarizada_Ñuñoa  \\\n",
       "18965                                         0                           0   \n",
       "1262                                          0                           0   \n",
       "16701                                         0                           0   \n",
       "19519                                         0                           0   \n",
       "8021                                          0                           0   \n",
       "...                                         ...                         ...   \n",
       "17497                                         0                           0   \n",
       "15198                                         0                           0   \n",
       "15269                                         0                           0   \n",
       "15678                                         0                           0   \n",
       "14662                                         0                           0   \n",
       "\n",
       "       cluster   Etiqueta  \n",
       "18965        3  Escritura  \n",
       "1262         4  Escritura  \n",
       "16701        3  Escritura  \n",
       "19519        1  Escritura  \n",
       "8021         4  Escritura  \n",
       "...        ...        ...  \n",
       "17497        2    Desiste  \n",
       "15198        3    Desiste  \n",
       "15269        3    Desiste  \n",
       "15678        3    Desiste  \n",
       "14662        3    Desiste  \n",
       "\n",
       "[19688 rows x 164 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Desiste      17757\n",
       "Escritura    17757\n",
       "Name: Etiqueta, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# concatenar el conjunto de entrenamiento\n",
    "X = pd.concat([X_train_1.drop('Etiqueta', axis=1), y_train], axis=1)\n",
    "\n",
    "# separar las clases\n",
    "escritura = X[X['Etiqueta'] == 'Escritura']\n",
    "desiste = X[X['Etiqueta'] == 'Desiste']\n",
    "\n",
    "# remuestrear  clase minoritaria\n",
    "desiste_upsampled = resample(desiste,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(escritura), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# recombinar resultados\n",
    "upsampled = pd.concat([escritura, desiste_upsampled])\n",
    "\n",
    "# chequear el número de elementos por clases\n",
    "upsampled['Etiqueta'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos de entrenamiento sobre-balanceados\n",
    "y_train_min = upsampled['Etiqueta']\n",
    "X_train_min = upsampled.drop('Etiqueta', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_min, X_te_min, y_tr_min, y_te_min = train_test_split(X_train_min, y_train_min, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[2684  878]\n",
      " [1398 2143]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.65752   0.75351   0.70225      3562\n",
      "   Escritura    0.70937   0.60520   0.65315      3541\n",
      "\n",
      "    accuracy                        0.67957      7103\n",
      "   macro avg    0.68344   0.67935   0.67770      7103\n",
      "weighted avg    0.68337   0.67957   0.67777      7103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upsampled = LogisticRegression(C=1.0,penalty='l2',random_state=1,solver=\"newton-cg\") # algoritmo de regresion logistica\n",
    "\n",
    "upsampled.fit(X_tr_min, y_tr_min)\n",
    "\n",
    "# metrics\n",
    "\n",
    "y_true =  list(y_te_min)\n",
    "y_pred = list(upsampled.predict(X_te_min))\n",
    "\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_te_min, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remuestreo Clase Mayoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Desiste      1931\n",
       "Escritura    1931\n",
       "Name: Etiqueta, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remuestreo clase mayoritaria\n",
    "escritura_downsampled = resample(escritura,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(desiste), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# recombinar resultados\n",
    "downsampled = pd.concat([escritura_downsampled, desiste])\n",
    "\n",
    "# chequear el número de elementos por clases\n",
    "downsampled['Etiqueta'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos de entrenamiento sub-balanceados\n",
    "\n",
    "y_train_may = downsampled['Etiqueta']\n",
    "X_train_may = downsampled.drop('Etiqueta', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_may, X_te_may, y_tr_may, y_te_may = train_test_split(X_train_may, y_train_may, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[268 105]\n",
      " [152 248]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.63810   0.71850   0.67591       373\n",
      "   Escritura    0.70255   0.62000   0.65870       400\n",
      "\n",
      "    accuracy                        0.66753       773\n",
      "   macro avg    0.67032   0.66925   0.66731       773\n",
      "weighted avg    0.67145   0.66753   0.66701       773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "undersampled = LogisticRegression(C=1.0,penalty='l2',random_state=1,solver=\"newton-cg\") # modelo de regresi+on logística\n",
    "\n",
    "undersampled.fit(X_tr_may, y_tr_may)\n",
    "# metrics\n",
    "\n",
    "y_true =  list(y_te_may)\n",
    "y_pred = list(undersampled.predict(X_te_may))\n",
    "\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_te_may, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w = best.coef_\n",
    "b = best.intercept_\n",
    "\n",
    "class1 = 'Escritura' #-1\n",
    "class2 = 'Desiste' #+1 -> linea 579 sklearn/linear_model/_stochastic_gradient.py fit_binary(self, 1, X, y, alpha, C,\n",
    "\n",
    "max_feat = min(len(w[0,:]),41)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5),nrows=1)\n",
    "p = sns.barplot(data= df_prepro, x=np.arange(max_feat), y=w[0,:max_feat], palette=\"rocket\",ax=ax)\n",
    "ax.set_xticklabels(X.columns[:max_feat])\n",
    "plt.setp(p.get_xticklabels(), rotation=90)\n",
    "ax.set_title('Coeficientes (Pesos Atributos) Clase -1=%s, Clase +1=%s'%(class1,class2))\n",
    "plt.show()\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>...</th>\n",
       "      <th>Comuna Estandarizada_VIII Región del Biobío</th>\n",
       "      <th>Comuna Estandarizada_Vitacura</th>\n",
       "      <th>Comuna Estandarizada_X Región de Los Lagos</th>\n",
       "      <th>Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo</th>\n",
       "      <th>Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena</th>\n",
       "      <th>Comuna Estandarizada_XIV Región de Los Ríos</th>\n",
       "      <th>Comuna Estandarizada_XV Región de Arica y Parinacota</th>\n",
       "      <th>Comuna Estandarizada_XVI Región de Ñuble</th>\n",
       "      <th>Comuna Estandarizada_Ñuñoa</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18965</th>\n",
       "      <td>-0.237820</td>\n",
       "      <td>-0.014490</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.389774</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>0.112674</td>\n",
       "      <td>-0.377468</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.316448</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16701</th>\n",
       "      <td>0.126693</td>\n",
       "      <td>0.126346</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.474094</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19519</th>\n",
       "      <td>-0.167721</td>\n",
       "      <td>0.158317</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>1.035375</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021</th>\n",
       "      <td>-0.097623</td>\n",
       "      <td>-0.377468</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>0.091561</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17497</th>\n",
       "      <td>0.112674</td>\n",
       "      <td>1.321269</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>5.936189</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15198</th>\n",
       "      <td>-0.237820</td>\n",
       "      <td>-0.261315</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.299787</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15269</th>\n",
       "      <td>-0.213286</td>\n",
       "      <td>-0.261315</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>2.261301</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.424761</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15678</th>\n",
       "      <td>-0.167721</td>\n",
       "      <td>-0.282324</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>-0.254120</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14662</th>\n",
       "      <td>-0.097623</td>\n",
       "      <td>0.048843</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>-0.091839</td>\n",
       "      <td>-0.02385</td>\n",
       "      <td>0.072543</td>\n",
       "      <td>-0.201095</td>\n",
       "      <td>-0.082281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19688 rows × 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "18965      -0.237820  -0.014490                   -0.022313   \n",
       "1262        0.112674  -0.377468                   -0.022313   \n",
       "16701       0.126693   0.126346                   -0.022313   \n",
       "19519      -0.167721   0.158317                   -0.022313   \n",
       "8021       -0.097623  -0.377468                   -0.022313   \n",
       "...              ...        ...                         ...   \n",
       "17497       0.112674   1.321269                   -0.022313   \n",
       "15198      -0.237820  -0.261315                   -0.022313   \n",
       "15269      -0.213286  -0.261315                   -0.022313   \n",
       "15678      -0.167721  -0.282324                   -0.022313   \n",
       "14662      -0.097623   0.048843                   -0.022313   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "18965               -0.046823              -0.068004   \n",
       "1262                -0.046823              -0.068004   \n",
       "16701               -0.046823              -0.068004   \n",
       "19519               -0.046823              -0.068004   \n",
       "8021                -0.046823              -0.068004   \n",
       "...                       ...                    ...   \n",
       "17497               -0.046823              -0.068004   \n",
       "15198               -0.046823              -0.068004   \n",
       "15269               -0.046823              -0.068004   \n",
       "15678               -0.046823              -0.068004   \n",
       "14662               -0.046823              -0.068004   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "18965                     -0.091839       -0.02385                 -0.389774   \n",
       "1262                      -0.091839       -0.02385                 -0.316448   \n",
       "16701                     -0.091839       -0.02385                 -0.474094   \n",
       "19519                     -0.091839       -0.02385                  1.035375   \n",
       "8021                      -0.091839       -0.02385                  0.091561   \n",
       "...                             ...            ...                       ...   \n",
       "17497                     -0.091839       -0.02385                  5.936189   \n",
       "15198                     -0.091839       -0.02385                 -0.299787   \n",
       "15269                      2.261301       -0.02385                 -0.424761   \n",
       "15678                     -0.091839       -0.02385                 -0.254120   \n",
       "14662                     -0.091839       -0.02385                  0.072543   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  ...  \\\n",
       "18965  -0.201095         -0.082281  ...   \n",
       "1262   -0.201095         -0.082281  ...   \n",
       "16701  -0.201095         -0.082281  ...   \n",
       "19519  -0.201095         -0.082281  ...   \n",
       "8021   -0.201095         -0.082281  ...   \n",
       "...          ...               ...  ...   \n",
       "17497  -0.201095         -0.082281  ...   \n",
       "15198  -0.201095         -0.082281  ...   \n",
       "15269  -0.201095         -0.082281  ...   \n",
       "15678  -0.201095         -0.082281  ...   \n",
       "14662  -0.201095         -0.082281  ...   \n",
       "\n",
       "       Comuna Estandarizada_VIII Región del Biobío  \\\n",
       "18965                                            0   \n",
       "1262                                             0   \n",
       "16701                                            0   \n",
       "19519                                            0   \n",
       "8021                                             0   \n",
       "...                                            ...   \n",
       "17497                                            0   \n",
       "15198                                            0   \n",
       "15269                                            1   \n",
       "15678                                            0   \n",
       "14662                                            0   \n",
       "\n",
       "       Comuna Estandarizada_Vitacura  \\\n",
       "18965                              0   \n",
       "1262                               0   \n",
       "16701                              0   \n",
       "19519                              0   \n",
       "8021                               0   \n",
       "...                              ...   \n",
       "17497                              1   \n",
       "15198                              0   \n",
       "15269                              0   \n",
       "15678                              0   \n",
       "14662                              0   \n",
       "\n",
       "       Comuna Estandarizada_X Región de Los Lagos  \\\n",
       "18965                                           0   \n",
       "1262                                            0   \n",
       "16701                                           0   \n",
       "19519                                           0   \n",
       "8021                                            0   \n",
       "...                                           ...   \n",
       "17497                                           0   \n",
       "15198                                           0   \n",
       "15269                                           0   \n",
       "15678                                           0   \n",
       "14662                                           0   \n",
       "\n",
       "       Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo  \\\n",
       "18965                                                  0                          \n",
       "1262                                                   0                          \n",
       "16701                                                  0                          \n",
       "19519                                                  0                          \n",
       "8021                                                   0                          \n",
       "...                                                  ...                          \n",
       "17497                                                  0                          \n",
       "15198                                                  0                          \n",
       "15269                                                  0                          \n",
       "15678                                                  0                          \n",
       "14662                                                  0                          \n",
       "\n",
       "       Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena  \\\n",
       "18965                                                  0                         \n",
       "1262                                                   0                         \n",
       "16701                                                  0                         \n",
       "19519                                                  0                         \n",
       "8021                                                   0                         \n",
       "...                                                  ...                         \n",
       "17497                                                  0                         \n",
       "15198                                                  0                         \n",
       "15269                                                  0                         \n",
       "15678                                                  0                         \n",
       "14662                                                  0                         \n",
       "\n",
       "       Comuna Estandarizada_XIV Región de Los Ríos  \\\n",
       "18965                                            0   \n",
       "1262                                             0   \n",
       "16701                                            0   \n",
       "19519                                            0   \n",
       "8021                                             0   \n",
       "...                                            ...   \n",
       "17497                                            0   \n",
       "15198                                            0   \n",
       "15269                                            0   \n",
       "15678                                            0   \n",
       "14662                                            0   \n",
       "\n",
       "       Comuna Estandarizada_XV Región de Arica y Parinacota  \\\n",
       "18965                                                  0      \n",
       "1262                                                   0      \n",
       "16701                                                  0      \n",
       "19519                                                  0      \n",
       "8021                                                   0      \n",
       "...                                                  ...      \n",
       "17497                                                  0      \n",
       "15198                                                  0      \n",
       "15269                                                  0      \n",
       "15678                                                  0      \n",
       "14662                                                  0      \n",
       "\n",
       "       Comuna Estandarizada_XVI Región de Ñuble  Comuna Estandarizada_Ñuñoa  \\\n",
       "18965                                         0                           0   \n",
       "1262                                          0                           0   \n",
       "16701                                         0                           0   \n",
       "19519                                         0                           0   \n",
       "8021                                          0                           0   \n",
       "...                                         ...                         ...   \n",
       "17497                                         0                           0   \n",
       "15198                                         0                           0   \n",
       "15269                                         0                           0   \n",
       "15678                                         0                           0   \n",
       "14662                                         0                           0   \n",
       "\n",
       "       cluster  \n",
       "18965        3  \n",
       "1262         4  \n",
       "16701        3  \n",
       "19519        1  \n",
       "8021         4  \n",
       "...        ...  \n",
       "17497        2  \n",
       "15198        3  \n",
       "15269        3  \n",
       "15678        3  \n",
       "14662        3  \n",
       "\n",
       "[19688 rows x 163 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w = best.coef_\n",
    "b = best.intercept_\n",
    "\n",
    "class1 = 'Escritura' #-1\n",
    "class2 = 'Desiste' #+1 -> linea 579 sklearn/linear_model/_stochastic_gradient.py fit_binary(self, 1, X, y, alpha, C,\n",
    "\n",
    "max_feat = 83\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5),nrows=1)\n",
    "p = sns.barplot(data= X_train, x=np.arange(max_feat-41), y=w[0,41:max_feat], palette=\"rocket\",ax=ax)\n",
    "ax.set_xticklabels(X.columns[41:max_feat])\n",
    "plt.setp(p.get_xticklabels(), rotation=90)\n",
    "ax.set_title('Coeficientes (Pesos Atributos) Clase -1=%s, Clase +1=%s'%(class1,class2))\n",
    "plt.show()\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>...</th>\n",
       "      <th>Comuna Estandarizada_Vitacura</th>\n",
       "      <th>Comuna Estandarizada_X Región de Los Lagos</th>\n",
       "      <th>Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo</th>\n",
       "      <th>Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena</th>\n",
       "      <th>Comuna Estandarizada_XIV Región de Los Ríos</th>\n",
       "      <th>Comuna Estandarizada_XV Región de Arica y Parinacota</th>\n",
       "      <th>Comuna Estandarizada_XVI Región de Ñuble</th>\n",
       "      <th>Comuna Estandarizada_Ñuñoa</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>10.00</td>\n",
       "      <td>399.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4544.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>408.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>30.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2547.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4732.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9890.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1594.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23210</th>\n",
       "      <td>50.00</td>\n",
       "      <td>982.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10317.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23251</th>\n",
       "      <td>10.00</td>\n",
       "      <td>1327.93</td>\n",
       "      <td>3096.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3114.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23472</th>\n",
       "      <td>50.00</td>\n",
       "      <td>764.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3618.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23710</th>\n",
       "      <td>50.00</td>\n",
       "      <td>4650.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12950.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23728</th>\n",
       "      <td>50.00</td>\n",
       "      <td>8813.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11863.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "261            10.00     399.09                        0.00   \n",
       "299            30.01       0.00                        0.00   \n",
       "424            10.00       0.00                        0.00   \n",
       "522            50.00       0.00                        0.00   \n",
       "534             5.00       0.00                        0.00   \n",
       "...              ...        ...                         ...   \n",
       "23210          50.00     982.00                        0.00   \n",
       "23251          10.00    1327.93                     3096.98   \n",
       "23472          50.00     764.00                        0.00   \n",
       "23710          50.00    4650.00                        0.00   \n",
       "23728          50.00    8813.00                        0.00   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "261                       0.0                    0.0   \n",
       "299                       0.0                    0.0   \n",
       "424                       0.0                    0.0   \n",
       "522                       0.0                    0.0   \n",
       "534                       0.0                    0.0   \n",
       "...                       ...                    ...   \n",
       "23210                     0.0                    0.0   \n",
       "23251                     0.0                    0.0   \n",
       "23472                     0.0                    0.0   \n",
       "23710                     0.0                    0.0   \n",
       "23728                     0.0                    0.0   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "261                             0.0            0.0                   4544.01   \n",
       "299                             0.0            0.0                   2547.00   \n",
       "424                             0.0            0.0                   4732.00   \n",
       "522                             0.0            0.0                   9890.00   \n",
       "534                             0.0            0.0                   1594.00   \n",
       "...                             ...            ...                       ...   \n",
       "23210                           0.0            0.0                  10317.00   \n",
       "23251                           0.0            0.0                   3114.00   \n",
       "23472                           0.0            0.0                   3618.00   \n",
       "23710                           0.0            0.0                  12950.00   \n",
       "23728                           0.0            0.0                  11863.00   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  ...  Comuna Estandarizada_Vitacura  \\\n",
       "261          0.0            408.91  ...                              0   \n",
       "299          0.0              0.00  ...                              0   \n",
       "424          0.0              0.00  ...                              0   \n",
       "522          0.0              0.00  ...                              0   \n",
       "534          0.0              0.00  ...                              0   \n",
       "...          ...               ...  ...                            ...   \n",
       "23210        0.0              0.00  ...                              0   \n",
       "23251        0.0              0.00  ...                              0   \n",
       "23472        0.0              0.00  ...                              0   \n",
       "23710        0.0              0.00  ...                              0   \n",
       "23728        0.0              0.00  ...                              0   \n",
       "\n",
       "       Comuna Estandarizada_X Región de Los Lagos  \\\n",
       "261                                             0   \n",
       "299                                             0   \n",
       "424                                             0   \n",
       "522                                             0   \n",
       "534                                             0   \n",
       "...                                           ...   \n",
       "23210                                           0   \n",
       "23251                                           0   \n",
       "23472                                           0   \n",
       "23710                                           0   \n",
       "23728                                           0   \n",
       "\n",
       "       Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo  \\\n",
       "261                                                    0                          \n",
       "299                                                    0                          \n",
       "424                                                    0                          \n",
       "522                                                    0                          \n",
       "534                                                    0                          \n",
       "...                                                  ...                          \n",
       "23210                                                  0                          \n",
       "23251                                                  0                          \n",
       "23472                                                  0                          \n",
       "23710                                                  0                          \n",
       "23728                                                  0                          \n",
       "\n",
       "       Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena  \\\n",
       "261                                                    0                         \n",
       "299                                                    0                         \n",
       "424                                                    0                         \n",
       "522                                                    0                         \n",
       "534                                                    0                         \n",
       "...                                                  ...                         \n",
       "23210                                                  0                         \n",
       "23251                                                  0                         \n",
       "23472                                                  0                         \n",
       "23710                                                  0                         \n",
       "23728                                                  0                         \n",
       "\n",
       "       Comuna Estandarizada_XIV Región de Los Ríos  \\\n",
       "261                                              0   \n",
       "299                                              0   \n",
       "424                                              0   \n",
       "522                                              0   \n",
       "534                                              0   \n",
       "...                                            ...   \n",
       "23210                                            0   \n",
       "23251                                            0   \n",
       "23472                                            0   \n",
       "23710                                            0   \n",
       "23728                                            0   \n",
       "\n",
       "       Comuna Estandarizada_XV Región de Arica y Parinacota  \\\n",
       "261                                                    0      \n",
       "299                                                    0      \n",
       "424                                                    0      \n",
       "522                                                    0      \n",
       "534                                                    0      \n",
       "...                                                  ...      \n",
       "23210                                                  0      \n",
       "23251                                                  0      \n",
       "23472                                                  0      \n",
       "23710                                                  0      \n",
       "23728                                                  0      \n",
       "\n",
       "       Comuna Estandarizada_XVI Región de Ñuble  Comuna Estandarizada_Ñuñoa  \\\n",
       "261                                           0                           0   \n",
       "299                                           0                           0   \n",
       "424                                           0                           0   \n",
       "522                                           0                           0   \n",
       "534                                           0                           0   \n",
       "...                                         ...                         ...   \n",
       "23210                                         0                           0   \n",
       "23251                                         0                           0   \n",
       "23472                                         0                           0   \n",
       "23710                                         0                           0   \n",
       "23728                                         0                           0   \n",
       "\n",
       "       cluster  Etiqueta  \n",
       "261          1   Desiste  \n",
       "299          4   Desiste  \n",
       "424          0   Desiste  \n",
       "522          0   Desiste  \n",
       "534          4   Desiste  \n",
       "...        ...       ...  \n",
       "23210        2   Desiste  \n",
       "23251        0   Desiste  \n",
       "23472        3   Desiste  \n",
       "23710        2   Desiste  \n",
       "23728        2   Desiste  \n",
       "\n",
       "[102 rows x 164 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro[(df_prepro['Comuna Estandarizada_Las Condes'] == 1)& (df_prepro['Etiqueta']=='Desiste')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>...</th>\n",
       "      <th>Comuna Estandarizada_Vitacura</th>\n",
       "      <th>Comuna Estandarizada_X Región de Los Lagos</th>\n",
       "      <th>Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo</th>\n",
       "      <th>Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena</th>\n",
       "      <th>Comuna Estandarizada_XIV Región de Los Ríos</th>\n",
       "      <th>Comuna Estandarizada_XV Región de Arica y Parinacota</th>\n",
       "      <th>Comuna Estandarizada_XVI Región de Ñuble</th>\n",
       "      <th>Comuna Estandarizada_Ñuñoa</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4464.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2302.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2587.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4452.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20655</th>\n",
       "      <td>14.0</td>\n",
       "      <td>498.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3578.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21646</th>\n",
       "      <td>5.0</td>\n",
       "      <td>189.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21696</th>\n",
       "      <td>20.0</td>\n",
       "      <td>437.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4759.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22188</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1427.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23214</th>\n",
       "      <td>40.0</td>\n",
       "      <td>390.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1466.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "701             10.0       0.00                         0.0   \n",
       "855              5.0       0.00                         0.0   \n",
       "885              5.0       0.00                         0.0   \n",
       "1668             5.0       0.00                         0.0   \n",
       "1749            14.0       0.00                         0.0   \n",
       "...              ...        ...                         ...   \n",
       "20655           14.0     498.00                         0.0   \n",
       "21646            5.0     189.84                         0.0   \n",
       "21696           20.0     437.00                         0.0   \n",
       "22188            5.0       0.00                         0.0   \n",
       "23214           40.0     390.59                         0.0   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "701                       0.0                    0.0   \n",
       "855                       0.0                    0.0   \n",
       "885                       0.0                    0.0   \n",
       "1668                      0.0                    0.0   \n",
       "1749                      0.0                    0.0   \n",
       "...                       ...                    ...   \n",
       "20655                     0.0                    0.0   \n",
       "21646                     0.0                    0.0   \n",
       "21696                     0.0                    0.0   \n",
       "22188                     0.0                    0.0   \n",
       "23214                     0.0                    0.0   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "701                             0.0            0.0                    4464.0   \n",
       "855                             0.0            0.0                    2302.0   \n",
       "885                             0.0            0.0                    1892.0   \n",
       "1668                            0.0            0.0                    2587.0   \n",
       "1749                            0.0            0.0                    4452.0   \n",
       "...                             ...            ...                       ...   \n",
       "20655                           0.0            0.0                    3578.0   \n",
       "21646                           0.0            0.0                    1905.0   \n",
       "21696                           0.0            0.0                    4759.0   \n",
       "22188                           0.0            0.0                    1427.0   \n",
       "23214                           0.0            0.0                    1466.0   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  ...  Comuna Estandarizada_Vitacura  \\\n",
       "701          0.0               0.0  ...                              0   \n",
       "855        131.0               0.0  ...                              0   \n",
       "885          0.0               0.0  ...                              0   \n",
       "1668       226.0               0.0  ...                              0   \n",
       "1749         0.0               0.0  ...                              0   \n",
       "...          ...               ...  ...                            ...   \n",
       "20655        0.0               0.0  ...                              0   \n",
       "21646        0.0               0.0  ...                              0   \n",
       "21696        0.0               0.0  ...                              0   \n",
       "22188        0.0               0.0  ...                              0   \n",
       "23214        0.0               0.0  ...                              0   \n",
       "\n",
       "       Comuna Estandarizada_X Región de Los Lagos  \\\n",
       "701                                             0   \n",
       "855                                             0   \n",
       "885                                             0   \n",
       "1668                                            0   \n",
       "1749                                            0   \n",
       "...                                           ...   \n",
       "20655                                           0   \n",
       "21646                                           0   \n",
       "21696                                           0   \n",
       "22188                                           0   \n",
       "23214                                           0   \n",
       "\n",
       "       Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo  \\\n",
       "701                                                    0                          \n",
       "855                                                    0                          \n",
       "885                                                    0                          \n",
       "1668                                                   0                          \n",
       "1749                                                   0                          \n",
       "...                                                  ...                          \n",
       "20655                                                  0                          \n",
       "21646                                                  0                          \n",
       "21696                                                  0                          \n",
       "22188                                                  0                          \n",
       "23214                                                  0                          \n",
       "\n",
       "       Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena  \\\n",
       "701                                                    0                         \n",
       "855                                                    0                         \n",
       "885                                                    0                         \n",
       "1668                                                   0                         \n",
       "1749                                                   0                         \n",
       "...                                                  ...                         \n",
       "20655                                                  0                         \n",
       "21646                                                  0                         \n",
       "21696                                                  0                         \n",
       "22188                                                  0                         \n",
       "23214                                                  0                         \n",
       "\n",
       "       Comuna Estandarizada_XIV Región de Los Ríos  \\\n",
       "701                                              0   \n",
       "855                                              0   \n",
       "885                                              0   \n",
       "1668                                             0   \n",
       "1749                                             0   \n",
       "...                                            ...   \n",
       "20655                                            0   \n",
       "21646                                            0   \n",
       "21696                                            0   \n",
       "22188                                            0   \n",
       "23214                                            0   \n",
       "\n",
       "       Comuna Estandarizada_XV Región de Arica y Parinacota  \\\n",
       "701                                                    0      \n",
       "855                                                    0      \n",
       "885                                                    0      \n",
       "1668                                                   0      \n",
       "1749                                                   0      \n",
       "...                                                  ...      \n",
       "20655                                                  0      \n",
       "21646                                                  0      \n",
       "21696                                                  0      \n",
       "22188                                                  0      \n",
       "23214                                                  0      \n",
       "\n",
       "       Comuna Estandarizada_XVI Región de Ñuble  Comuna Estandarizada_Ñuñoa  \\\n",
       "701                                           0                           0   \n",
       "855                                           0                           0   \n",
       "885                                           0                           0   \n",
       "1668                                          0                           0   \n",
       "1749                                          0                           0   \n",
       "...                                         ...                         ...   \n",
       "20655                                         0                           0   \n",
       "21646                                         0                           0   \n",
       "21696                                         0                           0   \n",
       "22188                                         0                           0   \n",
       "23214                                         0                           0   \n",
       "\n",
       "       cluster   Etiqueta  \n",
       "701          0  Escritura  \n",
       "855          4  Escritura  \n",
       "885          4  Escritura  \n",
       "1668         4  Escritura  \n",
       "1749         0  Escritura  \n",
       "...        ...        ...  \n",
       "20655        3  Escritura  \n",
       "21646        3  Escritura  \n",
       "21696        1  Escritura  \n",
       "22188        4  Escritura  \n",
       "23214        4  Escritura  \n",
       "\n",
       "[67 rows x 164 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro[(df_prepro['Comuna Estandarizada_La Granja'] == 1)& (df_prepro['Etiqueta']=='Escritura')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w = best.coef_\n",
    "b = best.intercept_\n",
    "\n",
    "class1 = 'Escritura' #-1\n",
    "class2 = 'Desiste' #+1 -> linea 579 sklearn/linear_model/_stochastic_gradient.py fit_binary(self, 1, X, y, alpha, C,\n",
    "\n",
    "max_feat = 124\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5),nrows=1)\n",
    "p = sns.barplot(data= df_prepro, x=np.arange(max_feat-83), y=w[0,83:max_feat], palette=\"rocket\",ax=ax)\n",
    "ax.set_xticklabels(X.columns[83:max_feat])\n",
    "plt.setp(p.get_xticklabels(), rotation=90)\n",
    "ax.set_title('Coeficientes (Pesos Atributos) Clase -1=%s, Clase +1=%s'%(class1,class2))\n",
    "plt.show()\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w = best.coef_\n",
    "b = best.intercept_\n",
    "\n",
    "class1 = 'Escritura' #-1\n",
    "class2 = 'Desiste' #+1 -> linea 579 sklearn/linear_model/_stochastic_gradient.py fit_binary(self, 1, X, y, alpha, C,\n",
    "\n",
    "max_feat = 163\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5),nrows=1)\n",
    "p = sns.barplot(data= df_prepro, x=np.arange(max_feat-124), y=w[0,124:max_feat], palette=\"rocket\",ax=ax)\n",
    "ax.set_xticklabels(X.columns[124:max_feat])\n",
    "plt.setp(p.get_xticklabels(), rotation=90)\n",
    "ax.set_title('Coeficientes (Pesos Atributos) Clase -1=%s, Clase +1=%s'%(class1,class2))\n",
    "plt.show()\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>...</th>\n",
       "      <th>Comuna Estandarizada_Vitacura</th>\n",
       "      <th>Comuna Estandarizada_X Región de Los Lagos</th>\n",
       "      <th>Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo</th>\n",
       "      <th>Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena</th>\n",
       "      <th>Comuna Estandarizada_XIV Región de Los Ríos</th>\n",
       "      <th>Comuna Estandarizada_XV Región de Arica y Parinacota</th>\n",
       "      <th>Comuna Estandarizada_XVI Región de Ñuble</th>\n",
       "      <th>Comuna Estandarizada_Ñuñoa</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.50</td>\n",
       "      <td>76.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2065.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2092.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2245.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>55.78</td>\n",
       "      <td>45.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>109.00</td>\n",
       "      <td>1332.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7589.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23704</th>\n",
       "      <td>10.00</td>\n",
       "      <td>350.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2210.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23724</th>\n",
       "      <td>15.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2583.00</td>\n",
       "      <td>243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23730</th>\n",
       "      <td>15.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2583.00</td>\n",
       "      <td>243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24359</th>\n",
       "      <td>7.00</td>\n",
       "      <td>235.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2499.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24415</th>\n",
       "      <td>0.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>950.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1741 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "4               3.50      76.50                         0.0   \n",
       "5               0.00      80.00                         0.0   \n",
       "6               0.00      80.00                         0.0   \n",
       "15             55.78      45.37                         0.0   \n",
       "23            109.00    1332.80                         0.0   \n",
       "...              ...        ...                         ...   \n",
       "23704          10.00     350.85                         0.0   \n",
       "23724          15.00       0.00                         0.0   \n",
       "23730          15.00       0.00                         0.0   \n",
       "24359           7.00     235.40                         0.0   \n",
       "24415           0.00      30.00                         0.0   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "4                         0.0                    0.0   \n",
       "5                         0.0                    0.0   \n",
       "6                         0.0                    0.0   \n",
       "15                        0.0                    0.0   \n",
       "23                        0.0                    0.0   \n",
       "...                       ...                    ...   \n",
       "23704                     0.0                    0.0   \n",
       "23724                     0.0                    0.0   \n",
       "23730                     0.0                    0.0   \n",
       "24359                     0.0                    0.0   \n",
       "24415                     0.0                    0.0   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "4                               0.0            0.0                   2065.91   \n",
       "5                               0.0            0.0                   2092.17   \n",
       "6                               0.0            0.0                   2245.69   \n",
       "15                              0.0            0.0                      0.00   \n",
       "23                              0.0            0.0                   7589.00   \n",
       "...                             ...            ...                       ...   \n",
       "23704                           0.0            0.0                   2210.00   \n",
       "23724                           0.0            0.0                   2583.00   \n",
       "23730                           0.0            0.0                   2583.00   \n",
       "24359                           0.0            0.0                   2499.00   \n",
       "24415                           0.0            0.0                    950.00   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  ...  Comuna Estandarizada_Vitacura  \\\n",
       "4            0.0               0.0  ...                              0   \n",
       "5            0.0               0.0  ...                              0   \n",
       "6            0.0               0.0  ...                              0   \n",
       "15           0.0               0.0  ...                              0   \n",
       "23           0.0               0.0  ...                              0   \n",
       "...          ...               ...  ...                            ...   \n",
       "23704        0.0               0.0  ...                              0   \n",
       "23724      243.0               0.0  ...                              0   \n",
       "23730      243.0               0.0  ...                              0   \n",
       "24359        0.0               0.0  ...                              0   \n",
       "24415        0.0               0.0  ...                              0   \n",
       "\n",
       "       Comuna Estandarizada_X Región de Los Lagos  \\\n",
       "4                                               0   \n",
       "5                                               0   \n",
       "6                                               0   \n",
       "15                                              0   \n",
       "23                                              0   \n",
       "...                                           ...   \n",
       "23704                                           0   \n",
       "23724                                           0   \n",
       "23730                                           0   \n",
       "24359                                           0   \n",
       "24415                                           0   \n",
       "\n",
       "       Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo  \\\n",
       "4                                                      0                          \n",
       "5                                                      0                          \n",
       "6                                                      0                          \n",
       "15                                                     0                          \n",
       "23                                                     0                          \n",
       "...                                                  ...                          \n",
       "23704                                                  0                          \n",
       "23724                                                  0                          \n",
       "23730                                                  0                          \n",
       "24359                                                  0                          \n",
       "24415                                                  0                          \n",
       "\n",
       "       Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena  \\\n",
       "4                                                      0                         \n",
       "5                                                      0                         \n",
       "6                                                      0                         \n",
       "15                                                     0                         \n",
       "23                                                     0                         \n",
       "...                                                  ...                         \n",
       "23704                                                  0                         \n",
       "23724                                                  0                         \n",
       "23730                                                  0                         \n",
       "24359                                                  0                         \n",
       "24415                                                  0                         \n",
       "\n",
       "       Comuna Estandarizada_XIV Región de Los Ríos  \\\n",
       "4                                                0   \n",
       "5                                                0   \n",
       "6                                                0   \n",
       "15                                               0   \n",
       "23                                               0   \n",
       "...                                            ...   \n",
       "23704                                            0   \n",
       "23724                                            0   \n",
       "23730                                            0   \n",
       "24359                                            0   \n",
       "24415                                            0   \n",
       "\n",
       "       Comuna Estandarizada_XV Región de Arica y Parinacota  \\\n",
       "4                                                      0      \n",
       "5                                                      0      \n",
       "6                                                      0      \n",
       "15                                                     0      \n",
       "23                                                     0      \n",
       "...                                                  ...      \n",
       "23704                                                  0      \n",
       "23724                                                  0      \n",
       "23730                                                  0      \n",
       "24359                                                  0      \n",
       "24415                                                  0      \n",
       "\n",
       "       Comuna Estandarizada_XVI Región de Ñuble  Comuna Estandarizada_Ñuñoa  \\\n",
       "4                                             0                           0   \n",
       "5                                             0                           0   \n",
       "6                                             0                           0   \n",
       "15                                            0                           0   \n",
       "23                                            0                           0   \n",
       "...                                         ...                         ...   \n",
       "23704                                         0                           0   \n",
       "23724                                         0                           0   \n",
       "23730                                         0                           0   \n",
       "24359                                         0                           0   \n",
       "24415                                         0                           0   \n",
       "\n",
       "       cluster   Etiqueta  \n",
       "4            3  Escritura  \n",
       "5            3  Escritura  \n",
       "6            3  Escritura  \n",
       "15           4  Escritura  \n",
       "23           1  Escritura  \n",
       "...        ...        ...  \n",
       "23704        3  Escritura  \n",
       "23724        3  Escritura  \n",
       "23730        3  Escritura  \n",
       "24359        3  Escritura  \n",
       "24415        4  Escritura  \n",
       "\n",
       "[1741 rows x 164 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro[(df_prepro['Comuna Estandarizada_V Región de Valparaíso']==1)&(df_prepro['Etiqueta']=='Escritura')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>...</th>\n",
       "      <th>Comuna Estandarizada_Vitacura</th>\n",
       "      <th>Comuna Estandarizada_X Región de Los Lagos</th>\n",
       "      <th>Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo</th>\n",
       "      <th>Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena</th>\n",
       "      <th>Comuna Estandarizada_XIV Región de Los Ríos</th>\n",
       "      <th>Comuna Estandarizada_XV Región de Arica y Parinacota</th>\n",
       "      <th>Comuna Estandarizada_XVI Región de Ñuble</th>\n",
       "      <th>Comuna Estandarizada_Ñuñoa</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.49</td>\n",
       "      <td>78.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2177.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2075.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>3.49</td>\n",
       "      <td>246.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2272.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>3.47</td>\n",
       "      <td>120.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2030.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>3.50</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2284.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23231</th>\n",
       "      <td>25.00</td>\n",
       "      <td>1171.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1226.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2578.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23336</th>\n",
       "      <td>25.00</td>\n",
       "      <td>430.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2114.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23352</th>\n",
       "      <td>14.00</td>\n",
       "      <td>348.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3810.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23383</th>\n",
       "      <td>40.00</td>\n",
       "      <td>91.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1190.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23469</th>\n",
       "      <td>25.00</td>\n",
       "      <td>1376.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1512.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Desiste</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "99              3.49      78.31                         0.0   \n",
       "115             0.00    2177.01                         0.0   \n",
       "208             3.49     246.99                         0.0   \n",
       "245             3.47     120.64                         0.0   \n",
       "246             3.50      80.00                         0.0   \n",
       "...              ...        ...                         ...   \n",
       "23231          25.00    1171.95                         0.0   \n",
       "23336          25.00     430.00                         0.0   \n",
       "23352          14.00     348.00                         0.0   \n",
       "23383          40.00      91.00                         0.0   \n",
       "23469          25.00    1376.00                         0.0   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "99                        0.0                   0.00   \n",
       "115                       0.0                   0.00   \n",
       "208                       0.0                   0.00   \n",
       "245                       0.0                   0.00   \n",
       "246                       0.0                   0.00   \n",
       "...                       ...                    ...   \n",
       "23231                     0.0                1226.05   \n",
       "23336                     0.0                   0.00   \n",
       "23352                     0.0                   0.00   \n",
       "23383                     0.0                   0.00   \n",
       "23469                     0.0                   0.00   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "99                            300.0            0.0                   2015.26   \n",
       "115                             0.0            0.0                   2075.20   \n",
       "208                             0.0            0.0                   2272.05   \n",
       "245                             0.0            0.0                   2030.26   \n",
       "246                             0.0            0.0                   2284.33   \n",
       "...                             ...            ...                       ...   \n",
       "23231                           0.0            0.0                   2578.00   \n",
       "23336                           0.0            0.0                   2114.00   \n",
       "23352                           0.0            0.0                   3810.00   \n",
       "23383                           0.0            0.0                   1190.00   \n",
       "23469                           0.0            0.0                   1512.00   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  ...  Comuna Estandarizada_Vitacura  \\\n",
       "99           0.0               0.0  ...                              0   \n",
       "115          0.0               0.0  ...                              0   \n",
       "208          0.0               0.0  ...                              0   \n",
       "245          0.0               0.0  ...                              0   \n",
       "246          0.0               0.0  ...                              0   \n",
       "...          ...               ...  ...                            ...   \n",
       "23231        0.0               0.0  ...                              0   \n",
       "23336        0.0               0.0  ...                              0   \n",
       "23352        0.0               0.0  ...                              0   \n",
       "23383        0.0               0.0  ...                              0   \n",
       "23469        0.0               0.0  ...                              0   \n",
       "\n",
       "       Comuna Estandarizada_X Región de Los Lagos  \\\n",
       "99                                              0   \n",
       "115                                             0   \n",
       "208                                             0   \n",
       "245                                             0   \n",
       "246                                             0   \n",
       "...                                           ...   \n",
       "23231                                           0   \n",
       "23336                                           0   \n",
       "23352                                           0   \n",
       "23383                                           0   \n",
       "23469                                           0   \n",
       "\n",
       "       Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo  \\\n",
       "99                                                     0                          \n",
       "115                                                    0                          \n",
       "208                                                    0                          \n",
       "245                                                    0                          \n",
       "246                                                    0                          \n",
       "...                                                  ...                          \n",
       "23231                                                  0                          \n",
       "23336                                                  0                          \n",
       "23352                                                  0                          \n",
       "23383                                                  0                          \n",
       "23469                                                  0                          \n",
       "\n",
       "       Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena  \\\n",
       "99                                                     0                         \n",
       "115                                                    0                         \n",
       "208                                                    0                         \n",
       "245                                                    0                         \n",
       "246                                                    0                         \n",
       "...                                                  ...                         \n",
       "23231                                                  0                         \n",
       "23336                                                  0                         \n",
       "23352                                                  0                         \n",
       "23383                                                  0                         \n",
       "23469                                                  0                         \n",
       "\n",
       "       Comuna Estandarizada_XIV Región de Los Ríos  \\\n",
       "99                                               0   \n",
       "115                                              0   \n",
       "208                                              0   \n",
       "245                                              0   \n",
       "246                                              0   \n",
       "...                                            ...   \n",
       "23231                                            0   \n",
       "23336                                            0   \n",
       "23352                                            0   \n",
       "23383                                            0   \n",
       "23469                                            0   \n",
       "\n",
       "       Comuna Estandarizada_XV Región de Arica y Parinacota  \\\n",
       "99                                                     0      \n",
       "115                                                    0      \n",
       "208                                                    0      \n",
       "245                                                    0      \n",
       "246                                                    0      \n",
       "...                                                  ...      \n",
       "23231                                                  0      \n",
       "23336                                                  0      \n",
       "23352                                                  0      \n",
       "23383                                                  0      \n",
       "23469                                                  0      \n",
       "\n",
       "       Comuna Estandarizada_XVI Región de Ñuble  Comuna Estandarizada_Ñuñoa  \\\n",
       "99                                            0                           0   \n",
       "115                                           0                           0   \n",
       "208                                           0                           0   \n",
       "245                                           0                           0   \n",
       "246                                           0                           0   \n",
       "...                                         ...                         ...   \n",
       "23231                                         0                           0   \n",
       "23336                                         0                           0   \n",
       "23352                                         0                           0   \n",
       "23383                                         0                           0   \n",
       "23469                                         0                           0   \n",
       "\n",
       "       cluster  Etiqueta  \n",
       "99           3   Desiste  \n",
       "115          4   Desiste  \n",
       "208          3   Desiste  \n",
       "245          3   Desiste  \n",
       "246          3   Desiste  \n",
       "...        ...       ...  \n",
       "23231        4   Desiste  \n",
       "23336        3   Desiste  \n",
       "23352        1   Desiste  \n",
       "23383        4   Desiste  \n",
       "23469        4   Desiste  \n",
       "\n",
       "[169 rows x 164 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro[(df_prepro['Comuna Estandarizada_V Región de Valparaíso']==1)&(df_prepro['Etiqueta']=='Desiste')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[   0  483]\n",
      " [   0 4440]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1    0.00000   0.00000   0.00000       483\n",
      "           1    0.90189   1.00000   0.94841      4440\n",
      "\n",
      "    accuracy                        0.90189      4923\n",
      "   macro avg    0.45094   0.50000   0.47421      4923\n",
      "weighted avg    0.81340   0.90189   0.85536      4923\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[  83  400]\n",
      " [ 423 4017]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1    0.16403   0.17184   0.16785       483\n",
      "           1    0.90944   0.90473   0.90708      4440\n",
      "\n",
      "    accuracy                        0.83283      4923\n",
      "   macro avg    0.53674   0.53829   0.53746      4923\n",
      "weighted avg    0.83631   0.83283   0.83455      4923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel = 'rbf', C= 10000)\n",
    "clf.fit(X_train, y_train_svm) \n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_test_svm,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_test_svm, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con Balanceo Clase Minoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[3066  496]\n",
      " [ 936 2605]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.76612   0.86075   0.81068      3562\n",
      "   Escritura    0.84005   0.73567   0.78440      3541\n",
      "\n",
      "    accuracy                        0.79840      7103\n",
      "   macro avg    0.80308   0.79821   0.79754      7103\n",
      "weighted avg    0.80297   0.79840   0.79758      7103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=10)\n",
    "clf.fit(X_tr_min, y_tr_min) \n",
    "\n",
    "y_pred = clf.predict(X_te_min)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_te_min,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_te_min, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con Balanceo Clase Mayoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[283  90]\n",
      " [151 249]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.65207   0.75871   0.70136       373\n",
      "   Escritura    0.73451   0.62250   0.67388       400\n",
      "\n",
      "    accuracy                        0.68823       773\n",
      "   macro avg    0.69329   0.69061   0.68762       773\n",
      "weighted avg    0.69473   0.68823   0.68714       773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel = 'rbf')\n",
    "clf.fit(X_tr_may, y_tr_may) \n",
    "\n",
    "y_pred = clf.predict(X_te_may)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_te_may,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_te_may, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[ 103  380]\n",
      " [ 441 3999]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.18934   0.21325   0.20058       483\n",
      "   Escritura    0.91322   0.90068   0.90691      4440\n",
      "\n",
      "    accuracy                        0.83323      4923\n",
      "   macro avg    0.55128   0.55696   0.55374      4923\n",
      "weighted avg    0.84220   0.83323   0.83761      4923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regr_2=tree.DecisionTreeClassifier()\n",
    "regr_2.fit(X_train,y_train)\n",
    "\n",
    "y_pred = regr_2.predict(X_test)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[  71  412]\n",
      " [ 311 4129]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.18586   0.14700   0.16416       483\n",
      "   Escritura    0.90927   0.92995   0.91950      4440\n",
      "\n",
      "    accuracy                        0.85314      4923\n",
      "   macro avg    0.54757   0.53848   0.54183      4923\n",
      "weighted avg    0.83830   0.85314   0.84539      4923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regr_2=tree.DecisionTreeClassifier(max_depth = 24)\n",
    "regr_2.fit(X_train,y_train)\n",
    "\n",
    "y_pred = regr_2.predict(X_test)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceo con Clase Minoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[2661  901]\n",
      " [1278 2263]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.67555   0.74705   0.70951      3562\n",
      "   Escritura    0.71523   0.63909   0.67502      3541\n",
      "\n",
      "    accuracy                        0.69323      7103\n",
      "   macro avg    0.69539   0.69307   0.69226      7103\n",
      "weighted avg    0.69533   0.69323   0.69231      7103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regr_2=tree.DecisionTreeClassifier(max_depth=5)\n",
    "regr_2.fit(X_tr_min,y_tr_min)\n",
    "\n",
    "y_pred = regr_2.predict(X_te_min)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_te_min,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_te_min, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceo con Clase Mayoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[222 151]\n",
      " [174 226]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.56061   0.59517   0.57737       373\n",
      "   Escritura    0.59947   0.56500   0.58172       400\n",
      "\n",
      "    accuracy                        0.57956       773\n",
      "   macro avg    0.58004   0.58009   0.57955       773\n",
      "weighted avg    0.58072   0.57956   0.57962       773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regr_2=tree.DecisionTreeClassifier()\n",
    "regr_2.fit(X_tr_may,y_tr_may)\n",
    "\n",
    "y_pred = regr_2.predict(X_te_may)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_te_may,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_te_may, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[ 463   20]\n",
      " [4181  259]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.09970   0.95859   0.18061       483\n",
      "   Escritura    0.92832   0.05833   0.10977      4440\n",
      "\n",
      "    accuracy                        0.14666      4923\n",
      "   macro avg    0.51401   0.50846   0.14519      4923\n",
      "weighted avg    0.84702   0.14666   0.11672      4923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remuestreo Clase minoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[ 461   22]\n",
      " [4126  314]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.10050   0.95445   0.18185       483\n",
      "   Escritura    0.93452   0.07072   0.13149      4440\n",
      "\n",
      "    accuracy                        0.15742      4923\n",
      "   macro avg    0.51751   0.51259   0.15667      4923\n",
      "weighted avg    0.85270   0.15742   0.13643      4923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train_min, y_train_min).predict(X_test)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remuestreo Clase Mayoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[  47  436]\n",
      " [ 338 4102]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.12208   0.09731   0.10829       483\n",
      "   Escritura    0.90392   0.92387   0.91379      4440\n",
      "\n",
      "    accuracy                        0.84278      4923\n",
      "   macro avg    0.51300   0.51059   0.51104      4923\n",
      "weighted avg    0.82721   0.84278   0.83476      4923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train_may, y_train_may).predict(X_test)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrategia: Ensamble de Modelos con Balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[  47  436]\n",
      " [ 338 4102]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.12208   0.09731   0.10829       483\n",
      "   Escritura    0.90392   0.92387   0.91379      4440\n",
      "\n",
      "    accuracy                        0.84278      4923\n",
      "   macro avg    0.51300   0.51059   0.51104      4923\n",
      "weighted avg    0.82721   0.84278   0.83476      4923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                sampling_strategy='auto',\n",
    "                                replacement=False,\n",
    "                                random_state=0)\n",
    "\n",
    "#Train the classifier.\n",
    "bbc.fit(X_train, y_train)\n",
    "pred_y = bbc.predict(X_test)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = NearMiss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_prepro.drop(['Etiqueta'], axis=1)\n",
    "y = df_prepro['Etiqueta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = nr.fit_sample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[416  70]\n",
      " [ 23 457]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.94761   0.85597   0.89946       486\n",
      "   Escritura    0.86717   0.95208   0.90765       480\n",
      "\n",
      "    accuracy                        0.90373       966\n",
      "   macro avg    0.90739   0.90403   0.90355       966\n",
      "weighted avg    0.90764   0.90373   0.90353       966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.5,penalty='l2',random_state=1,solver=\"newton-cg\").fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = clf.predict(X_te)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_te,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_te, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_prepro.drop(['Etiqueta'], axis=1)\n",
    "y = df_prepro['Etiqueta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = smt.fit_sample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Etiqueta'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['Etiqueta']\n",
    "X = X.drop(['Etiqueta'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_2, X_te_2, y_tr_2, y_te_2 = train_test_split(X, y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[3720  679]\n",
      " [2927 1480]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.55965   0.84565   0.67355      4399\n",
      "   Escritura    0.68550   0.33583   0.45081      4407\n",
      "\n",
      "    accuracy                        0.59051      8806\n",
      "   macro avg    0.62258   0.59074   0.56218      8806\n",
      "weighted avg    0.62263   0.59051   0.56208      8806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_2 = LogisticRegression().fit(X_tr_2, y_tr_2)\n",
    "\n",
    "y_pred = clf.predict(X_te_2)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_te_2,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_te_2, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalmente nos quedamos con la estrategia NearMiss y encontramos los mejores parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_tr[get_columns(0, 'Monto')])\n",
    "\n",
    "X_tr[get_columns(0, 'Monto')] = scaler.transform(X_tr[get_columns(0, 'Monto')])\n",
    "X_te[get_columns(0, 'Monto')] = scaler.transform(X_te[get_columns(0, 'Monto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:789: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  beta = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:789: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  beta = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:789: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  beta = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:789: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  beta = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:789: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  beta = 1. / C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:767: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  args = (X, target, 1. / C, sample_weight)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:167: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:213: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 769, in _logistic_regression_path\n",
      "    maxiter=max_iter, tol=tol)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py\", line 202, in _newton_cg\n",
      "    old_fval, old_old_fval, args=args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py\", line 43, in _line_search_wolfe12\n",
      "    **kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 98, in line_search_wolfe1\n",
      "    c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 167, in _logistic_loss\n",
      "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 576, in log_logistic\n",
      "    X = check_array(X, dtype=np.float64)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 645, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 99, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:767: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  args = (X, target, 1. / C, sample_weight)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:167: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:213: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 769, in _logistic_regression_path\n",
      "    maxiter=max_iter, tol=tol)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py\", line 202, in _newton_cg\n",
      "    old_fval, old_old_fval, args=args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py\", line 43, in _line_search_wolfe12\n",
      "    **kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 98, in line_search_wolfe1\n",
      "    c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 167, in _logistic_loss\n",
      "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 576, in log_logistic\n",
      "    X = check_array(X, dtype=np.float64)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 645, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 99, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:767: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  args = (X, target, 1. / C, sample_weight)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:167: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:213: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 769, in _logistic_regression_path\n",
      "    maxiter=max_iter, tol=tol)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py\", line 202, in _newton_cg\n",
      "    old_fval, old_old_fval, args=args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py\", line 43, in _line_search_wolfe12\n",
      "    **kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 98, in line_search_wolfe1\n",
      "    c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 167, in _logistic_loss\n",
      "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 576, in log_logistic\n",
      "    X = check_array(X, dtype=np.float64)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 645, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 99, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:767: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  args = (X, target, 1. / C, sample_weight)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:167: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:213: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 769, in _logistic_regression_path\n",
      "    maxiter=max_iter, tol=tol)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py\", line 202, in _newton_cg\n",
      "    old_fval, old_old_fval, args=args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py\", line 43, in _line_search_wolfe12\n",
      "    **kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 98, in line_search_wolfe1\n",
      "    c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 167, in _logistic_loss\n",
      "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 576, in log_logistic\n",
      "    X = check_array(X, dtype=np.float64)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 645, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 99, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:767: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  args = (X, target, 1. / C, sample_weight)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:167: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:213: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 769, in _logistic_regression_path\n",
      "    maxiter=max_iter, tol=tol)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py\", line 202, in _newton_cg\n",
      "    old_fval, old_old_fval, args=args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py\", line 43, in _line_search_wolfe12\n",
      "    **kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 98, in line_search_wolfe1\n",
      "    c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 172, in scalar_search_wolfe1\n",
      "    phi1 = phi(stp)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\linesearch.py\", line 84, in phi\n",
      "    return f(xk + s*pk, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 167, in _logistic_loss\n",
      "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 576, in log_logistic\n",
      "    X = check_array(X, dtype=np.float64)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 645, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 99, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:759: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  args=(X, target, 1. / C, sample_weight),\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:122: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:127: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 760, in _logistic_regression_path\n",
      "    options={\"iprint\": iprint, \"gtol\": tol, \"maxiter\": max_iter}\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 618, in minimize\n",
      "    callback=callback, **options)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 200, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 166, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 73, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 70, in fun_wrapped\n",
      "    return fun(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 122, in _logistic_loss_and_grad\n",
      "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 576, in log_logistic\n",
      "    X = check_array(X, dtype=np.float64)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 645, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 99, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:759: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  args=(X, target, 1. / C, sample_weight),\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:122: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:127: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 760, in _logistic_regression_path\n",
      "    options={\"iprint\": iprint, \"gtol\": tol, \"maxiter\": max_iter}\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 618, in minimize\n",
      "    callback=callback, **options)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 200, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 166, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 73, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 70, in fun_wrapped\n",
      "    return fun(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 122, in _logistic_loss_and_grad\n",
      "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 576, in log_logistic\n",
      "    X = check_array(X, dtype=np.float64)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 645, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 99, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:759: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  args=(X, target, 1. / C, sample_weight),\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:122: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:127: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 760, in _logistic_regression_path\n",
      "    options={\"iprint\": iprint, \"gtol\": tol, \"maxiter\": max_iter}\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 618, in minimize\n",
      "    callback=callback, **options)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 200, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 166, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 73, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 70, in fun_wrapped\n",
      "    return fun(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 122, in _logistic_loss_and_grad\n",
      "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 576, in log_logistic\n",
      "    X = check_array(X, dtype=np.float64)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 645, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 99, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:759: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  args=(X, target, 1. / C, sample_weight),\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:122: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:127: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 760, in _logistic_regression_path\n",
      "    options={\"iprint\": iprint, \"gtol\": tol, \"maxiter\": max_iter}\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 618, in minimize\n",
      "    callback=callback, **options)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 200, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 166, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 73, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 70, in fun_wrapped\n",
      "    return fun(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 122, in _logistic_loss_and_grad\n",
      "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 576, in log_logistic\n",
      "    X = check_array(X, dtype=np.float64)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 645, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 99, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:759: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  args=(X, target, 1. / C, sample_weight),\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:122: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:127: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 760, in _logistic_regression_path\n",
      "    options={\"iprint\": iprint, \"gtol\": tol, \"maxiter\": max_iter}\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 618, in minimize\n",
      "    callback=callback, **options)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 200, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 166, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 73, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 70, in fun_wrapped\n",
      "    return fun(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 74, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 68, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 122, in _logistic_loss_and_grad\n",
      "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 576, in log_logistic\n",
      "    X = check_array(X, dtype=np.float64)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 645, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 99, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 969, in _fit_liblinear\n",
      "    epsilon, sample_weight)\n",
      "  File \"sklearn\\svm\\_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
      "ValueError: b'C <= 0'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:791: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if step_size * alpha_scaled == 1:\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 801, in _logistic_regression_path\n",
      "    is_saga=(solver == 'saga'))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 326, in sag_solver\n",
      "    verbose)\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:791: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if step_size * alpha_scaled == 1:\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 801, in _logistic_regression_path\n",
      "    is_saga=(solver == 'saga'))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 326, in sag_solver\n",
      "    verbose)\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:791: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = 1. / C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if step_size * alpha_scaled == 1:\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 801, in _logistic_regression_path\n",
      "    is_saga=(solver == 'saga'))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 326, in sag_solver\n",
      "    verbose)\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:791: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if step_size * alpha_scaled == 1:\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 801, in _logistic_regression_path\n",
      "    is_saga=(solver == 'saga'))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 326, in sag_solver\n",
      "    verbose)\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:791: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if step_size * alpha_scaled == 1:\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 801, in _logistic_regression_path\n",
      "    is_saga=(solver == 'saga'))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 326, in sag_solver\n",
      "    verbose)\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:791: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if step_size * alpha_scaled == 1:\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 801, in _logistic_regression_path\n",
      "    is_saga=(solver == 'saga'))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 326, in sag_solver\n",
      "    verbose)\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:791: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if step_size * alpha_scaled == 1:\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 801, in _logistic_regression_path\n",
      "    is_saga=(solver == 'saga'))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 326, in sag_solver\n",
      "    verbose)\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:791: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if step_size * alpha_scaled == 1:\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 801, in _logistic_regression_path\n",
      "    is_saga=(solver == 'saga'))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 326, in sag_solver\n",
      "    verbose)\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:791: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if step_size * alpha_scaled == 1:\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 801, in _logistic_regression_path\n",
      "    is_saga=(solver == 'saga'))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 326, in sag_solver\n",
      "    verbose)\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:791: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = 1. / C\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if step_size * alpha_scaled == 1:\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1417, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 801, in _logistic_regression_path\n",
      "    is_saga=(solver == 'saga'))\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 326, in sag_solver\n",
      "    verbose)\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1360, in fit\n",
      "    sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 965, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 823, in _get_liblinear_solver_type\n",
      "    % (error_string, penalty, loss, dual))\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 446, in _check_solver\n",
      "    \"dual=False, got dual=%s\" % (solver, dual))\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1313, in fit\n",
      "    \" got (l1_ratio=%r)\" % self.l1_ratio)\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\utils\\optimize.py:212: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1321: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.899792 con {'C': 1.0, 'dual': False, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxi1\\miniconda3\\envs\\mat281\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'penalty': ('l1', 'l2', 'elasticnet', 'none'), 'dual': (True, False),\n",
    "              'C': np.linspace(0,10,21), 'solver': ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}\n",
    "lr = LogisticRegression()\n",
    "clf = GridSearchCV(lr, parameters)\n",
    "grid_result = clf.fit(X_tr, y_tr)\n",
    "print(\"Best: %f con %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[417  69]\n",
      " [ 20 460]]\n",
      "\n",
      "Métricas:\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Desiste    0.95423   0.85802   0.90358       486\n",
      "   Escritura    0.86957   0.95833   0.91179       480\n",
      "\n",
      "    accuracy                        0.90787       966\n",
      "   macro avg    0.91190   0.90818   0.90768       966\n",
      "weighted avg    0.91216   0.90787   0.90766       966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best = LogisticRegression(C=1, dual=False ,penalty='l1',random_state=1,solver=\"liblinear\").fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = best.predict(X_te)\n",
    "\n",
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(y_te,y_pred))\n",
    "\n",
    "print('\\nMétricas:\\n ')\n",
    "print(classification_report(y_te, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.concat([X_te,y_te], axis=1)\n",
    "df_pred['Prediccion'] = y_pred\n",
    "\n",
    "prob= best.predict_proba(X_te)\n",
    "df_pred = df_pred.reset_index()\n",
    "for i in range(df_pred.shape[0]):\n",
    "    df_pred.loc[i, ['Prob. Desiste (%)']] = round(prob[i][0]*100, 2)\n",
    "    df_pred.loc[i, ['Prob. Escritura (%)']] = round(prob[i][1]*100, 2)\n",
    "    \n",
    "df_pred= df_pred.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>...</th>\n",
       "      <th>Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena</th>\n",
       "      <th>Comuna Estandarizada_XIV Región de Los Ríos</th>\n",
       "      <th>Comuna Estandarizada_XV Región de Arica y Parinacota</th>\n",
       "      <th>Comuna Estandarizada_XVI Región de Ñuble</th>\n",
       "      <th>Comuna Estandarizada_Ñuñoa</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Etiqueta</th>\n",
       "      <th>Prediccion</th>\n",
       "      <th>Prob. Desiste (%)</th>\n",
       "      <th>Prob. Escritura (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>-0.117629</td>\n",
       "      <td>1.576286</td>\n",
       "      <td>-0.02328</td>\n",
       "      <td>-0.035132</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.182843</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>1.178942</td>\n",
       "      <td>-0.108162</td>\n",
       "      <td>-0.076515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>1.273216</td>\n",
       "      <td>0.340360</td>\n",
       "      <td>-0.02328</td>\n",
       "      <td>-0.035132</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.182843</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>0.283772</td>\n",
       "      <td>-0.108162</td>\n",
       "      <td>-0.076515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>-0.171452</td>\n",
       "      <td>-0.111932</td>\n",
       "      <td>-0.02328</td>\n",
       "      <td>-0.035132</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.182843</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>-0.000902</td>\n",
       "      <td>-0.108162</td>\n",
       "      <td>-0.076515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>99.89</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>-0.117629</td>\n",
       "      <td>0.264611</td>\n",
       "      <td>-0.02328</td>\n",
       "      <td>-0.035132</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.182843</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>1.136165</td>\n",
       "      <td>-0.108162</td>\n",
       "      <td>-0.076515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>-0.117629</td>\n",
       "      <td>-0.381620</td>\n",
       "      <td>-0.02328</td>\n",
       "      <td>-0.035132</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.182843</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>-0.161098</td>\n",
       "      <td>-0.108162</td>\n",
       "      <td>-0.076515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>99.77</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>0.297672</td>\n",
       "      <td>0.888137</td>\n",
       "      <td>-0.02328</td>\n",
       "      <td>-0.035132</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.182843</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>1.225910</td>\n",
       "      <td>-0.108162</td>\n",
       "      <td>-0.076515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>-0.117629</td>\n",
       "      <td>1.069780</td>\n",
       "      <td>-0.02328</td>\n",
       "      <td>-0.035132</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.182843</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>-1.286027</td>\n",
       "      <td>-0.108162</td>\n",
       "      <td>-0.076515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>-0.117629</td>\n",
       "      <td>0.222955</td>\n",
       "      <td>-0.02328</td>\n",
       "      <td>-0.035132</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.182843</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>0.067537</td>\n",
       "      <td>-0.108162</td>\n",
       "      <td>-0.076515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>99.85</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>0.048491</td>\n",
       "      <td>-0.119634</td>\n",
       "      <td>-0.02328</td>\n",
       "      <td>-0.035132</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.182843</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>-0.100899</td>\n",
       "      <td>-0.108162</td>\n",
       "      <td>-0.076515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>95.16</td>\n",
       "      <td>4.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>-0.200690</td>\n",
       "      <td>-0.381620</td>\n",
       "      <td>-0.02328</td>\n",
       "      <td>-0.035132</td>\n",
       "      <td>-0.047253</td>\n",
       "      <td>-0.182843</td>\n",
       "      <td>-0.022758</td>\n",
       "      <td>0.139411</td>\n",
       "      <td>-0.108162</td>\n",
       "      <td>-0.076515</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>Desiste</td>\n",
       "      <td>98.02</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>417 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "index                                                         \n",
       "783        -0.117629   1.576286                    -0.02328   \n",
       "695         1.273216   0.340360                    -0.02328   \n",
       "597        -0.171452  -0.111932                    -0.02328   \n",
       "2352       -0.117629   0.264611                    -0.02328   \n",
       "2341       -0.117629  -0.381620                    -0.02328   \n",
       "...              ...        ...                         ...   \n",
       "1543        0.297672   0.888137                    -0.02328   \n",
       "1198       -0.117629   1.069780                    -0.02328   \n",
       "1376       -0.117629   0.222955                    -0.02328   \n",
       "2141        0.048491  -0.119634                    -0.02328   \n",
       "891        -0.200690  -0.381620                    -0.02328   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "index                                                  \n",
       "783                 -0.035132              -0.047253   \n",
       "695                 -0.035132              -0.047253   \n",
       "597                 -0.035132              -0.047253   \n",
       "2352                -0.035132              -0.047253   \n",
       "2341                -0.035132              -0.047253   \n",
       "...                       ...                    ...   \n",
       "1543                -0.035132              -0.047253   \n",
       "1198                -0.035132              -0.047253   \n",
       "1376                -0.035132              -0.047253   \n",
       "2141                -0.035132              -0.047253   \n",
       "891                 -0.035132              -0.047253   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "index                                                                          \n",
       "783                       -0.182843      -0.022758                  1.178942   \n",
       "695                       -0.182843      -0.022758                  0.283772   \n",
       "597                       -0.182843      -0.022758                 -0.000902   \n",
       "2352                      -0.182843      -0.022758                  1.136165   \n",
       "2341                      -0.182843      -0.022758                 -0.161098   \n",
       "...                             ...            ...                       ...   \n",
       "1543                      -0.182843      -0.022758                  1.225910   \n",
       "1198                      -0.182843      -0.022758                 -1.286027   \n",
       "1376                      -0.182843      -0.022758                  0.067537   \n",
       "2141                      -0.182843      -0.022758                 -0.100899   \n",
       "891                       -0.182843      -0.022758                  0.139411   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  ...  \\\n",
       "index                               ...   \n",
       "783    -0.108162         -0.076515  ...   \n",
       "695    -0.108162         -0.076515  ...   \n",
       "597    -0.108162         -0.076515  ...   \n",
       "2352   -0.108162         -0.076515  ...   \n",
       "2341   -0.108162         -0.076515  ...   \n",
       "...          ...               ...  ...   \n",
       "1543   -0.108162         -0.076515  ...   \n",
       "1198   -0.108162         -0.076515  ...   \n",
       "1376   -0.108162         -0.076515  ...   \n",
       "2141   -0.108162         -0.076515  ...   \n",
       "891    -0.108162         -0.076515  ...   \n",
       "\n",
       "       Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena  \\\n",
       "index                                                                            \n",
       "783                                                    0                         \n",
       "695                                                    0                         \n",
       "597                                                    0                         \n",
       "2352                                                   0                         \n",
       "2341                                                   0                         \n",
       "...                                                  ...                         \n",
       "1543                                                   0                         \n",
       "1198                                                   0                         \n",
       "1376                                                   0                         \n",
       "2141                                                   0                         \n",
       "891                                                    0                         \n",
       "\n",
       "       Comuna Estandarizada_XIV Región de Los Ríos  \\\n",
       "index                                                \n",
       "783                                              0   \n",
       "695                                              0   \n",
       "597                                              0   \n",
       "2352                                             0   \n",
       "2341                                             0   \n",
       "...                                            ...   \n",
       "1543                                             0   \n",
       "1198                                             0   \n",
       "1376                                             0   \n",
       "2141                                             0   \n",
       "891                                              0   \n",
       "\n",
       "       Comuna Estandarizada_XV Región de Arica y Parinacota  \\\n",
       "index                                                         \n",
       "783                                                    0      \n",
       "695                                                    0      \n",
       "597                                                    0      \n",
       "2352                                                   0      \n",
       "2341                                                   0      \n",
       "...                                                  ...      \n",
       "1543                                                   0      \n",
       "1198                                                   0      \n",
       "1376                                                   0      \n",
       "2141                                                   0      \n",
       "891                                                    0      \n",
       "\n",
       "       Comuna Estandarizada_XVI Región de Ñuble  Comuna Estandarizada_Ñuñoa  \\\n",
       "index                                                                         \n",
       "783                                           0                           0   \n",
       "695                                           0                           0   \n",
       "597                                           0                           0   \n",
       "2352                                          0                           0   \n",
       "2341                                          0                           0   \n",
       "...                                         ...                         ...   \n",
       "1543                                          0                           0   \n",
       "1198                                          0                           0   \n",
       "1376                                          0                           0   \n",
       "2141                                          0                           0   \n",
       "891                                           0                           0   \n",
       "\n",
       "       cluster  Etiqueta  Prediccion  Prob. Desiste (%)  Prob. Escritura (%)  \n",
       "index                                                                         \n",
       "783          1   Desiste     Desiste             100.00                 0.00  \n",
       "695          3   Desiste     Desiste             100.00                 0.00  \n",
       "597          3   Desiste     Desiste              99.89                 0.11  \n",
       "2352         1   Desiste     Desiste             100.00                 0.00  \n",
       "2341         3   Desiste     Desiste              99.77                 0.23  \n",
       "...        ...       ...         ...                ...                  ...  \n",
       "1543         1   Desiste     Desiste             100.00                 0.00  \n",
       "1198         4   Desiste     Desiste             100.00                 0.00  \n",
       "1376         3   Desiste     Desiste              99.85                 0.15  \n",
       "2141         3   Desiste     Desiste              95.16                 4.84  \n",
       "891          3   Desiste     Desiste              98.02                 1.98  \n",
       "\n",
       "[417 rows x 167 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred[(df_pred['Etiqueta']=='Desiste')&(df_pred['Prediccion']=='Desiste')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_nuevas = df_prepro.drop(X_tr.index, axis=0)\n",
    "df_pred_nuevas = df_pred_nuevas.drop(X_te.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto Reserva</th>\n",
       "      <th>Monto Pie</th>\n",
       "      <th>Monto Carta de instruccion</th>\n",
       "      <th>Monto Beneficio Minero</th>\n",
       "      <th>Monto CH FFAA y Otros</th>\n",
       "      <th>Monto Crédito Complementario</th>\n",
       "      <th>Monto LEASING</th>\n",
       "      <th>Monto Vivienda principal</th>\n",
       "      <th>Monto CDP</th>\n",
       "      <th>Monto CDP Cheque</th>\n",
       "      <th>...</th>\n",
       "      <th>Comuna Estandarizada_Vitacura</th>\n",
       "      <th>Comuna Estandarizada_X Región de Los Lagos</th>\n",
       "      <th>Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo</th>\n",
       "      <th>Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena</th>\n",
       "      <th>Comuna Estandarizada_XIV Región de Los Ríos</th>\n",
       "      <th>Comuna Estandarizada_XV Región de Arica y Parinacota</th>\n",
       "      <th>Comuna Estandarizada_XVI Región de Ñuble</th>\n",
       "      <th>Comuna Estandarizada_Ñuñoa</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5904.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4829</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2349.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4830</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5265.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2911.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24606</th>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3156.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24607</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3973.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24608</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1481.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24609</th>\n",
       "      <td>10.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5472.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Escritura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19783 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Monto Reserva  Monto Pie  Monto Carta de instruccion  \\\n",
       "4828             0.0        0.0                         0.0   \n",
       "4829            50.0        0.0                         0.0   \n",
       "4830            10.0        0.0                         0.0   \n",
       "4831            20.0        0.0                         0.0   \n",
       "4832             0.0        0.0                         0.0   \n",
       "...              ...        ...                         ...   \n",
       "24606           10.0      300.0                         0.0   \n",
       "24607           10.0     1480.0                         0.0   \n",
       "24608           10.0     1481.0                         0.0   \n",
       "24609           10.0      504.0                         0.0   \n",
       "24610            0.0        0.0                         0.0   \n",
       "\n",
       "       Monto Beneficio Minero  Monto CH FFAA y Otros  \\\n",
       "4828                      0.0                    0.0   \n",
       "4829                      0.0                    0.0   \n",
       "4830                      0.0                    0.0   \n",
       "4831                      0.0                    0.0   \n",
       "4832                      0.0                    0.0   \n",
       "...                       ...                    ...   \n",
       "24606                     0.0                    0.0   \n",
       "24607                     0.0                    0.0   \n",
       "24608                     0.0                    0.0   \n",
       "24609                     0.0                    0.0   \n",
       "24610                     0.0                    0.0   \n",
       "\n",
       "       Monto Crédito Complementario  Monto LEASING  Monto Vivienda principal  \\\n",
       "4828                            0.0            0.0                    5904.0   \n",
       "4829                            0.0            0.0                    2349.0   \n",
       "4830                            0.0            0.0                    5265.0   \n",
       "4831                            0.0            0.0                    2911.0   \n",
       "4832                            0.0            0.0                    1295.0   \n",
       "...                             ...            ...                       ...   \n",
       "24606                           0.0            0.0                    3156.0   \n",
       "24607                           0.0            0.0                    3973.0   \n",
       "24608                           0.0            0.0                    5260.0   \n",
       "24609                           0.0            0.0                    5148.0   \n",
       "24610                           0.0            0.0                    5472.0   \n",
       "\n",
       "       Monto CDP  Monto CDP Cheque  ...  Comuna Estandarizada_Vitacura  \\\n",
       "4828         0.0               0.0  ...                              0   \n",
       "4829         0.0               0.0  ...                              0   \n",
       "4830         0.0               0.0  ...                              0   \n",
       "4831         0.0               0.0  ...                              0   \n",
       "4832         0.0               0.0  ...                              0   \n",
       "...          ...               ...  ...                            ...   \n",
       "24606        0.0               0.0  ...                              0   \n",
       "24607        0.0               0.0  ...                              0   \n",
       "24608        0.0               0.0  ...                              0   \n",
       "24609        0.0               0.0  ...                              0   \n",
       "24610        0.0               0.0  ...                              0   \n",
       "\n",
       "       Comuna Estandarizada_X Región de Los Lagos  \\\n",
       "4828                                            0   \n",
       "4829                                            0   \n",
       "4830                                            0   \n",
       "4831                                            0   \n",
       "4832                                            0   \n",
       "...                                           ...   \n",
       "24606                                           0   \n",
       "24607                                           0   \n",
       "24608                                           0   \n",
       "24609                                           0   \n",
       "24610                                           0   \n",
       "\n",
       "       Comuna Estandarizada_XI Región Aysén del General Carlos Ibáñez del Campo  \\\n",
       "4828                                                   0                          \n",
       "4829                                                   0                          \n",
       "4830                                                   0                          \n",
       "4831                                                   0                          \n",
       "4832                                                   0                          \n",
       "...                                                  ...                          \n",
       "24606                                                  0                          \n",
       "24607                                                  0                          \n",
       "24608                                                  0                          \n",
       "24609                                                  0                          \n",
       "24610                                                  0                          \n",
       "\n",
       "       Comuna Estandarizada_XII Región de Magallanes y de la Antártica Chilena  \\\n",
       "4828                                                   0                         \n",
       "4829                                                   0                         \n",
       "4830                                                   0                         \n",
       "4831                                                   0                         \n",
       "4832                                                   0                         \n",
       "...                                                  ...                         \n",
       "24606                                                  0                         \n",
       "24607                                                  0                         \n",
       "24608                                                  0                         \n",
       "24609                                                  0                         \n",
       "24610                                                  0                         \n",
       "\n",
       "       Comuna Estandarizada_XIV Región de Los Ríos  \\\n",
       "4828                                             0   \n",
       "4829                                             0   \n",
       "4830                                             0   \n",
       "4831                                             0   \n",
       "4832                                             0   \n",
       "...                                            ...   \n",
       "24606                                            0   \n",
       "24607                                            0   \n",
       "24608                                            0   \n",
       "24609                                            0   \n",
       "24610                                            0   \n",
       "\n",
       "       Comuna Estandarizada_XV Región de Arica y Parinacota  \\\n",
       "4828                                                   0      \n",
       "4829                                                   0      \n",
       "4830                                                   0      \n",
       "4831                                                   0      \n",
       "4832                                                   0      \n",
       "...                                                  ...      \n",
       "24606                                                  0      \n",
       "24607                                                  0      \n",
       "24608                                                  0      \n",
       "24609                                                  0      \n",
       "24610                                                  0      \n",
       "\n",
       "       Comuna Estandarizada_XVI Región de Ñuble  Comuna Estandarizada_Ñuñoa  \\\n",
       "4828                                          0                           0   \n",
       "4829                                          0                           1   \n",
       "4830                                          0                           0   \n",
       "4831                                          0                           0   \n",
       "4832                                          0                           0   \n",
       "...                                         ...                         ...   \n",
       "24606                                         0                           0   \n",
       "24607                                         0                           0   \n",
       "24608                                         0                           0   \n",
       "24609                                         0                           0   \n",
       "24610                                         0                           0   \n",
       "\n",
       "       cluster   Etiqueta  \n",
       "4828         0  Escritura  \n",
       "4829         4  Escritura  \n",
       "4830         0  Escritura  \n",
       "4831         4  Escritura  \n",
       "4832         4  Escritura  \n",
       "...        ...        ...  \n",
       "24606        3  Escritura  \n",
       "24607        3  Escritura  \n",
       "24608        1  Escritura  \n",
       "24609        1  Escritura  \n",
       "24610        1  Escritura  \n",
       "\n",
       "[19783 rows x 164 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_nuevas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_nuevas[get_columns(0, 'Monto')] = scaler.transform(df_pred_nuevas[get_columns(0, 'Monto')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nuevas = best.predict(df_pred_nuevas.drop('Etiqueta',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusion:\n",
      " \n",
      "[[ 1903   245]\n",
      " [12433  5202]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nMatriz de confusion:\\n ')\n",
    "print(confusion_matrix(df_pred_nuevas['Etiqueta'],y_pred_nuevas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real       Predicho       Probabilidad Desiste/Escritura\n",
      "\n",
      "Escritura Desiste [0.83092644 0.16907356]\n",
      "Escritura Escritura [0.49810653 0.50189347]\n",
      "Escritura Desiste [0.86718801 0.13281199]\n",
      "Escritura Desiste [0.51838956 0.48161044]\n",
      "Escritura Escritura [0.07046653 0.92953347]\n",
      "Escritura Escritura [0.03678663 0.96321337]\n",
      "Escritura Desiste [0.77485419 0.22514581]\n",
      "Escritura Escritura [0.15977387 0.84022613]\n",
      "Escritura Escritura [0.48651161 0.51348839]\n",
      "Escritura Desiste [0.64670937 0.35329063]\n",
      "Escritura Desiste [0.92341433 0.07658567]\n",
      "Escritura Escritura [0.07815479 0.92184521]\n",
      "Escritura Escritura [0.36238669 0.63761331]\n",
      "Escritura Escritura [0.49576027 0.50423973]\n",
      "Escritura Desiste [9.99613651e-01 3.86348836e-04]\n",
      "Escritura Escritura [0.36651545 0.63348455]\n",
      "Escritura Desiste [0.99120543 0.00879457]\n",
      "Escritura Escritura [0.16471367 0.83528633]\n",
      "Escritura Escritura [0.15591325 0.84408675]\n",
      "Escritura Escritura [0.46967629 0.53032371]\n",
      "Escritura Escritura [0.35930173 0.64069827]\n",
      "Escritura Escritura [0.15099166 0.84900834]\n",
      "Escritura Escritura [0.0860453 0.9139547]\n",
      "Escritura Desiste [0.99330995 0.00669005]\n",
      "Escritura Escritura [0.17854669 0.82145331]\n",
      "Escritura Desiste [0.61424939 0.38575061]\n",
      "Escritura Escritura [0.4554796 0.5445204]\n",
      "Escritura Escritura [0.2049207 0.7950793]\n",
      "Escritura Escritura [0.19217834 0.80782166]\n",
      "Escritura Desiste [0.85525532 0.14474468]\n",
      "Escritura Desiste [0.74374303 0.25625697]\n",
      "Escritura Escritura [0.36692422 0.63307578]\n",
      "Escritura Desiste [0.51283556 0.48716444]\n",
      "Escritura Escritura [0.09646802 0.90353198]\n",
      "Escritura Desiste [0.50131831 0.49868169]\n",
      "Escritura Desiste [0.63855321 0.36144679]\n",
      "Escritura Desiste [9.99916203e-01 8.37972918e-05]\n",
      "Escritura Desiste [0.98684304 0.01315696]\n",
      "Escritura Desiste [0.99841129 0.00158871]\n",
      "Escritura Desiste [0.55155312 0.44844688]\n",
      "Escritura Escritura [0.42038923 0.57961077]\n",
      "Escritura Desiste [0.99614387 0.00385613]\n",
      "Escritura Desiste [0.83450758 0.16549242]\n",
      "Escritura Desiste [0.70103377 0.29896623]\n",
      "Escritura Desiste [9.99998861e-01 1.13898736e-06]\n",
      "Escritura Escritura [0.29012773 0.70987227]\n",
      "Escritura Escritura [0.49656804 0.50343196]\n",
      "Escritura Escritura [0.25100864 0.74899136]\n",
      "Escritura Escritura [0.10910811 0.89089189]\n",
      "Escritura Desiste [0.73224372 0.26775628]\n",
      "Escritura Escritura [0.03383838 0.96616162]\n",
      "Escritura Escritura [0.39169449 0.60830551]\n",
      "Escritura Desiste [0.72032893 0.27967107]\n",
      "Escritura Desiste [0.99330484 0.00669516]\n",
      "Escritura Desiste [9.99993385e-01 6.61509259e-06]\n",
      "Escritura Escritura [0.30425853 0.69574147]\n",
      "Escritura Desiste [0.90912901 0.09087099]\n",
      "Escritura Desiste [0.95011576 0.04988424]\n",
      "Escritura Escritura [0.3106679 0.6893321]\n",
      "Escritura Desiste [0.9964632 0.0035368]\n",
      "Escritura Escritura [0.27641242 0.72358758]\n",
      "Escritura Desiste [0.57805882 0.42194118]\n",
      "Escritura Escritura [0.11516153 0.88483847]\n",
      "Escritura Escritura [0.26758671 0.73241329]\n",
      "Escritura Desiste [9.99975041e-01 2.49586320e-05]\n",
      "Escritura Escritura [0.21120741 0.78879259]\n",
      "Escritura Escritura [0.15905099 0.84094901]\n",
      "Escritura Escritura [0.12412813 0.87587187]\n",
      "Escritura Escritura [0.15895095 0.84104905]\n",
      "Escritura Escritura [0.05432109 0.94567891]\n",
      "Escritura Escritura [0.31308089 0.68691911]\n",
      "Escritura Desiste [9.99990083e-01 9.91711733e-06]\n",
      "Escritura Desiste [0.50651005 0.49348995]\n",
      "Escritura Escritura [0.24963312 0.75036688]\n",
      "Escritura Escritura [0.25989724 0.74010276]\n",
      "Escritura Desiste [0.96969679 0.03030321]\n",
      "Escritura Escritura [0.33828872 0.66171128]\n",
      "Escritura Escritura [0.14498399 0.85501601]\n",
      "Escritura Escritura [0.24869745 0.75130255]\n",
      "Escritura Escritura [0.20821666 0.79178334]\n",
      "Escritura Desiste [0.92116851 0.07883149]\n",
      "Escritura Desiste [0.87586766 0.12413234]\n",
      "Escritura Desiste [0.56063253 0.43936747]\n",
      "Escritura Desiste [0.72382814 0.27617186]\n",
      "Escritura Escritura [0.12208193 0.87791807]\n",
      "Escritura Escritura [0.06007894 0.93992106]\n",
      "Escritura Desiste [1.00000000e+00 7.57140196e-11]\n",
      "Escritura Desiste [0.53091694 0.46908306]\n",
      "Escritura Escritura [0.10152209 0.89847791]\n",
      "Escritura Desiste [0.99262661 0.00737339]\n",
      "Escritura Escritura [0.34377039 0.65622961]\n",
      "Escritura Escritura [0.44541733 0.55458267]\n",
      "Escritura Escritura [0.18947478 0.81052522]\n",
      "Escritura Escritura [0.27046799 0.72953201]\n",
      "Escritura Escritura [0.26907224 0.73092776]\n",
      "Escritura Escritura [0.4382569 0.5617431]\n",
      "Escritura Escritura [0.32264708 0.67735292]\n",
      "Escritura Escritura [0.21657538 0.78342462]\n",
      "Escritura Desiste [9.99584436e-01 4.15563593e-04]\n",
      "Escritura Desiste [9.99721293e-01 2.78706535e-04]\n",
      "Escritura Escritura [0.46754693 0.53245307]\n",
      "Escritura Escritura [0.27871507 0.72128493]\n",
      "Escritura Desiste [0.9793833 0.0206167]\n",
      "Escritura Desiste [0.77514204 0.22485796]\n",
      "Escritura Desiste [0.97338457 0.02661543]\n",
      "Escritura Escritura [0.44241516 0.55758484]\n",
      "Escritura Escritura [0.06581524 0.93418476]\n",
      "Escritura Escritura [0.22974795 0.77025205]\n",
      "Escritura Escritura [0.21100458 0.78899542]\n",
      "Escritura Escritura [0.12554749 0.87445251]\n",
      "Escritura Desiste [0.9885376 0.0114624]\n",
      "Escritura Desiste [0.99496982 0.00503018]\n",
      "Escritura Escritura [0.12868604 0.87131396]\n",
      "Escritura Desiste [0.94431781 0.05568219]\n",
      "Escritura Escritura [0.31474094 0.68525906]\n",
      "Escritura Desiste [0.71136124 0.28863876]\n",
      "Escritura Desiste [0.9988053 0.0011947]\n",
      "Escritura Desiste [0.99874408 0.00125592]\n",
      "Escritura Escritura [0.13275226 0.86724774]\n",
      "Escritura Desiste [0.96308228 0.03691772]\n",
      "Escritura Desiste [0.63942931 0.36057069]\n",
      "Escritura Escritura [0.21783847 0.78216153]\n",
      "Escritura Escritura [0.20006375 0.79993625]\n",
      "Escritura Escritura [0.23072644 0.76927356]\n",
      "Escritura Escritura [0.31798074 0.68201926]\n",
      "Escritura Desiste [0.99527004 0.00472996]\n",
      "Escritura Escritura [0.29817404 0.70182596]\n",
      "Escritura Escritura [0.21037037 0.78962963]\n",
      "Escritura Escritura [0.14787242 0.85212758]\n",
      "Escritura Desiste [9.99998244e-01 1.75619966e-06]\n",
      "Escritura Desiste [9.99997648e-01 2.35225603e-06]\n",
      "Escritura Desiste [0.97973609 0.02026391]\n",
      "Escritura Desiste [0.83050191 0.16949809]\n",
      "Escritura Escritura [0.09975842 0.90024158]\n",
      "Escritura Escritura [0.15654665 0.84345335]\n",
      "Escritura Desiste [0.66691354 0.33308646]\n",
      "Escritura Escritura [0.47372435 0.52627565]\n",
      "Escritura Desiste [0.53780912 0.46219088]\n",
      "Escritura Escritura [0.14817619 0.85182381]\n",
      "Escritura Escritura [0.2121209 0.7878791]\n",
      "Escritura Desiste [0.95375384 0.04624616]\n",
      "Escritura Desiste [0.68691983 0.31308017]\n",
      "Escritura Desiste [9.99998842e-01 1.15790085e-06]\n",
      "Escritura Escritura [0.08618252 0.91381748]\n",
      "Escritura Escritura [0.27036341 0.72963659]\n",
      "Escritura Escritura [0.1440323 0.8559677]\n",
      "Escritura Desiste [0.99856709 0.00143291]\n",
      "Escritura Escritura [0.19805394 0.80194606]\n",
      "Escritura Escritura [0.42609686 0.57390314]\n",
      "Escritura Escritura [0.30497891 0.69502109]\n",
      "Escritura Desiste [9.99964410e-01 3.55896394e-05]\n",
      "Escritura Desiste [0.99735641 0.00264359]\n",
      "Escritura Desiste [0.87494878 0.12505122]\n",
      "Escritura Escritura [0.25796304 0.74203696]\n",
      "Escritura Escritura [0.27677714 0.72322286]\n",
      "Escritura Desiste [0.97534429 0.02465571]\n",
      "Escritura Desiste [0.89186135 0.10813865]\n",
      "Escritura Desiste [9.99374168e-01 6.25831589e-04]\n",
      "Escritura Escritura [0.13663053 0.86336947]\n",
      "Escritura Escritura [0.30711647 0.69288353]\n",
      "Escritura Desiste [0.9807336 0.0192664]\n",
      "Escritura Escritura [0.43889754 0.56110246]\n",
      "Escritura Escritura [0.20606211 0.79393789]\n",
      "Escritura Desiste [0.97310795 0.02689205]\n",
      "Escritura Escritura [0.22282112 0.77717888]\n",
      "Escritura Escritura [0.28781419 0.71218581]\n",
      "Escritura Escritura [0.45202306 0.54797694]\n",
      "Escritura Desiste [0.56097284 0.43902716]\n",
      "Escritura Escritura [0.45250955 0.54749045]\n",
      "Escritura Desiste [0.58559329 0.41440671]\n",
      "Escritura Desiste [0.68156091 0.31843909]\n",
      "Escritura Desiste [0.91254046 0.08745954]\n",
      "Escritura Desiste [0.98349082 0.01650918]\n",
      "Escritura Escritura [0.06089009 0.93910991]\n",
      "Escritura Escritura [0.18384752 0.81615248]\n",
      "Escritura Escritura [0.36261529 0.63738471]\n",
      "Escritura Escritura [0.47720374 0.52279626]\n",
      "Escritura Desiste [0.99090795 0.00909205]\n",
      "Escritura Escritura [0.35350036 0.64649964]\n",
      "Escritura Desiste [0.51027236 0.48972764]\n",
      "Escritura Escritura [0.19645476 0.80354524]\n",
      "Escritura Desiste [0.87731069 0.12268931]\n",
      "Escritura Desiste [0.85487738 0.14512262]\n",
      "Escritura Desiste [0.85487738 0.14512262]\n",
      "Escritura Escritura [0.13643154 0.86356846]\n",
      "Escritura Escritura [0.47071697 0.52928303]\n",
      "Escritura Desiste [0.71359575 0.28640425]\n",
      "Escritura Desiste [0.5387534 0.4612466]\n",
      "Escritura Escritura [0.20479493 0.79520507]\n",
      "Escritura Escritura [0.13061029 0.86938971]\n",
      "Escritura Escritura [0.40636269 0.59363731]\n",
      "Escritura Desiste [0.98272056 0.01727944]\n",
      "Escritura Desiste [0.99262074 0.00737926]\n",
      "Escritura Escritura [0.17916712 0.82083288]\n",
      "Escritura Desiste [0.5305528 0.4694472]\n",
      "Escritura Desiste [0.99834075 0.00165925]\n",
      "Escritura Desiste [0.86567541 0.13432459]\n",
      "Escritura Desiste [0.99794774 0.00205226]\n",
      "Escritura Escritura [0.2809171 0.7190829]\n",
      "Escritura Desiste [0.99677435 0.00322565]\n",
      "Escritura Desiste [9.99978929e-01 2.10710098e-05]\n",
      "Escritura Desiste [9.99999904e-01 9.57285792e-08]\n",
      "Escritura Escritura [0.14929888 0.85070112]\n",
      "Escritura Desiste [0.9554644 0.0445356]\n",
      "Escritura Escritura [0.34067839 0.65932161]\n",
      "Escritura Escritura [0.29294101 0.70705899]\n",
      "Escritura Desiste [0.57897164 0.42102836]\n",
      "Escritura Escritura [0.08240748 0.91759252]\n",
      "Escritura Escritura [0.18439876 0.81560124]\n",
      "Escritura Desiste [0.9988548 0.0011452]\n",
      "Escritura Desiste [9.99992818e-01 7.18168291e-06]\n",
      "Escritura Escritura [0.14241381 0.85758619]\n",
      "Escritura Desiste [9.99545306e-01 4.54694394e-04]\n",
      "Escritura Escritura [0.14217027 0.85782973]\n",
      "Escritura Escritura [0.2136754 0.7863246]\n",
      "Escritura Desiste [0.74060605 0.25939395]\n",
      "Escritura Escritura [0.08015711 0.91984289]\n",
      "Escritura Desiste [0.8854303 0.1145697]\n",
      "Escritura Desiste [0.77159226 0.22840774]\n",
      "Escritura Escritura [0.25727901 0.74272099]\n",
      "Escritura Desiste [0.96894591 0.03105409]\n",
      "Escritura Escritura [0.35011859 0.64988141]\n",
      "Escritura Escritura [0.18876279 0.81123721]\n",
      "Escritura Desiste [0.62742623 0.37257377]\n",
      "Escritura Desiste [0.99346034 0.00653966]\n",
      "Escritura Desiste [9.99417617e-01 5.82383109e-04]\n",
      "Escritura Desiste [9.99946316e-01 5.36837965e-05]\n",
      "Escritura Escritura [0.11014241 0.88985759]\n",
      "Escritura Escritura [0.03865549 0.96134451]\n",
      "Escritura Desiste [0.82723016 0.17276984]\n",
      "Escritura Escritura [0.1937432 0.8062568]\n",
      "Escritura Desiste [0.63818839 0.36181161]\n",
      "Escritura Escritura [0.17146822 0.82853178]\n",
      "Escritura Escritura [0.18226196 0.81773804]\n",
      "Escritura Desiste [0.81588923 0.18411077]\n",
      "Escritura Desiste [0.58022694 0.41977306]\n",
      "Escritura Desiste [9.99407880e-01 5.92119546e-04]\n",
      "Escritura Desiste [9.99911822e-01 8.81779591e-05]\n",
      "Escritura Desiste [0.62627858 0.37372142]\n",
      "Escritura Desiste [0.98754297 0.01245703]\n",
      "Escritura Escritura [0.28792213 0.71207787]\n",
      "Escritura Desiste [9.99997443e-01 2.55749276e-06]\n",
      "Escritura Escritura [0.15762716 0.84237284]\n",
      "Escritura Desiste [0.9734477 0.0265523]\n",
      "Escritura Desiste [9.99790492e-01 2.09508222e-04]\n",
      "Escritura Desiste [0.99357446 0.00642554]\n",
      "Escritura Desiste [0.96892815 0.03107185]\n",
      "Escritura Escritura [0.38262206 0.61737794]\n",
      "Escritura Escritura [0.450387 0.549613]\n",
      "Escritura Desiste [0.68958993 0.31041007]\n",
      "Escritura Desiste [0.65899747 0.34100253]\n",
      "Escritura Escritura [0.44508828 0.55491172]\n",
      "Escritura Desiste [9.99999998e-01 1.61073402e-09]\n",
      "Escritura Desiste [0.56286134 0.43713866]\n",
      "Escritura Escritura [0.22667802 0.77332198]\n",
      "Escritura Desiste [9.99561261e-01 4.38738851e-04]\n",
      "Escritura Desiste [0.99753797 0.00246203]\n",
      "Escritura Desiste [0.96546245 0.03453755]\n",
      "Escritura Escritura [0.15733685 0.84266315]\n",
      "Escritura Escritura [0.35107803 0.64892197]\n",
      "Escritura Desiste [9.99999432e-01 5.68426382e-07]\n",
      "Escritura Desiste [0.58197418 0.41802582]\n",
      "Escritura Escritura [0.27263278 0.72736722]\n",
      "Escritura Desiste [0.9959092 0.0040908]\n",
      "Escritura Escritura [0.23246238 0.76753762]\n",
      "Escritura Escritura [0.12087206 0.87912794]\n",
      "Escritura Escritura [0.2496403 0.7503597]\n",
      "Escritura Desiste [9.99119198e-01 8.80801849e-04]\n",
      "Escritura Desiste [0.7673423 0.2326577]\n",
      "Escritura Escritura [0.26641609 0.73358391]\n",
      "Escritura Desiste [0.99588249 0.00411751]\n",
      "Escritura Desiste [0.63487088 0.36512912]\n",
      "Escritura Escritura [0.22499686 0.77500314]\n",
      "Escritura Escritura [0.08433113 0.91566887]\n",
      "Escritura Desiste [1.00000000e+00 2.01649498e-11]\n",
      "Escritura Escritura [0.08121478 0.91878522]\n",
      "Escritura Desiste [0.99726938 0.00273062]\n",
      "Escritura Escritura [0.49509441 0.50490559]\n",
      "Escritura Desiste [0.87265722 0.12734278]\n",
      "Escritura Escritura [0.26250272 0.73749728]\n",
      "Escritura Escritura [0.28122353 0.71877647]\n",
      "Escritura Desiste [0.95420343 0.04579657]\n",
      "Escritura Escritura [0.33037508 0.66962492]\n",
      "Escritura Desiste [0.81253124 0.18746876]\n",
      "Escritura Desiste [0.7120391 0.2879609]\n",
      "Escritura Desiste [0.84226053 0.15773947]\n",
      "Escritura Escritura [0.24722949 0.75277051]\n",
      "Escritura Escritura [0.45843986 0.54156014]\n",
      "Escritura Desiste [9.99980416e-01 1.95842137e-05]\n",
      "Escritura Desiste [9.99998317e-01 1.68255709e-06]\n",
      "Escritura Desiste [0.99858011 0.00141989]\n",
      "Escritura Escritura [0.14203209 0.85796791]\n",
      "Escritura Desiste [9.99872965e-01 1.27035265e-04]\n",
      "Escritura Desiste [9.99997742e-01 2.25841215e-06]\n",
      "Escritura Desiste [0.99869868 0.00130132]\n",
      "Escritura Escritura [0.10280776 0.89719224]\n",
      "Escritura Desiste [0.87288345 0.12711655]\n",
      "Escritura Escritura [0.4103752 0.5896248]\n",
      "Escritura Escritura [0.25949781 0.74050219]\n",
      "Escritura Escritura [0.17318886 0.82681114]\n",
      "Escritura Desiste [0.84943523 0.15056477]\n",
      "Escritura Escritura [0.15272523 0.84727477]\n",
      "Escritura Desiste [0.80878994 0.19121006]\n",
      "Escritura Escritura [0.15310517 0.84689483]\n",
      "Escritura Escritura [0.21239043 0.78760957]\n",
      "Escritura Escritura [0.19304951 0.80695049]\n",
      "Escritura Escritura [0.25657393 0.74342607]\n",
      "Escritura Desiste [0.8285581 0.1714419]\n",
      "Escritura Escritura [0.07061522 0.92938478]\n",
      "Escritura Escritura [0.46015998 0.53984002]\n",
      "Escritura Desiste [0.64481866 0.35518134]\n",
      "Escritura Escritura [0.21203985 0.78796015]\n",
      "Escritura Escritura [0.14155207 0.85844793]\n",
      "Escritura Desiste [0.97025454 0.02974546]\n",
      "Escritura Escritura [0.13063126 0.86936874]\n",
      "Escritura Desiste [0.80263058 0.19736942]\n",
      "Escritura Desiste [0.93349101 0.06650899]\n",
      "Escritura Escritura [0.03880536 0.96119464]\n",
      "Escritura Desiste [0.98793275 0.01206725]\n",
      "Escritura Desiste [0.5814708 0.4185292]\n",
      "Escritura Escritura [0.08358229 0.91641771]\n",
      "Escritura Desiste [0.51286 0.48714]\n",
      "Escritura Escritura [0.26941059 0.73058941]\n",
      "Escritura Desiste [0.83732437 0.16267563]\n",
      "Escritura Desiste [0.9531697 0.0468303]\n",
      "Escritura Desiste [9.99425999e-01 5.74001297e-04]\n",
      "Escritura Escritura [0.09775804 0.90224196]\n",
      "Escritura Escritura [0.19400086 0.80599914]\n",
      "Escritura Escritura [0.27976673 0.72023327]\n",
      "Escritura Escritura [0.15822115 0.84177885]\n",
      "Escritura Escritura [0.16680003 0.83319997]\n",
      "Escritura Escritura [0.11986369 0.88013631]\n",
      "Escritura Desiste [0.94526312 0.05473688]\n",
      "Escritura Escritura [0.4337408 0.5662592]\n",
      "Escritura Escritura [0.21179767 0.78820233]\n",
      "Escritura Desiste [0.70051343 0.29948657]\n",
      "Escritura Desiste [0.75396495 0.24603505]\n",
      "Escritura Desiste [0.99469501 0.00530499]\n",
      "Escritura Escritura [0.46946416 0.53053584]\n",
      "Escritura Desiste [0.61985246 0.38014754]\n",
      "Escritura Desiste [9.99860271e-01 1.39729378e-04]\n",
      "Escritura Escritura [0.04866781 0.95133219]\n",
      "Escritura Desiste [0.6843618 0.3156382]\n",
      "Escritura Desiste [0.83747271 0.16252729]\n",
      "Escritura Desiste [9.99933177e-01 6.68234713e-05]\n",
      "Escritura Desiste [9.99999979e-01 2.09055449e-08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Escritura [0.23879666 0.76120334]\n",
      "Escritura Escritura [0.10183102 0.89816898]\n",
      "Escritura Desiste [0.82974707 0.17025293]\n",
      "Escritura Desiste [0.84990418 0.15009582]\n",
      "Escritura Escritura [0.16505192 0.83494808]\n",
      "Escritura Desiste [0.72536752 0.27463248]\n",
      "Escritura Desiste [9.99998010e-01 1.98993612e-06]\n",
      "Escritura Desiste [0.99872889 0.00127111]\n",
      "Escritura Escritura [0.28053463 0.71946537]\n",
      "Escritura Escritura [0.24980219 0.75019781]\n",
      "Escritura Desiste [0.96961443 0.03038557]\n",
      "Escritura Escritura [0.38120409 0.61879591]\n",
      "Escritura Desiste [0.70861547 0.29138453]\n",
      "Escritura Desiste [0.93826035 0.06173965]\n",
      "Escritura Desiste [0.67857394 0.32142606]\n",
      "Escritura Escritura [0.22839257 0.77160743]\n",
      "Escritura Escritura [0.21123682 0.78876318]\n",
      "Escritura Escritura [0.11545495 0.88454505]\n",
      "Escritura Desiste [0.98173315 0.01826685]\n",
      "Escritura Desiste [0.99857739 0.00142261]\n",
      "Escritura Escritura [0.05710307 0.94289693]\n",
      "Escritura Desiste [0.98885488 0.01114512]\n",
      "Escritura Desiste [0.97888828 0.02111172]\n",
      "Escritura Escritura [0.10683828 0.89316172]\n",
      "Escritura Escritura [0.40618838 0.59381162]\n",
      "Escritura Desiste [0.87598952 0.12401048]\n",
      "Escritura Desiste [0.52445422 0.47554578]\n",
      "Escritura Escritura [0.18663981 0.81336019]\n",
      "Escritura Desiste [0.99477814 0.00522186]\n",
      "Escritura Desiste [0.55190944 0.44809056]\n",
      "Escritura Desiste [0.6722102 0.3277898]\n",
      "Escritura Desiste [9.99289082e-01 7.10918232e-04]\n",
      "Escritura Desiste [9.99629885e-01 3.70115212e-04]\n",
      "Escritura Desiste [0.98055082 0.01944918]\n",
      "Escritura Desiste [0.91873436 0.08126564]\n",
      "Escritura Desiste [0.81645775 0.18354225]\n",
      "Escritura Desiste [0.74539342 0.25460658]\n",
      "Escritura Desiste [9.99078065e-01 9.21934563e-04]\n",
      "Escritura Desiste [0.68336332 0.31663668]\n",
      "Desiste Desiste [9.99552487e-01 4.47513123e-04]\n",
      "Escritura Escritura [0.39255853 0.60744147]\n",
      "Escritura Desiste [0.90472306 0.09527694]\n",
      "Escritura Desiste [0.99639471 0.00360529]\n",
      "Escritura Escritura [0.1901286 0.8098714]\n",
      "Escritura Escritura [0.09911821 0.90088179]\n",
      "Escritura Desiste [0.55995081 0.44004919]\n",
      "Escritura Escritura [0.34675618 0.65324382]\n",
      "Escritura Desiste [0.99885803 0.00114197]\n",
      "Escritura Escritura [0.16486301 0.83513699]\n",
      "Escritura Desiste [0.64142504 0.35857496]\n",
      "Escritura Escritura [0.36290672 0.63709328]\n",
      "Escritura Escritura [0.31510718 0.68489282]\n",
      "Desiste Escritura [0.33367819 0.66632181]\n",
      "Escritura Desiste [0.92432171 0.07567829]\n",
      "Escritura Desiste [0.70370066 0.29629934]\n",
      "Escritura Desiste [0.94921745 0.05078255]\n",
      "Desiste Escritura [0.10669568 0.89330432]\n",
      "Escritura Desiste [9.99770885e-01 2.29114879e-04]\n",
      "Escritura Escritura [0.30745339 0.69254661]\n",
      "Escritura Desiste [0.81760618 0.18239382]\n",
      "Escritura Desiste [0.67963081 0.32036919]\n",
      "Escritura Escritura [0.14515759 0.85484241]\n",
      "Escritura Escritura [0.3295995 0.6704005]\n",
      "Escritura Escritura [0.21187959 0.78812041]\n",
      "Escritura Desiste [0.87826473 0.12173527]\n",
      "Escritura Desiste [0.5385643 0.4614357]\n",
      "Escritura Desiste [9.99975627e-01 2.43728118e-05]\n",
      "Escritura Escritura [0.0806211 0.9193789]\n",
      "Escritura Desiste [0.76266721 0.23733279]\n",
      "Escritura Desiste [0.99863249 0.00136751]\n",
      "Escritura Desiste [0.9617814 0.0382186]\n",
      "Escritura Desiste [9.99361951e-01 6.38048897e-04]\n",
      "Escritura Desiste [0.64879431 0.35120569]\n",
      "Escritura Desiste [0.88646283 0.11353717]\n",
      "Escritura Desiste [0.9750712 0.0249288]\n",
      "Escritura Desiste [0.56259308 0.43740692]\n",
      "Escritura Escritura [0.1365751 0.8634249]\n",
      "Escritura Desiste [0.95035846 0.04964154]\n",
      "Escritura Escritura [0.30222556 0.69777444]\n",
      "Escritura Escritura [0.26831165 0.73168835]\n",
      "Escritura Desiste [9.99999577e-01 4.22936241e-07]\n",
      "Escritura Desiste [0.9986052 0.0013948]\n",
      "Escritura Desiste [0.99831986 0.00168014]\n",
      "Escritura Desiste [0.70515642 0.29484358]\n",
      "Escritura Desiste [0.72434656 0.27565344]\n",
      "Escritura Desiste [0.63618618 0.36381382]\n",
      "Escritura Escritura [0.19884292 0.80115708]\n",
      "Escritura Desiste [0.59165521 0.40834479]\n",
      "Escritura Desiste [9.99830083e-01 1.69917309e-04]\n",
      "Escritura Desiste [0.96395124 0.03604876]\n",
      "Escritura Desiste [0.95719999 0.04280001]\n",
      "Escritura Escritura [0.3268246 0.6731754]\n",
      "Escritura Desiste [0.92068294 0.07931706]\n",
      "Escritura Escritura [0.1953954 0.8046046]\n",
      "Escritura Escritura [0.46758924 0.53241076]\n",
      "Escritura Desiste [0.85879371 0.14120629]\n",
      "Escritura Desiste [0.92896999 0.07103001]\n",
      "Escritura Escritura [0.43922669 0.56077331]\n",
      "Escritura Escritura [0.13775578 0.86224422]\n",
      "Escritura Desiste [0.78443366 0.21556634]\n",
      "Escritura Desiste [0.99786404 0.00213596]\n",
      "Escritura Desiste [0.76079201 0.23920799]\n",
      "Escritura Escritura [0.33377146 0.66622854]\n",
      "Escritura Escritura [0.27978941 0.72021059]\n",
      "Escritura Desiste [0.85945808 0.14054192]\n",
      "Escritura Escritura [0.31993194 0.68006806]\n",
      "Escritura Desiste [0.55231859 0.44768141]\n",
      "Escritura Desiste [0.9714369 0.0285631]\n",
      "Escritura Desiste [0.95075763 0.04924237]\n",
      "Escritura Desiste [0.99866096 0.00133904]\n",
      "Escritura Desiste [0.79474956 0.20525044]\n",
      "Escritura Desiste [0.89035484 0.10964516]\n",
      "Escritura Escritura [0.16903317 0.83096683]\n",
      "Escritura Escritura [0.30697592 0.69302408]\n",
      "Escritura Escritura [0.2099272 0.7900728]\n",
      "Escritura Escritura [0.34973175 0.65026825]\n",
      "Escritura Escritura [0.29248987 0.70751013]\n",
      "Escritura Escritura [0.0703221 0.9296779]\n",
      "Escritura Escritura [0.29248987 0.70751013]\n",
      "Escritura Escritura [0.33413729 0.66586271]\n",
      "Desiste Escritura [0.08409529 0.91590471]\n",
      "Escritura Desiste [0.93263442 0.06736558]\n",
      "Escritura Escritura [0.35803457 0.64196543]\n",
      "Escritura Desiste [0.99095873 0.00904127]\n",
      "Escritura Escritura [0.37594205 0.62405795]\n",
      "Escritura Escritura [0.30730313 0.69269687]\n",
      "Escritura Escritura [0.3748465 0.6251535]\n",
      "Escritura Escritura [0.0871308 0.9128692]\n",
      "Escritura Desiste [9.99994329e-01 5.67127977e-06]\n",
      "Escritura Escritura [0.25504051 0.74495949]\n",
      "Escritura Desiste [0.99874318 0.00125682]\n",
      "Escritura Desiste [0.79602943 0.20397057]\n",
      "Escritura Desiste [0.92244531 0.07755469]\n",
      "Escritura Desiste [0.94814324 0.05185676]\n",
      "Escritura Escritura [0.06787419 0.93212581]\n",
      "Escritura Desiste [0.99091181 0.00908819]\n",
      "Escritura Desiste [0.85692192 0.14307808]\n",
      "Escritura Escritura [0.37651405 0.62348595]\n",
      "Escritura Escritura [0.30570791 0.69429209]\n",
      "Escritura Escritura [0.20534294 0.79465706]\n",
      "Escritura Desiste [0.99412782 0.00587218]\n",
      "Escritura Escritura [0.23592875 0.76407125]\n",
      "Escritura Desiste [0.9981083 0.0018917]\n",
      "Escritura Desiste [0.99232974 0.00767026]\n",
      "Escritura Desiste [0.99879853 0.00120147]\n",
      "Escritura Escritura [0.39637974 0.60362026]\n",
      "Escritura Escritura [0.4407813 0.5592187]\n",
      "Escritura Desiste [0.9986088 0.0013912]\n",
      "Escritura Desiste [0.61993607 0.38006393]\n",
      "Escritura Desiste [0.84333772 0.15666228]\n",
      "Escritura Desiste [0.8271863 0.1728137]\n",
      "Escritura Escritura [0.41027313 0.58972687]\n",
      "Escritura Escritura [0.11550808 0.88449192]\n",
      "Escritura Desiste [9.99993465e-01 6.53486541e-06]\n",
      "Escritura Desiste [0.60989663 0.39010337]\n",
      "Escritura Escritura [0.29672711 0.70327289]\n",
      "Escritura Desiste [0.94203823 0.05796177]\n",
      "Escritura Escritura [0.1506855 0.8493145]\n",
      "Escritura Escritura [0.38385531 0.61614469]\n",
      "Escritura Escritura [0.39067726 0.60932274]\n",
      "Escritura Escritura [0.18667012 0.81332988]\n",
      "Escritura Escritura [0.31023903 0.68976097]\n",
      "Escritura Desiste [0.82350615 0.17649385]\n",
      "Escritura Escritura [0.30491231 0.69508769]\n",
      "Escritura Escritura [0.22807686 0.77192314]\n",
      "Escritura Desiste [0.77211004 0.22788996]\n",
      "Escritura Desiste [0.99136963 0.00863037]\n",
      "Escritura Escritura [0.42608495 0.57391505]\n",
      "Escritura Escritura [0.15592812 0.84407188]\n",
      "Escritura Desiste [0.96886423 0.03113577]\n",
      "Escritura Desiste [0.55846283 0.44153717]\n",
      "Escritura Desiste [0.99852128 0.00147872]\n",
      "Escritura Desiste [0.66856622 0.33143378]\n",
      "Escritura Escritura [0.31889959 0.68110041]\n",
      "Escritura Escritura [0.42131628 0.57868372]\n",
      "Escritura Escritura [0.4730752 0.5269248]\n",
      "Escritura Escritura [0.23003099 0.76996901]\n",
      "Escritura Escritura [0.13933529 0.86066471]\n",
      "Escritura Desiste [0.57872143 0.42127857]\n",
      "Escritura Desiste [0.90801941 0.09198059]\n",
      "Escritura Escritura [0.11673696 0.88326304]\n",
      "Escritura Escritura [0.0609894 0.9390106]\n",
      "Escritura Desiste [0.83462642 0.16537358]\n",
      "Escritura Desiste [0.99614555 0.00385445]\n",
      "Desiste Desiste [0.5544096 0.4455904]\n",
      "Escritura Desiste [0.76302937 0.23697063]\n",
      "Escritura Escritura [0.23286176 0.76713824]\n",
      "Escritura Escritura [0.08965348 0.91034652]\n",
      "Escritura Escritura [0.29623056 0.70376944]\n",
      "Escritura Desiste [0.9722345 0.0277655]\n",
      "Escritura Desiste [0.99856928 0.00143072]\n",
      "Escritura Desiste [0.99050528 0.00949472]\n",
      "Escritura Escritura [0.12974732 0.87025268]\n",
      "Escritura Escritura [0.25280311 0.74719689]\n",
      "Escritura Desiste [0.5428331 0.4571669]\n",
      "Desiste Escritura [0.36213909 0.63786091]\n",
      "Escritura Desiste [0.8123634 0.1876366]\n",
      "Escritura Escritura [0.27104007 0.72895993]\n",
      "Escritura Escritura [0.12389856 0.87610144]\n",
      "Escritura Escritura [0.19772474 0.80227526]\n",
      "Escritura Desiste [0.63352659 0.36647341]\n",
      "Escritura Escritura [0.17887567 0.82112433]\n",
      "Escritura Desiste [9.99526046e-01 4.73954390e-04]\n",
      "Escritura Desiste [0.80819916 0.19180084]\n",
      "Escritura Desiste [0.98433142 0.01566858]\n",
      "Escritura Desiste [9.99999973e-01 2.70778195e-08]\n",
      "Escritura Desiste [0.98002865 0.01997135]\n",
      "Escritura Escritura [0.26158173 0.73841827]\n",
      "Escritura Escritura [0.20955136 0.79044864]\n",
      "Escritura Desiste [0.9116481 0.0883519]\n",
      "Escritura Desiste [0.56518299 0.43481701]\n",
      "Escritura Desiste [9.99637599e-01 3.62401036e-04]\n",
      "Escritura Desiste [9.99537895e-01 4.62104580e-04]\n",
      "Escritura Desiste [9.99946614e-01 5.33863803e-05]\n",
      "Escritura Desiste [0.58543272 0.41456728]\n",
      "Escritura Desiste [9.99437781e-01 5.62219405e-04]\n",
      "Desiste Desiste [9.99981492e-01 1.85076899e-05]\n",
      "Escritura Desiste [0.98605114 0.01394886]\n",
      "Escritura Desiste [0.60796138 0.39203862]\n",
      "Escritura Desiste [0.57265596 0.42734404]\n",
      "Escritura Desiste [0.99864596 0.00135404]\n",
      "Desiste Escritura [0.38476078 0.61523922]\n",
      "Escritura Desiste [0.98443077 0.01556923]\n",
      "Escritura Escritura [0.32493459 0.67506541]\n",
      "Escritura Desiste [0.82604506 0.17395494]\n",
      "Escritura Escritura [0.14304725 0.85695275]\n",
      "Escritura Escritura [0.34932141 0.65067859]\n",
      "Desiste Escritura [0.20139707 0.79860293]\n",
      "Desiste Desiste [0.50629811 0.49370189]\n",
      "Desiste Escritura [0.12188675 0.87811325]\n",
      "Desiste Desiste [0.50821011 0.49178989]\n",
      "Escritura Escritura [0.12673024 0.87326976]\n",
      "Desiste Desiste [9.99720587e-01 2.79413139e-04]\n",
      "Desiste Desiste [0.89062615 0.10937385]\n",
      "Escritura Desiste [0.98027259 0.01972741]\n",
      "Escritura Escritura [0.33252953 0.66747047]\n",
      "Escritura Escritura [0.18811533 0.81188467]\n",
      "Escritura Escritura [0.4988625 0.5011375]\n",
      "Desiste Escritura [0.25380289 0.74619711]\n",
      "Escritura Desiste [0.56085139 0.43914861]\n",
      "Escritura Desiste [0.68917833 0.31082167]\n",
      "Escritura Escritura [0.48144873 0.51855127]\n",
      "Escritura Escritura [0.05107765 0.94892235]\n",
      "Escritura Escritura [0.0580332 0.9419668]\n",
      "Escritura Escritura [0.16783517 0.83216483]\n",
      "Escritura Escritura [0.38625649 0.61374351]\n",
      "Escritura Escritura [0.26227011 0.73772989]\n",
      "Escritura Escritura [0.36918383 0.63081617]\n",
      "Escritura Desiste [0.66108968 0.33891032]\n",
      "Escritura Desiste [1.00000000e+00 1.61724414e-33]\n",
      "Escritura Escritura [0.46504604 0.53495396]\n",
      "Escritura Desiste [0.98932032 0.01067968]\n",
      "Escritura Desiste [0.93413928 0.06586072]\n",
      "Escritura Desiste [0.50480049 0.49519951]\n",
      "Escritura Desiste [9.99867655e-01 1.32345419e-04]\n",
      "Escritura Escritura [0.05026472 0.94973528]\n",
      "Escritura Escritura [0.45822187 0.54177813]\n",
      "Escritura Escritura [0.13622285 0.86377715]\n",
      "Escritura Desiste [9.99985570e-01 1.44298542e-05]\n",
      "Escritura Escritura [0.26557863 0.73442137]\n",
      "Escritura Escritura [0.33545681 0.66454319]\n",
      "Escritura Escritura [0.34869975 0.65130025]\n",
      "Escritura Desiste [0.60653162 0.39346838]\n",
      "Escritura Desiste [0.67119828 0.32880172]\n",
      "Escritura Escritura [0.46927771 0.53072229]\n",
      "Desiste Desiste [0.93689988 0.06310012]\n",
      "Escritura Desiste [0.8817376 0.1182624]\n",
      "Escritura Desiste [0.99873181 0.00126819]\n",
      "Escritura Escritura [0.25570832 0.74429168]\n",
      "Escritura Desiste [9.99287507e-01 7.12493225e-04]\n",
      "Escritura Escritura [0.03791224 0.96208776]\n",
      "Escritura Desiste [0.63754127 0.36245873]\n",
      "Escritura Escritura [0.35974206 0.64025794]\n",
      "Escritura Escritura [0.4708 0.5292]\n",
      "Escritura Escritura [0.27748343 0.72251657]\n",
      "Escritura Escritura [0.12136338 0.87863662]\n",
      "Escritura Desiste [9.99313066e-01 6.86933996e-04]\n",
      "Escritura Desiste [0.71579492 0.28420508]\n",
      "Escritura Desiste [0.97080778 0.02919222]\n",
      "Escritura Escritura [0.37501676 0.62498324]\n",
      "Escritura Escritura [0.17942622 0.82057378]\n",
      "Escritura Desiste [0.99491243 0.00508757]\n",
      "Escritura Escritura [0.16302468 0.83697532]\n",
      "Escritura Escritura [0.36707763 0.63292237]\n",
      "Escritura Escritura [0.32093067 0.67906933]\n",
      "Escritura Escritura [0.29873645 0.70126355]\n",
      "Escritura Escritura [0.13099172 0.86900828]\n",
      "Escritura Escritura [0.25877047 0.74122953]\n",
      "Escritura Desiste [0.99522821 0.00477179]\n",
      "Escritura Desiste [0.83565824 0.16434176]\n",
      "Escritura Escritura [0.22289528 0.77710472]\n",
      "Escritura Desiste [0.99079531 0.00920469]\n",
      "Escritura Escritura [0.36581583 0.63418417]\n",
      "Escritura Desiste [9.99886922e-01 1.13078456e-04]\n",
      "Escritura Desiste [0.8379264 0.1620736]\n",
      "Escritura Escritura [0.35390852 0.64609148]\n",
      "Escritura Escritura [0.07477688 0.92522312]\n",
      "Escritura Escritura [0.05594541 0.94405459]\n",
      "Escritura Desiste [0.99703811 0.00296189]\n",
      "Escritura Desiste [0.72344858 0.27655142]\n",
      "Escritura Desiste [0.66376776 0.33623224]\n",
      "Escritura Desiste [0.93158459 0.06841541]\n",
      "Escritura Escritura [0.13350352 0.86649648]\n",
      "Escritura Desiste [0.94173517 0.05826483]\n",
      "Desiste Escritura [0.24182807 0.75817193]\n",
      "Escritura Escritura [0.34450984 0.65549016]\n",
      "Escritura Desiste [0.65827768 0.34172232]\n",
      "Escritura Escritura [0.20310418 0.79689582]\n",
      "Desiste Escritura [0.4481931 0.5518069]\n",
      "Escritura Desiste [9.99986791e-01 1.32089175e-05]\n",
      "Escritura Escritura [0.37300642 0.62699358]\n",
      "Escritura Desiste [0.85022249 0.14977751]\n",
      "Escritura Desiste [0.99409042 0.00590958]\n",
      "Desiste Desiste [0.84548727 0.15451273]\n",
      "Escritura Escritura [0.30232685 0.69767315]\n",
      "Escritura Desiste [0.81584751 0.18415249]\n",
      "Escritura Desiste [9.99980275e-01 1.97254709e-05]\n",
      "Escritura Desiste [9.99978075e-01 2.19246124e-05]\n",
      "Escritura Desiste [9.99991825e-01 8.17497732e-06]\n",
      "Escritura Desiste [0.9467899 0.0532101]\n",
      "Escritura Desiste [0.8451471 0.1548529]\n",
      "Desiste Desiste [0.9126461 0.0873539]\n",
      "Desiste Desiste [0.83433086 0.16566914]\n",
      "Escritura Escritura [0.24655027 0.75344973]\n",
      "Escritura Desiste [9.99139994e-01 8.60005653e-04]\n",
      "Escritura Desiste [0.93279509 0.06720491]\n",
      "Desiste Escritura [0.26657972 0.73342028]\n",
      "Escritura Escritura [0.18676658 0.81323342]\n",
      "Desiste Escritura [0.35494134 0.64505866]\n",
      "Escritura Escritura [0.10387166 0.89612834]\n",
      "Desiste Escritura [0.16830778 0.83169222]\n",
      "Desiste Desiste [0.99895216 0.00104784]\n",
      "Escritura Escritura [0.15495148 0.84504852]\n",
      "Escritura Desiste [9.99994444e-01 5.55609171e-06]\n",
      "Escritura Desiste [0.99651575 0.00348425]\n",
      "Escritura Escritura [0.09692685 0.90307315]\n",
      "Desiste Desiste [9.99995618e-01 4.38245553e-06]\n",
      "Escritura Desiste [0.99745006 0.00254994]\n",
      "Desiste Escritura [0.23591185 0.76408815]\n",
      "Escritura Escritura [0.20989204 0.79010796]\n",
      "Escritura Escritura [0.1241885 0.8758115]\n",
      "Desiste Desiste [0.99499368 0.00500632]\n",
      "Desiste Escritura [0.0752431 0.9247569]\n",
      "Desiste Escritura [0.21260982 0.78739018]\n",
      "Desiste Escritura [0.18699281 0.81300719]\n",
      "Desiste Desiste [9.99997086e-01 2.91408349e-06]\n",
      "Escritura Desiste [9.99379928e-01 6.20071655e-04]\n",
      "Escritura Desiste [9.99985491e-01 1.45085829e-05]\n",
      "Escritura Escritura [0.11050255 0.88949745]\n",
      "Escritura Desiste [0.97603775 0.02396225]\n",
      "Escritura Escritura [0.43231464 0.56768536]\n",
      "Escritura Escritura [0.43439996 0.56560004]\n",
      "Escritura Desiste [0.60637909 0.39362091]\n",
      "Escritura Escritura [0.47049205 0.52950795]\n",
      "Escritura Desiste [0.99570952 0.00429048]\n",
      "Escritura Desiste [9.99644340e-01 3.55659975e-04]\n",
      "Escritura Escritura [0.4239265 0.5760735]\n",
      "Escritura Desiste [0.6973737 0.3026263]\n",
      "Escritura Desiste [0.73650631 0.26349369]\n",
      "Escritura Escritura [0.33813145 0.66186855]\n",
      "Escritura Desiste [0.99403123 0.00596877]\n",
      "Escritura Desiste [0.99557869 0.00442131]\n",
      "Escritura Escritura [0.12210031 0.87789969]\n",
      "Escritura Desiste [9.99994139e-01 5.86107997e-06]\n",
      "Escritura Escritura [0.17021039 0.82978961]\n",
      "Escritura Desiste [0.95463444 0.04536556]\n",
      "Escritura Desiste [9.99454583e-01 5.45417284e-04]\n",
      "Escritura Desiste [0.95561408 0.04438592]\n",
      "Escritura Desiste [0.62633785 0.37366215]\n",
      "Escritura Escritura [0.23269352 0.76730648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escritura Desiste [0.98438914 0.01561086]\n",
      "Escritura Desiste [0.99877086 0.00122914]\n",
      "Escritura Escritura [0.19074992 0.80925008]\n",
      "Escritura Desiste [0.55897488 0.44102512]\n",
      "Escritura Escritura [0.39314392 0.60685608]\n",
      "Escritura Escritura [0.46736302 0.53263698]\n",
      "Escritura Desiste [0.6546494 0.3453506]\n",
      "Escritura Desiste [0.99261867 0.00738133]\n",
      "Escritura Escritura [0.17990702 0.82009298]\n",
      "Escritura Desiste [0.71353437 0.28646563]\n",
      "Escritura Escritura [0.19941137 0.80058863]\n",
      "Escritura Escritura [0.12051171 0.87948829]\n",
      "Escritura Escritura [0.29178492 0.70821508]\n",
      "Escritura Escritura [0.16196459 0.83803541]\n",
      "Escritura Desiste [0.9979353 0.0020647]\n",
      "Escritura Escritura [0.23302534 0.76697466]\n",
      "Desiste Escritura [0.20703794 0.79296206]\n",
      "Escritura Desiste [0.81830692 0.18169308]\n",
      "Escritura Desiste [0.64705562 0.35294438]\n",
      "Escritura Escritura [0.20871941 0.79128059]\n",
      "Escritura Escritura [0.1255741 0.8744259]\n",
      "Escritura Desiste [0.88500897 0.11499103]\n",
      "Escritura Desiste [0.98430442 0.01569558]\n",
      "Escritura Escritura [0.39189622 0.60810378]\n",
      "Desiste Desiste [0.78426885 0.21573115]\n",
      "Escritura Desiste [0.55255267 0.44744733]\n",
      "Escritura Escritura [0.23473565 0.76526435]\n",
      "Escritura Escritura [0.17390854 0.82609146]\n",
      "Escritura Escritura [0.33897213 0.66102787]\n",
      "Escritura Escritura [0.10298113 0.89701887]\n",
      "Escritura Desiste [0.97857009 0.02142991]\n",
      "Escritura Escritura [0.18839297 0.81160703]\n",
      "Escritura Desiste [0.97407089 0.02592911]\n",
      "Escritura Escritura [0.06680853 0.93319147]\n",
      "Escritura Escritura [0.19989092 0.80010908]\n",
      "Escritura Desiste [0.91067301 0.08932699]\n",
      "Desiste Escritura [0.09389426 0.90610574]\n",
      "Escritura Desiste [0.9739385 0.0260615]\n",
      "Escritura Desiste [0.82803099 0.17196901]\n",
      "Escritura Desiste [0.98663968 0.01336032]\n",
      "Escritura Desiste [0.99554132 0.00445868]\n",
      "Escritura Escritura [0.23311918 0.76688082]\n",
      "Desiste Desiste [0.99794845 0.00205155]\n",
      "Escritura Desiste [0.63606633 0.36393367]\n",
      "Escritura Desiste [0.97503999 0.02496001]\n",
      "Escritura Desiste [0.97418937 0.02581063]\n",
      "Escritura Desiste [9.99978028e-01 2.19716224e-05]\n",
      "Escritura Desiste [9.99976484e-01 2.35157424e-05]\n",
      "Desiste Desiste [0.99480651 0.00519349]\n",
      "Escritura Escritura [0.18788208 0.81211792]\n",
      "Escritura Desiste [0.95227902 0.04772098]\n",
      "Escritura Desiste [0.98597414 0.01402586]\n",
      "Escritura Escritura [0.29253213 0.70746787]\n",
      "Escritura Escritura [0.32190264 0.67809736]\n",
      "Escritura Escritura [0.24537235 0.75462765]\n",
      "Escritura Desiste [0.99798384 0.00201616]\n",
      "Escritura Desiste [0.97116976 0.02883024]\n",
      "Escritura Escritura [0.3984435 0.6015565]\n",
      "Escritura Escritura [0.27746642 0.72253358]\n",
      "Desiste Escritura [0.40032942 0.59967058]\n",
      "Escritura Desiste [0.96184856 0.03815144]\n",
      "Escritura Escritura [0.22061799 0.77938201]\n",
      "Escritura Desiste [9.99949821e-01 5.01785210e-05]\n",
      "Escritura Desiste [0.63357842 0.36642158]\n",
      "Escritura Desiste [0.99529097 0.00470903]\n",
      "Escritura Desiste [9.99705979e-01 2.94021128e-04]\n",
      "Escritura Escritura [0.09220968 0.90779032]\n",
      "Escritura Escritura [0.19121765 0.80878235]\n",
      "Desiste Desiste [0.63904432 0.36095568]\n",
      "Escritura Desiste [9.99264904e-01 7.35096337e-04]\n",
      "Desiste Desiste [0.74518396 0.25481604]\n",
      "Escritura Escritura [0.24609118 0.75390882]\n",
      "Escritura Desiste [0.81569172 0.18430828]\n",
      "Desiste Desiste [0.96843689 0.03156311]\n",
      "Desiste Desiste [0.98375471 0.01624529]\n",
      "Desiste Escritura [0.13070936 0.86929064]\n",
      "Escritura Escritura [0.2034048 0.7965952]\n",
      "Escritura Desiste [0.97349192 0.02650808]\n",
      "Escritura Desiste [0.78125722 0.21874278]\n",
      "Escritura Desiste [0.9981203 0.0018797]\n",
      "Escritura Escritura [0.23690911 0.76309089]\n",
      "Escritura Desiste [0.99030833 0.00969167]\n",
      "Escritura Escritura [0.3757269 0.6242731]\n",
      "Escritura Desiste [0.93072533 0.06927467]\n",
      "Escritura Escritura [0.41953023 0.58046977]\n",
      "Escritura Escritura [0.05303334 0.94696666]\n",
      "Escritura Desiste [0.99669135 0.00330865]\n",
      "Escritura Desiste [0.99206082 0.00793918]\n",
      "Escritura Escritura [0.0831657 0.9168343]\n",
      "Escritura Desiste [0.99679268 0.00320732]\n",
      "Escritura Desiste [0.8468 0.1532]\n",
      "Escritura Escritura [0.16326364 0.83673636]\n",
      "Escritura Escritura [0.09333735 0.90666265]\n",
      "Escritura Escritura [0.38089185 0.61910815]\n",
      "Escritura Desiste [0.99588934 0.00411066]\n",
      "Escritura Desiste [9.99623474e-01 3.76525854e-04]\n",
      "Escritura Escritura [0.23299355 0.76700645]\n",
      "Escritura Desiste [0.9977997 0.0022003]\n",
      "Escritura Desiste [0.83855668 0.16144332]\n",
      "Escritura Escritura [0.37563036 0.62436964]\n",
      "Escritura Desiste [0.89243347 0.10756653]\n",
      "Escritura Desiste [0.6232817 0.3767183]\n",
      "Escritura Desiste [0.67041041 0.32958959]\n",
      "Escritura Escritura [0.06563824 0.93436176]\n",
      "Escritura Escritura [0.12395759 0.87604241]\n",
      "Escritura Desiste [0.95683679 0.04316321]\n",
      "Escritura Escritura [0.4445017 0.5554983]\n",
      "Escritura Escritura [0.28516324 0.71483676]\n",
      "Escritura Desiste [0.96823035 0.03176965]\n",
      "Escritura Desiste [0.839764 0.160236]\n",
      "Escritura Escritura [0.16699329 0.83300671]\n",
      "Escritura Escritura [0.32033802 0.67966198]\n",
      "Escritura Desiste [0.99739873 0.00260127]\n",
      "Escritura Escritura [0.12089973 0.87910027]\n",
      "Escritura Escritura [0.12162365 0.87837635]\n",
      "Escritura Escritura [0.26086446 0.73913554]\n",
      "Escritura Desiste [0.98024851 0.01975149]\n",
      "Escritura Escritura [0.27538524 0.72461476]\n",
      "Escritura Desiste [0.64643197 0.35356803]\n",
      "Escritura Desiste [0.97331396 0.02668604]\n",
      "Escritura Escritura [0.10174856 0.89825144]\n",
      "Escritura Desiste [0.78648722 0.21351278]\n",
      "Escritura Desiste [0.99619312 0.00380688]\n",
      "Escritura Desiste [0.93218659 0.06781341]\n",
      "Escritura Desiste [0.53626769 0.46373231]\n",
      "Escritura Escritura [0.49263177 0.50736823]\n",
      "Escritura Escritura [0.09798379 0.90201621]\n",
      "Escritura Escritura [0.20775663 0.79224337]\n",
      "Escritura Desiste [0.95283036 0.04716964]\n",
      "Escritura Desiste [9.99914641e-01 8.53590236e-05]\n",
      "Escritura Escritura [0.13470958 0.86529042]\n",
      "Escritura Escritura [0.38215486 0.61784514]\n",
      "Escritura Escritura [0.47070132 0.52929868]\n",
      "Escritura Escritura [0.15564613 0.84435387]\n",
      "Escritura Desiste [9.99999993e-01 6.68244129e-09]\n",
      "Escritura Escritura [0.16758754 0.83241246]\n",
      "Escritura Desiste [0.89358255 0.10641745]\n",
      "Escritura Escritura [0.12770545 0.87229455]\n",
      "Escritura Escritura [0.28668762 0.71331238]\n",
      "Escritura Desiste [9.99622591e-01 3.77408696e-04]\n",
      "Escritura Escritura [0.43627342 0.56372658]\n",
      "Escritura Desiste [0.55871478 0.44128522]\n",
      "Escritura Escritura [0.34615944 0.65384056]\n",
      "Escritura Escritura [0.22244886 0.77755114]\n",
      "Escritura Escritura [0.49579249 0.50420751]\n",
      "Escritura Escritura [0.47988048 0.52011952]\n",
      "Escritura Escritura [0.14520354 0.85479646]\n",
      "Escritura Escritura [0.4924151 0.5075849]\n",
      "Escritura Desiste [0.73167369 0.26832631]\n",
      "Escritura Escritura [0.0903212 0.9096788]\n",
      "Escritura Desiste [9.99811225e-01 1.88775464e-04]\n",
      "Escritura Desiste [0.96453868 0.03546132]\n",
      "Escritura Escritura [0.06292247 0.93707753]\n",
      "Escritura Escritura [0.10796048 0.89203952]\n",
      "Escritura Desiste [9.99451967e-01 5.48033311e-04]\n",
      "Escritura Desiste [0.73669209 0.26330791]\n",
      "Escritura Desiste [0.75257135 0.24742865]\n",
      "Escritura Escritura [0.49709687 0.50290313]\n",
      "Escritura Desiste [0.96232396 0.03767604]\n",
      "Escritura Desiste [0.66428205 0.33571795]\n",
      "Escritura Escritura [0.42722068 0.57277932]\n",
      "Escritura Desiste [0.98106381 0.01893619]\n",
      "Escritura Desiste [0.99862497 0.00137503]\n",
      "Escritura Desiste [0.99395216 0.00604784]\n",
      "Escritura Escritura [0.13942914 0.86057086]\n",
      "Escritura Escritura [0.17241142 0.82758858]\n",
      "Escritura Desiste [0.98164922 0.01835078]\n",
      "Escritura Desiste [9.99777893e-01 2.22107349e-04]\n",
      "Escritura Desiste [0.63973368 0.36026632]\n",
      "Escritura Escritura [0.08916428 0.91083572]\n",
      "Escritura Escritura [0.11719804 0.88280196]\n",
      "Escritura Escritura [0.34666846 0.65333154]\n",
      "Escritura Desiste [0.99011893 0.00988107]\n",
      "Escritura Desiste [0.98964301 0.01035699]\n",
      "Escritura Desiste [0.99732748 0.00267252]\n",
      "Escritura Desiste [0.98266529 0.01733471]\n",
      "Escritura Desiste [0.99828152 0.00171848]\n",
      "Escritura Desiste [9.99908693e-01 9.13070425e-05]\n",
      "Escritura Desiste [0.99450199 0.00549801]\n",
      "Escritura Desiste [0.55340205 0.44659795]\n",
      "Escritura Escritura [0.20529075 0.79470925]\n",
      "Escritura Escritura [0.29878725 0.70121275]\n",
      "Escritura Desiste [9.99282323e-01 7.17677184e-04]\n",
      "Escritura Escritura [0.11225867 0.88774133]\n",
      "Escritura Escritura [0.23793585 0.76206415]\n",
      "Escritura Escritura [0.29791018 0.70208982]\n",
      "Escritura Desiste [0.9840321 0.0159679]\n",
      "Escritura Escritura [0.27804671 0.72195329]\n",
      "Escritura Escritura [0.12473318 0.87526682]\n",
      "Escritura Escritura [0.08517206 0.91482794]\n",
      "Escritura Desiste [0.60447781 0.39552219]\n",
      "Escritura Desiste [0.9397145 0.0602855]\n",
      "Escritura Escritura [0.26071562 0.73928438]\n",
      "Escritura Desiste [0.99380074 0.00619926]\n",
      "Escritura Escritura [0.16276135 0.83723865]\n",
      "Escritura Escritura [0.3463074 0.6536926]\n",
      "Escritura Escritura [0.17459846 0.82540154]\n",
      "Escritura Desiste [0.99785336 0.00214664]\n",
      "Escritura Desiste [0.77005366 0.22994634]\n",
      "Escritura Desiste [0.91880808 0.08119192]\n",
      "Escritura Escritura [0.20504278 0.79495722]\n",
      "Escritura Escritura [0.07085647 0.92914353]\n",
      "Escritura Escritura [0.10405881 0.89594119]\n",
      "Escritura Escritura [0.33224617 0.66775383]\n",
      "Escritura Escritura [0.36795645 0.63204355]\n",
      "Escritura Escritura [0.38272139 0.61727861]\n",
      "Escritura Escritura [0.13755265 0.86244735]\n",
      "Escritura Escritura [0.17239817 0.82760183]\n",
      "Escritura Desiste [0.56915258 0.43084742]\n",
      "Escritura Desiste [0.93552023 0.06447977]\n",
      "Escritura Escritura [0.1662668 0.8337332]\n",
      "Desiste Desiste [0.98928277 0.01071723]\n",
      "Escritura Escritura [0.39155646 0.60844354]\n",
      "Escritura Desiste [0.6759766 0.3240234]\n",
      "Escritura Escritura [0.20657364 0.79342636]\n",
      "Escritura Escritura [0.16866718 0.83133282]\n",
      "Escritura Escritura [0.14916053 0.85083947]\n",
      "Escritura Desiste [0.81942383 0.18057617]\n",
      "Escritura Escritura [0.05446845 0.94553155]\n",
      "Escritura Desiste [9.99631672e-01 3.68328499e-04]\n",
      "Escritura Desiste [0.92308431 0.07691569]\n",
      "Escritura Escritura [0.13710767 0.86289233]\n",
      "Escritura Escritura [0.13933303 0.86066697]\n",
      "Escritura Escritura [0.40805994 0.59194006]\n",
      "Escritura Desiste [0.88887563 0.11112437]\n",
      "Escritura Escritura [0.08224488 0.91775512]\n",
      "Escritura Desiste [0.98334031 0.01665969]\n",
      "Escritura Escritura [0.07868845 0.92131155]\n",
      "Escritura Escritura [0.25821209 0.74178791]\n",
      "Escritura Escritura [0.27384422 0.72615578]\n",
      "Desiste Desiste [0.98399966 0.01600034]\n",
      "Escritura Escritura [0.29916775 0.70083225]\n",
      "Escritura Escritura [0.08169465 0.91830535]\n",
      "Escritura Escritura [0.29862182 0.70137818]\n",
      "Escritura Desiste [9.99206021e-01 7.93978897e-04]\n",
      "Escritura Desiste [0.94050208 0.05949792]\n",
      "Escritura Desiste [9.99533724e-01 4.66276412e-04]\n",
      "Escritura Desiste [0.67611802 0.32388198]\n",
      "Escritura Desiste [0.99549048 0.00450952]\n",
      "Escritura Desiste [9.99831582e-01 1.68418426e-04]\n",
      "Escritura Escritura [0.43421902 0.56578098]\n",
      "Escritura Escritura [0.13882625 0.86117375]\n",
      "Escritura Desiste [0.79919855 0.20080145]\n",
      "Escritura Desiste [0.98398799 0.01601201]\n",
      "Escritura Desiste [9.99581368e-01 4.18631607e-04]\n",
      "Escritura Desiste [0.88542667 0.11457333]\n",
      "Escritura Escritura [0.2044328 0.7955672]\n",
      "Escritura Escritura [0.04245753 0.95754247]\n",
      "Escritura Desiste [0.68518409 0.31481591]\n",
      "Escritura Escritura [0.22705982 0.77294018]\n"
     ]
    }
   ],
   "source": [
    "print('Real      ', 'Predicho      ', 'Probabilidad Desiste/Escritura\\n')\n",
    "proba = best.predict_proba(df_pred_nuevas.drop('Etiqueta',axis=1))\n",
    "for i in range(len(y_te)):\n",
    "    print(df_pred_nuevas['Etiqueta'].tolist()[i], y_pred_nuevas[i], proba[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importacia de los atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w = best.coef_\n",
    "b = best.intercept_\n",
    "\n",
    "class1 = 'Escritura' #-1\n",
    "class2 = 'Desiste' #+1 -> linea 579 sklearn/linear_model/_stochastic_gradient.py fit_binary(self, 1, X, y, alpha, C,\n",
    "\n",
    "max_feat = min(len(w[0,:]),41)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5),nrows=1)\n",
    "p = sns.barplot(data= df_prepro, x=np.arange(max_feat), y=w[0,:max_feat], palette=\"rocket\",ax=ax)\n",
    "ax.set_xticklabels(X.columns[:max_feat])\n",
    "plt.setp(p.get_xticklabels(), rotation=90)\n",
    "ax.set_title('Coeficientes (Pesos Atributos) Clase -1=%s, Clase +1=%s'%(class1,class2))\n",
    "plt.show()\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w = best.coef_\n",
    "b = best.intercept_\n",
    "\n",
    "class1 = 'Escritura' #-1\n",
    "class2 = 'Desiste' #+1 -> linea 579 sklearn/linear_model/_stochastic_gradient.py fit_binary(self, 1, X, y, alpha, C,\n",
    "\n",
    "max_feat = 83\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5),nrows=1)\n",
    "p = sns.barplot(data= X_train, x=np.arange(max_feat-41), y=w[0,41:max_feat], palette=\"rocket\",ax=ax)\n",
    "ax.set_xticklabels(X.columns[41:max_feat])\n",
    "plt.setp(p.get_xticklabels(), rotation=90)\n",
    "ax.set_title('Coeficientes (Pesos Atributos) Clase -1=%s, Clase +1=%s'%(class1,class2))\n",
    "plt.show()\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w = best.coef_\n",
    "b = best.intercept_\n",
    "\n",
    "class1 = 'Escritura' #-1\n",
    "class2 = 'Desiste' #+1 -> linea 579 sklearn/linear_model/_stochastic_gradient.py fit_binary(self, 1, X, y, alpha, C,\n",
    "\n",
    "max_feat = 124\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5),nrows=1)\n",
    "p = sns.barplot(data= df_prepro, x=np.arange(max_feat-83), y=w[0,83:max_feat], palette=\"rocket\",ax=ax)\n",
    "ax.set_xticklabels(X.columns[83:max_feat])\n",
    "plt.setp(p.get_xticklabels(), rotation=90)\n",
    "ax.set_title('Coeficientes (Pesos Atributos) Clase -1=%s, Clase +1=%s'%(class1,class2))\n",
    "plt.show()\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w = best.coef_\n",
    "b = best.intercept_\n",
    "\n",
    "class1 = 'Escritura' #-1\n",
    "class2 = 'Desiste' #+1 -> linea 579 sklearn/linear_model/_stochastic_gradient.py fit_binary(self, 1, X, y, alpha, C,\n",
    "\n",
    "max_feat = 163\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,5),nrows=1)\n",
    "p = sns.barplot(data= df_prepro, x=np.arange(max_feat-124), y=w[0,124:max_feat], palette=\"rocket\",ax=ax)\n",
    "ax.set_xticklabels(X.columns[124:max_feat])\n",
    "plt.setp(p.get_xticklabels(), rotation=90)\n",
    "ax.set_title('Coeficientes (Pesos Atributos) Clase -1=%s, Clase +1=%s'%(class1,class2))\n",
    "plt.show()\n",
    "\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
